Implementing High Performance Multicast in a Managed Environment Krzysztof Ostrowski |
High Performance Multicast in a Managed Environment Krzysztof Ostrowski Cornell |
Performance Multicast in a Managed Environment Krzysztof Ostrowski Cornell University |
an Adaptive Distributed File System for Mobile Hosts Benjamin Atkin |
Multicast in a Managed Environment Krzysztof Ostrowski Cornell University Ken |
Adaptive Distributed File System for Mobile Hosts Benjamin Atkin and |
Transparent Error Correction for Communication Between Data Centers Mahesh Balakrishnan |
in a Managed Environment Krzysztof Ostrowski Cornell University Ken Birman |
Distributed File System for Mobile Hosts Benjamin Atkin and Kenneth |
this is changing rapidly all sorts of companies and governmental |
a Managed Environment Krzysztof Ostrowski Cornell University Ken Birman Cornell |
File System for Mobile Hosts Benjamin Atkin and Kenneth P |
is changing rapidly all sorts of companies and governmental organizations |
Managed Environment Krzysztof Ostrowski Cornell University Ken Birman Cornell University |
changing rapidly all sorts of companies and governmental organizations are |
Environment Krzysztof Ostrowski Cornell University Ken Birman Cornell University Abstract |
rapidly all sorts of companies and governmental organizations are suddenly |
Krzysztof Ostrowski Cornell University Ken Birman Cornell University Abstract motes |
all sorts of companies and governmental organizations are suddenly looking |
Ostrowski Cornell University Ken Birman Cornell University Abstract motes end |
sorts of companies and governmental organizations are suddenly looking towards |
of companies and governmental organizations are suddenly looking towards Web |
companies and governmental organizations are suddenly looking towards Web services |
and governmental organizations are suddenly looking towards Web services as |
governmental organizations are suddenly looking towards Web services as a |
edu Abstract MFS using file access traces from Windows NT |
organizations are suddenly looking towards Web services as a platform |
Abstract MFS using file access traces from Windows NT and |
are suddenly looking towards Web services as a platform that |
the company s own products are still implemented primarily in |
edu ABSTRACT We describe a practical auditing approach designed to |
MFS using file access traces from Windows NT and Unix |
suddenly looking towards Web services as a platform that might |
company s own products are still implemented primarily in unmanaged |
ABSTRACT We describe a practical auditing approach designed to encourage |
looking towards Web services as a platform that might support |
s own products are still implemented primarily in unmanaged C |
and a synthetic workload designed to emulate sharing patterns seen |
We describe a practical auditing approach designed to encourage fairness |
towards Web services as a platform that might support a |
a synthetic workload designed to emulate sharing patterns seen in |
Abstract The global network of data centers is emerging as |
describe a practical auditing approach designed to encourage fairness in |
Web services as a platform that might support a wide |
synthetic workload designed to emulate sharing patterns seen in Mobility |
The global network of data centers is emerging as an |
a practical auditing approach designed to encourage fairness in peer |
services as a platform that might support a wide range |
workload designed to emulate sharing patterns seen in Mobility is |
The multicast protocols employed by QSM were designed for performance |
global network of data centers is emerging as an important |
as a platform that might support a wide range of |
designed to emulate sharing patterns seen in Mobility is a |
multicast protocols employed by QSM were designed for performance and |
network of data centers is emerging as an important distributed |
a platform that might support a wide range of demanding |
Auditing is employed to ensure that correct nodes are able |
to emulate sharing patterns seen in Mobility is a critical |
protocols employed by QSM were designed for performance and scalability |
of data centers is emerging as an important distributed systems |
platform that might support a wide range of demanding applications |
incorporating a mixture of new ideas and ideas drawn from |
emulate sharing patterns seen in Mobility is a critical feature |
data centers is emerging as an important distributed systems paradigm |
is employed to ensure that correct nodes are able to |
a mixture of new ideas and ideas drawn from prior |
sharing patterns seen in Mobility is a critical feature of |
Examples of such systems include big banking and brokerage data |
centers is emerging as an important distributed systems paradigm commodity |
employed to ensure that correct nodes are able to receive |
mixture of new ideas and ideas drawn from prior systems |
patterns seen in Mobility is a critical feature of computer |
of such systems include big banking and brokerage data centers |
is emerging as an important distributed systems paradigm commodity clusters |
to ensure that correct nodes are able to receive streams |
online service centers for companies that operate on a global |
emerging as an important distributed systems paradigm commodity clusters running |
the aspects on which we focus here reflect architectural responses |
seen in Mobility is a critical feature of computer systems |
ensure that correct nodes are able to receive streams even |
service centers for companies that operate on a global scale |
as an important distributed systems paradigm commodity clusters running high |
aspects on which we focus here reflect architectural responses to |
that correct nodes are able to receive streams even in |
systems to operate critical infrastructures like electric power and transportation |
on which we focus here reflect architectural responses to scheduling |
correct nodes are able to receive streams even in the |
most applications that run on Existing work in cache management |
which we focus here reflect architectural responses to scheduling delays |
and government and military systems responsible for everything from intelligence |
nodes are able to receive streams even in the presence |
speed lambda networks across hundreds of milliseconds of network latency |
applications that run on Existing work in cache management for |
government and military systems responsible for everything from intelligence gathering |
are able to receive streams even in the presence of |
that run on Existing work in cache management for mobile |
and military systems responsible for everything from intelligence gathering to |
able to receive streams even in the presence of nodes |
haul networks can cripple the performance of applications and protocols |
run on Existing work in cache management for mobile file |
military systems responsible for everything from intelligence gathering to issuing |
to receive streams even in the presence of nodes that |
networks can cripple the performance of applications and protocols a |
on Existing work in cache management for mobile file systems |
forcing us to redesign and recode one layer of the |
systems responsible for everything from intelligence gathering to issuing Social |
receive streams even in the presence of nodes that do |
can cripple the performance of applications and protocols a loss |
Existing work in cache management for mobile file systems mobile |
us to redesign and recode one layer of the system |
responsible for everything from intelligence gathering to issuing Social Security |
streams even in the presence of nodes that do not |
cripple the performance of applications and protocols a loss rate |
work in cache management for mobile file systems mobile hosts |
to redesign and recode one layer of the system after |
for everything from intelligence gathering to issuing Social Security checks |
even in the presence of nodes that do not upload |
the performance of applications and protocols a loss rate as |
in cache management for mobile file systems mobile hosts lack |
redesign and recode one layer of the system after another |
in the presence of nodes that do not upload enough |
performance of applications and protocols a loss rate as low |
cache management for mobile file systems mobile hosts lack flexible |
the presence of nodes that do not upload enough data |
of applications and protocols a loss rate as low as |
a scalable system is one that can flexibly accommodate growth |
management for mobile file systems mobile hosts lack flexible mechanisms |
O calls and was rather casual about buffering and caching |
scalable system is one that can flexibly accommodate growth in |
for mobile file systems mobile hosts lack flexible mechanisms for |
system is one that can flexibly accommodate growth in its |
and scales well when compared to previous solutions that rely |
mobile file systems mobile hosts lack flexible mechanisms for data |
file systems mobile hosts lack flexible mechanisms for data access |
scales well when compared to previous solutions that rely on |
is one that can flexibly accommodate growth in its client |
systems mobile hosts lack flexible mechanisms for data access in |
well when compared to previous solutions that rely on tit |
one that can flexibly accommodate growth in its client base |
mobile hosts lack flexible mechanisms for data access in an |
hosts lack flexible mechanisms for data access in an en |
Such systems typically run on a clustered computer or in |
Maelstrom is an edge appliance that masks packet loss transparently |
systems typically run on a clustered computer or in a |
is an edge appliance that masks packet loss transparently and |
typically run on a clustered computer or in a large |
an edge appliance that masks packet loss transparently and quickly |
The finished system achieves extremely high performance with relatively modest |
run on a clustered computer or in a large data |
Untrusted local auditors run on all nodes in the system |
edge appliance that masks packet loss transparently and quickly from |
finished system achieves extremely high performance with relatively modest CPU |
and are responsible for collecting and maintaining accountable information regarding |
appliance that masks packet loss transparently and quickly from inter |
on a clustered computer or in a large data center |
system achieves extremely high performance with relatively modest CPU and |
are responsible for collecting and maintaining accountable information regarding data |
a clustered computer or in a large data center and |
achieves extremely high performance with relatively modest CPU and memory |
responsible for collecting and maintaining accountable information regarding data sent |
clustered computer or in a large data center and must |
extremely high performance with relatively modest CPU and memory loads |
speed encoding and using a new Forward Error Correction scheme |
for collecting and maintaining accountable information regarding data sent and |
computer or in a large data center and must be |
incorporates mechanisms for making efficient vironment with large and frequent |
encoding and using a new Forward Error Correction scheme to |
Although our paper is not about setting performance records the |
collecting and maintaining accountable information regarding data sent and received |
or in a large data center and must be able |
mechanisms for making efficient vironment with large and frequent variations |
and using a new Forward Error Correction scheme to handle |
our paper is not about setting performance records the absolute |
and maintaining accountable information regarding data sent and received by |
in a large data center and must be able to |
for making efficient vironment with large and frequent variations in |
using a new Forward Error Correction scheme to handle bursty |
paper is not about setting performance records the absolute numbers |
maintaining accountable information regarding data sent and received by each |
a large data center and must be able to handle |
making efficient vironment with large and frequent variations in network |
a new Forward Error Correction scheme to handle bursty loss |
is not about setting performance records the absolute numbers are |
accountable information regarding data sent and received by each node |
large data center and must be able to handle high |
efficient vironment with large and frequent variations in network connec |
not about setting performance records the absolute numbers are good |
data center and must be able to handle high loads |
one or more trusted global auditors periodically sample the state |
center and must be able to handle high loads or |
QSM outperforms the multicast platforms we ve worked with in |
or more trusted global auditors periodically sample the state of |
and must be able to handle high loads or sudden |
outperforms the multicast platforms we ve worked with in the |
more trusted global auditors periodically sample the state of participating |
must be able to handle high loads or sudden demand |
the multicast platforms we ve worked with in the past |
trusted global auditors periodically sample the state of participating nodes |
be able to handle high loads or sudden demand bursts |
in collaborative work adapting existing systems to cope with periods |
multicast platforms we ve worked with in the past systems |
able to handle high loads or sudden demand bursts and |
collaborative work adapting existing systems to cope with periods of |
platforms we ve worked with in the past systems that |
I NTRODUCTION T A conference version of this paper appeared |
to handle high loads or sudden demand bursts and a |
work adapting existing systems to cope with periods of low |
We demonstrate through simulation that our approach can successfully detect |
we ve worked with in the past systems that run |
NTRODUCTION T A conference version of this paper appeared in |
handle high loads or sudden demand bursts and a vast |
adapting existing systems to cope with periods of low bandwidth |
demonstrate through simulation that our approach can successfully detect and |
ve worked with in the past systems that run in |
T A conference version of this paper appeared in NSDI |
high loads or sudden demand bursts and a vast number |
particularly when wireless and wired users share in a style |
worked with in the past systems that run in unmanaged |
through simulation that our approach can successfully detect and react |
loads or sudden demand bursts and a vast number of |
when wireless and wired users share in a style which |
with in the past systems that run in unmanaged settings |
simulation that our approach can successfully detect and react to |
or sudden demand bursts and a vast number of users |
wireless and wired users share in a style which we |
that our approach can successfully detect and react to the |
and wired users share in a style which we will |
our approach can successfully detect and react to the presence |
They must reliably respond even in the event of failures |
wired users share in a style which we will refer |
approach can successfully detect and react to the presence of |
must reliably respond even in the event of failures or |
users share in a style which we will refer to |
we use QSM in a series of experiments that highlight |
can successfully detect and react to the presence of opportunistic |
reliably respond even in the event of failures or reconfiguration |
share in a style which we will refer to as |
use QSM in a series of experiments that highlight fundamental |
successfully detect and react to the presence of opportunistic nodes |
in a style which we will refer to as modal |
QSM in a series of experiments that highlight fundamental factors |
detect and react to the presence of opportunistic nodes in |
a style which we will refer to as modal adaptation |
managed and automate as many routine services such as backups |
and react to the presence of opportunistic nodes in streaming |
These reveal linkages between achievable performance and the costs and |
HE emergence of commodity clusters and data centers has enabled |
and automate as many routine services such as backups and |
react to the presence of opportunistic nodes in streaming sessions |
reveal linkages between achievable performance and the costs and characteristics |
emergence of commodity clusters and data centers has enabled a |
automate as many routine services such as backups and component |
linkages between achievable performance and the costs and characteristics of |
of commodity clusters and data centers has enabled a new |
when for adapting data access to network variability in the |
as many routine services such as backups and component upgrades |
between achievable performance and the costs and characteristics of the |
commodity clusters and data centers has enabled a new class |
for adapting data access to network variability in the context |
many routine services such as backups and component upgrades as |
achievable performance and the costs and characteristics of the managed |
clusters and data centers has enabled a new class of |
adapting data access to network variability in the context of |
INTRODUCTION Video and audio streaming account for a large percentage |
routine services such as backups and component upgrades as possible |
performance and the costs and characteristics of the managed framework |
and data centers has enabled a new class of globally |
data access to network variability in the context of bandwidth |
Video and audio streaming account for a large percentage of |
Many settings also require security against attempted intrusions and distributed |
data centers has enabled a new class of globally distributed |
access to network variability in the context of bandwidth falls |
Doing so sheds light on the challenges of working in |
and audio streaming account for a large percentage of content |
settings also require security against attempted intrusions and distributed denial |
centers has enabled a new class of globally distributed highperformance |
to network variability in the context of bandwidth falls below |
so sheds light on the challenges of working in a |
audio streaming account for a large percentage of content accessed |
has enabled a new class of globally distributed highperformance applications |
network variability in the context of bandwidth falls below a |
sheds light on the challenges of working in a kind |
streaming account for a large percentage of content accessed over |
enabled a new class of globally distributed highperformance applications that |
variability in the context of bandwidth falls below a threshold |
light on the challenges of working in a kind of |
account for a large percentage of content accessed over the |
a new class of globally distributed highperformance applications that coordinate |
on the challenges of working in a kind of environment |
for a large percentage of content accessed over the web |
new class of globally distributed highperformance applications that coordinate over |
the challenges of working in a kind of environment that |
One popular style of streaming on the web is on |
class of globally distributed highperformance applications that coordinate over vast |
the second builds on the first and supports a way |
We bandwidth mode in which communication is restricted or deshow |
challenges of working in a kind of environment that will |
popular style of streaming on the web is on demand |
of globally distributed highperformance applications that coordinate over vast geographical |
second builds on the first and supports a way to |
bandwidth mode in which communication is restricted or deshow how |
of working in a kind of environment that will be |
globally distributed highperformance applications that coordinate over vast geographical distances |
Another style requires streams to be generated and disseminated in |
mode in which communication is restricted or deshow how MFS |
working in a kind of environment that will be more |
builds on the first and supports a way to build |
style requires streams to be generated and disseminated in real |
in which communication is restricted or deshow how MFS is |
in a kind of environment that will be more and |
a financial firm s New York City data center may |
on the first and supports a way to build scripts |
which communication is restricted or deshow how MFS is able |
a kind of environment that will be more and more |
financial firm s New York City data center may receive |
the first and supports a way to build scripts of |
communication is restricted or deshow how MFS is able to |
kind of environment that will be more and more prevalent |
firm s New York City data center may receive real |
first and supports a way to build scripts of simpler |
is restricted or deshow how MFS is able to adapt |
of environment that will be more and more prevalent in |
and supports a way to build scripts of simpler transactions |
restricted or deshow how MFS is able to adapt to |
interested users ideally want to receive the stream without much |
cache data in London for locality and mirror it to |
or deshow how MFS is able to adapt to widely |
environment that will be more and more prevalent in years |
users ideally want to receive the stream without much delay |
Some might argue that all reliability needs can be recast |
data in London for locality and mirror it to Kansas |
deshow how MFS is able to adapt to widely varying |
that will be more and more prevalent in years to |
ideally want to receive the stream without much delay from |
might argue that all reliability needs can be recast in |
in London for locality and mirror it to Kansas for |
how MFS is able to adapt to widely varying bandwidth |
will be more and more prevalent in years to come |
want to receive the stream without much delay from its |
argue that all reliability needs can be recast in terms |
London for locality and mirror it to Kansas for disaster |
MFS is able to adapt to widely varying bandwidth ferred |
to receive the stream without much delay from its original |
Our insights should be of value to developers of other |
that all reliability needs can be recast in terms of |
receive the stream without much delay from its original transmission |
insights should be of value to developers of other high |
all reliability needs can be recast in terms of transactions |
an application has a small number of levels through the |
application has a small number of levels through the use |
streaming systems now allow large numbers of interested users to |
has a small number of levels through the use of |
the past three decades have seen one failed attempt after |
systems now allow large numbers of interested users to receive |
Raw bandwidth is ubiquitous and cheaply available in the form |
a small number of levels through the use of modeless |
past three decades have seen one failed attempt after another |
now allow large numbers of interested users to receive streamed |
bandwidth is ubiquitous and cheaply available in the form of |
small number of levels through the use of modeless adaptation |
three decades have seen one failed attempt after another to |
allow large numbers of interested users to receive streamed data |
is ubiquitous and cheaply available in the form of existing |
as an extension of the component integration features of the |
decades have seen one failed attempt after another to build |
and evaluate the possible modes and chooses the appropriate one |
large numbers of interested users to receive streamed data in |
ubiquitous and cheaply available in the form of existing dark |
an extension of the component integration features of the Microsoft |
have seen one failed attempt after another to build everything |
evaluate the possible modes and chooses the appropriate one based |
numbers of interested users to receive streamed data in near |
and cheaply available in the form of existing dark fiber |
seen one failed attempt after another to build everything over |
the possible modes and chooses the appropriate one based on |
of interested users to receive streamed data in near real |
one failed attempt after another to build everything over a |
possible modes and chooses the appropriate one based on the |
interested users to receive streamed data in near real time |
failed attempt after another to build everything over a database |
experiments reveal a series of problematic interactions between its high |
modes and chooses the appropriate one based on the benefit |
attempt after another to build everything over a database system |
and chooses the appropriate one based on the benefit of |
chooses the appropriate one based on the benefit of mechanisms |
and it s now clear that many kinds of systems |
the appropriate one based on the benefit of mechanisms for |
it s now clear that many kinds of systems just |
appropriate one based on the benefit of mechanisms for improving |
s now clear that many kinds of systems just don |
one based on the benefit of mechanisms for improving file |
now clear that many kinds of systems just don t |
based on the benefit of mechanisms for improving file system |
We addressed these and achieved high performance by making some |
clear that many kinds of systems just don t match |
where nodes interested in receiving data also help disseminate it |
on the benefit of mechanisms for improving file system performance |
addressed these and achieved high performance by making some unusual |
that many kinds of systems just don t match the |
nodes interested in receiving data also help disseminate it to |
the benefit of mechanisms for improving file system performance currently |
these and achieved high performance by making some unusual architectural |
many kinds of systems just don t match the model |
interested in receiving data also help disseminate it to each |
benefit of mechanisms for improving file system performance currently available |
These intrinsically distributed systems make use of direct communication between |
in receiving data also help disseminate it to each other |
and achieved high performance by making some unusual architectural decisions |
of mechanisms for improving file system performance currently available bandwidth |
intrinsically distributed systems make use of direct communication between programs |
distributed systems make use of direct communication between programs via |
systems make use of direct communication between programs via the |
make use of direct communication between programs via the Trans |
in the Coda file and cache consistency using microbenchmarks and |
based overlay of nodes through which data would be pushed |
the Coda file and cache consistency using microbenchmarks and file |
Coda file and cache consistency using microbenchmarks and file system |
file and cache consistency using microbenchmarks and file system system |
today s Web services standards seem to answer these needs |
have shown that the use of a mesh of connected |
shown that the use of a mesh of connected nodes |
that the use of a mesh of connected nodes and |
the use of a mesh of connected nodes and a |
use of a mesh of connected nodes and a pull |
based data dissemination approach can provide similar results with better |
data dissemination approach can provide similar results with better resilience |
a new multicast platform designed to achieve high performance in |
dissemination approach can provide similar results with better resilience to |
new multicast platform designed to achieve high performance in managed |
which affects the policy for writing changes to files back |
Reliability provides for reliable handoff between a client system and |
approach can provide similar results with better resilience to failures |
multicast platform designed to achieve high performance in managed environments |
affects the policy for writing changes to files back to |
provides for reliable handoff between a client system and a |
can provide similar results with better resilience to failures and |
the policy for writing changes to files back to the |
Memoryrelated overheads and phenomena related to scheduling are shown to |
for reliable handoff between a client system and a queuing |
provide similar results with better resilience to failures and churn |
policy for writing changes to files back to the server |
overheads and phenomena related to scheduling are shown to dominate |
reliable handoff between a client system and a queuing system |
and phenomena related to scheduling are shown to dominate the |
handoff between a client system and a queuing system residing |
phenomena related to scheduling are shown to dominate the behavior |
between a client system and a queuing system residing between |
related to scheduling are shown to dominate the behavior of |
a client system and a queuing system residing between the |
to scheduling are shown to dominate the behavior of the |
such as switching network access from an Ethernet to a |
client system and a queuing system residing between the client |
scheduling are shown to dominate the behavior of the system |
as switching network access from an Ethernet to a modem |
system and a queuing system residing between the client and |
and request packets from their neighbors based on the received |
but Mobility is now an major feature of computer systems |
We discuss techniques that helped us to alleviate these problems |
and a queuing system residing between the client and some |
request packets from their neighbors based on the received notifications |
a queuing system residing between the client and some service |
and argue that they reveal general principles applicable to other |
argue that they reveal general principles applicable to other kinds |
held devices capable of wireless availability is less predictable and |
the standard isn t nearly as comprehensive as the name |
that they reveal general principles applicable to other kinds of |
where they are used to disseminate television channels to thousands |
devices capable of wireless availability is less predictable and varies |
standard isn t nearly as comprehensive as the name implies |
they reveal general principles applicable to other kinds of high |
they are used to disseminate television channels to thousands of |
capable of wireless availability is less predictable and varies over |
are used to disseminate television channels to thousands of users |
of wireless availability is less predictable and varies over a |
wireless availability is less predictable and varies over a larger |
IP has three major problems when used over such networks |
availability is less predictable and varies over a larger possible |
Reliability boils down to a few options that a client |
Introduction A component integration revolution is transforming the development of |
is less predictable and varies over a larger possible network |
P paradigm allows systems to scale with the number of |
boils down to a few options that a client can |
A component integration revolution is transforming the development of desktop |
less predictable and varies over a larger possible network access |
IP suffers throughput collapse if the network is even slightly |
paradigm allows systems to scale with the number of users |
down to a few options that a client can use |
component integration revolution is transforming the development of desktop applications |
predictable and varies over a larger possible network access have |
suffers throughput collapse if the network is even slightly prone |
to a few options that a client can use to |
and varies over a larger possible network access have become |
Opportunistic nodes attempt to receive a stream without uploading their |
throughput collapse if the network is even slightly prone to |
a few options that a client can use to tell |
varies over a larger possible network access have become common |
nodes attempt to receive a stream without uploading their fair |
EE promote an application development style in which components are |
collapse if the network is even slightly prone to packet |
few options that a client can use to tell the |
attempt to receive a stream without uploading their fair share |
promote an application development style in which components are implemented |
if the network is even slightly prone to packet loss |
options that a client can use to tell the queuing |
to receive a stream without uploading their fair share of |
Applications that run on hosts in wireless neting on how |
an application development style in which components are implemented independently |
that a client can use to tell the queuing system |
Conservative flow control mechanisms designed to deal with the systematic |
receive a stream without uploading their fair share of data |
that run on hosts in wireless neting on how much |
application development style in which components are implemented independently and |
a client can use to tell the queuing system whether |
flow control mechanisms designed to deal with the systematic congestion |
run on hosts in wireless neting on how much data |
development style in which components are implemented independently and heavily |
client can use to tell the queuing system whether or |
control mechanisms designed to deal with the systematic congestion of |
on hosts in wireless neting on how much data the |
not much work has been done in studying mechanisms to |
style in which components are implemented independently and heavily reused |
can use to tell the queuing system whether or not |
mechanisms designed to deal with the systematic congestion of the |
hosts in wireless neting on how much data the application |
much work has been done in studying mechanisms to avoid |
use to tell the queuing system whether or not to |
designed to deal with the systematic congestion of the commodity |
in wireless neting on how much data the application is |
work has been done in studying mechanisms to avoid their |
to tell the queuing system whether or not to reissue |
to deal with the systematic congestion of the commodity Internet |
wireless neting on how much data the application is trying |
has been done in studying mechanisms to avoid their presence |
tell the queuing system whether or not to reissue a |
deal with the systematic congestion of the commodity Internet react |
neting on how much data the application is trying to |
been done in studying mechanisms to avoid their presence in |
the queuing system whether or not to reissue a request |
with the systematic congestion of the commodity Internet react too |
on how much data the application is trying to send |
done in studying mechanisms to avoid their presence in live |
our project is interested in leveraging these benefits to help |
queuing system whether or not to reissue a request if |
the systematic congestion of the commodity Internet react too sharply |
so that works must cope with constraints on access to |
The goal of this The authors were supported by AFRL |
system whether or not to reissue a request if a |
systematic congestion of the commodity Internet react too sharply to |
project is interested in leveraging these benefits to help developers |
that works must cope with constraints on access to data |
goal of this The authors were supported by AFRL award |
whether or not to reissue a request if a failure |
is interested in leveraging these benefits to help developers implement |
works must cope with constraints on access to data that |
of this The authors were supported by AFRL award FA |
or not to reissue a request if a failure occurs |
interested in leveraging these benefits to help developers implement robust |
must cope with constraints on access to data that are |
in leveraging these benefits to help developers implement robust and |
and a way to timestamp requests so that a service |
cope with constraints on access to data that are genit |
leveraging these benefits to help developers implement robust and scalable |
a way to timestamp requests so that a service can |
provisioned links a single packet in ten thousand is enough |
with constraints on access to data that are genit may |
these benefits to help developers implement robust and scalable computing |
way to timestamp requests so that a service can detect |
links a single packet in ten thousand is enough to |
constraints on access to data that are genit may make |
benefits to help developers implement robust and scalable computing services |
to timestamp requests so that a service can detect duplicates |
a single packet in ten thousand is enough to reduce |
on access to data that are genit may make sense |
to help developers implement robust and scalable computing services that |
single packet in ten thousand is enough to reduce TCP |
access to data that are genit may make sense to |
help developers implement robust and scalable computing services that will |
to data that are genit may make sense to adjust |
developers implement robust and scalable computing services that will run |
data that are genit may make sense to adjust network |
implement robust and scalable computing services that will run on |
One is aimed at applications that perform database transactions with |
that are genit may make sense to adjust network usage |
and one in a thousand drops it by an order |
robust and scalable computing services that will run on clusters |
is aimed at applications that perform database transactions with the |
are genit may make sense to adjust network usage when |
one in a thousand drops it by an order of |
and scalable computing services that will run on clusters or |
aimed at applications that perform database transactions with the usual |
genit may make sense to adjust network usage when the |
in a thousand drops it by an order of magnitude |
scalable computing services that will run on clusters or in |
at applications that perform database transactions with the usual ACID |
may make sense to adjust network usage when the bandwidth |
computing services that will run on clusters or in datacenters |
make sense to adjust network usage when the bandwidth erally |
time or interactive applications are impacted by the reliance of |
sense to adjust network usage when the bandwidth erally not |
or interactive applications are impacted by the reliance of reliability |
to adjust network usage when the bandwidth erally not present |
adjust network usage when the bandwidth erally not present in |
interactive applications are impacted by the reliance of reliability mechanisms |
Early users of our platform are creating applications in areas |
network usage when the bandwidth erally not present in wired |
applications are impacted by the reliance of reliability mechanisms on |
users of our platform are creating applications in areas such |
usage when the bandwidth erally not present in wired networks |
or the Remote Procedure Call and that can t tolerate |
are impacted by the reliance of reliability mechanisms on acknowledgments |
of our platform are creating applications in areas such as |
the Remote Procedure Call and that can t tolerate delay |
impacted by the reliance of reliability mechanisms on acknowledgments and |
our platform are creating applications in areas such as parallelized |
by the reliance of reliability mechanisms on acknowledgments and retransmissions |
These systems lack databases clean separation of stored data from |
platform are creating applications in areas such as parallelized data |
systems lack databases clean separation of stored data from code |
limiting the latency of packet recovery to at least the |
are creating applications in areas such as parallelized data mining |
contention with other hosts or processes on the same host |
the latency of packet recovery to at least the Round |
and any attempt to force them into that model results |
latency of packet recovery to at least the Round Trip |
any attempt to force them into that model results in |
Selecting a mode according to the available bandwidth can uninterference |
of packet recovery to at least the Round Trip Time |
Developers of clustered services need reliable multicast protocols for data |
attempt to force them into that model results in unacceptable |
of clustered services need reliable multicast protocols for data replication |
and switching between different wireless media all necessarily constrain communication |
to force them into that model results in unacceptable loss |
and in light of our broader goal of leveraging the |
force them into that model results in unacceptable loss of |
in light of our broader goal of leveraging the power |
since it ignores what data compound the variability in network |
them into that model results in unacceptable loss of performance |
light of our broader goal of leveraging the power and |
it ignores what data compound the variability in network performance |
of our broader goal of leveraging the power and component |
IP requires massive buffers at the communicating endhosts to fully |
ignores what data compound the variability in network performance to |
our broader goal of leveraging the power and component integration |
requires massive buffers at the communicating endhosts to fully exploit |
what data compound the variability in network performance to which |
broader goal of leveraging the power and component integration features |
massive buffers at the communicating endhosts to fully exploit the |
the existing reliability options simply don t address the requirement |
data compound the variability in network performance to which apthe |
goal of leveraging the power and component integration features of |
A LESSON FROM THE PAST What sorts of scaling and |
compound the variability in network performance to which apthe application |
buffers at the communicating endhosts to fully exploit the bandwidth |
of leveraging the power and component integration features of a |
LESSON FROM THE PAST What sorts of scaling and reliability |
the variability in network performance to which apthe application actually |
at the communicating endhosts to fully exploit the bandwidth of |
leveraging the power and component integration features of a managed |
FROM THE PAST What sorts of scaling and reliability features |
variability in network performance to which apthe application actually wants |
the communicating endhosts to fully exploit the bandwidth of a |
the power and component integration features of a managed framework |
THE PAST What sorts of scaling and reliability features are |
in network performance to which apthe application actually wants to |
communicating endhosts to fully exploit the bandwidth of a long |
The views and conclusions herein are those of the authors |
But little is known about highperformance protocols in managed environments |
network performance to which apthe application actually wants to send |
PAST What sorts of scaling and reliability features are lacking |
performance to which apthe application actually wants to send over |
What sorts of scaling and reliability features are lacking in |
to which apthe application actually wants to send over the |
sorts of scaling and reliability features are lacking in Web |
which apthe application actually wants to send over the network |
of scaling and reliability features are lacking in Web services |
where standardization is the key to low and predictable maintenance |
scaling and reliability features are lacking in Web services standards |
standardization is the key to low and predictable maintenance costs |
and reliability features are lacking in Web services standards today |
ring writing back all modifications to files may not be |
neither is eliminating loss events on a network that could |
writing back all modifications to files may not be a |
is eliminating loss events on a network that could span |
back all modifications to files may not be a sensible |
Building a server that scales to handle load often requires |
eliminating loss events on a network that could span thousands |
all modifications to files may not be a sensible This |
a server that scales to handle load often requires replicating |
loss events on a network that could span thousands of |
modifications to files may not be a sensible This paper |
server that scales to handle load often requires replicating data |
events on a network that could span thousands of miles |
to files may not be a sensible This paper focuses |
that scales to handle load often requires replicating data on |
files may not be a sensible This paper focuses on |
scales to handle load often requires replicating data on multiple |
may not be a sensible This paper focuses on adaptation |
there is a need to mask loss on the link |
to handle load often requires replicating data on multiple nodes |
not be a sensible This paper focuses on adaptation techniques |
is a need to mask loss on the link from |
handle load often requires replicating data on multiple nodes of |
be a sensible This paper focuses on adaptation techniques for |
a need to mask loss on the link from the |
load often requires replicating data on multiple nodes of a |
a sensible This paper focuses on adaptation techniques for management |
need to mask loss on the link from the commodity |
often requires replicating data on multiple nodes of a cluster |
sensible This paper focuses on adaptation techniques for management policy |
to mask loss on the link from the commodity protocols |
This paper focuses on adaptation techniques for management policy if |
mask loss on the link from the commodity protocols running |
paper focuses on adaptation techniques for management policy if those |
loss on the link from the commodity protocols running at |
focuses on adaptation techniques for management policy if those are |
on the link from the commodity protocols running at end |
A company that buys a cluster probably wants to guarantee |
on adaptation techniques for management policy if those are the |
company that buys a cluster probably wants to guarantee that |
Minimum and average download rates across all nodes when using |
adaptation techniques for management policy if those are the only |
The embedding of QSM into Windows yielded an unexpected benefit |
that buys a cluster probably wants to guarantee that some |
and average download rates across all nodes when using the |
techniques for management policy if those are the only messages |
because recovery delays for lost packets translate into dramatic reductions |
buys a cluster probably wants to guarantee that some service |
average download rates across all nodes when using the BAR |
for management policy if those are the only messages available |
recovery delays for lost packets translate into dramatic reductions in |
a cluster probably wants to guarantee that some service will |
these are abstract data types in which content evolves over |
download rates across all nodes when using the BAR Gossip |
management policy if those are the only messages available to |
delays for lost packets translate into dramatic reductions in application |
cluster probably wants to guarantee that some service will be |
are abstract data types in which content evolves over time |
rates across all nodes when using the BAR Gossip and |
policy if those are the only messages available to send |
probably wants to guarantee that some service will be responsive |
across all nodes when using the BAR Gossip and Chainsaw |
because applications and OS networking stacks in commodity data centers |
wants to guarantee that some service will be responsive enough |
all nodes when using the BAR Gossip and Chainsaw protocols |
the current state of the object is imported and the |
applications and OS networking stacks in commodity data centers cannot |
paper is to propose and evaluate a mechanism that can |
current state of the object is imported and the object |
to guarantee that some service will be responsive enough to |
and OS networking stacks in commodity data centers cannot be |
is to propose and evaluate a mechanism that can defend |
state of the object is imported and the object can |
guarantee that some service will be responsive enough to keep |
OS networking stacks in commodity data centers cannot be rewritten |
a client cache manager for a manager for a distributed |
to propose and evaluate a mechanism that can defend against |
of the object is imported and the object can send |
that some service will be responsive enough to keep its |
networking stacks in commodity data centers cannot be rewritten from |
client cache manager for a manager for a distributed file |
propose and evaluate a mechanism that can defend against this |
the object is imported and the object can send and |
some service will be responsive enough to keep its customers |
stacks in commodity data centers cannot be rewritten from scratch |
cache manager for a manager for a distributed file system |
and evaluate a mechanism that can defend against this problem |
object is imported and the object can send and receive |
service will be responsive enough to keep its customers happy |
manager for a manager for a distributed file system client |
is imported and the object can send and receive updates |
will be responsive enough to keep its customers happy even |
The approach that most closely relates to our work is |
imported and the object can send and receive updates at |
We concentrate on distributed file systraditional cache manager design in |
approach that most closely relates to our work is the |
be responsive enough to keep its customers happy even when |
and the object can send and receive updates at high |
concentrate on distributed file systraditional cache manager design in two |
that most closely relates to our work is the BAR |
responsive enough to keep its customers happy even when demand |
the object can send and receive updates at high data |
on distributed file systraditional cache manager design in two important |
most closely relates to our work is the BAR Gossip |
enough to keep its customers happy even when demand is |
object can send and receive updates at high data rates |
distributed file systraditional cache manager design in two important respects |
closely relates to our work is the BAR Gossip protocol |
to keep its customers happy even when demand is high |
An object could be a place in a game like |
object could be a place in a game like Second |
could be a place in a game like Second Life |
tems because systems in this area are highly developed and |
packet recovery latency is independent of the RTT of the |
because systems in this area are highly developed and have |
recovery latency is independent of the RTT of the link |
cycle services that can launch an application on demand or |
systems in this area are highly developed and have MFS |
While FEC codes have been used for decades within link |
services that can launch an application on demand or restart |
a node only sends as much data to another node |
in this area are highly developed and have MFS uses |
that can launch an application on demand or restart a |
node only sends as much data to another node as |
this area are highly developed and have MFS uses an |
can launch an application on demand or restart a failed |
only sends as much data to another node as it |
and we plan to pursue the concept in future work |
area are highly developed and have MFS uses an RPC |
launch an application on demand or restart a failed component |
sends as much data to another node as it receives |
are highly developed and have MFS uses an RPC library |
as much data to another node as it receives back |
this use of QSM raises performance and scalability issues beyond |
highly developed and have MFS uses an RPC library supporting |
Or load balancers and technology to automate management of a |
use of QSM raises performance and scalability issues beyond the |
It provides an elegant solution shown to tolerate both opportunistic |
developed and have MFS uses an RPC library supporting priorities |
load balancers and technology to automate management of a machine |
of QSM raises performance and scalability issues beyond the ones |
provides an elegant solution shown to tolerate both opportunistic behavior |
and have MFS uses an RPC library supporting priorities to |
balancers and technology to automate management of a machine cluster |
QSM raises performance and scalability issues beyond the ones seen |
an elegant solution shown to tolerate both opportunistic behavior and |
have MFS uses an RPC library supporting priorities to enable |
and technology to automate management of a machine cluster running |
technology to automate management of a machine cluster running Web |
elegant solution shown to tolerate both opportunistic behavior and other |
end FEC is very attractive for communication between data centers |
MFS uses an RPC library supporting priorities to enable modewell |
raises performance and scalability issues beyond the ones seen in |
to automate management of a machine cluster running Web services |
solution shown to tolerate both opportunistic behavior and other malicious |
uses an RPC library supporting priorities to enable modewell understood |
and does not require specialized equipment in the network linking |
automate management of a machine cluster running Web services applications |
shown to tolerate both opportunistic behavior and other malicious attacks |
performance and scalability issues beyond the ones seen in our |
an RPC library supporting priorities to enable modewell understood semantics |
does not require specialized equipment in the network linking the |
and scalability issues beyond the ones seen in our original |
not require specialized equipment in the network linking the data |
scalability issues beyond the ones seen in our original target |
require specialized equipment in the network linking the data centers |
issues beyond the ones seen in our original target domain |
the data source should ensure that packets are evenly spread |
data source should ensure that packets are evenly spread across |
source should ensure that packets are evenly spread across the |
should ensure that packets are evenly spread across the system |
we leave detailed discussion of the idea for the future |
which allocates available bandwidth based should be broadly applicable in |
ensure that packets are evenly spread across the system by |
allocates available bandwidth based should be broadly applicable in other |
that packets are evenly spread across the system by sending |
available bandwidth based should be broadly applicable in other application |
packets are evenly spread across the system by sending data |
bandwidth based should be broadly applicable in other application environments |
are evenly spread across the system by sending data to |
evenly spread across the system by sending data to a |
spread across the system by sending data to a fixed |
across the system by sending data to a fixed proportion |
the system by sending data to a fixed proportion of |
By assigning priorities such as caching dynamic Internet content or |
system by sending data to a fixed proportion of nodes |
assigning priorities such as caching dynamic Internet content or caching |
stable traffic rates and performs poorly if the data rate |
priorities such as caching dynamic Internet content or caching to |
traffic rates and performs poorly if the data rate in |
such as caching dynamic Internet content or caching to improve |
rates and performs poorly if the data rate in the |
it requires the source and all nodes to have full |
a silver bullet to solve every problem related to older |
as caching dynamic Internet content or caching to improve appropriately |
speed event stream filtering and data mining system to obtain |
and performs poorly if the data rate in the channel |
requires the source and all nodes to have full membership |
silver bullet to solve every problem related to older mainframe |
event stream filtering and data mining system to obtain a |
performs poorly if the data rate in the channel is |
the source and all nodes to have full membership knowledge |
bullet to solve every problem related to older mainframe and |
stream filtering and data mining system to obtain a scalable |
poorly if the data rate in the channel is low |
to solve every problem related to older mainframe and batch |
We evaluate proceed concurrently with background activities such as writing |
These restrictions affect scalability when the data source has bounded |
if the data rate in the channel is low and |
solve every problem related to older mainframe and batch systems |
evaluate proceed concurrently with background activities such as writing The |
restrictions affect scalability when the data source has bounded upload |
the data rate in the channel is low and sporadic |
proceed concurrently with background activities such as writing The authors |
Group used for System Management Service B X Y Z |
Companies rushed to move everything from mainframe settings to client |
affect scalability when the data source has bounded upload bandwidth |
concurrently with background activities such as writing The authors were |
used for System Management Service B X Y Z X |
with background activities such as writing The authors were supported |
for System Management Service B X Y Z X Y |
we fixed the upload capacity of a data source at |
background activities such as writing The authors were supported in |
but it quickly became apparent that the early platforms were |
System Management Service B X Y Z X Y Z |
activities such as writing The authors were supported in part |
it quickly became apparent that the early platforms were strikingly |
Management Service B X Y Z X Y Z X |
such as writing The authors were supported in part by |
quickly became apparent that the early platforms were strikingly immature |
Service B X Y Z X Y Z X Y |
we present the Maelstrom Error Correction appliance a rack of |
as writing The authors were supported in part by DARPA |
B X Y Z X Y Z X Y Z |
present the Maelstrom Error Correction appliance a rack of proxies |
for which we fixed the source s upload bandwidth to |
writing The authors were supported in part by DARPA under |
X Y Z X Y Z X Y Z A |
the Maelstrom Error Correction appliance a rack of proxies residing |
The authors were supported in part by DARPA under AFRL |
Y Z X Y Z X Y Z A B |
Maelstrom Error Correction appliance a rack of proxies residing between |
authors were supported in part by DARPA under AFRL grant |
Z X Y Z X Y Z A B Service |
Error Correction appliance a rack of proxies residing between a |
The total cost of ownership proved to be unexpectedly and |
were supported in part by DARPA under AFRL grant RADC |
X Y Z X Y Z A B Service c |
of both protocols when the number of nodes is increased |
Correction appliance a rack of proxies residing between a data |
total cost of ownership proved to be unexpectedly and unacceptably |
supported in part by DARPA under AFRL grant RADC back |
Y Z X Y Z A B Service c A |
appliance a rack of proxies residing between a data center |
cost of ownership proved to be unexpectedly and unacceptably high |
BAR Gossip is not able to sustain its performance without |
in part by DARPA under AFRL grant RADC back changes |
Z X Y Z A B Service c A B |
a rack of proxies residing between a data center and |
Gossip is not able to sustain its performance without scaling |
X Y Z A B Service c A B W |
server era is that incomplete platforms can t support major |
rack of proxies residing between a data center and its |
is not able to sustain its performance without scaling the |
Y Z A B Service c A B W Figure |
of proxies residing between a data center and its WAN |
not able to sustain its performance without scaling the upload |
proxies residing between a data center and its WAN link |
My concern is that the Web services community is about |
able to sustain its performance without scaling the upload capacity |
concern is that the Web services community is about to |
to sustain its performance without scaling the upload capacity of |
is that the Web services community is about to face |
sustain its performance without scaling the upload capacity of the |
that the Web services community is about to face the |
Maelstrom encodes FEC packets over traffic flowing through it and |
its performance without scaling the upload capacity of the source |
the Web services community is about to face the same |
encodes FEC packets over traffic flowing through it and routes |
One could imagine an approach to laying out components on |
performance without scaling the upload capacity of the source proportionally |
Web services community is about to face the same problem |
FEC packets over traffic flowing through it and routes them |
could imagine an approach to laying out components on a |
without scaling the upload capacity of the source proportionally with |
packets over traffic flowing through it and routes them to |
imagine an approach to laying out components on a cluster |
scaling the upload capacity of the source proportionally with the |
over traffic flowing through it and routes them to a |
an approach to laying out components on a cluster that |
the upload capacity of the source proportionally with the size |
while closing their eyes to the dangerous potholes in the |
traffic flowing through it and routes them to a corresponding |
approach to laying out components on a cluster that would |
upload capacity of the source proportionally with the size of |
closing their eyes to the dangerous potholes in the road |
flowing through it and routes them to a corresponding appliance |
to laying out components on a cluster that would result |
capacity of the source proportionally with the size of the |
their eyes to the dangerous potholes in the road ahead |
through it and routes them to a corresponding appliance at |
laying out components on a cluster that would result in |
of the source proportionally with the size of the system |
it and routes them to a corresponding appliance at the |
ARCHITECTURAL STANDARDS FOR SCALABILITY To properly address scalability in Web |
out components on a cluster that would result in irregular |
and routes them to a corresponding appliance at the destination |
STANDARDS FOR SCALABILITY To properly address scalability in Web services |
Chainsaw is able to scale well even with a fixed |
components on a cluster that would result in irregular layouts |
routes them to a corresponding appliance at the destination data |
is able to scale well even with a fixed lower |
on a cluster that would result in irregular layouts of |
we need more than a long list of reliability and |
them to a corresponding appliance at the destination data center |
able to scale well even with a fixed lower upload |
a cluster that would result in irregular layouts of groups |
need more than a long list of reliability and management |
to scale well even with a fixed lower upload bandwidth |
more than a long list of reliability and management standards |
Maelstrom is completely transparent it does not require modification of |
scale well even with a fixed lower upload bandwidth at |
is completely transparent it does not require modification of end |
well even with a fixed lower upload bandwidth at the |
we need a new methodology suitable for supporting a scalable |
additional support from Microsoft Research and from the Intel Corporation |
but for reasons of brevity the discussion in the remainder |
even with a fixed lower upload bandwidth at the source |
host software and is agnostic to the network connecting the |
need a new methodology suitable for supporting a scalable data |
for reasons of brevity the discussion in the remainder of |
software and is agnostic to the network connecting the data |
a new methodology suitable for supporting a scalable data center |
Application programs background processing incoming traffic cache consistency demand fetch |
reasons of brevity the discussion in the remainder of the |
and is agnostic to the network connecting the data centers |
new methodology suitable for supporting a scalable data center architecture |
programs background processing incoming traffic cache consistency demand fetch access |
of brevity the discussion in the remainder of the paper |
Our auditing approach establishes a minimum threshold for the amount |
background processing incoming traffic cache consistency demand fetch access monitoring |
it eliminates the dependence of FEC recovery latency on the |
brevity the discussion in the remainder of the paper focuses |
auditing approach establishes a minimum threshold for the amount of |
processing incoming traffic cache consistency demand fetch access monitoring prefetch |
eliminates the dependence of FEC recovery latency on the data |
the discussion in the remainder of the paper focuses on |
approach establishes a minimum threshold for the amount of data |
incoming traffic cache consistency demand fetch access monitoring prefetch outgoing |
the dependence of FEC recovery latency on the data rate |
discussion in the remainder of the paper focuses on regular |
establishes a minimum threshold for the amount of data sent |
traffic cache consistency demand fetch access monitoring prefetch outgoing traffic |
dependence of FEC recovery latency on the data rate in |
recommends that developers think in terms of a reliable arraystructured |
a minimum threshold for the amount of data sent by |
cache consistency demand fetch access monitoring prefetch outgoing traffic synchronous |
of FEC recovery latency on the data rate in any |
Initial users of our system haven t had any difficulty |
that developers think in terms of a reliable arraystructured partitioned |
minimum threshold for the amount of data sent by any |
consistency demand fetch access monitoring prefetch outgoing traffic synchronous writeback |
FEC recovery latency on the data rate in any single |
users of our system haven t had any difficulty with |
developers think in terms of a reliable arraystructured partitioned service |
threshold for the amount of data sent by any node |
demand fetch access monitoring prefetch outgoing traffic synchronous writeback update |
recovery latency on the data rate in any single node |
of our system haven t had any difficulty with this |
for the amount of data sent by any node in |
fetch access monitoring prefetch outgoing traffic synchronous writeback update logging |
our system haven t had any difficulty with this constraint |
the amount of data sent by any node in the |
access monitoring prefetch outgoing traffic synchronous writeback update logging asynchronous |
node channel by encoding over the aggregated traffic leaving the |
amount of data sent by any node in the system |
monitoring prefetch outgoing traffic synchronous writeback update logging asynchronous writeback |
channel by encoding over the aggregated traffic leaving the data |
prefetch outgoing traffic synchronous writeback update logging asynchronous writeback MFS |
and removes nodes that upload less data than the threshold |
by encoding over the aggregated traffic leaving the data center |
outgoing traffic synchronous writeback update logging asynchronous writeback MFS server |
The lower level implements subservices using groups of programs that |
but a review of prior systems convinced us that no |
traffic synchronous writeback update logging asynchronous writeback MFS server Adaptive |
lower level implements subservices using groups of programs that run |
a review of prior systems convinced us that no existing |
we focus on encouraging nodes to respect the established protocol |
synchronous writeback update logging asynchronous writeback MFS server Adaptive RPC |
level implements subservices using groups of programs that run on |
review of prior systems convinced us that no existing system |
Maelstrom s positioning as a network appliance reflects the physical |
writeback update logging asynchronous writeback MFS server Adaptive RPC library |
Nodes are forced to provide accountable information regarding packets sent |
implements subservices using groups of programs that run on multiple |
of prior systems convinced us that no existing system would |
s positioning as a network appliance reflects the physical infrastructure |
update logging asynchronous writeback MFS server Adaptive RPC library MFS |
are forced to provide accountable information regarding packets sent to |
subservices using groups of programs that run on multiple machines |
prior systems convinced us that no existing system would work |
positioning as a network appliance reflects the physical infrastructure of |
logging asynchronous writeback MFS server Adaptive RPC library MFS cache |
forced to provide accountable information regarding packets sent to and |
systems convinced us that no existing system would work well |
as a network appliance reflects the physical infrastructure of modern |
asynchronous writeback MFS server Adaptive RPC library MFS cache manager |
The groups replicate data so that each can handle any |
to provide accountable information regarding packets sent to and received |
convinced us that no existing system would work well in |
a network appliance reflects the physical infrastructure of modern data |
writeback MFS server Adaptive RPC library MFS cache manager will |
groups replicate data so that each can handle any incoming |
provide accountable information regarding packets sent to and received from |
us that no existing system would work well in the |
network appliance reflects the physical infrastructure of modern data centers |
MFS server Adaptive RPC library MFS cache manager will be |
replicate data so that each can handle any incoming query |
accountable information regarding packets sent to and received from neighbors |
that no existing system would work well in the scenarios |
appliance reflects the physical infrastructure of modern data centers clean |
and the auditing system is responsible for detecting and removing |
data so that each can handle any incoming query for |
no existing system would work well in the scenarios targeted |
server Adaptive RPC library MFS cache manager will be penalised |
reflects the physical infrastructure of modern data centers clean insertion |
the auditing system is responsible for detecting and removing misbehaving |
so that each can handle any incoming query for its |
existing system would work well in the scenarios targeted by |
Adaptive RPC library MFS cache manager will be penalised first |
the physical infrastructure of modern data centers clean insertion points |
auditing system is responsible for detecting and removing misbehaving nodes |
that each can handle any incoming query for its range |
system would work well in the scenarios targeted by our |
physical infrastructure of modern data centers clean insertion points for |
Modeless adaptation using prioritised communication also allows MFS to be |
each can handle any incoming query for its range within |
would work well in the scenarios targeted by our project |
Notice that identifying the misbehaving nodes is not a trivial |
infrastructure of modern data centers clean insertion points for proxy |
adaptation using prioritised communication also allows MFS to be more |
can handle any incoming query for its range within the |
that identifying the misbehaving nodes is not a trivial task |
This forced us to build a new system that combines |
of modern data centers clean insertion points for proxy devices |
using prioritised communication also allows MFS to be more flexible |
handle any incoming query for its range within the keys |
forced us to build a new system that combines features |
since there is no fixed minimum amount of data that |
modern data centers clean insertion points for proxy devices exist |
prioritised communication also allows MFS to be more flexible in |
us to build a new system that combines features from |
there is no fixed minimum amount of data that nodes |
data centers clean insertion points for proxy devices exist on |
communication also allows MFS to be more flexible in response |
to build a new system that combines features from a |
is no fixed minimum amount of data that nodes should |
centers clean insertion points for proxy devices exist on the |
also allows MFS to be more flexible in response to |
tailer such as Amazon might use to personalize a product |
build a new system that combines features from a number |
no fixed minimum amount of data that nodes should contribute |
clean insertion points for proxy devices exist on the high |
allows MFS to be more flexible in response to bandwidth |
such as Amazon might use to personalize a product recommendation |
a new system that combines features from a number of |
fixed minimum amount of data that nodes should contribute to |
MFS to be more flexible in response to bandwidth variations |
speed lambda links that interconnect individual data centers to each |
new system that combines features from a number of prior |
minimum amount of data that nodes should contribute to the |
to be more flexible in response to bandwidth variations than |
the service ranks matching products differently to maximize the chance |
lambda links that interconnect individual data centers to each other |
system that combines features from a number of prior systems |
amount of data that nodes should contribute to the system |
Maelstrom can operate as either a passive or active device |
Our decision not to use some existing multicast system reflects |
be more flexible in response to bandwidth variations than would |
service ranks matching products differently to maximize the chance of |
can operate as either a passive or active device on |
If we assume a model where misbehaving nodes simply did |
decision not to use some existing multicast system reflects a |
more flexible in response to bandwidth variations than would be |
ranks matching products differently to maximize the chance of a |
operate as either a passive or active device on these |
we assume a model where misbehaving nodes simply did not |
not to use some existing multicast system reflects a number |
flexible in response to bandwidth variations than would be possible |
matching products differently to maximize the chance of a purchase |
as either a passive or active device on these links |
assume a model where misbehaving nodes simply did not upload |
to use some existing multicast system reflects a number of |
in response to bandwidth variations than would be possible with |
a model where misbehaving nodes simply did not upload any |
use some existing multicast system reflects a number of issues |
response to bandwidth variations than would be possible with a |
model where misbehaving nodes simply did not upload any data |
Maelstrom solves the first two throughput collapse and realtime recovery |
to bandwidth variations than would be possible with a modal |
the service assigns the search request to the RACS handling |
solves the first two throughput collapse and realtime recovery delays |
Most prior multicast systems were designed to replicate state within |
bandwidth variations than would be possible with a modal scheme |
service assigns the search request to the RACS handling all |
the first two throughput collapse and realtime recovery delays while |
prior multicast systems were designed to replicate state within just |
once we assume that misbehaving nodes may adjust their contribution |
assigns the search request to the RACS handling all Ds |
first two throughput collapse and realtime recovery delays while operating |
multicast systems were designed to replicate state within just a |
MFS incorporates a new cache consistency algorithm to efficiently provide |
we assume that misbehaving nodes may adjust their contribution level |
two throughput collapse and realtime recovery delays while operating as |
systems were designed to replicate state within just a single |
incorporates a new cache consistency algorithm to efficiently provide a |
assume that misbehaving nodes may adjust their contribution level based |
throughput collapse and realtime recovery delays while operating as a |
were designed to replicate state within just a single group |
The load balancer then routes the request to the appropriate |
a new cache consistency algorithm to efficiently provide a high |
that misbehaving nodes may adjust their contribution level based on |
collapse and realtime recovery delays while operating as a passive |
designed to replicate state within just a single group at |
load balancer then routes the request to the appropriate program |
new cache consistency algorithm to efficiently provide a high degree |
misbehaving nodes may adjust their contribution level based on the |
and realtime recovery delays while operating as a passive device |
to replicate state within just a single group at a |
balancer then routes the request to the appropriate program for |
cache consistency algorithm to efficiently provide a high degree of |
nodes may adjust their contribution level based on the policy |
realtime recovery delays while operating as a passive device that |
replicate state within just a single group at a time |
then routes the request to the appropriate program for processing |
consistency algorithm to efficiently provide a high degree of consistency |
may adjust their contribution level based on the policy used |
recovery delays while operating as a passive device that does |
routes the request to the appropriate program for processing in |
algorithm to efficiently provide a high degree of consistency for |
adjust their contribution level based on the policy used by |
delays while operating as a passive device that does not |
the request to the appropriate program for processing in this |
while others have overheads linear in the number of groups |
to efficiently provide a high degree of consistency for access |
their contribution level based on the policy used by an |
while operating as a passive device that does not intervene |
request to the appropriate program for processing in this case |
others have overheads linear in the number of groups to |
efficiently provide a high degree of consistency for access to |
contribution level based on the policy used by an auditing |
operating as a passive device that does not intervene in |
have overheads linear in the number of groups to which |
provide a high degree of consistency for access to shared |
level based on the policy used by an auditing system |
as a passive device that does not intervene in the |
overheads linear in the number of groups to which a |
it s possible to tackle a wide range of secondary |
a high degree of consistency for access to shared files |
a passive device that does not intervene in the critical |
linear in the number of groups to which a node |
s possible to tackle a wide range of secondary issues |
This paper presents and evaluates an auditing model based on |
passive device that does not intervene in the critical communication |
in the number of groups to which a node belongs |
paper presents and evaluates an auditing model based on sampling |
device that does not intervene in the critical communication path |
describes the MFS design and differences from existing distributed and |
presents and evaluates an auditing model based on sampling the |
the MFS design and differences from existing distributed and mobile |
and evaluates an auditing model based on sampling the system |
MFS design and differences from existing distributed and mobile file |
Maelstrom handles the additional problem of massive buffering requirements as |
evaluates an auditing model based on sampling the system and |
Such a basic architecture is effectively a framework to resolve |
design and differences from existing distributed and mobile file systems |
handles the additional problem of massive buffering requirements as well |
an auditing model based on sampling the system and using |
a component of the JBoss platform which runs in a |
at the cost of adding a point of failure in |
as well as giving an overview of the MFS RPC |
auditing model based on sampling the system and using the |
a basic architecture is effectively a framework to resolve other |
component of the JBoss platform which runs in a managed |
the cost of adding a point of failure in the |
well as giving an overview of the MFS RPC library |
model based on sampling the system and using the sampled |
basic architecture is effectively a framework to resolve other related |
of the JBoss platform which runs in a managed Java |
cost of adding a point of failure in the network |
based on sampling the system and using the sampled information |
architecture is effectively a framework to resolve other related issues |
the JBoss platform which runs in a managed Java framework |
describes the use of prioritised communication in MFS and experiments |
of adding a point of failure in the network path |
on sampling the system and using the sampled information to |
the use of prioritised communication in MFS and experiments to |
JGroups wasn t designed to support large numbers of overlapping |
GROUP REPLICATION Web services currently lacks support for building scalable |
sampling the system and using the sampled information to build |
use of prioritised communication in MFS and experiments to evaluate |
wasn t designed to support large numbers of overlapping groups |
REPLICATION Web services currently lacks support for building scalable services |
the system and using the sampled information to build a |
of prioritised communication in MFS and experiments to evaluate its |
system and using the sampled information to build a global |
prioritised communication in MFS and experiments to evaluate its effectiveness |
and argue that the rate sensitivity of FEC codes and |
and using the sampled information to build a global view |
node server that responds to requests from some set of |
There has been a great deal of work on P |
argue that the rate sensitivity of FEC codes and the |
using the sampled information to build a global view of |
server that responds to requests from some set of clients |
presents and explains experimental results for the MFS prefetching mechanism |
that the rate sensitivity of FEC codes and the opacity |
but there s no way to turn that single server |
the sampled information to build a global view of how |
the rate sensitivity of FEC codes and the opacity of |
there s no way to turn that single server into |
sampled information to build a global view of how the |
rate sensitivity of FEC codes and the opacity of their |
s no way to turn that single server into a |
information to build a global view of how the system |
sensitivity of FEC codes and the opacity of their implementations |
no way to turn that single server into a RACS |
to build a global view of how the system is |
of FEC codes and the opacity of their implementations present |
way to turn that single server into a RACS or |
build a global view of how the system is currently |
systems in this class incur steep overheads associated with content |
FEC codes and the opacity of their implementations present major |
to turn that single server into a RACS or turn |
a global view of how the system is currently behaving |
in this class incur steep overheads associated with content filtering |
The most important part of MFS is the cache manager |
codes and the opacity of their implementations present major obstacles |
turn that single server into a RACS or turn a |
and the opacity of their implementations present major obstacles to |
auditors employ strategies to identify the misbehaving nodes that should |
that single server into a RACS or turn a set |
which intercepts file system operations from application programs and resolves |
the opacity of their implementations present major obstacles to their |
employ strategies to identify the misbehaving nodes that should be |
single server into a RACS or turn a set of |
intercepts file system operations from application programs and resolves them |
opacity of their implementations present major obstacles to their usage |
these factors would degrade the performance of the replicated application |
strategies to identify the misbehaving nodes that should be punished |
server into a RACS or turn a set of RACS |
file system operations from application programs and resolves them into |
into a RACS or turn a set of RACS into |
a gateway appliance that transparently aggregates traffic and encodes over |
system operations from application programs and resolves them into accesses |
a RACS or turn a set of RACS into a |
gateway appliance that transparently aggregates traffic and encodes over the |
operations from application programs and resolves them into accesses to |
RACS or turn a set of RACS into a RAPS |
appliance that transparently aggregates traffic and encodes over the resulting |
we state the exact problem that we aim to solve |
from application programs and resolves them into accesses to its |
that transparently aggregates traffic and encodes over the resulting high |
state the exact problem that we aim to solve and |
application programs and resolves them into accesses to its local |
it would be easy to bridge the gap if vendors |
there is really only one Use of QSM in our |
the exact problem that we aim to solve and the |
programs and resolves them into accesses to its local MFS |
a new FEC scheme used by Maelstrom where for constant |
is really only one Use of QSM in our target |
exact problem that we aim to solve and the assumptions |
would be easy to bridge the gap if vendors and |
and resolves them into accesses to its local MFS cache |
new FEC scheme used by Maelstrom where for constant encoding |
really only one Use of QSM in our target settings |
problem that we aim to solve and the assumptions considered |
be easy to bridge the gap if vendors and platform |
resolves them into accesses to its local MFS cache or |
FEC scheme used by Maelstrom where for constant encoding overhead |
only one Use of QSM in our target settings gives |
that we aim to solve and the assumptions considered in |
easy to bridge the gap if vendors and platform builders |
them into accesses to its local MFS cache or RPCs |
scheme used by Maelstrom where for constant encoding overhead the |
one Use of QSM in our target settings gives rise |
we aim to solve and the assumptions considered in this |
to bridge the gap if vendors and platform builders wanted |
into accesses to its local MFS cache or RPCs to |
used by Maelstrom where for constant encoding overhead the latency |
Use of QSM in our target settings gives rise to |
aim to solve and the assumptions considered in this work |
bridge the gap if vendors and platform builders wanted to |
accesses to its local MFS cache or RPCs to a |
by Maelstrom where for constant encoding overhead the latency of |
of QSM in our target settings gives rise to potentially |
the gap if vendors and platform builders wanted to do |
to its local MFS cache or RPCs to a server |
Maelstrom where for constant encoding overhead the latency of packet |
QSM in our target settings gives rise to potentially large |
gap if vendors and platform builders wanted to do so |
where for constant encoding overhead the latency of packet recovery |
those in solid boxes are part of the core system |
followed by a description of our novel auditing approach in |
in our target settings gives rise to potentially large numbers |
for constant encoding overhead the latency of packet recovery degrades |
by a description of our novel auditing approach in section |
those in dashed boxes are optional extensions which are described |
our target settings gives rise to potentially large numbers of |
constant encoding overhead the latency of packet recovery degrades gracefully |
in dashed boxes are optional extensions which are described in |
target settings gives rise to potentially large numbers of overlapping |
encoding overhead the latency of packet recovery degrades gracefully as |
dashed boxes are optional extensions which are described in subsequent |
settings gives rise to potentially large numbers of overlapping communication |
overhead the latency of packet recovery degrades gracefully as losses |
boxes are optional extensions which are described in subsequent sections |
and briefly describe how to extend our model for heterogeneous |
the latency of packet recovery degrades gracefully as losses get |
gives rise to potentially large numbers of overlapping communication groups |
briefly describe how to extend our model for heterogeneous systems |
latency of packet recovery degrades gracefully as losses get burstier |
The service assigns a digital camera search request to the |
MFS overview MFS differs from earlier mobile file systems in |
service assigns a digital camera search request to the clustered |
the primary goal is to support data replication in scalable |
overview MFS differs from earlier mobile file systems in adjusting |
assigns a digital camera search request to the clustered server |
MFS differs from earlier mobile file systems in adjusting to |
a digital camera search request to the clustered server handling |
differs from earlier mobile file systems in adjusting to changing |
in which sets of components are interconnected and cooperate to |
digital camera search request to the clustered server handling all |
from earlier mobile file systems in adjusting to changing network |
which sets of components are interconnected and cooperate to perform |
camera search request to the clustered server handling all Ds |
earlier mobile file systems in adjusting to changing network conditions |
sets of components are interconnected and cooperate to perform requests |
mobile file systems in adjusting to changing network conditions using |
PROBLEM STATEMENT Our approach focuses on a target streaming system |
and a load balancer routes it to the appropriate process |
and recovers packets with latency independent of the RTT of |
STATEMENT Our approach focuses on a target streaming system consisting |
file systems in adjusting to changing network conditions using modeless |
recovers packets with latency independent of the RTT of the |
Old and familiar technologies The most standard form of system |
Our approach focuses on a target streaming system consisting of |
systems in adjusting to changing network conditions using modeless adaptation |
each of its constituent components will need to replicate its |
packets with latency independent of the RTT of the link |
and familiar technologies The most standard form of system support |
approach focuses on a target streaming system consisting of one |
of its constituent components will need to replicate its portion |
with latency independent of the RTT of the link and |
familiar technologies The most standard form of system support for |
focuses on a target streaming system consisting of one data |
its constituent components will need to replicate its portion of |
and a number of subsystems that perform different kinds of |
latency independent of the RTT of the link and the |
technologies The most standard form of system support for building |
on a target streaming system consisting of one data source |
constituent components will need to replicate its portion of the |
a number of subsystems that perform different kinds of adaptation |
independent of the RTT of the link and the rate |
The most standard form of system support for building a |
components will need to replicate its portion of the service |
of the RTT of the link and the rate in |
most standard form of system support for building a RAPS |
will need to replicate its portion of the service state |
the RTT of the link and the rate in any |
standard form of system support for building a RAPS of |
which disseminates data at a fixed rate to a dynamic |
RTT of the link and the rate in any single |
form of system support for building a RAPS of RACS |
disseminates data at a fixed rate to a dynamic set |
while subsequent sections do the same for the three main |
of the link and the rate in any single channel |
this results in a pattern of communication groups that are |
of system support for building a RAPS of RACS would |
data at a fixed rate to a dynamic set of |
subsequent sections do the same for the three main subsystems |
results in a pattern of communication groups that are exactly |
system support for building a RAPS of RACS would draw |
at a fixed rate to a dynamic set of receivers |
in a pattern of communication groups that are exactly overlapped |
Packet loss typically occurs at two points in an end |
support for building a RAPS of RACS would draw on |
We begin with an overview of mobile file system design |
each replicated component will have one or more associated groups |
for building a RAPS of RACS would draw on virtual |
and hence can only send data directly to a small |
begin with an overview of mobile file system design and |
building a RAPS of RACS would draw on virtual synchrony |
hence can only send data directly to a small subset |
with an overview of mobile file system design and the |
can only send data directly to a small subset of |
an overview of mobile file system design and the relation |
only send data directly to a small subset of interested |
overview of mobile file system design and the relation of |
send data directly to a small subset of interested receivers |
Loss in the lambda link can occur for many reasons |
of mobile file system design and the relation of MFS |
mobile file system design and the relation of MFS to |
Participating nodes are consequently required to forward packets to their |
file system design and the relation of MFS to previous |
s and used today to run the New York and |
nodes are consequently required to forward packets to their neighbors |
system design and the relation of MFS to previous work |
and used today to run the New York and Swiss |
used today to run the New York and Swiss stock |
low receiver power and burst switching contention are some reasons |
The streamed data should be received by all nodes within |
then briefly describe the adaptive RPC library used in MFS |
today to run the New York and Swiss stock exchange |
streamed data should be received by all nodes within a |
to run the New York and Swiss stock exchange systems |
data should be received by all nodes within a fixed |
Cluster management systems use groups for purposes other than component |
should be received by all nodes within a fixed latency |
management systems use groups for purposes other than component replication |
be received by all nodes within a fixed latency from |
received by all nodes within a fixed latency from the |
MFS design and related work The core of MFS follows |
by all nodes within a fixed latency from the source |
IBM s Websphere platform and the Windows Vista clustering system |
design and related work The core of MFS follows a |
all nodes within a fixed latency from the source s |
s Websphere platform and the Windows Vista clustering system also |
and related work The core of MFS follows a design |
nodes within a fixed latency from the source s original |
Websphere platform and the Windows Vista clustering system also use |
The result is an environment in which there will be |
related work The core of MFS follows a design common |
within a fixed latency from the source s original transmission |
platform and the Windows Vista clustering system also use versions |
result is an environment in which there will be a |
work The core of MFS follows a design common to |
and the Windows Vista clustering system also use versions of |
is an environment in which there will be a hierarchy |
The core of MFS follows a design common to many |
the Windows Vista clustering system also use versions of the |
core of MFS follows a design common to many mobile |
Windows Vista clustering system also use versions of the model |
of MFS follows a design common to many mobile file |
MFS follows a design common to many mobile file systems |
these are usually cheap commodity machines prone to temporary overloads |
are usually cheap commodity machines prone to temporary overloads that |
which is implemented in scalable file systems and other ultrareliable |
QSM is highly effective in supporting this style of use |
we briefly discuss how to extend our model to work |
usually cheap commodity machines prone to temporary overloads that cause |
is implemented in scalable file systems and other ultrareliable server |
briefly discuss how to extend our model to work in |
cheap commodity machines prone to temporary overloads that cause packets |
implemented in scalable file systems and other ultrareliable server designs |
discuss how to extend our model to work in heterogeneous |
commodity machines prone to temporary overloads that cause packets to |
how to extend our model to work in heterogeneous scenarios |
machines prone to temporary overloads that cause packets to be |
prone to temporary overloads that cause packets to be dropped |
to temporary overloads that cause packets to be dropped by |
A natural option would be to offer them in the |
consisting of a small set of servers to which client |
temporary overloads that cause packets to be dropped by the |
natural option would be to offer them in the context |
of a small set of servers to which client systems |
requesting data as needed and sending data as requested from |
overloads that cause packets to be dropped by the kernel |
option would be to offer them in the context of |
a small set of servers to which client systems connect |
data as needed and sending data as requested from them |
The design of MFS is closest in structure to that |
that cause packets to be dropped by the kernel in |
would be to offer them in the context of WS |
design of MFS is closest in structure to that of |
cause packets to be dropped by the kernel in bursts |
Altrustic nodes are a subgroup of correct nodes that are |
of MFS is closest in structure to that of Coda |
nodes are a subgroup of correct nodes that are willing |
These filter the ordered multicast stream and relay messages back |
are a subgroup of correct nodes that are willing to |
filter the ordered multicast stream and relay messages back out |
a subgroup of correct nodes that are willing to upload |
the ordered multicast stream and relay messages back out to |
if you re replicating data within some form of group |
subgroup of correct nodes that are willing to upload more |
ordered multicast stream and relay messages back out to receivers |
you can just as easily imagine that it has a |
of correct nodes that are willing to upload more data |
can just as easily imagine that it has a subject |
correct nodes that are willing to upload more data than |
This approach can support huge numbers of groups with irregular |
just as easily imagine that it has a subject name |
nodes that are willing to upload more data than required |
approach can support huge numbers of groups with irregular overlap |
as easily imagine that it has a subject name in |
that are willing to upload more data than required from |
can support huge numbers of groups with irregular overlap patterns |
easily imagine that it has a subject name in a |
are willing to upload more data than required from them |
A host acting as a client of an MFS file |
imagine that it has a subject name in a publish |
host acting as a client of an MFS file system |
acting as a client of an MFS file system runs |
These considerations convinced us that a new system was needed |
as a client of an MFS file system runs a |
we employ the term opportunistic to refer to a subgroup |
a client of an MFS file system runs a user |
QSM implements a approach similar to Spread s lightweight group |
employ the term opportunistic to refer to a subgroup of |
implements a approach similar to Spread s lightweight group abstraction |
the term opportunistic to refer to a subgroup of Byzantine |
which receives file system operations intercepted by a kernel module |
term opportunistic to refer to a subgroup of Byzantine nodes |
opportunistic to refer to a subgroup of Byzantine nodes that |
interacting with the VFS layer of the local file system |
We define a region of overlap to be a set |
to refer to a subgroup of Byzantine nodes that attempt |
We adopt the same approach to intercepting VFS operations as |
define a region of overlap to be a set of |
refer to a subgroup of Byzantine nodes that attempt to |
adopt the same approach to intercepting VFS operations as LBFS |
a region of overlap to be a set of nodes |
to a subgroup of Byzantine nodes that attempt to give |
W e b Te c h n o l o |
region of overlap to be a set of nodes with |
making use of the kernel module provided as part of |
a subgroup of Byzantine nodes that attempt to give less |
e b Te c h n o l o g |
of overlap to be a set of nodes with approximately |
use of the kernel module provided as part of the |
subgroup of Byzantine nodes that attempt to give less data |
b Te c h n o l o g i |
overlap to be a set of nodes with approximately the |
of the kernel module provided as part of the Arla |
of Byzantine nodes that attempt to give less data than |
Te c h n o l o g i e |
to be a set of nodes with approximately the same |
the kernel module provided as part of the Arla AFS |
Byzantine nodes that attempt to give less data than they |
c h n o l o g i e s |
be a set of nodes with approximately the same group |
kernel module provided as part of the Arla AFS client |
nodes that attempt to give less data than they would |
h n o l o g i e s Concerns |
a set of nodes with approximately the same group membership |
that attempt to give less data than they would if |
n o l o g i e s Concerns Experience |
attempt to give less data than they would if they |
o l o g i e s Concerns Experience with |
to give less data than they would if they behaved |
l o g i e s Concerns Experience with Corba |
give less data than they would if they behaved as |
o g i e s Concerns Experience with Corba Even |
less data than they would if they behaved as correct |
g i e s Concerns Experience with Corba Even good |
data than they would if they behaved as correct nodes |
i e s Concerns Experience with Corba Even good ideas |
When a VFS operation is intercepted for a file that |
e s Concerns Experience with Corba Even good ideas can |
a VFS operation is intercepted for a file that is |
with the intention of obtaining as much data as possible |
QSM uses regions for multicast dissemination and for recovery of |
s Concerns Experience with Corba Even good ideas can be |
VFS operation is intercepted for a file that is not |
the intention of obtaining as much data as possible at |
uses regions for multicast dissemination and for recovery of lost |
Concerns Experience with Corba Even good ideas can be used |
operation is intercepted for a file that is not in |
intention of obtaining as much data as possible at least |
regions for multicast dissemination and for recovery of lost packets |
Experience with Corba Even good ideas can be used in |
is intercepted for a file that is not in the |
of obtaining as much data as possible at least feasible |
perhaps because such links are a relatively recent addition to |
with Corba Even good ideas can be used in ways |
intercepted for a file that is not in the cache |
obtaining as much data as possible at least feasible cost |
because such links are a relatively recent addition to the |
Corba Even good ideas can be used in ways that |
such links are a relatively recent addition to the networking |
Even good ideas can be used in ways that developers |
links are a relatively recent addition to the networking landscape |
good ideas can be used in ways that developers dislike |
are a relatively recent addition to the networking landscape and |
ideas can be used in ways that developers dislike and |
or a more elaborate strategy that allows them to cheat |
a relatively recent addition to the networking landscape and their |
can be used in ways that developers dislike and ultimately |
a more elaborate strategy that allows them to cheat without |
relatively recent addition to the networking landscape and their ownership |
be used in ways that developers dislike and ultimately reject |
more elaborate strategy that allows them to cheat without being |
recent addition to the networking landscape and their ownership is |
elaborate strategy that allows them to cheat without being easily |
addition to the networking landscape and their ownership is still |
A good example of this occurred when the Corba community |
strategy that allows them to cheat without being easily detected |
to the networking landscape and their ownership is still mostly |
good example of this occurred when the Corba community decided |
the networking landscape and their ownership is still mostly restricted |
Notice that our model diverges from the one used in |
example of this occurred when the Corba community decided to |
networking landscape and their ownership is still mostly restricted to |
Though scheme for minimising bandwidth utilisation when transferring files is |
that our model diverges from the one used in BAR |
of this occurred when the Corba community decided to tackle |
landscape and their ownership is still mostly restricted to commercial |
scheme for minimising bandwidth utilisation when transferring files is not |
our model diverges from the one used in BAR Gossip |
this occurred when the Corba community decided to tackle replication |
aggregating information that can be used by a group sender |
and their ownership is still mostly restricted to commercial organizations |
for minimising bandwidth utilisation when transferring files is not used |
occurred when the Corba community decided to tackle replication for |
information that can be used by a group sender to |
their ownership is still mostly restricted to commercial organizations disinclined |
minimising bandwidth utilisation when transferring files is not used in |
when the Corba community decided to tackle replication for fault |
that can be used by a group sender to retransmit |
ownership is still mostly restricted to commercial organizations disinclined to |
bandwidth utilisation when transferring files is not used in MFS |
the Corba community decided to tackle replication for fault tolerance |
can be used by a group sender to retransmit packets |
is still mostly restricted to commercial organizations disinclined to reveal |
rational nodes attempt to maximize their utility while still following |
Corba community decided to tackle replication for fault tolerance but |
although it is orthogonal to MFS adaptation and could be |
be used by a group sender to retransmit packets that |
still mostly restricted to commercial organizations disinclined to reveal such |
nodes attempt to maximize their utility while still following the |
community decided to tackle replication for fault tolerance but then |
it is orthogonal to MFS adaptation and could be added |
used by a group sender to retransmit packets that were |
mostly restricted to commercial organizations disinclined to reveal such information |
attempt to maximize their utility while still following the defined |
decided to tackle replication for fault tolerance but then stumbled |
is orthogonal to MFS adaptation and could be added to |
by a group sender to retransmit packets that were missed |
to maximize their utility while still following the defined protocol |
to tackle replication for fault tolerance but then stumbled by |
orthogonal to MFS adaptation and could be added to further |
a group sender to retransmit packets that were missed by |
tackle replication for fault tolerance but then stumbled by presenting |
to MFS adaptation and could be added to further improve |
an optical network interconnecting major supercomputing sites in the US |
group sender to retransmit packets that were missed by entire |
nodes employing strategies to maximize their utility are classified as |
replication for fault tolerance but then stumbled by presenting the |
MFS adaptation and could be added to further improve performance |
sender to retransmit packets that were missed by entire regions |
TeraGrid has a monitoring framework within which ten sites periodically |
employing strategies to maximize their utility are classified as Byzantine |
for fault tolerance but then stumbled by presenting the technology |
has a monitoring framework within which ten sites periodically send |
The server that stores a file is responsible for maintaining |
fault tolerance but then stumbled by presenting the technology to |
a monitoring framework within which ten sites periodically send each |
server that stores a file is responsible for maintaining the |
based system in which any node not contributing its fair |
tolerance but then stumbled by presenting the technology to developers |
monitoring framework within which ten sites periodically send each other |
a token circulates to provide loss recovery at the level |
that stores a file is responsible for maintaining the mutual |
system in which any node not contributing its fair share |
but then stumbled by presenting the technology to developers in |
token circulates to provide loss recovery at the level of |
Gbps streams of UDP packets and measure the resulting loss |
stores a file is responsible for maintaining the mutual consistency |
in which any node not contributing its fair share of |
then stumbled by presenting the technology to developers in a |
circulates to provide loss recovery at the level of nodes |
streams of UDP packets and measure the resulting loss rate |
a file is responsible for maintaining the mutual consistency of |
which any node not contributing its fair share of data |
stumbled by presenting the technology to developers in a way |
to provide loss recovery at the level of nodes belonging |
file is responsible for maintaining the mutual consistency of the |
any node not contributing its fair share of data may |
by presenting the technology to developers in a way that |
presenting the technology to developers in a way that was |
is responsible for maintaining the mutual consistency of the copies |
node not contributing its fair share of data may be |
Each site measures the loss rate to every other site |
provide loss recovery at the level of nodes belonging to |
the technology to developers in a way that was much |
responsible for maintaining the mutual consistency of the copies cached |
not contributing its fair share of data may be expelled |
site measures the loss rate to every other site once |
loss recovery at the level of nodes belonging to the |
technology to developers in a way that was much too |
for maintaining the mutual consistency of the copies cached by |
contributing its fair share of data may be expelled from |
measures the loss rate to every other site once an |
recovery at the level of nodes belonging to the region |
to developers in a way that was much too limiting |
maintaining the mutual consistency of the copies cached by clients |
its fair share of data may be expelled from the |
the loss rate to every other site once an hour |
developers in a way that was much too limiting for |
fair share of data may be expelled from the system |
in a way that was much too limiting for general |
a way that was much too limiting for general use |
MFS implements a variation of the scheme used by Coda |
Throughout the paper we use the terms upload factor and |
the paper we use the terms upload factor and download |
paper we use the terms upload factor and download factor |
we use the terms upload factor and download factor to |
use the terms upload factor and download factor to refer |
but the programming tools built over this model prevent developers |
the terms upload factor and download factor to refer to |
obliging it to inform the client through a callback if |
the programming tools built over this model prevent developers from |
terms upload factor and download factor to refer to the |
it to inform the client through a callback if another |
programming tools built over this model prevent developers from using |
upload factor and download factor to refer to the ratio |
to inform the client through a callback if another host |
tools built over this model prevent developers from using threads |
we plan to experiment with larger configurations and will work |
factor and download factor to refer to the ratio between |
inform the client through a callback if another host modifies |
plan to experiment with larger configurations and will work with |
and download factor to refer to the ratio between an |
the client through a callback if another host modifies the |
to experiment with larger configurations and will work with deeper |
download factor to refer to the ratio between an upload |
client through a callback if another host modifies the file |
experiment with larger configurations and will work with deeper hierarchies |
factor to refer to the ratio between an upload or |
If the callback promise expires without a callback being issued |
The QSM recovery protocol uses tokens to track message status |
to refer to the ratio between an upload or download |
refer to the ratio between an upload or download rate |
to the ratio between an upload or download rate and |
The cache consistency algorithm is described in more detail in |
the ratio between an upload or download rate and the |
cache consistency algorithm is described in more detail in Section |
ratio between an upload or download rate and the original |
between an upload or download rate and the original stream |
an upload or download rate and the original stream rate |
a developer who obeys this long list of constraints can |
developer who obeys this long list of constraints can do |
who obeys this long list of constraints can do lockstep |
obeys this long list of constraints can do lockstep replication |
this long list of constraints can do lockstep replication of |
Adaptive RPC library The fundamental difference between MFS and other |
NAK implosion problems with which reliable multicast protocols traditionally have |
long list of constraints can do lockstep replication of a |
RPC library The fundamental difference between MFS and other file |
implosion problems with which reliable multicast protocols traditionally have struggled |
list of constraints can do lockstep replication of a program |
library The fundamental difference between MFS and other file systems |
of constraints can do lockstep replication of a program for |
The fundamental difference between MFS and other file systems we |
constraints can do lockstep replication of a program for tolerance |
the sender may not find out for quite a while |
fundamental difference between MFS and other file systems we have |
can do lockstep replication of a program for tolerance of |
difference between MFS and other file systems we have described |
do lockstep replication of a program for tolerance of hardware |
between MFS and other file systems we have described is |
lockstep replication of a program for tolerance of hardware faults |
this isn t a major issue because most message losses |
MFS and other file systems we have described is in |
isn t a major issue because most message losses can |
and other file systems we have described is in the |
t a major issue because most message losses can be |
STREAMING SYSTEM MODEL Our auditing approach is used over the |
other file systems we have described is in the communication |
a major issue because most message losses can be corrected |
SYSTEM MODEL Our auditing approach is used over the Chainsaw |
file systems we have described is in the communication between |
major issue because most message losses can be corrected locally |
MODEL Our auditing approach is used over the Chainsaw protocol |
systems we have described is in the communication between the |
we have described is in the communication between the cache |
have described is in the communication between the cache manager |
Conventional wisdom states that optical links do not drop packets |
All nodes participating in the system are organized into a |
described is in the communication between the cache manager and |
The basic idea is to perform recovery as locally as |
grade optical equipment is configured to shut down beyond bit |
is in the communication between the cache manager and servers |
nodes participating in the system are organized into a fully |
basic idea is to perform recovery as locally as possible |
While LBFS uses a variant of the NFS RPC protocol |
participating in the system are organized into a fully connected |
optical equipment is configured to shut down beyond bit error |
in the system are organized into a fully connected mesh |
equipment is configured to shut down beyond bit error rates |
the system are organized into a fully connected mesh overlay |
is configured to shut down beyond bit error rates of |
If a message is available within the same token ring |
They supported many of the mechanisms needed to build and |
supported many of the mechanisms needed to build and manage |
some process that has a A A C AC AB |
The source is randomly connected to a small subset of |
many of the mechanisms needed to build and manage a |
process that has a A A C AC AB ABC |
source is randomly connected to a small subset of the |
of the mechanisms needed to build and manage a RAPS |
that has a A A C AC AB ABC BC |
the reliability of the lambda network is clearly not equal |
is randomly connected to a small subset of the nodes |
the mechanisms needed to build and manage a RAPS of |
has a A A C AC AB ABC BC B |
the RPC used in MFS incorporates novel features to allow |
reliability of the lambda network is clearly not equal to |
mechanisms needed to build and manage a RAPS of RACS |
a A A C AC AB ABC BC B C |
RPC used in MFS incorporates novel features to allow it |
of the lambda network is clearly not equal to the |
which breaks the data stream into packets and sends notifications |
A A C AC AB ABC BC B C B |
used in MFS incorporates novel features to allow it to |
and their successes have clearly demonstrated the model s effectiveness |
the lambda network is clearly not equal to the sum |
breaks the data stream into packets and sends notifications to |
A C AC AB ABC BC B C B Figure |
in MFS incorporates novel features to allow it to adapt |
lambda network is clearly not equal to the sum of |
the data stream into packets and sends notifications to its |
MFS incorporates novel features to allow it to adapt to |
network is clearly not equal to the sum of its |
data stream into packets and sends notifications to its neighbors |
incorporates novel features to allow it to adapt to network |
is clearly not equal to the sum of its optical |
Nodes belong to the same region if they have similar |
stream into packets and sends notifications to its neighbors as |
machine approach as used in the Paxos algorithm is also |
novel features to allow it to adapt to network variability |
clearly not equal to the sum of its optical parts |
belong to the same region if they have similar group |
into packets and sends notifications to its neighbors as soon |
approach as used in the Paxos algorithm is also becoming |
to the same region if they have similar group membership |
The MFS RPC library is implemented on top of the |
packets and sends notifications to its neighbors as soon as |
as used in the Paxos algorithm is also becoming more |
MFS RPC library is implemented on top of the Adaptive |
and sends notifications to its neighbors as soon as it |
used in the Paxos algorithm is also becoming more popular |
RPC library is implemented on top of the Adaptive Transport |
speed network are instead subjected to unexpectedly high loss rates |
sends notifications to its neighbors as soon as it has |
library is implemented on top of the Adaptive Transport Protocol |
The key insight is that these successes use similar ideas |
a node multicasts a message to each of the regions |
notifications to its neighbors as soon as it has packets |
these numbers reflect the loss rate specifically experienced by UDP |
key insight is that these successes use similar ideas but |
node multicasts a message to each of the regions separately |
to its neighbors as soon as it has packets to |
numbers reflect the loss rate specifically experienced by UDP traffic |
insight is that these successes use similar ideas but in |
we give an overview of the parts of ATP which |
its neighbors as soon as it has packets to disseminate |
reflect the loss rate specifically experienced by UDP traffic on |
is that these successes use similar ideas but in ways |
give an overview of the parts of ATP which are |
the loss rate specifically experienced by UDP traffic on an |
Our approach makes it easy to aggregate messages across different |
that these successes use similar ideas but in ways very |
These notifications are small messages used only to inform neighbors |
an overview of the parts of ATP which are most |
loss rate specifically experienced by UDP traffic on an end |
approach makes it easy to aggregate messages across different groups |
these successes use similar ideas but in ways very different |
notifications are small messages used only to inform neighbors of |
overview of the parts of ATP which are most relevant |
successes use similar ideas but in ways very different from |
are small messages used only to inform neighbors of the |
If a node has two messages to send to a |
use similar ideas but in ways very different from what |
we do not know if packets were dropped within the |
of the parts of ATP which are most relevant to |
small messages used only to inform neighbors of the availability |
a node has two messages to send to a pair |
similar ideas but in ways very different from what the |
do not know if packets were dropped within the optical |
the parts of ATP which are most relevant to MFS |
messages used only to inform neighbors of the availability of |
node has two messages to send to a pair of |
ideas but in ways very different from what the Corba |
not know if packets were dropped within the optical network |
used only to inform neighbors of the availability of new |
has two messages to send to a pair of groups |
ATP and its design motivations have been described in more |
but in ways very different from what the Corba fault |
know if packets were dropped within the optical network or |
only to inform neighbors of the availability of new packets |
two messages to send to a pair of groups G |
and its design motivations have been described in more detail |
if packets were dropped within the optical network or at |
its design motivations have been described in more detail in |
What we need today is a modern revisiting of this |
packets were dropped within the optical network or at intermediate |
design motivations have been described in more detail in our |
we need today is a modern revisiting of this technology |
were dropped within the optical network or at intermediate devices |
and the source satisfies as many requests as allowed by |
motivations have been described in more detail in our earlier |
need today is a modern revisiting of this technology that |
dropped within the optical network or at intermediate devices within |
the source satisfies as many requests as allowed by its |
Apps Send to A Send to B Group Senders A |
have been described in more detail in our earlier work |
today is a modern revisiting of this technology that draws |
within the optical network or at intermediate devices within either |
source satisfies as many requests as allowed by its upload |
Send to A Send to B Group Senders A B |
is a modern revisiting of this technology that draws on |
the optical network or at intermediate devices within either data |
satisfies as many requests as allowed by its upload capacity |
to A Send to B Group Senders A B C |
a modern revisiting of this technology that draws on group |
optical network or at intermediate devices within either data center |
A Send to B Group Senders A B C Region |
The hypothesis underlying ATP is that adapting to network variation |
modern revisiting of this technology that draws on group communication |
Send to B Group Senders A B C Region Senders |
though it s unlikely that they were dropped at the |
with Chainsaw the upload capacity of the source does not |
hypothesis underlying ATP is that adapting to network variation by |
revisiting of this technology that draws on group communication but |
to B Group Senders A B C Region Senders A |
it s unlikely that they were dropped at the end |
Chainsaw the upload capacity of the source does not need |
underlying ATP is that adapting to network variation by structuring |
of this technology that draws on group communication but packages |
B Group Senders A B C Region Senders A AB |
the upload capacity of the source does not need to |
ATP is that adapting to network variation by structuring applications |
many of the measurements lost just one or two packets |
this technology that draws on group communication but packages it |
Group Senders A B C Region Senders A AB AC |
upload capacity of the source does not need to increase |
is that adapting to network variation by structuring applications according |
of the measurements lost just one or two packets whereas |
technology that draws on group communication but packages it in |
Senders A B C Region Senders A AB AC ABC |
capacity of the source does not need to increase with |
that adapting to network variation by structuring applications according to |
the measurements lost just one or two packets whereas kernel |
that draws on group communication but packages it in a |
A B C Region Senders A AB AC ABC B |
of the source does not need to increase with the |
adapting to network variation by structuring applications according to modes |
draws on group communication but packages it in a way |
B C Region Senders A AB AC ABC B C |
the source does not need to increase with the size |
to network variation by structuring applications according to modes is |
on group communication but packages it in a way that |
C Region Senders A AB AC ABC B C BC |
source does not need to increase with the size of |
network variation by structuring applications according to modes is not |
loss occurred on paths where levels of optical link utilization |
group communication but packages it in a way that developers |
Region Senders A AB AC ABC B C BC region |
does not need to increase with the size of the |
variation by structuring applications according to modes is not always |
communication but packages it in a way that developers perceive |
Senders A AB AC ABC B C BC region leader |
not need to increase with the size of the system |
by structuring applications according to modes is not always appropriate |
but packages it in a way that developers perceive as |
A AB AC ABC B C BC region leader Figure |
even an upload capacity of twice the stream rate is |
packages it in a way that developers perceive as solving |
an upload capacity of twice the stream rate is sufficient |
it in a way that developers perceive as solving their |
upload capacity of twice the stream rate is sufficient to |
in a way that developers perceive as solving their most |
shows the results of an experiment in which modeless adaptation |
capacity of twice the stream rate is sufficient to ensure |
QSM sends a copy to each of the underlying regions |
a way that developers perceive as solving their most pressing |
the results of an experiment in which modeless adaptation over |
of twice the stream rate is sufficient to ensure that |
What are some possible causes for such high loss rates |
way that developers perceive as solving their most pressing scalability |
results of an experiment in which modeless adaptation over ATP |
twice the stream rate is sufficient to ensure that the |
are some possible causes for such high loss rates on |
that developers perceive as solving their most pressing scalability problems |
of an experiment in which modeless adaptation over ATP achieves |
the stream rate is sufficient to ensure that the system |
some possible causes for such high loss rates on TeraGrid |
developers perceive as solving their most pressing scalability problems and |
an experiment in which modeless adaptation over ATP achieves higher |
stream rate is sufficient to ensure that the system performs |
perceive as solving their most pressing scalability problems and that |
A likely hypothesis is device clutter the critical communication path |
experiment in which modeless adaptation over ATP achieves higher bandwidth |
rate is sufficient to ensure that the system performs and |
as solving their most pressing scalability problems and that flexibly |
making it possible to access the communication subsystem directly from |
likely hypothesis is device clutter the critical communication path between |
in which modeless adaptation over ATP achieves higher bandwidth utilisation |
is sufficient to ensure that the system performs and scales |
solving their most pressing scalability problems and that flexibly matches |
it possible to access the communication subsystem directly from the |
hypothesis is device clutter the critical communication path between nodes |
which modeless adaptation over ATP achieves higher bandwidth utilisation than |
sufficient to ensure that the system performs and scales well |
their most pressing scalability problems and that flexibly matches their |
possible to access the communication subsystem directly from the Windows |
is device clutter the critical communication path between nodes in |
modeless adaptation over ATP achieves higher bandwidth utilisation than we |
most pressing scalability problems and that flexibly matches their preferred |
to access the communication subsystem directly from the Windows GUI |
device clutter the critical communication path between nodes in different |
adaptation over ATP achieves higher bandwidth utilisation than we will |
pressing scalability problems and that flexibly matches their preferred styles |
clutter the critical communication path between nodes in different data |
over ATP achieves higher bandwidth utilisation than we will concentrate |
scalability problems and that flexibly matches their preferred styles and |
the user can store a shortcut to a QSM stream |
the critical communication path between nodes in different data centers |
ATP achieves higher bandwidth utilisation than we will concentrate on |
problems and that flexibly matches their preferred styles and tools |
user can store a shortcut to a QSM stream in |
critical communication path between nodes in different data centers is |
achieves higher bandwidth utilisation than we will concentrate on a |
can store a shortcut to a QSM stream in the |
communication path between nodes in different data centers is littered |
higher bandwidth utilisation than we will concentrate on a system |
store a shortcut to a QSM stream in the file |
path between nodes in different data centers is littered with |
since a participant will have multiple possible sources for each |
bandwidth utilisation than we will concentrate on a system with |
a shortcut to a QSM stream in the file system |
The user simply designs a data structure and employs multicast |
between nodes in different data centers is littered with multiple |
a participant will have multiple possible sources for each packet |
utilisation than we will concentrate on a system with a |
user simply designs a data structure and employs multicast technology |
nodes in different data centers is littered with multiple electronic |
than we will concentrate on a system with a single |
The mesh overlay defines a predetermined set of neighbors for |
simply designs a data structure and employs multicast technology to |
click to attach a previewer or a viewer to an |
in different data centers is littered with multiple electronic devices |
we will concentrate on a system with a single server |
mesh overlay defines a predetermined set of neighbors for each |
designs a data structure and employs multicast technology to transmit |
to attach a previewer or a viewer to an event |
overlay defines a predetermined set of neighbors for each peer |
Another possibility is that such loss rates may be typical |
a data structure and employs multicast technology to transmit updates |
attach a previewer or a viewer to an event stream |
possibility is that such loss rates may be typical for |
data structure and employs multicast technology to transmit updates to |
which also makes it hard for malicious peers to round |
is that such loss rates may be typical for any |
structure and employs multicast technology to transmit updates to the |
also makes it hard for malicious peers to round up |
that such loss rates may be typical for any large |
and employs multicast technology to transmit updates to the group |
makes it hard for malicious peers to round up on |
employs multicast technology to transmit updates to the group members |
scale network where the cost of immediately detecting and fixing |
it hard for malicious peers to round up on individual |
network where the cost of immediately detecting and fixing failures |
hard for malicious peers to round up on individual peers |
where the cost of immediately detecting and fixing failures is |
for malicious peers to round up on individual peers since |
the cost of immediately detecting and fixing failures is prohibitively |
malicious peers to round up on individual peers since attackers |
cost of immediately detecting and fixing failures is prohibitively high |
peers to round up on individual peers since attackers lack |
to round up on individual peers since attackers lack a |
Examples of updates include a stock trade or stock market |
round up on individual peers since attackers lack a deterministic |
of updates include a stock trade or stock market quote |
we found through dialogue with the administrators that the steady |
up on individual peers since attackers lack a deterministic means |
a new object detected by radar in an air traffic |
found through dialogue with the administrators that the steady loss |
on individual peers since attackers lack a deterministic means of |
new object detected by radar in an air traffic control |
through dialogue with the administrators that the steady loss rate |
individual peers since attackers lack a deterministic means of acquiring |
object detected by radar in an air traffic control system |
dialogue with the administrators that the steady loss rate experienced |
peers since attackers lack a deterministic means of acquiring control |
with the administrators that the steady loss rate experienced by |
since attackers lack a deterministic means of acquiring control of |
the administrators that the steady loss rate experienced by the |
or the addition of a node to a distributed data |
attackers lack a deterministic means of acquiring control of all |
administrators that the steady loss rate experienced by the Indiana |
the addition of a node to a distributed data structure |
lack a deterministic means of acquiring control of all of |
that the steady loss rate experienced by the Indiana University |
addition of a node to a distributed data structure containing |
a deterministic means of acquiring control of all of its |
the steady loss rate experienced by the Indiana University site |
of a node to a distributed data structure containing an |
deterministic means of acquiring control of all of its neighbors |
steady loss rate experienced by the Indiana University site was |
a node to a distributed data structure containing an index |
loss rate experienced by the Indiana University site was due |
node to a distributed data structure containing an index of |
All nodes with exception of the source have a fixed |
rate experienced by the Indiana University site was due to |
to a distributed data structure containing an index of pending |
nodes with exception of the source have a fixed upper |
experienced by the Indiana University site was due to a |
a distributed data structure containing an index of pending orders |
with exception of the source have a fixed upper limit |
by the Indiana University site was due to a faulty |
distributed data structure containing an index of pending orders in |
exception of the source have a fixed upper limit on |
the Indiana University site was due to a faulty line |
data structure containing an index of pending orders in an |
of the source have a fixed upper limit on their |
Indiana University site was due to a faulty line card |
structure containing an index of pending orders in an online |
the source have a fixed upper limit on their upload |
containing an index of pending orders in an online warehouse |
and the measurements showed that the error persisting over at |
source have a fixed upper limit on their upload contribution |
the measurements showed that the error persisting over at least |
measurements showed that the error persisting over at least a |
showed that the error persisting over at least a three |
that the error persisting over at least a three month |
the error persisting over at least a three month period |
copy will forward it to the process missing the message |
Computer chrony service can run at rates well in excess |
chrony service can run at rates well in excess of |
who attempt to reduce it with the goal of uploading |
attempt to reduce it with the goal of uploading less |
QSM also uses this idea at the level of partitions |
to reduce it with the goal of uploading less data |
it s possible to maintain rates of thousands per second |
s possible to maintain rates of thousands per second on |
each node stores packets and forwards them to other peers |
possible to maintain rates of thousands per second on typical |
node stores packets and forwards them to other peers only |
to maintain rates of thousands per second on typical hardware |
stores packets and forwards them to other peers only while |
packets and forwards them to other peers only while the |
The virtual synchrony and statemachine models show how a tremendous |
including data used to perform rate control and information used |
and forwards them to other peers only while the packet |
virtual synchrony and statemachine models show how a tremendous range |
data used to perform rate control and information used to |
forwards them to other peers only while the packet is |
synchrony and statemachine models show how a tremendous range of |
used to perform rate control and information used to trigger |
them to other peers only while the packet is within |
and statemachine models show how a tremendous range of application |
to perform rate control and information used to trigger garbage |
to other peers only while the packet is within its |
statemachine models show how a tremendous range of application requirements |
perform rate control and information used to trigger garbage collection |
other peers only while the packet is within its availability |
models show how a tremendous range of application requirements can |
peers only while the packet is within its availability window |
The overall system configuration is managed by what we call |
show how a tremendous range of application requirements can map |
overall system configuration is managed by what we call the |
how a tremendous range of application requirements can map down |
system configuration is managed by what we call the Configuration |
a tremendous range of application requirements can map down to |
We expect privately managed lambdas to exhibit higher loss rates |
configuration is managed by what we call the Configuration Management |
tremendous range of application requirements can map down to a |
expect privately managed lambdas to exhibit higher loss rates due |
is managed by what we call the Configuration Management Service |
range of application requirements can map down to a rigorously |
privately managed lambdas to exhibit higher loss rates due to |
which represents the set of packets in which the peer |
of application requirements can map down to a rigorously precise |
managed lambdas to exhibit higher loss rates due to the |
represents the set of packets in which the peer is |
application requirements can map down to a rigorously precise execution |
lambdas to exhibit higher loss rates due to the inherent |
and uses these to generate a sequence of membership views |
the set of packets in which the peer is currently |
requirements can map down to a rigorously precise execution model |
to exhibit higher loss rates due to the inherent tradeoff |
uses these to generate a sequence of membership views for |
set of packets in which the peer is currently interested |
and the right graph shows a scheme in which there |
exhibit higher loss rates due to the inherent tradeoff between |
these to generate a sequence of membership views for each |
which in turn can be used to validate a platform |
the right graph shows a scheme in which there are |
higher loss rates due to the inherent tradeoff between fiber |
to generate a sequence of membership views for each multicast |
Nodes choose packets to request from each of its neighbors |
right graph shows a scheme in which there are four |
graph shows a scheme in which there are four classes |
shows a scheme in which there are four classes of |
and even use theorem provers to assist developers in testing |
a scheme in which there are four classes of messages |
and tracks the mapping from group views to region views |
even use theorem provers to assist developers in testing their |
scheme in which there are four classes of messages being |
respecting a maximum limit l on the number of outstanding |
use theorem provers to assist developers in testing their most |
as well as the difficulty of performing routine maintenance on |
in which there are four classes of messages being sent |
a maximum limit l on the number of outstanding requests |
theorem provers to assist developers in testing their most critical |
well as the difficulty of performing routine maintenance on longdistance |
machine replicated version in the future to eliminate the risk |
which there are four classes of messages being sent simultaneously |
maximum limit l on the number of outstanding requests to |
provers to assist developers in testing their most critical application |
as the difficulty of performing routine maintenance on longdistance links |
replicated version in the future to eliminate the risk of |
limit l on the number of outstanding requests to each |
to assist developers in testing their most critical application components |
version in the future to eliminate the risk of single |
and the highest priority of data being sent during a |
the highest priority of data being sent during a second |
One reason that we lack this sort of support today |
highest priority of data being sent during a second on |
reason that we lack this sort of support today is |
In the longer term we will move to a hierarchically |
priority of data being sent during a second on the |
that we lack this sort of support today is that |
the longer term we will move to a hierarchically structured |
of data being sent during a second on the right |
we lack this sort of support today is that vendors |
longer term we will move to a hierarchically structured CMS |
lack this sort of support today is that vendors and |
this sort of support today is that vendors and platform |
sort of support today is that vendors and platform developers |
of support today is that vendors and platform developers worry |
support today is that vendors and platform developers worry that |
today is that vendors and platform developers worry that these |
IP is the default reliable communication option for contemporary networked |
is that vendors and platform developers worry that these forms |
is the default reliable communication option for contemporary networked applications |
while the modal scheme is dependent on a rapid and |
alarm queue application thread operating system kernel Implementation QSM QSM |
that vendors and platform developers worry that these forms of |
the modal scheme is dependent on a rapid and accurate |
queue application thread operating system kernel Implementation QSM QSM request |
vendors and platform developers worry that these forms of replication |
modal scheme is dependent on a rapid and accurate estimate |
application thread operating system kernel Implementation QSM QSM request queue |
and platform developers worry that these forms of replication haven |
most applications requiring reliable communication over any form of network |
scheme is dependent on a rapid and accurate estimate of |
thread operating system kernel Implementation QSM QSM request queue core |
platform developers worry that these forms of replication haven t |
applications requiring reliable communication over any form of network use |
is dependent on a rapid and accurate estimate of the |
operating system kernel Implementation QSM QSM request queue core thread |
developers worry that these forms of replication haven t achieved |
requiring reliable communication over any form of network use TCP |
dependent on a rapid and accurate estimate of the available |
system kernel Implementation QSM QSM request queue core thread I |
worry that these forms of replication haven t achieved huge |
on a rapid and accurate estimate of the available bandwidth |
that these forms of replication haven t achieved huge market |
a rapid and accurate estimate of the available bandwidth in |
these forms of replication haven t achieved huge market success |
rapid and accurate estimate of the available bandwidth in order |
and accurate estimate of the available bandwidth in order to |
accurate estimate of the available bandwidth in order to select |
estimate of the available bandwidth in order to select its |
of the available bandwidth in order to select its correct |
the available bandwidth in order to select its correct operating |
available bandwidth in order to select its correct operating mode |
tolerant groups mechanism that was based on the virtual synchrony |
groups mechanism that was based on the virtual synchrony model |
IP is unable to distinguish between ephemeral loss modes due |
is unable to distinguish between ephemeral loss modes due to |
the Corba standard is widely viewed as rigid and limited |
unable to distinguish between ephemeral loss modes due to transient |
our intent was to leverage the component integration tools available |
to distinguish between ephemeral loss modes due to transient congestion |
intent was to leverage the component integration tools available on |
I believe that the Corba community erred by embedding a |
was to leverage the component integration tools available on the |
believe that the Corba community erred by embedding a powerful |
to leverage the component integration tools available on the Windows |
Other experiments have shown that modeless adaptation can achieve improvements |
that the Corba community erred by embedding a powerful solution |
The loss of one packet out of ten thousand is |
leverage the component integration tools available on the Windows platform |
experiments have shown that modeless adaptation can achieve improvements of |
the Corba community erred by embedding a powerful solution into |
loss of one packet out of ten thousand is sufficient |
Corba community erred by embedding a powerful solution into a |
of one packet out of ten thousand is sufficient to |
existence with the managed environment would require any special architectural |
community erred by embedding a powerful solution into a tool |
one packet out of ten thousand is sufficient to reduce |
with the managed environment would require any special architectural features |
erred by embedding a powerful solution into a tool mismatched |
packet out of ten thousand is sufficient to reduce TCP |
and it is possible to construct cases in which the |
by embedding a powerful solution into a tool mismatched to |
it is possible to construct cases in which the improvement |
is possible to construct cases in which the improvement is |
embedding a powerful solution into a tool mismatched to developer |
possible to construct cases in which the improvement is even |
a powerful solution into a tool mismatched to developer needs |
to construct cases in which the improvement is even greater |
IP s fundamental reliance on loss as a signal of |
Work on adaptation in mobile file systems has generally relied |
s fundamental reliance on loss as a signal of congestion |
on adaptation in mobile file systems has generally relied on |
but the Corba community s failed effort to implement virtual |
While recent approaches have sought to replace loss with delay |
adaptation in mobile file systems has generally relied on modal |
the Corba community s failed effort to implement virtual synchrony |
recent approaches have sought to replace loss with delay as |
in mobile file systems has generally relied on modal schemes |
Corba community s failed effort to implement virtual synchrony carries |
approaches have sought to replace loss with delay as a |
community s failed effort to implement virtual synchrony carries an |
have sought to replace loss with delay as a congestion |
s failed effort to implement virtual synchrony carries an important |
sought to replace loss with delay as a congestion signal |
failed effort to implement virtual synchrony carries an important lesson |
effort to implement virtual synchrony carries an important lesson to |
to implement virtual synchrony carries an important lesson to current |
but our evaluation of ATP demonstrated that it could also |
implement virtual synchrony carries an important lesson to current researchers |
our evaluation of ATP demonstrated that it could also improve |
evaluation of ATP demonstrated that it could also improve the |
of ATP demonstrated that it could also improve the performance |
ATP demonstrated that it could also improve the performance of |
Any technology offered to developers must support the programming styles |
demonstrated that it could also improve the performance of file |
technology offered to developers must support the programming styles they |
Windows understands QSM to be the handler for operations on |
that it could also improve the performance of file system |
offered to developers must support the programming styles they prefer |
understands QSM to be the handler for operations on new |
QSM to be the handler for operations on new kind |
We discuss the implementation of modeless adaptation in MFS further |
MANAGEMENT POLICIES A scalable services architecture for building RAPS of |
to be the handler for operations on new kind of |
discuss the implementation of modeless adaptation in MFS further in |
POLICIES A scalable services architecture for building RAPS of RACS |
be the handler for operations on new kind of event |
the implementation of modeless adaptation in MFS further in Section |
A scalable services architecture for building RAPS of RACS alone |
the handler for operations on new kind of event stream |
IP uses positive acknowledgments and retransmissions to ensure reliability the |
scalable services architecture for building RAPS of RACS alone isn |
uses positive acknowledgments and retransmissions to ensure reliability the sender |
services architecture for building RAPS of RACS alone isn t |
positive acknowledgments and retransmissions to ensure reliability the sender buffers |
architecture for building RAPS of RACS alone isn t enough |
and can then invoke methods on those handles to send |
acknowledgments and retransmissions to ensure reliability the sender buffers packets |
in which messages of an arbitrary size can be reliably |
can then invoke methods on those handles to send events |
and retransmissions to ensure reliability the sender buffers packets until |
scale systems that will likely soon rely on standardized Web |
which messages of an arbitrary size can be reliably transmitted |
retransmissions to ensure reliability the sender buffers packets until their |
systems that will likely soon rely on standardized Web services |
messages of an arbitrary size can be reliably transmitted with |
to ensure reliability the sender buffers packets until their receipt |
that will likely soon rely on standardized Web services including |
of an arbitrary size can be reliably transmitted with their |
O event representing a received packet is retrieved for a |
ensure reliability the sender buffers packets until their receipt is |
will likely soon rely on standardized Web services including global |
an arbitrary size can be reliably transmitted with their boundaries |
event representing a received packet is retrieved for a given |
reliability the sender buffers packets until their receipt is acknowledged |
likely soon rely on standardized Web services including global banks |
arbitrary size can be reliably transmitted with their boundaries preserved |
representing a received packet is retrieved for a given socket |
the sender buffers packets until their receipt is acknowledged by |
size can be reliably transmitted with their boundaries preserved at |
sender buffers packets until their receipt is acknowledged by the |
the socket is drained to minimize the probability of loss |
can be reliably transmitted with their boundaries preserved at the |
buffers packets until their receipt is acknowledged by the receiver |
and the supervisory control and data acquisition systems that operate |
be reliably transmitted with their boundaries preserved at the receiver |
Several aspects of the architecture are noteworthy because of their |
the supervisory control and data acquisition systems that operate the |
and resends if an acknowledgment is not received within some |
reliably transmitted with their boundaries preserved at the receiver s |
aspects of the architecture are noteworthy because of their performance |
supervisory control and data acquisition systems that operate the US |
resends if an acknowledgment is not received within some time |
transmitted with their boundaries preserved at the receiver s side |
of the architecture are noteworthy because of their performance implications |
control and data acquisition systems that operate the US power |
if an acknowledgment is not received within some time period |
and data acquisition systems that operate the US power grid |
In the latter case the sender provides a function to |
data acquisition systems that operate the US power grid will |
the latter case the sender provides a function to be |
a lost packet is received in the form of a |
acquisition systems that operate the US power grid will also |
latter case the sender provides a function to be executed |
lost packet is received in the form of a retransmission |
systems that operate the US power grid will also require |
case the sender provides a function to be executed when |
packet is received in the form of a retransmission that |
that operate the US power grid will also require policies |
the sender provides a function to be executed when transmission |
is received in the form of a retransmission that arrives |
operate the US power grid will also require policies to |
sender provides a function to be executed when transmission of |
received in the form of a retransmission that arrives no |
the US power grid will also require policies to manage |
provides a function to be executed when transmission of the |
in the form of a retransmission that arrives no earlier |
US power grid will also require policies to manage security |
a function to be executed when transmission of the message |
the form of a retransmission that arrives no earlier than |
power grid will also require policies to manage security keys |
function to be executed when transmission of the message completes |
this is similar to the Queued RPC developed for Rover |
The sender has to buffer each packet until it s |
and by prioritizing control packets over data we reduce delays |
sender has to buffer each packet until it s acknowledged |
Automated tools for monitoring large complex systems will be needed |
by prioritizing control packets over data we reduce delays in |
tools for monitoring large complex systems will be needed as |
prioritizing control packets over data we reduce delays in reacting |
ATP also allows the sender to attach a priority to |
for monitoring large complex systems will be needed as well |
control packets over data we reduce delays in reacting to |
and it has to perform additional work to retransmit the |
also allows the sender to attach a priority to each |
Download and upload factors of nodes in an ideal system |
packets over data we reduce delays in reacting to packet |
it has to perform additional work to retransmit the packet |
allows the sender to attach a priority to each message |
and upload factors of nodes in an ideal system where |
over data we reduce delays in reacting to packet loss |
to control the order in which the queued messages are |
has to perform additional work to retransmit the packet if |
upload factors of nodes in an ideal system where all |
researchers must think about how monitoring and management policies in |
data we reduce delays in reacting to packet loss or |
control the order in which the queued messages are transmitted |
to perform additional work to retransmit the packet if it |
factors of nodes in an ideal system where all nodes |
must think about how monitoring and management policies in different |
we reduce delays in reacting to packet loss or other |
perform additional work to retransmit the packet if it does |
Messages are queued at the sender according to their receivers |
of nodes in an ideal system where all nodes behave |
think about how monitoring and management policies in different organizations |
reduce delays in reacting to packet loss or other control |
Messages of the same priority within a queue are transmitted |
nodes in an ideal system where all nodes behave correctly |
about how monitoring and management policies in different organizations should |
additional work to retransmit the packet if it does not |
of the same priority within a queue are transmitted in |
This limit not only improves the general flow of packets |
work to retransmit the packet if it does not receive |
how monitoring and management policies in different organizations should talk |
the same priority within a queue are transmitted in first |
to retransmit the packet if it does not receive the |
but also makes it harder for malicious peers to overrequest |
The pros and cons of using threads in eventoriented systems |
monitoring and management policies in different organizations should talk to |
retransmit the packet if it does not receive the acknowledgment |
also makes it harder for malicious peers to overrequest packets |
ATP also allows a sender to specify a send timeout |
and management policies in different organizations should talk to one |
pros and cons of using threads in eventoriented systems are |
makes it harder for malicious peers to overrequest packets from |
also allows a sender to specify a send timeout for |
management policies in different organizations should talk to one another |
any packets that arrive with higher sequence numbers than that |
and cons of using threads in eventoriented systems are hotly |
it harder for malicious peers to overrequest packets from their |
allows a sender to specify a send timeout for a |
policies in different organizations should talk to one another when |
packets that arrive with higher sequence numbers than that of |
cons of using threads in eventoriented systems are hotly debated |
harder for malicious peers to overrequest packets from their neighbors |
a sender to specify a send timeout for a message |
in different organizations should talk to one another when Web |
that arrive with higher sequence numbers than that of a |
different organizations should talk to one another when Web services |
which causes the transmission to be suspended if it expires |
arrive with higher sequence numbers than that of a lost |
Although we used threads rather casually in the first year |
organizations should talk to one another when Web services interactions |
with higher sequence numbers than that of a lost packet |
we used threads rather casually in the first year of |
should talk to one another when Web services interactions cross |
Expected Behavior Our first goal is to explore the typical |
higher sequence numbers than that of a lost packet must |
used threads rather casually in the first year of our |
send timeouts do not play a major role in MFS |
talk to one another when Web services interactions cross boundaries |
Behavior Our first goal is to explore the typical signature |
sequence numbers than that of a lost packet must be |
threads rather casually in the first year of our effort |
An additional use for timeouts would be to detect prefetches |
Our first goal is to explore the typical signature of |
numbers than that of a lost packet must be queued |
additional use for timeouts would be to detect prefetches which |
first goal is to explore the typical signature of the |
that version of the system was annoyingly process requests incoming |
than that of a lost packet must be queued while |
use for timeouts would be to detect prefetches which are |
goal is to explore the typical signature of the system |
version of the system was annoyingly process requests incoming control |
that of a lost packet must be queued while the |
for timeouts would be to detect prefetches which are not |
of the system was annoyingly process requests incoming control outgoing |
a scalable technology for distributed monitoring and control that has |
since an understanding of the behavior of pullbased dissemination in |
of a lost packet must be queued while the receiver |
timeouts would be to detect prefetches which are not making |
the system was annoyingly process requests incoming control outgoing control |
scalable technology for distributed monitoring and control that has attracted |
an understanding of the behavior of pullbased dissemination in the |
a lost packet must be queued while the receiver waits |
would be to detect prefetches which are not making progress |
system was annoyingly process requests incoming control outgoing control outgoing |
technology for distributed monitoring and control that has attracted tremendous |
understanding of the behavior of pullbased dissemination in the absence |
lost packet must be queued while the receiver waits for |
be to detect prefetches which are not making progress and |
was annoyingly process requests incoming control outgoing control outgoing data |
for distributed monitoring and control that has attracted tremendous interest |
of the behavior of pullbased dissemination in the absence of |
packet must be queued while the receiver waits for the |
to detect prefetches which are not making progress and reissue |
annoyingly process requests incoming control outgoing control outgoing data feed |
distributed monitoring and control that has attracted tremendous interest and |
the behavior of pullbased dissemination in the absence of opportunistic |
must be queued while the receiver waits for the lost |
detect prefetches which are not making progress and reissue a |
process requests incoming control outgoing control outgoing data feed sink |
monitoring and control that has attracted tremendous interest and attention |
behavior of pullbased dissemination in the absence of opportunistic nodes |
be queued while the receiver waits for the lost packet |
prefetches which are not making progress and reissue a prefetch |
Researchers at other institutions are working on other promising solutions |
of pullbased dissemination in the absence of opportunistic nodes will |
queued while the receiver waits for the lost packet to |
requests incoming control outgoing control outgoing data feed sink limit |
which are not making progress and reissue a prefetch for |
pullbased dissemination in the absence of opportunistic nodes will turn |
while the receiver waits for the lost packet to arrive |
incoming control outgoing control outgoing data feed sink limit sending |
are not making progress and reissue a prefetch for a |
dissemination in the absence of opportunistic nodes will turn out |
it s also a mindset with ramifications at many levels |
control outgoing control outgoing data feed sink limit sending rate |
not making progress and reissue a prefetch for a different |
in the absence of opportunistic nodes will turn out to |
throughput financial banking application running in a data center in |
outgoing control outgoing data feed sink limit sending rate limit |
making progress and reissue a prefetch for a different file |
the absence of opportunistic nodes will turn out to be |
financial banking application running in a data center in New |
control outgoing data feed sink limit sending rate limit concurrency |
Web services platforms must begin to standardize application architectures that |
absence of opportunistic nodes will turn out to be important |
banking application running in a data center in New York |
outgoing data feed sink limit sending rate limit concurrency limit |
services platforms must begin to standardize application architectures that promote |
of opportunistic nodes will turn out to be important when |
application running in a data center in New York City |
data feed sink limit sending rate limit concurrency limit window |
ATP administers priorities by deriving an estimate for the bandwidth |
platforms must begin to standardize application architectures that promote reliability |
opportunistic nodes will turn out to be important when we |
feed sink limit sending rate limit concurrency limit window size |
administers priorities by deriving an estimate for the bandwidth available |
must begin to standardize application architectures that promote reliability and |
nodes will turn out to be important when we set |
sink limit sending rate limit concurrency limit window size Figure |
priorities by deriving an estimate for the bandwidth available between |
begin to standardize application architectures that promote reliability and interoperability |
will turn out to be important when we set out |
by deriving an estimate for the bandwidth available between the |
to standardize application architectures that promote reliability and interoperability when |
turn out to be important when we set out to |
deriving an estimate for the bandwidth available between the sender |
standardize application architectures that promote reliability and interoperability when developers |
out to be important when we set out to introduce |
registers the intent to send with a sink that may |
an estimate for the bandwidth available between the sender and |
application architectures that promote reliability and interoperability when developers build |
to be important when we set out to introduce auditing |
the intent to send with a sink that may be |
estimate for the bandwidth available between the sender and receiver |
architectures that promote reliability and interoperability when developers build systems |
intent to send with a sink that may be controlled |
milliseconds or more between the original packet send and the |
that promote reliability and interoperability when developers build systems of |
In order to minimise the transmission delay when a new |
to send with a sink that may be controlled by |
or more between the original packet send and the receipt |
promote reliability and interoperability when developers build systems of systems |
order to minimise the transmission delay when a new message |
send with a sink that may be controlled by a |
more between the original packet send and the receipt of |
to minimise the transmission delay when a new message is |
with a sink that may be controlled by a policy |
work with intrinsically distributed programs that don t fit a |
between the original packet send and the receipt of its |
minimise the transmission delay when a new message is sent |
a sink that may be controlled by a policy limiting |
with intrinsically distributed programs that don t fit a transactional |
the original packet send and the receipt of its retransmission |
sink that may be controlled by a policy limiting the |
intrinsically distributed programs that don t fit a transactional model |
original packet send and the receipt of its retransmission have |
that may be controlled by a policy limiting the send |
We varied the maximum upload factor of nodes to see |
packet send and the receipt of its retransmission have to |
may be controlled by a policy limiting the send rate |
varied the maximum upload factor of nodes to see how |
send and the receipt of its retransmission have to be |
Applications with these sorts of requirements are already in the |
the maximum upload factor of nodes to see how it |
twentieth of the available bandwidth is used during a single |
and the receipt of its retransmission have to be buffered |
with these sorts of requirements are already in the pipeline |
maximum upload factor of nodes to see how it affected |
of the available bandwidth is used during a single send |
the receipt of its retransmission have to be buffered at |
these sorts of requirements are already in the pipeline and |
upload factor of nodes to see how it affected both |
the available bandwidth is used during a single send period |
receipt of its retransmission have to be buffered at the |
O events according to priorities incoming data policy get messages |
sorts of requirements are already in the pipeline and even |
factor of nodes to see how it affected both the |
of its retransmission have to be buffered at the receiver |
events according to priorities incoming data policy get messages pre |
ATP would send as much data as it could on |
of requirements are already in the pipeline and even more |
of nodes to see how it affected both the download |
would send as much data as it could on receipt |
requirements are already in the pipeline and even more of |
nodes to see how it affected both the download and |
the loss of a single packet stops all traffic in |
O events process timer events register to send app app |
send as much data as it could on receipt of |
are already in the pipeline and even more of them |
to see how it affected both the download and upload |
loss of a single packet stops all traffic in the |
events process timer events register to send app app f |
as much data as it could on receipt of a |
already in the pipeline and even more of them are |
see how it affected both the download and upload factors |
of a single packet stops all traffic in the channel |
much data as it could on receipt of a low |
in the pipeline and even more of them are on |
how it affected both the download and upload factors of |
a single packet stops all traffic in the channel to |
the pipeline and even more of them are on drawing |
it affected both the download and upload factors of nodes |
single packet stops all traffic in the channel to the |
One can think of QSM as a collection of protocol |
and this data could then be buffered at an intermediate |
pipeline and even more of them are on drawing boards |
affected both the download and upload factors of nodes across |
packet stops all traffic in the channel to the application |
can think of QSM as a collection of protocol stacks |
this data could then be buffered at an intermediate link |
and even more of them are on drawing boards in |
both the download and upload factors of nodes across the |
stops all traffic in the channel to the application for |
think of QSM as a collection of protocol stacks in |
even more of them are on drawing boards in government |
the download and upload factors of nodes across the system |
all traffic in the channel to the application for a |
of QSM as a collection of protocol stacks in which |
The disadvantage of this scheme is that heavy contention at |
traffic in the channel to the application for a seventh |
The maximum upload factor is a fixed parameter which defines |
QSM as a collection of protocol stacks in which components |
disadvantage of this scheme is that heavy contention at the |
in the channel to the application for a seventh of |
maximum upload factor is a fixed parameter which defines the |
as a collection of protocol stacks in which components act |
The only option for the Web services community is to |
of this scheme is that heavy contention at the sender |
the channel to the application for a seventh of a |
upload factor is a fixed parameter which defines the maximum |
a collection of protocol stacks in which components act as |
only option for the Web services community is to take |
this scheme is that heavy contention at the sender may |
channel to the application for a seventh of a second |
factor is a fixed parameter which defines the maximum rate |
collection of protocol stacks in which components act as both |
option for the Web services community is to take on |
scheme is that heavy contention at the sender may delay |
is a fixed parameter which defines the maximum rate at |
of protocol stacks in which components act as both feeds |
a sequence of such blocks can have devastating effect on |
for the Web services community is to take on the |
is that heavy contention at the sender may delay a |
a fixed parameter which defines the maximum rate at which |
protocol stacks in which components act as both feeds and |
sequence of such blocks can have devastating effect on a |
the Web services community is to take on the challenge |
that heavy contention at the sender may delay a new |
fixed parameter which defines the maximum rate at which a |
stacks in which components act as both feeds and as |
of such blocks can have devastating effect on a high |
heavy contention at the sender may delay a new message |
parameter which defines the maximum rate at which a node |
in which components act as both feeds and as sinks |
contention at the sender may delay a new message by |
Web services are going to be the ubiquitous platform technology |
which defines the maximum rate at which a node will |
at the sender may delay a new message by as |
services are going to be the ubiquitous platform technology for |
defines the maximum rate at which a node will upload |
the sender may delay a new message by as much |
are going to be the ubiquitous platform technology for next |
the maximum rate at which a node will upload data |
sender may delay a new message by as much as |
a lost packet can potentially trigger a butterfly effect of |
O was to reduce staleness by postponing the creation of |
maximum rate at which a node will upload data to |
lost packet can potentially trigger a butterfly effect of missed |
and we ve no one but ourselves to blame if |
was to reduce staleness by postponing the creation of control |
rate at which a node will upload data to all |
packet can potentially trigger a butterfly effect of missed deadlines |
we ve no one but ourselves to blame if these |
to reduce staleness by postponing the creation of control messages |
at which a node will upload data to all its |
This inefficiency of the ATP implementation is most visible when |
can potentially trigger a butterfly effect of missed deadlines along |
ve no one but ourselves to blame if these systems |
reduce staleness by postponing the creation of control messages until |
which a node will upload data to all its neighbors |
inefficiency of the ATP implementation is most visible when there |
potentially trigger a butterfly effect of missed deadlines along a |
no one but ourselves to blame if these systems don |
staleness by postponing the creation of control messages until the |
of the ATP implementation is most visible when there is |
trigger a butterfly effect of missed deadlines along a distributed |
we would like all nodes to upload data at a |
one but ourselves to blame if these systems don t |
by postponing the creation of control messages until the time |
the ATP implementation is most visible when there is contention |
a butterfly effect of missed deadlines along a distributed workflow |
would like all nodes to upload data at a factor |
but ourselves to blame if these systems don t work |
postponing the creation of control messages until the time when |
ATP implementation is most visible when there is contention between |
like all nodes to upload data at a factor as |
ourselves to blame if these systems don t work properly |
the creation of control messages until the time when transmission |
implementation is most visible when there is contention between different |
all nodes to upload data at a factor as close |
creation of control messages until the time when transmission is |
is most visible when there is contention between different priorities |
Do we really want to create a world in which |
nodes to upload data at a factor as close as |
of control messages until the time when transmission is actually |
with each lost packet driving the system further and further |
most visible when there is contention between different priorities at |
we really want to create a world in which minor |
to upload data at a factor as close as possible |
control messages until the time when transmission is actually about |
each lost packet driving the system further and further out |
visible when there is contention between different priorities at high |
really want to create a world in which minor computer |
upload data at a factor as close as possible to |
messages until the time when transmission is actually about to |
lost packet driving the system further and further out of |
when there is contention between different priorities at high bandwidth |
want to create a world in which minor computer glitches |
until the time when transmission is actually about to take |
packet driving the system further and further out of sync |
to create a world in which minor computer glitches shut |
the time when transmission is actually about to take place |
driving the system further and further out of sync with |
create a world in which minor computer glitches shut down |
the system further and further out of sync with respect |
a world in which minor computer glitches shut down massive |
MFS implementation The version of MFS described in this paper |
system further and further out of sync with respect to |
world in which minor computer glitches shut down massive critical |
implementation The version of MFS described in this paper is |
average and maximum download factors across the nodes when the |
further and further out of sync with respect to its |
in which minor computer glitches shut down massive critical applications |
The version of MFS described in this paper is implemented |
and maximum download factors across the nodes when the maximum |
An unintended benefit is that the pull architecture slashes buffering |
and further out of sync with respect to its real |
which minor computer glitches shut down massive critical applications and |
version of MFS described in this paper is implemented in |
maximum download factors across the nodes when the maximum upload |
unintended benefit is that the pull architecture slashes buffering and |
minor computer glitches shut down massive critical applications and in |
of MFS described in this paper is implemented in C |
download factors across the nodes when the maximum upload factor |
benefit is that the pull architecture slashes buffering and memory |
computer glitches shut down massive critical applications and in which |
MFS described in this paper is implemented in C and |
IP uses fixed size buffers at receivers to prevent overflows |
is that the pull architecture slashes buffering and memory overheads |
glitches shut down massive critical applications and in which hackers |
factors across the nodes when the maximum upload factor of |
described in this paper is implemented in C and runs |
the sender never pushes more unacknowledged data into the network |
shut down massive critical applications and in which hackers can |
across the nodes when the maximum upload factor of nodes |
in this paper is implemented in C and runs on |
sender never pushes more unacknowledged data into the network than |
down massive critical applications and in which hackers can readily |
the nodes when the maximum upload factor of nodes is |
this paper is implemented in C and runs on FreeBSD |
In QSM each element of a protocol stack acts as |
never pushes more unacknowledged data into the network than the |
massive critical applications and in which hackers can readily disrupt |
nodes when the maximum upload factor of nodes is increased |
QSM each element of a protocol stack acts as a |
pushes more unacknowledged data into the network than the receiver |
critical applications and in which hackers can readily disrupt access |
each element of a protocol stack acts as a feed |
more unacknowledged data into the network than the receiver is |
applications and in which hackers can readily disrupt access to |
Both the client and server have multiple threads to cope |
element of a protocol stack acts as a feed that |
unacknowledged data into the network than the receiver is capable |
and in which hackers can readily disrupt access to banking |
the client and server have multiple threads to cope with |
the discrepancy among the upload factors of individual nodes also |
of a protocol stack acts as a feed that has |
data into the network than the receiver is capable of |
in which hackers can readily disrupt access to banking records |
client and server have multiple threads to cope with simultaneous |
discrepancy among the upload factors of individual nodes also increases |
a protocol stack acts as a feed that has data |
into the network than the receiver is capable of holding |
and server have multiple threads to cope with simultaneous file |
protocol stack acts as a feed that has data to |
server have multiple threads to cope with simultaneous file system |
some nodes participate more actively in dissemination while others end |
Current halfway solutions will tempt developers to embark on a |
have multiple threads to cope with simultaneous file system requests |
stack acts as a feed that has data to send |
nodes participate more actively in dissemination while others end up |
the size of the fluctuating window at the sender is |
halfway solutions will tempt developers to embark on a path |
participate more actively in dissemination while others end up contributing |
size of the fluctuating window at the sender is bounded |
solutions will tempt developers to embark on a path that |
therefore there are two mandatory thread context switches on any |
more actively in dissemination while others end up contributing less |
of the fluctuating window at the sender is bounded by |
will tempt developers to embark on a path that will |
there are two mandatory thread context switches on any message |
the fluctuating window at the sender is bounded by the |
tempt developers to embark on a path that will soon |
are two mandatory thread context switches on any message send |
fluctuating window at the sender is bounded by the size |
developers to embark on a path that will soon lead |
two mandatory thread context switches on any message send or |
we do not want to punish nodes that are willing |
window at the sender is bounded by the size of |
to embark on a path that will soon lead many |
mandatory thread context switches on any message send or receive |
Rather than creating a message and handing it down to |
do not want to punish nodes that are willing to |
at the sender is bounded by the size of the |
embark on a path that will soon lead many of |
thread context switches on any message send or receive operation |
than creating a message and handing it down to the |
not want to punish nodes that are willing to contribute |
the sender is bounded by the size of the buffer |
on a path that will soon lead many of them |
creating a message and handing it down to the sink |
want to punish nodes that are willing to contribute but |
sender is bounded by the size of the buffer at |
a path that will soon lead many of them into |
some subsystems have additional threads to carry out background processing |
to punish nodes that are willing to contribute but cannot |
is bounded by the size of the buffer at the |
a feed registers the intent to send a message with |
path that will soon lead many of them into real |
our experiments were conducted with a default client cache size |
bounded by the size of the buffer at the receiver |
feed registers the intent to send a message with the |
punish nodes that are willing to contribute but cannot do |
that will soon lead many of them into real trouble |
experiments were conducted with a default client cache size of |
registers the intent to send a message with the sink |
nodes that are willing to contribute but cannot do so |
that are willing to contribute but cannot do so because |
are willing to contribute but cannot do so because of |
The message can be created at this time and buffered |
the quantity of inflight unacknowledged data has to be extremely |
willing to contribute but cannot do so because of factors |
and vendors as well as the government have a shared |
message can be created at this time and buffered in |
quantity of inflight unacknowledged data has to be extremely high |
to contribute but cannot do so because of factors such |
vendors as well as the government have a shared obligation |
can be created at this time and buffered in the |
RPCs with priorities MFS RPCs are implemented on top of |
of inflight unacknowledged data has to be extremely high for |
contribute but cannot do so because of factors such as |
as well as the government have a shared obligation to |
be created at this time and buffered in the feed |
with priorities MFS RPCs are implemented on top of ATP |
inflight unacknowledged data has to be extremely high for the |
but cannot do so because of factors such as their |
well as the government have a shared obligation to make |
priorities MFS RPCs are implemented on top of ATP in |
unacknowledged data has to be extremely high for the flow |
but the creation may also be postponed until the time |
cannot do so because of factors such as their physical |
as the government have a shared obligation to make Web |
MFS RPCs are implemented on top of ATP in the |
data has to be extremely high for the flow to |
the creation may also be postponed until the time when |
do so because of factors such as their physical positioning |
the government have a shared obligation to make Web services |
RPCs are implemented on top of ATP in the natural |
has to be extremely high for the flow to saturate |
creation may also be postponed until the time when the |
so because of factors such as their physical positioning in |
government have a shared obligation to make Web services better |
are implemented on top of ATP in the natural way |
to be extremely high for the flow to saturate the |
may also be postponed until the time when the sink |
because of factors such as their physical positioning in the |
be extremely high for the flow to saturate the network |
S Ken Birman is a professor in the Department of |
also be postponed until the time when the sink polls |
Priorities are used to differentiate types of RPCs to improve |
Ken Birman is a professor in the Department of Computer |
of factors such as their physical positioning in the system |
Since the size of the receiver window limits the sending |
be postponed until the time when the sink polls the |
are used to differentiate types of RPCs to improve performance |
Birman is a professor in the Department of Computer Science |
the size of the receiver window limits the sending envelope |
In all our future experiments we set the maximum upload |
postponed until the time when the sink polls the feed |
is a professor in the Department of Computer Science at |
all our future experiments we set the maximum upload factor |
until the time when the sink polls the feed for |
a professor in the Department of Computer Science at Cornell |
or those which would cause an interactive client to block |
our future experiments we set the maximum upload factor to |
the time when the sink polls the feed for messages |
professor in the Department of Computer Science at Cornell University |
IP implementations are in the range of tens of kilobytes |
time when the sink polls the feed for messages to |
when the sink polls the feed for messages to transmit |
and consequently inadequate receiver buffering is the first hurdle faced |
consequently inadequate receiver buffering is the first hurdle faced by |
The sink determines its readiness to send based on a |
inadequate receiver buffering is the first hurdle faced by most |
sink determines its readiness to send based on a control |
receiver buffering is the first hurdle faced by most practical |
determines its readiness to send based on a control policy |
Effect of Opportunistic Behavior Our next goal was to understand |
buffering is the first hurdle faced by most practical deployments |
of Opportunistic Behavior Our next goal was to understand the |
Opportunistic Behavior Our next goal was to understand the expected |
A natural solution is to increase the size of the |
Behavior Our next goal was to understand the expected behavior |
Assigning priorities to RPCs allows MFS to adapt to bandwidth |
natural solution is to increase the size of the receiver |
Our next goal was to understand the expected behavior of |
priorities to RPCs allows MFS to adapt to bandwidth variation |
solution is to increase the size of the receiver buffers |
When the socket at the root of the tree is |
next goal was to understand the expected behavior of correct |
to RPCs allows MFS to adapt to bandwidth variation in |
the socket at the root of the tree is ready |
goal was to understand the expected behavior of correct nodes |
RPCs allows MFS to adapt to bandwidth variation in a |
host may not have the spare memory capacity to buffer |
was to understand the expected behavior of correct nodes under |
socket at the root of the tree is ready for |
allows MFS to adapt to bandwidth variation in a straightforward |
may not have the spare memory capacity to buffer the |
to understand the expected behavior of correct nodes under different |
at the root of the tree is ready for transmission |
MFS to adapt to bandwidth variation in a straightforward way |
not have the spare memory capacity to buffer the entire |
understand the expected behavior of correct nodes under different scenarios |
have the spare memory capacity to buffer the entire bandwidth |
messages will be recursively pulled from the tree of protocol |
the expected behavior of correct nodes under different scenarios where |
will be recursively pulled from the tree of protocol stack |
expected behavior of correct nodes under different scenarios where opportunistic |
be recursively pulled from the tree of protocol stack components |
behavior of correct nodes under different scenarios where opportunistic nodes |
The need for larger buffers is orthogonal to the flow |
of correct nodes under different scenarios where opportunistic nodes compromise |
need for larger buffers is orthogonal to the flow control |
correct nodes under different scenarios where opportunistic nodes compromise the |
for larger buffers is orthogonal to the flow control mechanisms |
nodes under different scenarios where opportunistic nodes compromise the system |
Feeds that no longer have data to send are automatically |
larger buffers is orthogonal to the flow control mechanisms used |
that no longer have data to send are automatically deregistered |
We therefore studied how the download and contribution rates of |
buffers is orthogonal to the flow control mechanisms used within |
therefore studied how the download and contribution rates of correct |
is orthogonal to the flow control mechanisms used within TCP |
studied how the download and contribution rates of correct nodes |
how the download and contribution rates of correct nodes are |
the download and contribution rates of correct nodes are affected |
directory contents write back directory and metadata updates write back |
download and contribution rates of correct nodes are affected under |
contents write back directory and metadata updates write back shared |
and contribution rates of correct nodes are affected under these |
write back directory and metadata updates write back shared files |
contribution rates of correct nodes are affected under these conditions |
back directory and metadata updates write back shared files write |
When we decided to take control over event processing order |
directory and metadata updates write back shared files write back |
Opportunistic nodes may contribute with some data in an attempt |
and metadata updates write back shared files write back unshared |
nodes may contribute with some data in an attempt to |
metadata updates write back shared files write back unshared files |
may contribute with some data in an attempt to disguise |
updates write back shared files write back unshared files prefetch |
grained scheduling eliminated convoy behavior and oscillatory throughput of the |
contribute with some data in an attempt to disguise their |
c data and error correction packets are sent over the |
write back shared files write back unshared files prefetch file |
scheduling eliminated convoy behavior and oscillatory throughput of the sort |
with some data in an attempt to disguise their opportunistic |
data and error correction packets are sent over the channel |
back shared files write back unshared files prefetch file data |
eliminated convoy behavior and oscillatory throughput of the sort that |
some data in an attempt to disguise their opportunistic behavior |
shared files write back unshared files prefetch file data section |
convoy behavior and oscillatory throughput of the sort that can |
redundancy information cannot be generated and sent until all r |
behavior and oscillatory throughput of the sort that can disrupt |
information cannot be generated and sent until all r data |
and oscillatory throughput of the sort that can disrupt reliable |
cannot be generated and sent until all r data packets |
oscillatory throughput of the sort that can disrupt reliable multicast |
be generated and sent until all r data packets are |
throughput of the sort that can disrupt reliable multicast systems |
generated and sent until all r data packets are available |
of the sort that can disrupt reliable multicast systems when |
and sent until all r data packets are available for |
the sort that can disrupt reliable multicast systems when they |
sent until all r data packets are available for sending |
sort that can disrupt reliable multicast systems when they run |
that can disrupt reliable multicast systems when they run at |
can disrupt reliable multicast systems when they run at high |
disrupt reliable multicast systems when they run at high data |
the latency of packet recovery is determined by the rate |
reliable multicast systems when they run at high data rates |
latency of packet recovery is determined by the rate at |
multicast systems when they run at high data rates on |
of packet recovery is determined by the rate at which |
Cornell University Within the community developing the Web Services architecture |
systems when they run at high data rates on a |
level priority mechanism at the IP packet level to further |
packet recovery is determined by the rate at which the |
University Within the community developing the Web Services architecture and |
when they run at high data rates on a large |
priority mechanism at the IP packet level to further reduce |
recovery is determined by the rate at which the sender |
Within the community developing the Web Services architecture and products |
they run at high data rates on a large scale |
mechanism at the IP packet level to further reduce interference |
is determined by the rate at which the sender transmits |
at the IP packet level to further reduce interference between |
determined by the rate at which the sender transmits data |
The last aspect relates to the creation of new messages |
the IP packet level to further reduce interference between writeback |
presents the average and minimum download factors among all correct |
Marketing materials assure us that Web Services are a breakthrough |
IP packet level to further reduce interference between writeback traffic |
Generating error correction packets from less than r data packets |
the average and minimum download factors among all correct nodes |
Readers who have implemented multicast protocols will know that most |
packet level to further reduce interference between writeback traffic and |
error correction packets from less than r data packets at |
average and minimum download factors among all correct nodes under |
who have implemented multicast protocols will know that most existing |
level to further reduce interference between writeback traffic and other |
They portray Web Services as a seamless interconnection layer that |
correction packets from less than r data packets at the |
and minimum download factors among all correct nodes under different |
have implemented multicast protocols will know that most existing systems |
to further reduce interference between writeback traffic and other network |
portray Web Services as a seamless interconnection layer that will |
packets from less than r data packets at the sender |
minimum download factors among all correct nodes under different configurations |
implemented multicast protocols will know that most existing systems are |
further reduce interference between writeback traffic and other network traffic |
Web Services as a seamless interconnection layer that will propel |
from less than r data packets at the sender is |
multicast protocols will know that most existing systems are push |
reduce interference between writeback traffic and other network traffic sent |
Services as a seamless interconnection layer that will propel computer |
less than r data packets at the sender is not |
interference between writeback traffic and other network traffic sent by |
than r data packets at the sender is not a |
and all correct nodes had a maximum upload factor of |
between writeback traffic and other network traffic sent by the |
r data packets at the sender is not a viable |
and lower layers then buffer that message until it can |
writeback traffic and other network traffic sent by the client |
And they use language evocative of marketing for distributed object |
data packets at the sender is not a viable option |
lower layers then buffer that message until it can be |
they use language evocative of marketing for distributed object middleware |
packets at the sender is not a viable option even |
layers then buffer that message until it can be sent |
at the sender is not a viable option even though |
the sender is not a viable option even though the |
sender is not a viable option even though the data |
This makes sense under the assumption that senders often generate |
is not a viable option even though the data rate |
in an essay entitled Web Services are not distributed objects |
makes sense under the assumption that senders often generate bursts |
not a viable option even though the data rate in |
sense under the assumption that senders often generate bursts of |
Werner Vogels argues that Web Services will work well for |
a viable option even though the data rate in this |
under the assumption that senders often generate bursts of packets |
The third column gives the section in which the corresponding |
Vogels argues that Web Services will work well for important |
viable option even though the data rate in this channel |
nodes and increasing percentages of opportunistic nodes in the system |
third column gives the section in which the corresponding RPC |
argues that Web Services will work well for important classes |
option even though the data rate in this channel is |
the communication subsystem can smooth the traffic flow and keep |
column gives the section in which the corresponding RPC types |
that Web Services will work well for important classes of |
even though the data rate in this channel is low |
communication subsystem can smooth the traffic flow and keep the |
gives the section in which the corresponding RPC types are |
Web Services will work well for important classes of applications |
subsystem can smooth the traffic flow and keep the network |
the section in which the corresponding RPC types are described |
can smooth the traffic flow and keep the network interface |
section in which the corresponding RPC types are described in |
smooth the traffic flow and keep the network interface busy |
in which the corresponding RPC types are described in detail |
One consequence is that messages can linger for a while |
consequence is that messages can linger for a while before |
we can observe that the download factors of correct nodes |
is that messages can linger for a while before they |
can observe that the download factors of correct nodes decreases |
H A B C D X X E F G |
that messages can linger for a while before they are |
observe that the download factors of correct nodes decreases since |
A B C D X X E F G H |
messages can linger for a while before they are sent |
update logging is fundamentally unsuitable for use at high bandwidth |
that the download factors of correct nodes decreases since the |
B C D X X E F G H X |
the download factors of correct nodes decreases since the aggregated |
C D X X E F G H X X |
since it imposes a delay on transmitting updates to the |
download factors of correct nodes decreases since the aggregated upload |
D X X E F G H X X A |
it imposes a delay on transmitting updates to the server |
that state may be stale by the time it s |
factors of correct nodes decreases since the aggregated upload capacity |
X X E F G H X X A C |
state may be stale by the time it s sent |
of correct nodes decreases since the aggregated upload capacity in |
Systems using update logging must therefore switch to a synchronous |
Web Services are the most recent in a long series |
X E F G H X X A C B |
correct nodes decreases since the aggregated upload capacity in the |
using update logging must therefore switch to a synchronous writes |
Services are the most recent in a long series of |
E F G H X X A C B E |
nodes decreases since the aggregated upload capacity in the system |
update logging must therefore switch to a synchronous writes when |
are the most recent in a long series of object |
F G H X X A C B E D |
decreases since the aggregated upload capacity in the system becomes |
costs of the domain crossing between the application and QSM |
logging must therefore switch to a synchronous writes when bandwidth |
the most recent in a long series of object oriented |
G H X X A C B E D A |
must therefore switch to a synchronous writes when bandwidth is |
most recent in a long series of object oriented interoperability |
therefore switch to a synchronous writes when bandwidth is high |
recent in a long series of object oriented interoperability platforms |
Our goal is to arrive at a deep understanding of |
goal is to arrive at a deep understanding of the |
The mode switch also changes the semantics of the file |
is to arrive at a deep understanding of the performance |
mode switch also changes the semantics of the file system |
to arrive at a deep understanding of the performance limits |
arrive at a deep understanding of the performance limits of |
and the developers of Coda have noted that undetected mode |
at a deep understanding of the performance limits of QSM |
the developers of Coda have noted that undetected mode changes |
Developers using popular middleware platforms can transform a program object |
a deep understanding of the performance limits of QSM when |
developers of Coda have noted that undetected mode changes can |
using popular middleware platforms can transform a program object into |
deep understanding of the performance limits of QSM when operating |
of Coda have noted that undetected mode changes can surprise |
separate encoding for odd and even packets could be operating |
popular middleware platforms can transform a program object into a |
understanding of the performance limits of QSM when operating at |
Coda have noted that undetected mode changes can surprise the |
encoding for odd and even packets could be operating at |
middleware platforms can transform a program object into a Web |
of the performance limits of QSM when operating at high |
have noted that undetected mode changes can surprise the user |
for odd and even packets could be operating at near |
platforms can transform a program object into a Web Services |
the performance limits of QSM when operating at high data |
noted that undetected mode changes can surprise the user in |
odd and even packets could be operating at near full |
can transform a program object into a Web Services object |
performance limits of QSM when operating at high data rates |
that undetected mode changes can surprise the user in undesirable |
and even packets could be operating at near full capacity |
limits of QSM when operating at high data rates with |
undetected mode changes can surprise the user in undesirable ways |
even packets could be operating at near full capacity with |
of QSM when operating at high data rates with large |
packets could be operating at near full capacity with data |
Major application providers are planning to offer WS interfaces to |
QSM when operating at high data rates with large numbers |
could be operating at near full capacity with data from |
application providers are planning to offer WS interfaces to their |
when operating at high data rates with large numbers of |
be operating at near full capacity with data from other |
providers are planning to offer WS interfaces to their products |
operating at high data rates with large numbers of overlapping |
such as cache inconsistencies arising due to unexpectedly delayed writes |
operating at near full capacity with data from other senders |
at high data rates with large numbers of overlapping groups |
So it makes perfect sense that the marketing community would |
Rather than relying on a modal adaptation scheme incorporating a |
it makes perfect sense that the marketing community would feel |
than relying on a modal adaptation scheme incorporating a transition |
makes perfect sense that the marketing community would feel that |
we are unable to undertake a detailed analysis of oscillatory |
relying on a modal adaptation scheme incorporating a transition to |
perfect sense that the marketing community would feel that finally |
are unable to undertake a detailed analysis of oscillatory phenomena |
on a modal adaptation scheme incorporating a transition to update |
unable to undertake a detailed analysis of oscillatory phenomena in |
a modal adaptation scheme incorporating a transition to update logging |
to undertake a detailed analysis of oscillatory phenomena in this |
modal adaptation scheme incorporating a transition to update logging when |
undertake a detailed analysis of oscillatory phenomena in this paper |
adaptation scheme incorporating a transition to update logging when bandwidth |
is a standard encoding technique used to combat bursty loss |
has an understandable emphasis on facts on the ground and |
scheme incorporating a transition to update logging when bandwidth is |
where error correction packets are generated from alternate disjoint sub |
an understandable emphasis on facts on the ground and the |
incorporating a transition to update logging when bandwidth is low |
understandable emphasis on facts on the ground and the Vogels |
emphasis on facts on the ground and the Vogels essay |
Event prioritization eliminated such problems in the configurations tested by |
on facts on the ground and the Vogels essay reflects |
prioritization eliminated such problems in the configurations tested by our |
facts on the ground and the Vogels essay reflects the |
eliminated such problems in the configurations tested by our experiments |
on the ground and the Vogels essay reflects the realities |
the encoder would create correction packets separately from three disjoint |
when an application performs an operation that changes a file |
the ground and the Vogels essay reflects the realities of |
encoder would create correction packets separately from three disjoint sub |
ground and the Vogels essay reflects the realities of an |
and the Vogels essay reflects the realities of an architecture |
the Vogels essay reflects the realities of an architecture focused |
Vogels essay reflects the realities of an architecture focused at |
essay reflects the realities of an architecture focused at its |
reflects the realities of an architecture focused at its core |
the realities of an architecture focused at its core on |
realities of an architecture focused at its core on using |
the performance of QSM is ultimately limited by overheads associated |
of an architecture focused at its core on using document |
performance of QSM is ultimately limited by overheads associated with |
an architecture focused at its core on using document exchange |
of QSM is ultimately limited by overheads associated with memory |
architecture focused at its core on using document exchange to |
which sends it to the server when there is sufficient |
QSM is ultimately limited by overheads associated with memory management |
focused at its core on using document exchange to access |
sends it to the server when there is sufficient bandwidth |
is ultimately limited by overheads associated with memory management in |
at its core on using document exchange to access backend |
ultimately limited by overheads associated with memory management in the |
its core on using document exchange to access backend servers |
asynchronous writeback therefore only delays updates when there is foreground |
limited by overheads associated with memory management in the managed |
writeback therefore only delays updates when there is foreground traffic |
This core has been extended with such mechanisms as RPC |
by overheads associated with memory management in the managed environment |
core has been extended with such mechanisms as RPC and |
has been extended with such mechanisms as RPC and asynchronous |
been extended with such mechanisms as RPC and asynchronous messaging |
the performance of asynchronous writeback should be comparable to purely |
performance of asynchronous writeback should be comparable to purely synchronous |
of asynchronous writeback should be comparable to purely synchronous writes |
the higher the overheads of the memory management subsystem and |
higher the overheads of the memory management subsystem and the |
the overheads of the memory management subsystem and the more |
overheads of the memory management subsystem and the more CPU |
of the memory management subsystem and the more CPU time |
the memory management subsystem and the more CPU time it |
memory management subsystem and the more CPU time it consumes |
But the primary usage case remains that of a client |
the primary usage case remains that of a client sending |
an implementation without priorities will result in the completion times |
primary usage case remains that of a client sending documents |
Minimum and average download factors across all correct nodes when |
implementation without priorities will result in the completion times for |
usage case remains that of a client sending documents to |
and average download factors across all correct nodes when opportunistic |
without priorities will result in the completion times for all |
and the costs grow linearly in the amount of memory |
case remains that of a client sending documents to a |
average download factors across all correct nodes when opportunistic nodes |
priorities will result in the completion times for all RPCs |
the costs grow linearly in the amount of memory in |
remains that of a client sending documents to a back |
download factors across all correct nodes when opportunistic nodes are |
will result in the completion times for all RPCs increasing |
costs grow linearly in the amount of memory in use |
factors across all correct nodes when opportunistic nodes are present |
result in the completion times for all RPCs increasing uniformly |
Each curve corresponds to a different contribution rate used by |
curve corresponds to a different contribution rate used by opportunistic |
The assumption is that the application can tolerate substantial delay |
corresponds to a different contribution rate used by opportunistic nodes |
assumption is that the application can tolerate substantial delay before |
is that the application can tolerate substantial delay before a |
that the application can tolerate substantial delay before a response |
the application can tolerate substantial delay before a response arrives |
minimizing the memory footprint turns out to be the key |
the memory footprint turns out to be the key to |
and mechanisms capable of introducing delays are scattered throughout the |
memory footprint turns out to be the key to high |
mechanisms capable of introducing delays are scattered throughout the architecture |
footprint turns out to be the key to high performance |
Our design is based on the assumption that when bandwidth |
design is based on the assumption that when bandwidth is |
The more basic assumption is that it all boils down |
is based on the assumption that when bandwidth is low |
more basic assumption is that it all boils down to |
basic assumption is that it all boils down to moving |
assumption is that it all boils down to moving documents |
an assignment of differentiated priorities will improve the response times |
is that it all boils down to moving documents around |
assignment of differentiated priorities will improve the response times for |
that it all boils down to moving documents around whereas |
of differentiated priorities will improve the response times for interactive |
it all boils down to moving documents around whereas the |
Interleaving adds burst tolerance to FEC but exacerbates its sensitivity |
differentiated priorities will improve the response times for interactive tasks |
all boils down to moving documents around whereas the most |
adds burst tolerance to FEC but exacerbates its sensitivity to |
boils down to moving documents around whereas the most basic |
burst tolerance to FEC but exacerbates its sensitivity to sending |
If a task which predominantly performs reads executes in parallel |
down to moving documents around whereas the most basic assumption |
tolerance to FEC but exacerbates its sensitivity to sending rate |
a task which predominantly performs reads executes in parallel to |
to moving documents around whereas the most basic assumption of |
to FEC but exacerbates its sensitivity to sending rate with |
task which predominantly performs reads executes in parallel to a |
moving documents around whereas the most basic assumption of a |
FEC but exacerbates its sensitivity to sending rate with an |
which predominantly performs reads executes in parallel to a task |
documents around whereas the most basic assumption of a distributed |
but exacerbates its sensitivity to sending rate with an interleave |
predominantly performs reads executes in parallel to a task which |
around whereas the most basic assumption of a distributed object |
exacerbates its sensitivity to sending rate with an interleave index |
performs reads executes in parallel to a task which performs |
whereas the most basic assumption of a distributed object system |
its sensitivity to sending rate with an interleave index of |
reads executes in parallel to a task which performs many |
the most basic assumption of a distributed object system is |
sensitivity to sending rate with an interleave index of i |
executes in parallel to a task which performs many writes |
most basic assumption of a distributed object system is that |
to sending rate with an interleave index of i and |
basic assumption of a distributed object system is that the |
sending rate with an interleave index of i and an |
assumption of a distributed object system is that the world |
rate with an interleave index of i and an encoding |
the first task will receive a higher share of the |
of a distributed object system is that the world consists |
with an interleave index of i and an encoding rate |
first task will receive a higher share of the bandwidth |
a distributed object system is that the world consists of |
an interleave index of i and an encoding rate of |
distributed object system is that the world consists of programs |
object system is that the world consists of programs and |
system is that the world consists of programs and data |
many applications have patterns of interactive file access involving both |
applications have patterns of interactive file access involving both reads |
have patterns of interactive file access involving both reads and |
The gist of Vogel s essay is that even with |
patterns of interactive file access involving both reads and writes |
gist of Vogel s essay is that even with all |
of Vogel s essay is that even with all the |
Vogel s essay is that even with all the contemplated |
s essay is that even with all the contemplated extensions |
The dilemma underlying the debate is that the platforms one |
dilemma underlying the debate is that the platforms one uses |
Such an application will have improved read performance when there |
underlying the debate is that the platforms one uses to |
an application will have improved read performance when there is |
sensitive settings rate sensitivity and burst susceptibility are interlinked through |
the debate is that the platforms one uses to create |
but these intervals are sometimes so small that they may |
application will have improved read performance when there is contention |
settings rate sensitivity and burst susceptibility are interlinked through the |
debate is that the platforms one uses to create WScompatible |
these intervals are sometimes so small that they may not |
will have improved read performance when there is contention with |
rate sensitivity and burst susceptibility are interlinked through the tuning |
is that the platforms one uses to create WScompatible objects |
intervals are sometimes so small that they may not always |
have improved read performance when there is contention with other |
sensitivity and burst susceptibility are interlinked through the tuning knobs |
that the platforms one uses to create WScompatible objects impose |
are sometimes so small that they may not always be |
improved read performance when there is contention with other applications |
the platforms one uses to create WScompatible objects impose no |
sometimes so small that they may not always be visible |
platforms one uses to create WScompatible objects impose no such |
one uses to create WScompatible objects impose no such restrictions |
provides tolerance to a burst of up to c i |
This does not match our design goal of having interactive |
tolerance to a burst of up to c i consecutive |
to a burst of up to c i consecutive packets |
NET that warns a user that an intended use of |
that warns a user that an intended use of the |
warns a user that an intended use of the architecture |
a user that an intended use of the architecture may |
the burst tolerance of an FEC code can be changed |
user that an intended use of the architecture may be |
burst tolerance of an FEC code can be changed by |
that an intended use of the architecture may be inappropriate |
tolerance of an FEC code can be changed by modulating |
of an FEC code can be changed by modulating either |
an FEC code can be changed by modulating either the |
used in several existing systems and incorporated in MFS for |
FEC code can be changed by modulating either the c |
much of the excitement reflects the realization that with Web |
in several existing systems and incorporated in MFS for the |
code can be changed by modulating either the c or |
of the excitement reflects the realization that with Web Services |
several existing systems and incorporated in MFS for the purposes |
Minimum and average upload factors across all correct nodes when |
can be changed by modulating either the c or the |
existing systems and incorporated in MFS for the purposes of |
and average upload factors across all correct nodes when opportunistic |
be changed by modulating either the c or the i |
systems and incorporated in MFS for the purposes of comparison |
average upload factors across all correct nodes when opportunistic nodes |
changed by modulating either the c or the i parameters |
upload factors across all correct nodes when opportunistic nodes are |
and it is natural to applaud a widely adopted advance |
factors across all correct nodes when opportunistic nodes are present |
Increasing c enhances burst tolerance at the cost of network |
c enhances burst tolerance at the cost of network and |
Each curve corresponds to a different contribution rate used by |
enhances burst tolerance at the cost of network and encoding |
curve corresponds to a different contribution rate used by opportunistic |
burst tolerance at the cost of network and encoding overhead |
corresponds to a different contribution rate used by opportunistic nodes |
but assign priorities according to some notion of relative importance |
assign priorities according to some notion of relative importance of |
priorities according to some notion of relative importance of processes |
based direct sales systems are turning to the WS architecture |
increasing i trades off recovery latency for better burst tolerance |
direct sales systems are turning to the WS architecture as |
i trades off recovery latency for better burst tolerance without |
sales systems are turning to the WS architecture as a |
existing operating systems and applications generally do not provide this |
trades off recovery latency for better burst tolerance without adding |
Memory Overheads on the Sender We begin by showing that |
systems are turning to the WS architecture as a means |
operating systems and applications generally do not provide this information |
off recovery latency for better burst tolerance without adding overhead |
Overheads on the Sender We begin by showing that memory |
are turning to the WS architecture as a means of |
presents the average and minimum upload factors among all correct |
recovery latency for better burst tolerance without adding overhead as |
The cache manager s writeback thread divides updates into metadata |
turning to the WS architecture as a means of enlarging |
the average and minimum upload factors among all correct nodes |
on the Sender We begin by showing that memory overhead |
latency for better burst tolerance without adding overhead as mentioned |
cache manager s writeback thread divides updates into metadata operations |
to the WS architecture as a means of enlarging their |
the Sender We begin by showing that memory overhead at |
the WS architecture as a means of enlarging their markets |
Sender We begin by showing that memory overhead at the |
the encoder has to wait for more data packets to |
We begin by showing that memory overhead at the sender |
encoder has to wait for more data packets to be |
begin by showing that memory overhead at the sender is |
The two types of operations are queued and replayed to |
has to wait for more data packets to be transmitted |
by showing that memory overhead at the sender is a |
It is interesting to note that the average upload factor |
two types of operations are queued and replayed to the |
party application developers can access their datacenters from a diversity |
showing that memory overhead at the sender is a central |
is interesting to note that the average upload factor among |
to wait for more data packets to be transmitted before |
types of operations are queued and replayed to the server |
application developers can access their datacenters from a diversity of |
that memory overhead at the sender is a central to |
interesting to note that the average upload factor among correct |
wait for more data packets to be transmitted before it |
of operations are queued and replayed to the server separately |
developers can access their datacenters from a diversity of end |
memory overhead at the sender is a central to throughput |
to note that the average upload factor among correct nodes |
for more data packets to be transmitted before it can |
so that a metadata RPC can proceed in parallel with |
note that the average upload factor among correct nodes initially |
more data packets to be transmitted before it can send |
that a metadata RPC can proceed in parallel with a |
that the average upload factor among correct nodes initially increases |
data packets to be transmitted before it can send error |
a metadata RPC can proceed in parallel with a file |
query the fulfillment system to track order status or billing |
packets to be transmitted before it can send error correction |
and then starts falling when the percentage of opportunistic nodes |
metadata RPC can proceed in parallel with a file writeback |
the fulfillment system to track order status or billing data |
to be transmitted before it can send error correction packets |
then starts falling when the percentage of opportunistic nodes increases |
starts falling when the percentage of opportunistic nodes increases significantly |
the sender has more work to do than the receivers |
sender has more work to do than the receivers and |
we say that the update has been committed at the |
has more work to do than the receivers and on |
once the FEC encoding is parameterized with a rate and |
say that the update has been committed at the server |
correct nodes start contributing more to compensate for the lack |
more work to do than the receivers and on our |
the FEC encoding is parameterized with a rate and an |
nodes start contributing more to compensate for the lack of |
work to do than the receivers and on our clusters |
FEC encoding is parameterized with a rate and an interleave |
start contributing more to compensate for the lack of data |
Web Service components will play a critical role in tremendous |
encoding is parameterized with a rate and an interleave to |
contributing more to compensate for the lack of data provided |
Service components will play a critical role in tremendous numbers |
is parameterized with a rate and an interleave to tolerate |
more to compensate for the lack of data provided by |
components will play a critical role in tremendous numbers of |
parameterized with a rate and an interleave to tolerate a |
to compensate for the lack of data provided by a |
will play a critical role in tremendous numbers of end |
with a rate and an interleave to tolerate a certain |
compensate for the lack of data provided by a small |
we report the highest combined send rate that the system |
metadata RPCs from file writes allows remote clients to see |
a rate and an interleave to tolerate a certain burst |
for the lack of data provided by a small percentage |
report the highest combined send rate that the system could |
RPCs from file writes allows remote clients to see statems |
rate and an interleave to tolerate a certain burst length |
the lack of data provided by a small percentage of |
Outages that plague human users of Web browsers don t |
the highest combined send rate that the system could sustain |
and an interleave to tolerate a certain burst length B |
lack of data provided by a small percentage of opportunistic |
that plague human users of Web browsers don t cause |
highest combined send rate that the system could sustain without |
of data provided by a small percentage of opportunistic nodes |
plague human users of Web browsers don t cause much |
combined send rate that the system could sustain without developing |
human users of Web browsers don t cause much harm |
send rate that the system could sustain without developing backlogs |
the system collapses and correct nodes are not able to |
rate that the system could sustain without developing backlogs at |
system collapses and correct nodes are not able to keep |
that the system could sustain without developing backlogs at the |
collapses and correct nodes are not able to keep contributing |
the system could sustain without developing backlogs at the senders |
Another important point to note is that the minimum upload |
important point to note is that the minimum upload factor |
computer pathway buried deep within an application on which an |
point to note is that the minimum upload factor does |
pathway buried deep within an application on which an enterprise |
to note is that the minimum upload factor does not |
buried deep within an application on which an enterprise has |
note is that the minimum upload factor does not follow |
tus changes to files without having to wait for intervening |
deep within an application on which an enterprise has become |
is that the minimum upload factor does not follow a |
changes to files without having to wait for intervening writequirement |
within an application on which an enterprise has become dependent |
that the minimum upload factor does not follow a clearly |
to files without having to wait for intervening writequirement that |
the minimum upload factor does not follow a clearly defined |
files without having to wait for intervening writequirement that processes |
It is too easy to dismiss these concerns by arguing |
minimum upload factor does not follow a clearly defined pattern |
without having to wait for intervening writequirement that processes wait |
is too easy to dismiss these concerns by arguing that |
all losses occurring in bursts of size less than or |
having to wait for intervening writequirement that processes wait for |
making it hard to estimate the minimum contribution of correct |
too easy to dismiss these concerns by arguing that the |
losses occurring in bursts of size less than or equal |
to wait for intervening writequirement that processes wait for writes |
it hard to estimate the minimum contribution of correct nodes |
Running this test again in a profiler reveals that the |
easy to dismiss these concerns by arguing that the Web |
occurring in bursts of size less than or equal to |
hard to estimate the minimum contribution of correct nodes under |
this test again in a profiler reveals that the percentage |
to dismiss these concerns by arguing that the Web is |
in bursts of size less than or equal to B |
to estimate the minimum contribution of correct nodes under compromised |
A similar motivation underlies the cache consisupdate to the server |
test again in a profiler reveals that the percentage of |
dismiss these concerns by arguing that the Web is extremely |
bursts of size less than or equal to B are |
estimate the minimum contribution of correct nodes under compromised scenarios |
similar motivation underlies the cache consisupdate to the server as |
again in a profiler reveals that the percentage of time |
these concerns by arguing that the Web is extremely scalable |
of size less than or equal to B are recovered |
motivation underlies the cache consisupdate to the server as soon |
in a profiler reveals that the percentage of time spent |
concerns by arguing that the Web is extremely scalable and |
size less than or equal to B are recovered with |
underlies the cache consisupdate to the server as soon as |
a profiler reveals that the percentage of time spent in |
by arguing that the Web is extremely scalable and robust |
streaming system against opportunistic behavior is motivated by the graphs |
less than or equal to B are recovered with the |
the cache consisupdate to the server as soon as a |
profiler reveals that the percentage of time spent in QSM |
system against opportunistic behavior is motivated by the graphs presented |
than or equal to B are recovered with the same |
cache consisupdate to the server as soon as a file |
reveals that the percentage of time spent in QSM code |
A human can deal with the many error conditions the |
against opportunistic behavior is motivated by the graphs presented in |
or equal to B are recovered with the same latency |
consisupdate to the server as soon as a file is |
that the percentage of time spent in QSM code is |
human can deal with the many error conditions the Web |
opportunistic behavior is motivated by the graphs presented in the |
equal to B are recovered with the same latency and |
to the server as soon as a file is closed |
the percentage of time spent in QSM code is decreasing |
can deal with the many error conditions the Web exposes |
behavior is motivated by the graphs presented in the previous |
to B are recovered with the same latency and this |
is motivated by the graphs presented in the previous section |
B are recovered with the same latency and this latency |
write contention environments we logs the update and periodically flushes |
are recovered with the same latency and this latency depends |
we propose to employ auditing to ensure that all nodes |
contention environments we logs the update and periodically flushes logged |
recovered with the same latency and this latency depends on |
propose to employ auditing to ensure that all nodes in |
when we take what was once a batch service or |
environments we logs the update and periodically flushes logged updates |
with the same latency and this latency depends on the |
to employ auditing to ensure that all nodes in the |
we take what was once a batch service or a |
we logs the update and periodically flushes logged updates to |
the same latency and this latency depends on the i |
employ auditing to ensure that all nodes in the system |
take what was once a batch service or a Web |
logs the update and periodically flushes logged updates to the |
same latency and this latency depends on the i parameter |
shows that the main culprit behind the increase of overhead |
auditing to ensure that all nodes in the system contribute |
what was once a batch service or a Web site |
the update and periodically flushes logged updates to the describe |
that the main culprit behind the increase of overhead is |
to ensure that all nodes in the system contribute more |
was once a batch service or a Web site and |
update and periodically flushes logged updates to the describe in |
we d like to parameterize the encoding to tolerate a |
the main culprit behind the increase of overhead is a |
ensure that all nodes in the system contribute more than |
once a batch service or a Web site and transform |
and periodically flushes logged updates to the describe in Section |
d like to parameterize the encoding to tolerate a maximum |
main culprit behind the increase of overhead is a Figure |
that all nodes in the system contribute more than a |
a batch service or a Web site and transform it |
like to parameterize the encoding to tolerate a maximum burst |
all nodes in the system contribute more than a particular |
batch service or a Web site and transform it into |
to parameterize the encoding to tolerate a maximum burst length |
nodes in the system contribute more than a particular specified |
service or a Web site and transform it into a |
The percentages of the profiler samples taken from QSM and |
parameterize the encoding to tolerate a maximum burst length and |
to improve read performance and reduce write traffic by aggregat |
in the system contribute more than a particular specified threshold |
or a Web site and transform it into a Web |
percentages of the profiler samples taken from QSM and CLR |
the encoding to tolerate a maximum burst length and then |
a Web site and transform it into a Web Service |
back lies in resolving dependencies between metadata operations ing updates |
of the profiler samples taken from QSM and CLR DLLs |
encoding to tolerate a maximum burst length and then have |
we illustrate the potential benefit from using auditing in a |
lies in resolving dependencies between metadata operations ing updates to |
there is no way to enforce appropriate patterns of use |
to tolerate a maximum burst length and then have recovery |
illustrate the potential benefit from using auditing in a system |
in resolving dependencies between metadata operations ing updates to the |
tolerate a maximum burst length and then have recovery latency |
the potential benefit from using auditing in a system where |
What s to stop a Web client from trying to |
Memory allocation and garbage collection overheads on the sender node |
resolving dependencies between metadata operations ing updates to the same |
a maximum burst length and then have recovery latency depend |
s to stop a Web client from trying to download |
dependencies between metadata operations ing updates to the same file |
maximum burst length and then have recovery latency depend on |
to stop a Web client from trying to download Amazon |
between metadata operations ing updates to the same file in |
burst length and then have recovery latency depend on the |
metadata operations ing updates to the same file in the |
length and then have recovery latency depend on the actual |
operations ing updates to the same file in the log |
and then have recovery latency depend on the actual burstiness |
ing updates to the same file in the log before |
then have recovery latency depend on the actual burstiness of |
no punishment was applied in an attempt to simulate a |
updates to the same file in the log before they |
have recovery latency depend on the actual burstiness of the |
punishment was applied in an attempt to simulate a system |
to the same file in the log before they are |
recovery latency depend on the actual burstiness of the loss |
This configuration is typical of the host environment expected for |
was applied in an attempt to simulate a system with |
one might argue that none of these uses are what |
the same file in the log before they are transmitted |
configuration is typical of the host environment expected for our |
applied in an attempt to simulate a system with no |
might argue that none of these uses are what the |
we would like the encoding to have a constant rate |
is typical of the host environment expected for our target |
in an attempt to simulate a system with no auditing |
argue that none of these uses are what the architecture |
would like the encoding to have a constant rate for |
typical of the host environment expected for our target applications |
that none of these uses are what the architecture is |
like the encoding to have a constant rate for network |
Update logging separates communication with the server into modified and |
none of these uses are what the architecture is intended |
the encoding to have a constant rate for network provisioning |
logging separates communication with the server into modified and closed |
of these uses are what the architecture is intended to |
auditing is enabled and opportunistic nodes start to be expelled |
encoding to have a constant rate for network provisioning and |
of the overhead is the allocation of byte arrays to |
these uses are what the architecture is intended to support |
is enabled and opportunistic nodes start to be expelled from |
and the length of the metadata queue may two distinct |
to have a constant rate for network provisioning and stability |
the overhead is the allocation of byte arrays to send |
enabled and opportunistic nodes start to be expelled from the |
the length of the metadata queue may two distinct streams |
overhead is the allocation of byte arrays to send in |
and opportunistic nodes start to be expelled from the system |
is the allocation of byte arrays to send in the |
an FEC scheme is required where latency of recovery degrades |
opportunistic nodes start to be expelled from the system for |
the allocation of byte arrays to send in the application |
FEC scheme is required where latency of recovery degrades gracefully |
and all be enough to mean that the file update |
nodes start to be expelled from the system for low |
scheme is required where latency of recovery degrades gracefully as |
all be enough to mean that the file update would |
start to be expelled from the system for low contribution |
is required where latency of recovery degrades gracefully as losses |
be enough to mean that the file update would be |
required where latency of recovery degrades gracefully as losses get |
Enterprises fell over themselves in a kind of technology gold |
the minimum upload factor for nodes to stay in the |
enough to mean that the file update would be initiated |
where latency of recovery degrades gracefully as losses get burstier |
fell over themselves in a kind of technology gold rush |
minimum upload factor for nodes to stay in the system |
to mean that the file update would be initiated first |
upload factor for nodes to stay in the system was |
factor for nodes to stay in the system was set |
for nodes to stay in the system was set to |
the total cost of ownership for clientserver systems remains excessively |
These two types of communication are scheduled this case the |
of time is spent exclusively on copying memory in the |
total cost of ownership for clientserver systems remains excessively high |
two types of communication are scheduled this case the file |
time is spent exclusively on copying memory in the CLR |
types of communication are scheduled this case the file update |
the number of system administrators remains roughly proportional to the |
of communication are scheduled this case the file update must |
number of system administrators remains roughly proportional to the size |
communication are scheduled this case the file update must wait |
of system administrators remains roughly proportional to the size of |
system administrators remains roughly proportional to the size of the |
administrators remains roughly proportional to the size of the deployment |
The increase in the memory allocation overhead and the activity |
test activity GC grep compile grep write read compile read |
increase in the memory allocation overhead and the activity of |
activity GC grep compile grep write read compile read write |
a list like these comments might have seemed like an |
in the memory allocation overhead and the activity of the |
GC grep compile grep write read compile read write GW |
list like these comments might have seemed like an indictment |
the memory allocation overhead and the activity of the garbage |
grep compile grep write read compile read write GW RC |
like these comments might have seemed like an indictment of |
memory allocation overhead and the activity of the garbage collector |
compile grep write read compile read write GW RC RW |
these comments might have seemed like an indictment of the |
allocation overhead and the activity of the garbage collector are |
grep write read compile read write GW RC RW synchronous |
comments might have seemed like an indictment of the technology |
overhead and the activity of the garbage collector are caused |
write read compile read write GW RC RW synchronous uniform |
and the activity of the garbage collector are caused by |
read compile read write GW RC RW synchronous uniform priorities |
the activity of the garbage collector are caused by the |
activity of the garbage collector are caused by the increasing |
of the garbage collector are caused by the increasing memory |
the garbage collector are caused by the increasing memory usage |
reflectsan increase of the average number of multicasts pending completion |
and are beginning to understand how to build solutions on |
are beginning to understand how to build solutions on an |
beginning to understand how to build solutions on an Internet |
a copy is kept by the sender for possible loss |
to understand how to build solutions on an Internet scale |
copy is kept by the sender for possible loss recovery |
average and maximum download factors across correct nodes varying along |
If we freeze the sender process and inspect the contents |
we freeze the sender process and inspect the contents of |
freeze the sender process and inspect the contents of the |
the sender process and inspect the contents of the managed |
sender process and inspect the contents of the managed heap |
we find that the number of objects in memory is |
find that the number of objects in memory is more |
but spawned a new generation of technologies based on distributed |
auditing has the potential to improve the quality of streamed |
that the number of objects in memory is more than |
spawned a new generation of technologies based on distributed hash |
has the potential to improve the quality of streamed sessions |
the number of objects in memory is more than twice |
a new generation of technologies based on distributed hash tables |
the potential to improve the quality of streamed sessions significantly |
number of objects in memory is more than twice the |
new generation of technologies based on distributed hash tables and |
of objects in memory is more than twice the number |
generation of technologies based on distributed hash tables and epidemic |
objects in memory is more than twice the number of |
One important concern is that if the specified threshold is |
of technologies based on distributed hash tables and epidemic communication |
in memory is more than twice the number of multicasts |
important concern is that if the specified threshold is too |
technologies based on distributed hash tables and epidemic communication protocols |
memory is more than twice the number of multicasts pending |
concern is that if the specified threshold is too high |
is more than twice the number of multicasts pending acknowledgement |
scalable tools for dealing with enormous numbers of components scattered |
tools for dealing with enormous numbers of components scattered over |
for dealing with enormous numbers of components scattered over a |
dealing with enormous numbers of components scattered over a network |
The growing amount of unacknowledged data is caused by the |
growing amount of unacknowledged data is caused by the increase |
amount of unacknowledged data is caused by the increase of |
of unacknowledged data is caused by the increase of the |
unacknowledged data is caused by the increase of the average |
data is caused by the increase of the average time |
is caused by the increase of the average time to |
the Web Services community decided not to adapt the CORBA |
Auditing components We now give some additional details of the |
caused by the increase of the average time to acknowledge |
Web Services community decided not to adapt the CORBA fault |
components We now give some additional details of the auditing |
by the increase of the average time to acknowledge a |
We now give some additional details of the auditing architecture |
the increase of the average time to acknowledge a message |
it was based on the virtual synchrony model colleagues of |
was based on the virtual synchrony model colleagues of mine |
M AELSTROM D ESIGN AND I MPLEMENTATION We describe the |
based on the virtual synchrony model colleagues of mine and |
AELSTROM D ESIGN AND I MPLEMENTATION We describe the Maelstrom |
collecting accountable information about the download and upload factors of |
on the virtual synchrony model colleagues of mine and I |
This grows because of the increasing time to circulate a |
D ESIGN AND I MPLEMENTATION We describe the Maelstrom appliance |
accountable information about the download and upload factors of individual |
the virtual synchrony model colleagues of mine and I developed |
grows because of the increasing time to circulate a token |
ESIGN AND I MPLEMENTATION We describe the Maelstrom appliance as |
information about the download and upload factors of individual nodes |
virtual synchrony model colleagues of mine and I developed in |
because of the increasing time to circulate a token around |
AND I MPLEMENTATION We describe the Maelstrom appliance as a |
about the download and upload factors of individual nodes in |
synchrony model colleagues of mine and I developed in work |
of the increasing time to circulate a token around the |
I MPLEMENTATION We describe the Maelstrom appliance as a single |
the download and upload factors of individual nodes in the |
model colleagues of mine and I developed in work on |
the increasing time to circulate a token around the region |
MPLEMENTATION We describe the Maelstrom appliance as a single machine |
download and upload factors of individual nodes in the system |
colleagues of mine and I developed in work on the |
increasing time to circulate a token around the region for |
We describe the Maelstrom appliance as a single machine later |
of mine and I developed in work on the Isis |
time to circulate a token around the region for purposes |
mine and I developed in work on the Isis Toolkit |
we will show how more machines can be added to |
to circulate a token around the region for purposes of |
establishing and applying the best threshold at any given time |
will show how more machines can be added to the |
circulate a token around the region for purposes of state |
and applying the best threshold at any given time during |
show how more machines can be added to the appliance |
a token around the region for purposes of state aggregation |
Perhaps the issue is the way the technology was used |
how more machines can be added to the appliance to |
applying the best threshold at any given time during execution |
more machines can be added to the appliance to balance |
machines can be added to the appliance to balance encoding |
The time to acknowledge is only slightly higher than the |
We employ two types of components to perform these two |
can be added to the appliance to balance encoding load |
time to acknowledge is only slightly higher than the expected |
employ two types of components to perform these two roles |
be added to the appliance to balance encoding load and |
added to the appliance to balance encoding load and scale |
Isis runs the New York Stock Exchange quote and trade |
to the appliance to balance encoding load and scale to |
Local auditors are executed on the nodes participating in the |
runs the New York Stock Exchange quote and trade reporting |
the appliance to balance encoding load and scale to multiple |
auditors are executed on the nodes participating in the system |
the New York Stock Exchange quote and trade reporting system |
appliance to balance encoding load and scale to multiple gigabits |
These experiments show that the critical factor determining performance is |
to balance encoding load and scale to multiple gigabits per |
experiments show that the critical factor determining performance is the |
balance encoding load and scale to multiple gigabits per second |
Global auditors are trusted components that run on dedicated external |
show that the critical factor determining performance is the time |
encoding load and scale to multiple gigabits per second of |
auditors are trusted components that run on dedicated external nodes |
that the critical factor determining performance is the time needed |
load and scale to multiple gigabits per second of traffic |
the critical factor determining performance is the time needed for |
There can be just one or a few global auditors |
critical factor determining performance is the time needed for the |
factor determining performance is the time needed for the system |
determining performance is the time needed for the system to |
Basic Mechanism The basic operation of Maelstrom is shown in |
performance is the time needed for the system to aggregate |
Mechanism The basic operation of Maelstrom is shown in Figure |
is the time needed for the system to aggregate state |
Leslie Lamport s Paxos protocol has been used to build |
the time needed for the system to aggregate state over |
Lamport s Paxos protocol has been used to build file |
time needed for the system to aggregate state over regions |
s Paxos protocol has been used to build file systems |
Paxos protocol has been used to build file systems and |
which interacts with other local auditors and has two main |
it intercepts outgoing data packets and routes them to the |
protocol has been used to build file systems and scalable |
they shed light on a mechanism that links latency to |
interacts with other local auditors and has two main roles |
intercepts outgoing data packets and routes them to the destination |
has been used to build file systems and scalable clusters |
shed light on a mechanism that links latency to throughput |
outgoing data packets and routes them to the destination data |
n s local auditor periodically compiles and distributes the history |
data packets and routes them to the destination data center |
via increased memory consumption and the resulting increase in allocation |
s local auditor periodically compiles and distributes the history of |
increased memory consumption and the resulting increase in allocation and |
local auditor periodically compiles and distributes the history of packets |
generating and injecting FEC repair packets into the stream in |
memory consumption and the resulting increase in allocation and garbage |
auditor periodically compiles and distributes the history of packets exchanged |
and injecting FEC repair packets into the stream in their |
consumption and the resulting increase in allocation and garbage collection |
periodically compiles and distributes the history of packets exchanged by |
injecting FEC repair packets into the stream in their wake |
these technologies could take the Web Services architecture to a |
and the resulting increase in allocation and garbage collection overheads |
compiles and distributes the history of packets exchanged by n |
technologies could take the Web Services architecture to a new |
A repair packet consists of a recipe list of data |
could take the Web Services architecture to a new level |
repair packet consists of a recipe list of data packet |
packet consists of a recipe list of data packet identifiers |
it queries the local streaming application running on n for |
consists of a recipe list of data packet identifiers and |
queries the local streaming application running on n for the |
of a recipe list of data packet identifiers and FEC |
the local streaming application running on n for the set |
a recipe list of data packet identifiers and FEC information |
local streaming application running on n for the set of |
recipe list of data packet identifiers and FEC information generated |
streaming application running on n for the set of packets |
list of data packet identifiers and FEC information generated from |
application running on n for the set of packets it |
of data packet identifiers and FEC information generated from these |
running on n for the set of packets it sent |
data packet identifiers and FEC information generated from these packets |
on n for the set of packets it sent and |
n for the set of packets it sent and received |
for the set of packets it sent and received using |
the set of packets it sent and received using the |
set of packets it sent and received using the streaming |
of packets it sent and received using the streaming protocol |
packets it sent and received using the streaming protocol in |
it sent and received using the streaming protocol in the |
It s time for the Web Services community to come |
The size of the XOR is equal to the MTU |
sent and received using the streaming protocol in the most |
s time for the Web Services community to come to |
ve identified could be to reduce the latency of state |
size of the XOR is equal to the MTU of |
and received using the streaming protocol in the most recent |
time for the Web Services community to come to grips |
identified could be to reduce the latency of state aggregation |
of the XOR is equal to the MTU of the |
received using the streaming protocol in the most recent time |
for the Web Services community to come to grips with |
the XOR is equal to the MTU of the data |
using the streaming protocol in the most recent time interval |
the Web Services community to come to grips with the |
XOR is equal to the MTU of the data center |
this might be achieved by using a deeper hierarchy of |
Web Services community to come to grips with the needs |
is equal to the MTU of the data center network |
might be achieved by using a deeper hierarchy of rings |
Services community to come to grips with the needs of |
The local auditor signs and publishes the collected history to |
and by letting tokens in each of these rings circulate |
and to avoid fragmentation of repair packets we require that |
community to come to grips with the needs of their |
local auditor signs and publishes the collected history to an |
by letting tokens in each of these rings circulate independently |
to avoid fragmentation of repair packets we require that the |
to come to grips with the needs of their customer |
auditor signs and publishes the collected history to an assigned |
avoid fragmentation of repair packets we require that the MTU |
come to grips with the needs of their customer base |
signs and publishes the collected history to an assigned subset |
fragmentation of repair packets we require that the MTU of |
and publishes the collected history to an assigned subset of |
of repair packets we require that the MTU of the |
publishes the collected history to an assigned subset of its |
but found that neither can substitute for lowering the latency |
repair packets we require that the MTU of the long |
the collected history to an assigned subset of its neighboring |
found that neither can substitute for lowering the latency of |
collected history to an assigned subset of its neighboring nodes |
that neither can substitute for lowering the latency of the |
neither can substitute for lowering the latency of the recovery |
can substitute for lowering the latency of the recovery state |
since gigabit links very often use Jumbo frames of up |
substitute for lowering the latency of the recovery state aggregation |
This level of indirection is used to prevent nodes from |
gigabit links very often use Jumbo frames of up to |
a solution that tries to do better will probably overreach |
level of indirection is used to prevent nodes from masking |
Our first approach varies the rate of aggregation by increasing |
of indirection is used to prevent nodes from masking their |
But you can t get there if you close your |
first approach varies the rate of aggregation by increasing the |
indirection is used to prevent nodes from masking their real |
you can t get there if you close your eyes |
approach varies the rate of aggregation by increasing the rate |
is used to prevent nodes from masking their real upload |
can t get there if you close your eyes to |
varies the rate of aggregation by increasing the rate at |
used to prevent nodes from masking their real upload and |
t get there if you close your eyes to the |
the rate of aggregation by increasing the rate at which |
to prevent nodes from masking their real upload and download |
get there if you close your eyes to the way |
rate of aggregation by increasing the rate at which tokens |
prevent nodes from masking their real upload and download factors |
there if you close your eyes to the way the |
of aggregation by increasing the rate at which tokens are |
nodes from masking their real upload and download factors by |
if you close your eyes to the way the customers |
aggregation by increasing the rate at which tokens are released |
from masking their real upload and download factors by presenting |
you close your eyes to the way the customers are |
masking their real upload and download factors by presenting different |
the appliance examines incoming repair packets and uses them to |
close your eyes to the way the customers are likely |
their real upload and download factors by presenting different information |
appliance examines incoming repair packets and uses them to recover |
your eyes to the way the customers are likely to |
real upload and download factors by presenting different information to |
examines incoming repair packets and uses them to recover missing |
eyes to the way the customers are likely to use |
upload and download factors by presenting different information to different |
incoming repair packets and uses them to recover missing data |
to the way the customers are likely to use the |
and download factors by presenting different information to different auditors |
repair packets and uses them to recover missing data packets |
the way the customers are likely to use the technology |
n s local auditor periodically audits the published histories of |
Will the Web Services community have the wisdom to tackle |
the data packet is injected transparently into the stream to |
s local auditor periodically audits the published histories of the |
the Web Services community have the wisdom to tackle the |
data packet is injected transparently into the stream to the |
local auditor periodically audits the published histories of the nodes |
Web Services community have the wisdom to tackle the tough |
packet is injected transparently into the stream to the receiving |
auditor periodically audits the published histories of the nodes with |
Services community have the wisdom to tackle the tough issues |
is injected transparently into the stream to the receiving end |
periodically audits the published histories of the nodes with whom |
community have the wisdom to tackle the tough issues before |
audits the published histories of the nodes with whom n |
have the wisdom to tackle the tough issues before circumstances |
the published histories of the nodes with whom n exchanges |
the wisdom to tackle the tough issues before circumstances force |
published histories of the nodes with whom n exchanges packets |
wisdom to tackle the tough issues before circumstances force it |
to tackle the tough issues before circumstances force it upon |
tackle the tough issues before circumstances force it upon them |
and hence it is vital that packets be recovered by |
hence it is vital that packets be recovered by the |
n s local auditor compares these three nodes histories with |
it is vital that packets be recovered by the appliance |
s local auditor compares these three nodes histories with n |
is vital that packets be recovered by the appliance extremely |
local auditor compares these three nodes histories with n s |
vital that packets be recovered by the appliance extremely quickly |
auditor compares these three nodes histories with n s own |
that packets be recovered by the appliance extremely quickly to |
compares these three nodes histories with n s own history |
packets be recovered by the appliance extremely quickly to avoid |
be recovered by the appliance extremely quickly to avoid triggering |
Memory used on sender and the number of multicast requests |
recovered by the appliance extremely quickly to avoid triggering mechanisms |
used on sender and the number of multicast requests in |
by the appliance extremely quickly to avoid triggering mechanisms in |
on sender and the number of multicast requests in progress |
the appliance extremely quickly to avoid triggering mechanisms in commodity |
the amount of data sent by these nodes satisfies the |
and has worked on reliability and scalability issues in distributed |
appliance extremely quickly to avoid triggering mechanisms in commodity stacks |
amount of data sent by these nodes satisfies the defined |
has worked on reliability and scalability issues in distributed systems |
extremely quickly to avoid triggering mechanisms in commodity stacks that |
of data sent by these nodes satisfies the defined minimum |
worked on reliability and scalability issues in distributed systems since |
Token roundtrip time and an average time to acknowledge a |
quickly to avoid triggering mechanisms in commodity stacks that interpret |
data sent by these nodes satisfies the defined minimum threshold |
on reliability and scalability issues in distributed systems since starting |
roundtrip time and an average time to acknowledge a message |
to avoid triggering mechanisms in commodity stacks that interpret out |
sent by these nodes satisfies the defined minimum threshold for |
reliability and scalability issues in distributed systems since starting his |
by these nodes satisfies the defined minimum threshold for the |
and scalability issues in distributed systems since starting his research |
these nodes satisfies the defined minimum threshold for the system |
scalability issues in distributed systems since starting his research career |
He is the author of many articles on the subject |
Our second approach increased the amount of feedback to the |
the set of packets they claim to have sent to |
second approach increased the amount of feedback to the sender |
set of packets they claim to have sent to and |
of packets they claim to have sent to and received |
packets they claim to have sent to and received from |
and Applications will be published by Springer Verlag in Fall |
they claim to have sent to and received from node |
representing the maximum number such that messages with this and |
claim to have sent to and received from node n |
the maximum number such that messages with this and all |
to have sent to and received from node n corresponds |
maximum number such that messages with this and all lower |
have sent to and received from node n corresponds to |
number such that messages with this and all lower numbers |
sent to and received from node n corresponds to the |
such that messages with this and all lower numbers are |
to and received from node n corresponds to the set |
that messages with this and all lower numbers are stable |
and received from node n corresponds to the set of |
messages with this and all lower numbers are stable in |
received from node n corresponds to the set of packets |
with this and all lower numbers are stable in the |
from node n corresponds to the set of packets n |
IP packets as conventional IP packets and routes them through |
this and all lower numbers are stable in the region |
node n corresponds to the set of packets n claims |
packets as conventional IP packets and routes them through without |
n corresponds to the set of packets n claims to |
as conventional IP packets and routes them through without modification |
corresponds to the set of packets n claims to have |
we permit ACK to contain up to k numeric ranges |
to the set of packets n claims to have respectively |
the set of packets n claims to have respectively received |
set of packets n claims to have respectively received from |
of packets n claims to have respectively received from and |
packets n claims to have respectively received from and sent |
n claims to have respectively received from and sent to |
claims to have respectively received from and sent to them |
the local auditor issues an accusation against the node to |
local auditor issues an accusation against the node to a |
auditor issues an accusation against the node to a global |
issues an accusation against the node to a global auditor |
the local auditor is not able to prove the neighbor |
local auditor is not able to prove the neighbor s |
auditor is not able to prove the neighbor s misbehavior |
snooping outgoing and incoming traffic at the data center s |
it instructs its local streaming application to not further exchange |
outgoing and incoming traffic at the data center s edge |
instructs its local streaming application to not further exchange packets |
and incoming traffic at the data center s edge its |
its local streaming application to not further exchange packets with |
incoming traffic at the data center s edge its failure |
local streaming application to not further exchange packets with the |
The system can now cleanup message sequences that have as |
traffic at the data center s edge its failure does |
streaming application to not further exchange packets with the misbehaving |
system can now cleanup message sequences that have as gaps |
at the data center s edge its failure does not |
application to not further exchange packets with the misbehaving neighbor |
the data center s edge its failure does not disrupt |
data center s edge its failure does not disrupt the |
More complex types of checks may also be performed to |
center s edge its failure does not disrupt the flow |
complex types of checks may also be performed to address |
s edge its failure does not disrupt the flow of |
types of checks may also be performed to address other |
edge its failure does not disrupt the flow of packets |
of checks may also be performed to address other types |
its failure does not disrupt the flow of packets between |
checks may also be performed to address other types of |
failure does not disrupt the flow of packets between the |
may also be performed to address other types of Byzantine |
does not disrupt the flow of packets between the two |
also be performed to address other types of Byzantine behavior |
not disrupt the flow of packets between the two data |
disrupt the flow of packets between the two data centers |
and the overall throughput is actually lower because token processing |
the overall throughput is actually lower because token processing becomes |
overall throughput is actually lower because token processing becomes more |
throughput is actually lower because token processing becomes more costly |
terminating connections and sending back ACKs immediately before relaying data |
connections and sending back ACKs immediately before relaying data on |
and sending back ACKs immediately before relaying data on appliance |
Split mode is extremely useful when endhosts have limited buffering |
mode is extremely useful when endhosts have limited buffering capacity |
While the sender can cleanup any portion of the message |
the sender can cleanup any portion of the message sequence |
side appliance to buffer incoming data over the highspeed long |
but the pattern is similar to what we saw earlier |
merely having more cached data is enough to slow them |
having more cached data is enough to slow them down |
inserted into the critical communication path its failure disconnects the |
into the critical communication path its failure disconnects the communication |
the critical communication path its failure disconnects the communication path |
critical communication path its failure disconnects the communication path between |
communication path its failure disconnects the communication path between the |
path its failure disconnects the communication path between the two |
Aware Adaptation Techniques for Mobile File Systems Benjamin Atkin Kenneth |
its failure disconnects the communication path between the two data |
Adaptation Techniques for Mobile File Systems Benjamin Atkin Kenneth P |
failure disconnects the communication path between the two data centers |
edu Abstract therefore react to bandwidth variations in a fine |
write Wireless networks present unusual challenges for mobile file contention |
IP flow control allows it to steal bandwidth from other |
flow control allows it to steal bandwidth from other competing |
control allows it to steal bandwidth from other competing flows |
allows it to steal bandwidth from other competing flows running |
since they are characterised by unpredictable tion time of up |
it to steal bandwidth from other competing flows running without |
they are characterised by unpredictable tion time of up to |
to steal bandwidth from other competing flows running without FEC |
Memory Overheads on the Receiver The reader may doubt that |
steal bandwidth from other competing flows running without FEC in |
Overheads on the Receiver The reader may doubt that memory |
bandwidth from other competing flows running without FEC in the |
on the Receiver The reader may doubt that memory overhead |
from other competing flows running without FEC in the link |
the Receiver The reader may doubt that memory overhead on |
Receiver The reader may doubt that memory overhead on receivers |
The reader may doubt that memory overhead on receivers is |
reader may doubt that memory overhead on receivers is the |
may doubt that memory overhead on receivers is the real |
doubt that memory overhead on receivers is the real issue |
The traditional approach to adapting network communication to these conditions |
traditional approach to adapting network communication to these conditions is |
approach to adapting network communication to these conditions is to |
to adapting network communication to these conditions is to write |
adapting network communication to these conditions is to write back |
Global Auditing There are two ways in which a node |
network communication to these conditions is to write back file |
IP flows is not a primary protocol design goal on |
Auditing There are two ways in which a node could |
communication to these conditions is to write back file updates |
flows is not a primary protocol design goal on over |
There are two ways in which a node could pretend |
to these conditions is to write back file updates asynchronously |
are two ways in which a node could pretend to |
these conditions is to write back file updates asynchronously when |
two ways in which a node could pretend to be |
conditions is to write back file updates asynchronously when bandwidth |
ways in which a node could pretend to be sending |
in which we vary the number of receivers that cache |
is to write back file updates asynchronously when bandwidth is |
in which a node could pretend to be sending more |
We see evidence for this assertion in the routine use |
which we vary the number of receivers that cache a |
which a node could pretend to be sending more or |
see evidence for this assertion in the routine use of |
we vary the number of receivers that cache a copy |
this can lead to underutilisation of bandwidth and inconsistencies between |
evidence for this assertion in the routine use of parallel |
a node could pretend to be sending more or receiving |
vary the number of receivers that cache a copy of |
can lead to underutilisation of bandwidth and inconsistencies between clients |
for this assertion in the routine use of parallel flows |
node could pretend to be sending more or receiving less |
the number of receivers that cache a copy of each |
could pretend to be sending more or receiving less data |
We describe a new Mobile access to shared data is |
number of receivers that cache a copy of each message |
pretend to be sending more or receiving less data than |
describe a new Mobile access to shared data is complicated |
to be sending more or receiving less data than it |
a new Mobile access to shared data is complicated by |
be sending more or receiving less data than it actually |
new Mobile access to shared data is complicated by an |
sending more or receiving less data than it actually does |
Mobile access to shared data is complicated by an unpredictable |
Increasing this value results in a linear increase of memory |
access to shared data is complicated by an unpredictable mobile |
this value results in a linear increase of memory usage |
to shared data is complicated by an unpredictable mobile file |
value results in a linear increase of memory usage on |
shared data is complicated by an unpredictable mobile file system |
results in a linear increase of memory usage on receivers |
n could send a history to p pretending to send |
could send a history to p pretending to send more |
If memory overheads were not a significant issue on half |
send a history to p pretending to send more data |
a history to p pretending to send more data to |
the network or a particular destination of file system performance |
history to p pretending to send more data to q |
network or a particular destination of file system performance as |
to p pretending to send more data to q than |
or a particular destination of file system performance as bandwidth |
both in commercial deployments and by researchers seeking to transfer |
p pretending to send more data to q than it |
a particular destination of file system performance as bandwidth is |
in commercial deployments and by researchers seeking to transfer large |
pretending to send more data to q than it actually |
particular destination of file system performance as bandwidth is reduced |
a slow increase of the number of messages pending ACK |
commercial deployments and by researchers seeking to transfer large amounts |
to send more data to q than it actually did |
slow increase of the number of messages pending ACK on |
deployments and by researchers seeking to transfer large amounts of |
increase of the number of messages pending ACK on the |
and by researchers seeking to transfer large amounts of data |
while it sends a different history to q where it |
of the number of messages pending ACK on the sender |
by researchers seeking to transfer large amounts of data over |
it sends a different history to q where it pretends |
researchers seeking to transfer large amounts of data over high |
sends a different history to q where it pretends to |
Each test consists of two concurrent processes executing different workloads |
a different history to q where it pretends to send |
different history to q where it pretends to send more |
history to q where it pretends to send more data |
to q where it pretends to send more data to |
q where it pretends to send more data to p |
where it pretends to send more data to p than |
The increased activity of the garbage collector and allocation overheads |
it pretends to send more data to p than it |
increased activity of the garbage collector and allocation overheads slow |
pretends to send more data to p than it actually |
activity of the garbage collector and allocation overheads slow the |
to send more data to p than it actually did |
Note that elapsed times for write workloads give the time |
life measurements of available bandwidth between a mobile host on |
of the garbage collector and allocation overheads slow the system |
that elapsed times for write workloads give the time until |
measurements of available bandwidth between a mobile host on a |
n s goal would be to send less data while |
the garbage collector and allocation overheads slow the system down |
elapsed times for write workloads give the time until the |
of available bandwidth between a mobile host on a wireless |
s goal would be to send less data while not |
garbage collector and allocation overheads slow the system down and |
times for write workloads give the time until the process |
available bandwidth between a mobile host on a wireless network |
goal would be to send less data while not being |
collector and allocation overheads slow the system down and processing |
for write workloads give the time until the process running |
would be to send less data while not being caught |
and allocation overheads slow the system down and processing of |
write workloads give the time until the process running the |
be to send less data while not being caught by |
allocation overheads slow the system down and processing of the |
workloads give the time until the process running the workload |
to send less data while not being caught by any |
overheads slow the system down and processing of the incoming |
give the time until the process running the workload finishes |
send less data while not being caught by any of |
factors such as the distance to the base station and |
slow the system down and processing of the incoming packets |
less data while not being caught by any of its |
such as the distance to the base station and local |
the system down and processing of the incoming packets and |
data while not being caught by any of its neighbors |
as the distance to the base station and local interference |
system down and processing of the incoming packets and tokens |
which requires the file update RPC to be cancelled if |
the distance to the base station and local interference cause |
down and processing of the incoming packets and tokens takes |
The process of publishing a node s history to a |
requires the file update RPC to be cancelled if it |
distance to the base station and local interference cause the |
and processing of the incoming packets and tokens takes more |
process of publishing a node s history to a predefined |
the file update RPC to be cancelled if it is |
to the base station and local interference cause the host |
processing of the incoming packets and tokens takes more time |
of publishing a node s history to a predefined set |
file update RPC to be cancelled if it is still |
the base station and local interference cause the host s |
publishing a node s history to a predefined set of |
update RPC to be cancelled if it is still in |
Although the effect is not significant when considering a single |
base station and local interference cause the host s network |
a node s history to a predefined set of neighbors |
RPC to be cancelled if it is still in transmission |
the effect is not significant when considering a single node |
station and local interference cause the host s network card |
node s history to a predefined set of neighbors ensures |
to be cancelled if it is still in transmission when |
effect is not significant when considering a single node in |
and local interference cause the host s network card to |
s history to a predefined set of neighbors ensures that |
be cancelled if it is still in transmission when the |
is not significant when considering a single node in isolation |
local interference cause the host s network card to switch |
history to a predefined set of neighbors ensures that the |
cancelled if it is still in transmission when the remove |
interference cause the host s network card to switch to |
a token must visit all nodes in a region to |
to a predefined set of neighbors ensures that the node |
if it is still in transmission when the remove RPC |
cause the host s network card to switch to higher |
token must visit all nodes in a region to aggregate |
a predefined set of neighbors ensures that the node cannot |
it is still in transmission when the remove RPC is |
must visit all nodes in a region to aggregate the |
predefined set of neighbors ensures that the node cannot send |
is still in transmission when the remove RPC is initiated |
visit all nodes in a region to aggregate the recovery |
set of neighbors ensures that the node cannot send conflicting |
all nodes in a region to aggregate the recovery state |
An update to a file will supersede any previous queued |
Such switching causes available bandwidth to oscillate Distributed file systems |
of neighbors ensures that the node cannot send conflicting histories |
update to a file will supersede any previous queued updates |
switching causes available bandwidth to oscillate Distributed file systems are |
neighbors ensures that the node cannot send conflicting histories to |
causes available bandwidth to oscillate Distributed file systems are a |
QSM is configured so that five nodes in each region |
compiles the entire MFS file system and its RPC library |
ensures that the node cannot send conflicting histories to different |
available bandwidth to oscillate Distributed file systems are a common |
is configured so that five nodes in each region cache |
that the node cannot send conflicting histories to different neighbors |
bandwidth to oscillate Distributed file systems are a common feature |
configured so that five nodes in each region cache each |
the node cannot send conflicting histories to different neighbors undetected |
to oscillate Distributed file systems are a common feature of |
so that five nodes in each region cache each packet |
oscillate Distributed file systems are a common feature of large |
Distributed file systems are a common feature of large com |
A node could also lie about the set of packets |
This workload performs an intensive pattern of reads and writes |
node could also lie about the set of packets sent |
workload performs an intensive pattern of reads and writes files |
could also lie about the set of packets sent to |
performs an intensive pattern of reads and writes files without |
also lie about the set of packets sent to or |
an intensive pattern of reads and writes files without raising |
since they simplify sharing data between sure that clients file |
lie about the set of packets sent to or received |
FEC encoding is simply an XOR of the r data |
they simplify sharing data between sure that clients file operations |
intensive pattern of reads and writes files without raising the |
about the set of packets sent to or received from |
encoding is simply an XOR of the r data packets |
simplify sharing data between sure that clients file operations are |
pattern of reads and writes files without raising the issue |
the set of packets sent to or received from a |
is simply an XOR of the r data packets hence |
sharing data between sure that clients file operations are executed |
of reads and writes files without raising the issue of |
Varying the number of caching replicas per message in a |
data between sure that clients file operations are executed in |
set of packets sent to or received from a particular |
reads and writes files without raising the issue of concurrent |
in layered interleaving each data packet is included in c |
between sure that clients file operations are executed in a |
of packets sent to or received from a particular neighbor |
and writes files without raising the issue of concurrent accesses |
layered interleaving each data packet is included in c XORs |
sure that clients file operations are executed in a timely |
packets sent to or received from a particular neighbor p |
that clients file operations are executed in a timely way |
each of which is generated at different interleaves from the |
of which is generated at different interleaves from the original |
which is generated at different interleaves from the original data |
is generated at different interleaves from the original data stream |
p will be able to identify that the node has |
a form of the oscillating state we encountered in Figure |
will be able to identify that the node has lied |
be able to identify that the node has lied and |
Four combined workloads were then generated by running a foreground |
able to identify that the node has lied and will |
combined workloads were then generated by running a foreground and |
ensures that the c XORs containing a data packet do |
to identify that the node has lied and will therefore |
workloads were then generated by running a foreground and a |
the amount of memory in use at the sender ceases |
that the c XORs containing a data packet do not |
supporting mobile clients requires coping Existing systems tailored to low |
were then generated by running a foreground and a background |
amount of memory in use at the sender ceases to |
identify that the node has lied and will therefore stop |
the c XORs containing a data packet do not have |
then generated by running a foreground and a background workload |
of memory in use at the sender ceases to be |
bandwidth clients differenwith the atypical patterns of connectivity that characterise |
that the node has lied and will therefore stop exchanging |
c XORs containing a data packet do not have any |
generated by running a foreground and a background workload concurrently |
memory in use at the sender ceases to be a |
clients differenwith the atypical patterns of connectivity that characterise them |
the node has lied and will therefore stop exchanging packets |
XORs containing a data packet do not have any other |
in use at the sender ceases to be a good |
node has lied and will therefore stop exchanging packets with |
containing a data packet do not have any other data |
use at the sender ceases to be a good predictor |
has lied and will therefore stop exchanging packets with n |
a data packet do not have any other data packet |
at the sender ceases to be a good predictor of |
data packet do not have any other data packet in |
the sender ceases to be a good predictor of the |
Given that an opportunistic node s goal is to maximize |
packet do not have any other data packet in common |
sender ceases to be a good predictor of the amount |
that an opportunistic node s goal is to maximize its |
ceases to be a good predictor of the amount of |
an opportunistic node s goal is to maximize its utility |
to be a good predictor of the amount of memory |
be a good predictor of the amount of memory in |
a good predictor of the amount of memory in use |
it should have no interest in losing data exchange partners |
good predictor of the amount of memory in use at |
The aim of the experiments was to demonstrate that priorities |
with each XOR generated from r data packets and each |
predictor of the amount of memory in use at receivers |
aim of the experiments was to demonstrate that priorities improve |
each XOR generated from r data packets and each data |
of the experiments was to demonstrate that priorities improve the |
XOR generated from r data packets and each data packet |
violating what turns out to be an implicit requirement of |
the experiments was to demonstrate that priorities improve the performance |
generated from r data packets and each data packet included |
what turns out to be an implicit requirement of our |
Local auditing ensures that correct information is available regarding the |
experiments was to demonstrate that priorities improve the performance of |
from r data packets and each data packet included in |
turns out to be an implicit requirement of our flow |
writes back changes to files asynbandwidth to perform all its |
auditing ensures that correct information is available regarding the set |
was to demonstrate that priorities improve the performance of the |
r data packets and each data packet included in c |
back changes to files asynbandwidth to perform all its file |
ensures that correct information is available regarding the set of |
to demonstrate that priorities improve the performance of the foreground |
data packets and each data packet included in c XORs |
changes to files asynbandwidth to perform all its file operations |
that correct information is available regarding the set of data |
demonstrate that priorities improve the performance of the foreground workloads |
Overheads in a Perturbed System The reader might wonder whether |
to files asynbandwidth to perform all its file operations in |
correct information is available regarding the set of data sent |
in a Perturbed System The reader might wonder whether our |
The four combined workloads were executed on top of MFS |
files asynbandwidth to perform all its file operations in a |
information is available regarding the set of data sent and |
a Perturbed System The reader might wonder whether our results |
four combined workloads were executed on top of MFS configured |
asynbandwidth to perform all its file operations in a timely |
is available regarding the set of data sent and received |
Perturbed System The reader might wonder whether our results would |
combined workloads were executed on top of MFS configured with |
to perform all its file operations in a timely fashion |
available regarding the set of data sent and received by |
System The reader might wonder whether our results would be |
workloads were executed on top of MFS configured with either |
regarding the set of data sent and received by any |
The reader might wonder whether our results would be different |
were executed on top of MFS configured with either synchronous |
the set of data sent and received by any node |
reader might wonder whether our results would be different if |
executed on top of MFS configured with either synchronous writes |
might wonder whether our results would be different if the |
assigns lower priorities to asynMobile file systems typically assume that |
and allows nodes to monitor each other s contribution rates |
wonder whether our results would be different if the system |
lower priorities to asynMobile file systems typically assume that a |
The update logging mechanism was configured to delay flushing an |
whether our results would be different if the system experienced |
priorities to asynMobile file systems typically assume that a client |
update logging mechanism was configured to delay flushing an update |
our results would be different if the system experienced high |
to asynMobile file systems typically assume that a client is |
logging mechanism was configured to delay flushing an update for |
standard FEC schemes can be made resistant to a certain |
results would be different if the system experienced high loss |
asynMobile file systems typically assume that a client is strongly |
mechanism was configured to delay flushing an update for at |
FEC schemes can be made resistant to a certain loss |
would be different if the system experienced high loss rates |
Global Auditors Global auditors are trusted components with global membership |
was configured to delay flushing an update for at least |
schemes can be made resistant to a certain loss burst |
chronous operations at the IP level to reduce interference with |
be different if the system experienced high loss rates or |
Auditors Global auditors are trusted components with global membership knowledge |
configured to delay flushing an update for at least a |
can be made resistant to a certain loss burst length |
operations at the IP level to reduce interference with connected |
different if the system experienced high loss rates or was |
to delay flushing an update for at least a second |
be made resistant to a certain loss burst length at |
who interact with one another and with the local auditors |
at the IP level to reduce interference with connected like |
if the system experienced high loss rates or was otherwise |
made resistant to a certain loss burst length at the |
Every experiment was repeated ten times at each of five |
the IP level to reduce interference with connected like a |
the system experienced high loss rates or was otherwise perturbed |
resistant to a certain loss burst length at the cost |
experiment was repeated ten times at each of five possible |
IP level to reduce interference with connected like a desktop |
to a certain loss burst length at the cost of |
was repeated ten times at each of five possible bandwidth |
level to reduce interference with connected like a desktop host |
a certain loss burst length at the cost of increased |
we performed an experiment in which one of the receiver |
repeated ten times at each of five possible bandwidth values |
Global auditors periodically sample the state of the system by |
certain loss burst length at the cost of increased recovery |
performed an experiment in which one of the receiver nodes |
auditors periodically sample the state of the system by querying |
loss burst length at the cost of increased recovery latency |
shows the time taken for each workload at a bandwidth |
an experiment in which one of the receiver nodes experiences |
periodically sample the state of the system by querying local |
burst length at the cost of increased recovery latency for |
the time taken for each workload at a bandwidth of |
experiment in which one of the receiver nodes experiences a |
sample the state of the system by querying local auditors |
length at the cost of increased recovery latency for all |
in which one of the receiver nodes experiences a periodic |
at the cost of increased recovery latency for all lost |
adaptation by deferred transmission of file upwidth lies between these |
the cost of increased recovery latency for all lost packets |
by deferred transmission of file upwidth lies between these extremes |
and on this basis compute the minimum upload contribution threshold |
demonstrate the benefit of priorities when there is high contention |
assuming weak connectivity dates has the disadvantage of increasing the |
the benefit of priorities when there is high contention between |
Different strategies may be employed for choosing the best possible |
weak connectivity dates has the disadvantage of increasing the delay |
benefit of priorities when there is high contention between high |
layered interleaving provides graceful degradation in the face of bursty |
strategies may be employed for choosing the best possible threshold |
connectivity dates has the disadvantage of increasing the delay before |
interleaving provides graceful degradation in the face of bursty loss |
dates has the disadvantage of increasing the delay before upcan |
provides graceful degradation in the face of bursty loss for |
has the disadvantage of increasing the delay before upcan be |
adding priorities decreases the time required for the foreground workload |
graceful degradation in the face of bursty loss for constant |
the disadvantage of increasing the delay before upcan be too |
priorities decreases the time required for the foreground workload to |
degradation in the face of bursty loss for constant encoding |
disadvantage of increasing the delay before upcan be too conservative |
decreases the time required for the foreground workload to execute |
in the face of bursty loss for constant encoding overhead |
Global auditors are also responsible for verifying accusations issued by |
the face of bursty loss for constant encoding overhead singleton |
since it delays sending updates to the dates are applied |
auditors are also responsible for verifying accusations issued by local |
face of bursty loss for constant encoding overhead singleton random |
it delays sending updates to the dates are applied at |
are also responsible for verifying accusations issued by local auditors |
of bursty loss for constant encoding overhead singleton random losses |
delays sending updates to the dates are applied at the |
also responsible for verifying accusations issued by local auditors against |
bursty loss for constant encoding overhead singleton random losses are |
where the foreground workload generates heavy contention by fetching a |
sending updates to the dates are applied at the file |
responsible for verifying accusations issued by local auditors against particular |
loss for constant encoding overhead singleton random losses are recovered |
the foreground workload generates heavy contention by fetching a large |
updates to the dates are applied at the file server |
for verifying accusations issued by local auditors against particular nodes |
for constant encoding overhead singleton random losses are recovered as |
foreground workload generates heavy contention by fetching a large volume |
and therefore reduces the deserver in order to aggregate modifications |
constant encoding overhead singleton random losses are recovered as quickly |
workload generates heavy contention by fetching a large volume of |
encoding overhead singleton random losses are recovered as quickly as |
generates heavy contention by fetching a large volume of data |
overhead singleton random losses are recovered as quickly as possible |
Validation involves verifying that the accused node s history indeed |
For its own This paper examines the effectiveness of MAFS |
The greatest benefits are observable for the combination of asynchronous |
involves verifying that the accused node s history indeed indicates |
greatest benefits are observable for the combination of asynchronous writes |
verifying that the accused node s history indeed indicates that |
benefits are observable for the combination of asynchronous writes with |
and each successive layer of XORs generated at a higher |
that the accused node s history indeed indicates that the |
are observable for the combination of asynchronous writes with priorities |
bandwidth client may decide to delay sending a file system |
each successive layer of XORs generated at a higher interleave |
the accused node s history indeed indicates that the node |
client may decide to delay sending a file system that |
since here the performance of the background workload can also |
successive layer of XORs generated at a higher interleave catches |
accused node s history indeed indicates that the node is |
may decide to delay sending a file system that propagates |
It doesn t appear to be correlated to the amount |
here the performance of the background workload can also improve |
layer of XORs generated at a higher interleave catches larger |
node s history indeed indicates that the node is sending |
decide to delay sending a file system that propagates file |
doesn t appear to be correlated to the amount of |
the performance of the background workload can also improve by |
of XORs generated at a higher interleave catches larger bursts |
s history indeed indicates that the node is sending less |
to delay sending a file system that propagates file modifications |
t appear to be correlated to the amount of loss |
performance of the background workload can also improve by not |
XORs generated at a higher interleave catches larger bursts missed |
history indeed indicates that the node is sending less data |
delay sending a file system that propagates file modifications asynchronously |
of the background workload can also improve by not having |
generated at a higher interleave catches larger bursts missed by |
indeed indicates that the node is sending less data than |
sending a file system that propagates file modifications asynchronously file |
the background workload can also improve by not having to |
at a higher interleave catches larger bursts missed by the |
indicates that the node is sending less data than the |
a file system that propagates file modifications asynchronously file s |
background workload can also improve by not having to wait |
a higher interleave catches larger bursts missed by the previous |
that the node is sending less data than the current |
file system that propagates file modifications asynchronously file s update |
workload can also improve by not having to wait for |
higher interleave catches larger bursts missed by the previous layer |
the node is sending less data than the current threshold |
system that propagates file modifications asynchronously file s update to |
The implementation of this algorithm is simple and shown in |
can also improve by not having to wait for its |
that propagates file modifications asynchronously file s update to the |
implementation of this algorithm is simple and shown in Figure |
Expurging a node involves informing the nodes immediate neighbors of |
also improve by not having to wait for its writes |
propagates file modifications asynchronously file s update to the file |
a node involves informing the nodes immediate neighbors of its |
improve by not having to wait for its writes to |
file modifications asynchronously file s update to the file server |
performance does not appear to be directly correlated to the |
node involves informing the nodes immediate neighbors of its status |
by not having to wait for its writes to be |
a set of repair bins is maintained for each layer |
does not appear to be directly correlated to the observed |
but this decision may also affect at all bandwidth levels |
involves informing the nodes immediate neighbors of its status and |
not having to wait for its writes to be committed |
not appear to be directly correlated to the observed packet |
informing the nodes immediate neighbors of its status and forcing |
A repair bin consists of a partially constructed repair packet |
having to wait for its writes to be committed at |
appear to be directly correlated to the observed packet loss |
MAFS other clients that would like to read the file |
the nodes immediate neighbors of its status and forcing the |
to wait for its writes to be committed at the |
an XOR and the recipe list of identifiers of data |
nodes immediate neighbors of its status and forcing the removal |
wait for its writes to be committed at the server |
Optimistic concuruses RPC priorities to reduce interference between read and |
throughput is uncorrelated with memory use both on the perturbed |
XOR and the recipe list of identifiers of data packets |
immediate neighbors of its status and forcing the removal of |
concuruses RPC priorities to reduce interference between read and rency |
is uncorrelated with memory use both on the perturbed receiver |
and the recipe list of identifiers of data packets that |
neighbors of its status and forcing the removal of the |
RPC priorities to reduce interference between read and rency control |
the recipe list of identifiers of data packets that compose |
of its status and forcing the removal of the node |
In addition to comparing MFS with and without prioritised RPCs |
priorities to reduce interference between read and rency control and |
recipe list of identifiers of data packets that compose the |
its status and forcing the removal of the node from |
to reduce interference between read and rency control and reconciliation |
we also investigate the performance impact of replacing synchronous RPCs |
list of identifiers of data packets that compose the XOR |
status and forcing the removal of the node from the |
reduce interference between read and rency control and reconciliation of |
also investigate the performance impact of replacing synchronous RPCs for |
and forcing the removal of the node from the overlay |
Each intercepted data packet is added to each layer where |
interference between read and rency control and reconciliation of conflicting |
investigate the performance impact of replacing synchronous RPCs for file |
forcing the removal of the node from the overlay mesh |
a consequence of the cooperative caching policy described in Section |
intercepted data packet is added to each layer where adding |
between read and rency control and reconciliation of conflicting updates |
the performance impact of replacing synchronous RPCs for file updates |
data packet is added to each layer where adding to |
The number of global auditors may vary according to different |
read and rency control and reconciliation of conflicting updates are |
performance impact of replacing synchronous RPCs for file updates with |
packet is added to each layer where adding to a |
number of global auditors may vary according to different parameters |
and rency control and reconciliation of conflicting updates are typwrite |
impact of replacing synchronous RPCs for file updates with asynchronous |
is added to each layer where adding to a layer |
rency control and reconciliation of conflicting updates are typwrite traffic |
of replacing synchronous RPCs for file updates with asynchronous writeback |
added to each layer where adding to a layer simply |
We conclude that the drop in performance in these scenarios |
control and reconciliation of conflicting updates are typwrite traffic at |
The use of more global auditors distributes the load of |
to each layer where adding to a layer simply means |
conclude that the drop in performance in these scenarios can |
The performance of these alternatives is compared in a set |
and reconciliation of conflicting updates are typwrite traffic at low |
use of more global auditors distributes the load of sampling |
each layer where adding to a layer simply means choosing |
that the drop in performance in these scenarios can t |
performance of these alternatives is compared in a set of |
reconciliation of conflicting updates are typwrite traffic at low bandwidth |
of more global auditors distributes the load of sampling and |
layer where adding to a layer simply means choosing a |
the drop in performance in these scenarios can t be |
of these alternatives is compared in a set of microbenchmarks |
more global auditors distributes the load of sampling and improves |
To ensure that file modifications ically used to resolve inconsistencies |
where adding to a layer simply means choosing a repair |
drop in performance in these scenarios can t be explained |
global auditors distributes the load of sampling and improves efficiency |
and with workloads gathered from Windows NT file system traces |
adding to a layer simply means choosing a repair bin |
in performance in these scenarios can t be explained by |
auditors distributes the load of sampling and improves efficiency in |
to a layer simply means choosing a repair bin from |
performance in these scenarios can t be explained by correlation |
distributes the load of sampling and improves efficiency in reacting |
a layer simply means choosing a repair bin from the |
in these scenarios can t be explained by correlation with |
the load of sampling and improves efficiency in reacting to |
layer simply means choosing a repair bin from the layer |
When bandwidth are rapidly propagated to the clients that need |
these scenarios can t be explained by correlation with CPU |
load of sampling and improves efficiency in reacting to accusations |
simply means choosing a repair bin from the layer s |
The client machine makes use of the Dummynet trafficshaping module |
bandwidth are rapidly propagated to the clients that need them |
scenarios can t be explained by correlation with CPU activity |
of sampling and improves efficiency in reacting to accusations against |
means choosing a repair bin from the layer s set |
client machine makes use of the Dummynet trafficshaping module in |
sampling and improves efficiency in reacting to accusations against nodes |
this can be an acceptable price to pay for the |
machine makes use of the Dummynet trafficshaping module in FreeBSD |
but that it does appear correlated to slower cleanup and |
Global auditors are also perfect candidates to perform membership tasks |
and adding the data packet s header to the recipe |
makes use of the Dummynet trafficshaping module in FreeBSD to |
can be an acceptable price to pay for the abilalso |
that it does appear correlated to slower cleanup and the |
auditors are also perfect candidates to perform membership tasks such |
adding the data packet s header to the recipe list |
use of the Dummynet trafficshaping module in FreeBSD to limit |
be an acceptable price to pay for the abilalso incorporates |
A counter is incremented as each data packet arrives at |
are also perfect candidates to perform membership tasks such as |
of the Dummynet trafficshaping module in FreeBSD to limit its |
it does appear correlated to slower cleanup and the resulting |
an acceptable price to pay for the abilalso incorporates a |
counter is incremented as each data packet arrives at the |
also perfect candidates to perform membership tasks such as acting |
the Dummynet trafficshaping module in FreeBSD to limit its incoming |
does appear correlated to slower cleanup and the resulting memory |
acceptable price to pay for the abilalso incorporates a new |
is incremented as each data packet arrives at the appliance |
perfect candidates to perform membership tasks such as acting as |
The effect is much stronger than in the undisturbed experiments |
and choosing the repair bin from the layer s set |
Dummynet trafficshaping module in FreeBSD to limit its incoming and |
the number of pending messages starts at a higher level |
price to pay for the abilalso incorporates a new invalidation |
choosing the repair bin from the layer s set is |
candidates to perform membership tasks such as acting as entry |
trafficshaping module in FreeBSD to limit its incoming and outgoing |
the repair bin from the layer s set is done |
to perform membership tasks such as acting as entry points |
based update propagation ity to continue accessing a file server |
module in FreeBSD to limit its incoming and outgoing bandwidth |
repair bin from the layer s set is done by |
perform membership tasks such as acting as entry points to |
bin from the layer s set is done by taking |
The experiments we conduct in this section have a constant |
membership tasks such as acting as entry points to the |
from the layer s set is done by taking the |
experiments we conduct in this section have a constant bandwidth |
tasks such as acting as entry points to the P |
the layer s set is done by taking the modulo |
we conduct in this section have a constant bandwidth over |
layer s set is done by taking the modulo of |
conduct in this section have a constant bandwidth over the |
s set is done by taking the modulo of the |
in this section have a constant bandwidth over the duration |
set is done by taking the modulo of the counter |
this section have a constant bandwidth over the duration of |
since they are required to have full membership knowledge of |
and can like file systems therefore switch between a low |
is done by taking the modulo of the counter with |
section have a constant bandwidth over the duration of the |
they are required to have full membership knowledge of the |
done by taking the modulo of the counter with the |
have a constant bandwidth over the duration of the experiment |
are required to have full membership knowledge of the system |
by taking the modulo of the counter with the number |
required to have full membership knowledge of the system for |
but we analyse the performance of MFS when the bandwidth |
acThe authors were supported in part by DARPA under AFRL |
to have full membership knowledge of the system for performing |
taking the modulo of the counter with the number of |
we analyse the performance of MFS when the bandwidth varies |
authors were supported in part by DARPA under AFRL grant |
have full membership knowledge of the system for performing their |
the modulo of the counter with the number of bins |
analyse the performance of MFS when the bandwidth varies over |
were supported in part by DARPA under AFRL grant RADC |
full membership knowledge of the system for performing their auditing |
modulo of the counter with the number of bins in |
the performance of MFS when the bandwidth varies over the |
supported in part by DARPA under AFRL grant RADC cording |
membership knowledge of the system for performing their auditing roles |
of the counter with the number of bins in each |
performance of MFS when the bandwidth varies over the course |
in part by DARPA under AFRL grant RADC cording to |
the counter with the number of bins in each layer |
of MFS when the bandwidth varies over the course of |
part by DARPA under AFRL grant RADC cording to the |
MFS when the bandwidth varies over the course of an |
Global auditing monitors the global health of the system to |
by DARPA under AFRL grant RADC cording to the available |
when the bandwidth varies over the course of an experiment |
auditing monitors the global health of the system to identify |
DARPA under AFRL grant RADC cording to the available bandwidth |
the bandwidth varies over the course of an experiment in |
monitors the global health of the system to identify the |
bandwidth varies over the course of an experiment in Section |
the global health of the system to identify the best |
global health of the system to identify the best value |
health of the system to identify the best value for |
When a repair bin fills up its recipe list contains |
of the system to identify the best value for the |
a repair bin fills up its recipe list contains r |
Although it would be hard to precisely measure these delays |
the system to identify the best value for the minimum |
repair bin fills up its recipe list contains r data |
system to identify the best value for the minimum upload |
measuring alarm delays sheds light on the magnitude of the |
Microbenchmarks The first set of experiments compares different MFS configurations |
bin fills up its recipe list contains r data packets |
to identify the best value for the minimum upload threshold |
alarm delays sheds light on the magnitude of the problem |
The first set of experiments compares different MFS configurations for |
fills up its recipe list contains r data packets it |
Recall that our timesharing policy assigns quanta to different types |
first set of experiments compares different MFS configurations for specific |
identify the best value for the minimum upload threshold at |
up its recipe list contains r data packets it fires |
that our timesharing policy assigns quanta to different types of |
set of experiments compares different MFS configurations for specific types |
the best value for the minimum upload threshold at any |
our timesharing policy assigns quanta to different types of events |
a repair packet is generated consisting of the XOR and |
of experiments compares different MFS configurations for specific types of |
best value for the minimum upload threshold at any time |
repair packet is generated consisting of the XOR and the |
experiments compares different MFS configurations for specific types of contention |
value for the minimum upload threshold at any time during |
will cause QSM to use a larger fraction of its |
packet is generated consisting of the XOR and the recipe |
for the minimum upload threshold at any time during a |
cause QSM to use a larger fraction of its I |
is generated consisting of the XOR and the recipe list |
the minimum upload threshold at any time during a streaming |
generated consisting of the XOR and the recipe list and |
minimum upload threshold at any time during a streaming session |
consisting of the XOR and the recipe list and is |
of the XOR and the recipe list and is scheduled |
the XOR and the recipe list and is scheduled for |
This effect is magnified each time QSM is preempted by |
XOR and the recipe list and is scheduled for sending |
effect is magnified each time QSM is preempted by other |
is magnified each time QSM is preempted by other processes |
magnified each time QSM is preempted by other processes or |
each time QSM is preempted by other processes or by |
time QSM is preempted by other processes or by its |
QSM is preempted by other processes or by its own |
is preempted by other processes or by its own garbage |
preempted by other processes or by its own garbage collector |
a low threshold may not be sufficient to identify opportunistic |
low threshold may not be sufficient to identify opportunistic nodes |
if all the data packets contained in the repair s |
variations in bandwidth can occur without the user s with |
all the data packets contained in the repair s recipe |
in bandwidth can occur without the user s with additional |
the data packets contained in the repair s recipe list |
bandwidth can occur without the user s with additional support |
We considered different strategies for the choice of the minimum |
data packets contained in the repair s recipe list have |
can occur without the user s with additional support from |
considered different strategies for the choice of the minimum contribution |
packets contained in the repair s recipe list have been |
MB files from the local file system into the MFS |
occur without the user s with additional support from Microsoft |
s intervals are indeed much larger in the perturbed experiments |
different strategies for the choice of the minimum contribution t |
contained in the repair s recipe list have been received |
files from the local file system into the MFS file |
without the user s with additional support from Microsoft Research |
strategies for the choice of the minimum contribution t hreshold |
in the repair s recipe list have been received successfully |
from the local file system into the MFS file system |
the user s with additional support from Microsoft Research and |
for the choice of the minimum contribution t hreshold used |
user s with additional support from Microsoft Research and from |
the choice of the minimum contribution t hreshold used for |
If the repair s recipe list contains a single missing |
s with additional support from Microsoft Research and from the |
choice of the minimum contribution t hreshold used for identifying |
the repair s recipe list contains a single missing data |
with additional support from Microsoft Research and from the Intel |
of the minimum contribution t hreshold used for identifying misbehaving |
repair s recipe list contains a single missing data packet |
The maximum delay measured on receivers in the perturbed runs |
additional support from Microsoft Research and from the Intel Corporation |
the minimum contribution t hreshold used for identifying misbehaving nodes |
maximum delay measured on receivers in the perturbed runs is |
the final contents depend on the client that closed it |
final contents depend on the client that closed it last |
that is renewed each time the client communicates with the |
is renewed each time the client communicates with the file |
renewed each time the client communicates with the file server |
the problem could be alleviated by making our priority scheduling |
problem could be alleviated by making our priority scheduling more |
could be alleviated by making our priority scheduling more fine |
One downside of using a fixed threshold is that opportunistic |
downside of using a fixed threshold is that opportunistic nodes |
of using a fixed threshold is that opportunistic nodes that |
using a fixed threshold is that opportunistic nodes that learn |
a fixed threshold is that opportunistic nodes that learn the |
or by assigning priorities to feeds in the sending stack |
fixed threshold is that opportunistic nodes that learn the threshold |
threshold is that opportunistic nodes that learn the threshold can |
is that opportunistic nodes that learn the threshold can simply |
that opportunistic nodes that learn the threshold can simply contribute |
opportunistic nodes that learn the threshold can simply contribute at |
nodes that learn the threshold can simply contribute at the |
that learn the threshold can simply contribute at the lowest |
learn the threshold can simply contribute at the lowest possible |
the threshold can simply contribute at the lowest possible upload |
Token roundtrip time and the time to recover in the |
threshold can simply contribute at the lowest possible upload factor |
Token roundtrip time and the time to recover in the |
it is clear that such a stretagy may disrupt the |
is clear that such a stretagy may disrupt the streaming |
clear that such a stretagy may disrupt the streaming session |
It is worth noting that the doubled token roundtrip time |
can t be accounted for by the increase in memory |
t be accounted for by the increase in memory overhead |
be accounted for by the increase in memory overhead or |
accounted for by the increase in memory overhead or CPU |
for by the increase in memory overhead or CPU activity |
by the increase in memory overhead or CPU activity on |
the increase in memory overhead or CPU activity on the |
increase in memory overhead or CPU activity on the receivers |
as was the case in experiments where we varied the |
was the case in experiments where we varied the replication |
the case in experiments where we varied the replication factor |
The forwarded messages tend to get ahead of the token |
Adaptive RPC is based on our earlier work in modes |
RPC is based on our earlier work in modes can |
is based on our earlier work in modes can be |
based on our earlier work in modes can be ill |
Global auditors sample the system to identify the average download |
auditors sample the system to identify the average download factor |
but insufficient for a client to ignore it a typical |
insufficient for a client to ignore it a typical RPC |
for a client to ignore it a typical RPC system |
a client to ignore it a typical RPC system in |
client to ignore it a typical RPC system in allowing |
to ignore it a typical RPC system in allowing applications |
ignore it a typical RPC system in allowing applications to |
combining the XOR in the repair with the other successfully |
it a typical RPC system in allowing applications to control |
the XOR in the repair with the other successfully received |
the threshold may be reduced back to its initial value |
a typical RPC system in allowing applications to control how |
XOR in the repair with the other successfully received data |
typical RPC system in allowing applications to control how concurrent |
in the repair with the other successfully received data packets |
Loaded System So far the evaluation has focused on scenarios |
This stepwise approach allows the system to catch opportunistic nodes |
RPC system in allowing applications to control how concurrent RPCs |
System So far the evaluation has focused on scenarios where |
stepwise approach allows the system to catch opportunistic nodes in |
system in allowing applications to control how concurrent RPCs are |
So far the evaluation has focused on scenarios where the |
it cannot be used immediately for recovery it is instead |
approach allows the system to catch opportunistic nodes in case |
in allowing applications to control how concurrent RPCs are transmitted |
far the evaluation has focused on scenarios where the system |
cannot be used immediately for recovery it is instead stored |
allows the system to catch opportunistic nodes in case their |
the evaluation has focused on scenarios where the system was |
be used immediately for recovery it is instead stored in |
and special handling for failwhen deciding what to send over |
the system to catch opportunistic nodes in case their presence |
evaluation has focused on scenarios where the system was heavily |
used immediately for recovery it is instead stored in a |
special handling for failwhen deciding what to send over the |
system to catch opportunistic nodes in case their presence starts |
has focused on scenarios where the system was heavily loaded |
immediately for recovery it is instead stored in a table |
handling for failwhen deciding what to send over the network |
to catch opportunistic nodes in case their presence starts affecting |
for recovery it is instead stored in a table that |
catch opportunistic nodes in case their presence starts affecting the |
recovery it is instead stored in a table that maps |
Adaptive RPC requests and replies can contain an arbitrary amount |
opportunistic nodes in case their presence starts affecting the performance |
it is instead stored in a table that maps missing |
RPC requests and replies can contain an arbitrary amount of |
nodes in case their presence starts affecting the performance of |
is instead stored in a table that maps missing data |
requests and replies can contain an arbitrary amount of data |
in case their presence starts affecting the performance of the |
the growth in memory consumption causes slowdowns that amplify the |
instead stored in a table that maps missing data packets |
case their presence starts affecting the performance of the system |
A sender also attaches a priority and timeout to the |
growth in memory consumption causes slowdowns that amplify the increased |
stored in a table that maps missing data packets to |
sender also attaches a priority and timeout to the send |
in memory consumption causes slowdowns that amplify the increased latencies |
in a table that maps missing data packets to repair |
also attaches a priority and timeout to the send operation |
memory consumption causes slowdowns that amplify the increased latencies associated |
a table that maps missing data packets to repair packets |
consumption causes slowdowns that amplify the increased latencies associated with |
causes slowdowns that amplify the increased latencies associated with the |
slowdowns that amplify the increased latencies associated with the growth |
that amplify the increased latencies associated with the growth in |
for computing the threshold based on periodically sampled download and |
amplify the increased latencies associated with the growth in traffic |
this table is checked to see if any XORs now |
computing the threshold based on periodically sampled download and upload |
table is checked to see if any XORs now have |
To show this we designed experiments that vary the multicast |
the threshold based on periodically sampled download and upload factors |
is checked to see if any XORs now have singleton |
show this we designed experiments that vary the multicast rate |
checked to see if any XORs now have singleton losses |
The average download factors once again are used for detecting |
to see if any XORs now have singleton losses due |
so that an application need not block waiting for the |
average download factors once again are used for detecting whether |
see if any XORs now have singleton losses due to |
that an application need not block waiting for the result |
download factors once again are used for detecting whether the |
negligible loss rates and the nearly flat curve of memory |
if any XORs now have singleton losses due to the |
factors once again are used for detecting whether the threshold |
intem designed to support efficient access to a remote file |
loss rates and the nearly flat curve of memory consumption |
any XORs now have singleton losses due to the presence |
once again are used for detecting whether the threshold should |
designed to support efficient access to a remote file server |
XORs now have singleton losses due to the presence of |
again are used for detecting whether the threshold should be |
to support efficient access to a remote file server stead |
now have singleton losses due to the presence of the |
are used for detecting whether the threshold should be varied |
have singleton losses due to the presence of the new |
used for detecting whether the threshold should be varied or |
singleton losses due to the presence of the new packet |
Since an application can perform multiple RPCs concurby mobile clients |
for detecting whether the threshold should be varied or not |
losses due to the presence of the new packet and |
an application can perform multiple RPCs concurby mobile clients that |
due to the presence of the new packet and can |
application can perform multiple RPCs concurby mobile clients that must |
to the presence of the new packet and can be |
can perform multiple RPCs concurby mobile clients that must cope |
the presence of the new packet and can be used |
perform multiple RPCs concurby mobile clients that must cope with |
presence of the new packet and can be used for |
multiple RPCs concurby mobile clients that must cope with variations |
of the new packet and can be used for recovering |
RPCs concurby mobile clients that must cope with variations in |
the new packet and can be used for recovering other |
if the system seems to be in a compromised state |
concurby mobile clients that must cope with variations in available |
new packet and can be used for recovering other missing |
mobile clients that must cope with variations in available bandwidth |
packet and can be used for recovering other missing packets |
the collected upload factors are ordered and the value dividing |
Combined with a linear growth of CPU usage due to |
collected upload factors are ordered and the value dividing the |
with a linear growth of CPU usage due to the |
upload factors are ordered and the value dividing the lowest |
XORs received from different layers interact to recover missing data |
a linear growth of CPU usage due to the increasing |
received from different layers interact to recover missing data packets |
linear growth of CPU usage due to the increasing volume |
growth of CPU usage due to the increasing volume of |
of CPU usage due to the increasing volume of traffic |
since an XOR received at a higher interleave can recover |
an XOR received at a higher interleave can recover a |
XOR received at a higher interleave can recover a packet |
received at a higher interleave can recover a packet that |
and on fact that if the system s performance is |
at a higher interleave can recover a packet that makes |
on fact that if the system s performance is not |
The increasing number of unacknowledged requests and the resulting overheads |
Attaching priorities to RPCs allows applications to control this scheduling |
a higher interleave can recover a packet that makes an |
fact that if the system s performance is not satisfactory |
increasing number of unacknowledged requests and the resulting overheads rise |
priorities to RPCs allows applications to control this scheduling policy |
higher interleave can recover a packet that makes an earlier |
number of unacknowledged requests and the resulting overheads rise sharply |
interleave can recover a packet that makes an earlier XOR |
of unacknowledged requests and the resulting overheads rise sharply at |
can recover a packet that makes an earlier XOR at |
unacknowledged requests and the resulting overheads rise sharply at the |
recover a packet that makes an earlier XOR at a |
requests and the resulting overheads rise sharply at the highest |
File access model based on the importance of their results |
a packet that makes an earlier XOR at a lower |
and the resulting overheads rise sharply at the highest rates |
access model based on the importance of their results to |
packet that makes an earlier XOR at a lower interleave |
the resulting overheads rise sharply at the highest rates because |
model based on the importance of their results to the |
that makes an earlier XOR at a lower interleave usable |
resulting overheads rise sharply at the highest rates because of |
we evaluate the performance of our proposed auditing strategy over |
based on the importance of their results to the user |
makes an earlier XOR at a lower interleave usable hence |
overheads rise sharply at the highest rates because of the |
evaluate the performance of our proposed auditing strategy over the |
rise sharply at the highest rates because of the increasing |
the performance of our proposed auditing strategy over the original |
sharply at the highest rates because of the increasing token |
when a file is accessed assigns priorities to the classes |
performance of our proposed auditing strategy over the original streaming |
at the highest rates because of the increasing token roundtrip |
of our proposed auditing strategy over the original streaming protocol |
the highest rates because of the increasing token roundtrip time |
a client fetches the entire file from the file based |
its recovery power is much higher and comes close to |
client fetches the entire file from the file based on |
recovery power is much higher and comes close to standard |
driven simulator and used it to simulate streaming sessions on |
fetches the entire file from the file based on priorities |
This delays tokens as a function of the growing volume |
simulator and used it to simulate streaming sessions on networks |
the entire file from the file based on priorities whenever |
delays tokens as a function of the growing volume of |
and used it to simulate streaming sessions on networks with |
entire file from the file based on priorities whenever there |
tokens as a function of the growing volume of multicast |
file from the file based on priorities whenever there is |
as a function of the growing volume of multicast traffic |
from the file based on priorities whenever there is insufficient |
Limiting In the naive implementation of the layered interleaving algorithm |
the file based on priorities whenever there is insufficient bandwidth |
file based on priorities whenever there is insufficient bandwidth to |
repair packets are transmitted as soon as repair bins fill |
based on priorities whenever there is insufficient bandwidth to server |
packets are transmitted as soon as repair bins fill and |
on priorities whenever there is insufficient bandwidth to server and |
are transmitted as soon as repair bins fill and allow |
priorities whenever there is insufficient bandwidth to server and caches |
transmitted as soon as repair bins fill and allow them |
whenever there is insufficient bandwidth to server and caches it |
we would expect latency to decrease as the sending rate |
as soon as repair bins fill and allow them to |
would expect latency to decrease as the sending rate increases |
The target streaming rate in the experiments was fixed to |
soon as repair bins fill and allow them to be |
MAFS only sends the server the contents transmit competing RPCs |
expect latency to decrease as the sending rate increases because |
as repair bins fill and allow them to be constructed |
only sends the server the contents transmit competing RPCs without |
latency to decrease as the sending rate increases because the |
sends the server the contents transmit competing RPCs without a |
to decrease as the sending rate increases because the system |
the server the contents transmit competing RPCs without a noticeable |
all the repair bins in a layer fill in quick |
decrease as the sending rate increases because the system operates |
Each pair of graphs in shows the speedup of one |
server the contents transmit competing RPCs without a noticeable delay |
the repair bins in a layer fill in quick succession |
as the sending rate increases because the system operates more |
pair of graphs in shows the speedup of one of |
the sending rate increases because the system operates more smoothly |
RPCs of a modified file when it is closed by |
of graphs in shows the speedup of one of three |
of a modified file when it is closed by an |
avoiding context switching overheads and the extra latencies caused by |
graphs in shows the speedup of one of three cache |
a modified file when it is closed by an application |
context switching overheads and the extra latencies caused by the |
in shows the speedup of one of three cache manager |
switching overheads and the extra latencies caused by the small |
shows the speedup of one of three cache manager configurations |
overheads and the extra latencies caused by the small amount |
the source of the stream has an upload capacity of |
and the extra latencies caused by the small amount of |
relative to the time taken by uniform priorities with synchronous |
source of the stream has an upload capacity of four |
the extra latencies caused by the small amount of buffering |
to the time taken by uniform priorities with synchronous RPCs |
of the stream has an upload capacity of four times |
extra latencies caused by the small amount of buffering in |
the time taken by uniform priorities with synchronous RPCs at |
the stream has an upload capacity of four times the |
This ensures that the directory contents and apply changes locally |
latencies caused by the small amount of buffering in our |
stream has an upload capacity of four times the stream |
caused by the small amount of buffering in our protocol |
has an upload capacity of four times the stream rate |
This behavior leads to a large number of repair packets |
by the small amount of buffering in our protocol stack |
ing an RPC to apply the changes to the server |
behavior leads to a large number of repair packets being |
an RPC to apply the changes to the server s |
the graphs also show curves for differentiated priorities and synchronous |
leads to a large number of repair packets being generated |
RPC to apply the changes to the server s copy |
graphs also show curves for differentiated priorities and synchronous RPCs |
to a large number of repair packets being generated and |
a large number of repair packets being generated and sent |
Whole since lower bandwidth translates into longer delays for lowerfile |
large number of repair packets being generated and sent within |
since lower bandwidth translates into longer delays for lowerfile caching |
due to the longer pipeline at the receive side and |
number of repair packets being generated and sent within a |
Other nodes have enough download capacity to receive the stream |
lower bandwidth translates into longer delays for lowerfile caching is |
to the longer pipeline at the receive side and other |
of repair packets being generated and sent within a short |
bandwidth translates into longer delays for lowerfile caching is effective |
the longer pipeline at the receive side and other phenomena |
repair packets being generated and sent within a short period |
translates into longer delays for lowerfile caching is effective if |
longer pipeline at the receive side and other phenomena just |
packets being generated and sent within a short period of |
due to the overhead of priorities for small RPCs mentioned |
pipeline at the receive side and other phenomena just mentioned |
into longer delays for lowerfile caching is effective if a |
being generated and sent within a short period of time |
to the overhead of priorities for small RPCs mentioned in |
longer delays for lowerfile caching is effective if a client |
the overhead of priorities for small RPCs mentioned in Section |
delays for lowerfile caching is effective if a client s |
for lowerfile caching is effective if a client s connectivity |
lowerfile caching is effective if a client s connectivity is |
caching is effective if a client s connectivity is uncertain |
limit transmissions of repair packets to one for every r |
Comparing the execution time of the foreground workloads with synchronous |
the above observations are consistent with the sharp rise of |
transmissions of repair packets to one for every r data |
the execution time of the foreground workloads with synchronous writes |
we evaluate the average download factors of correct nodes during |
above observations are consistent with the sharp rise of the |
RPC timeouts allow the application to prevent since the client |
of repair packets to one for every r data packets |
evaluate the average download factors of correct nodes during a |
update logging and asynchronous writeback reveals that the latter two |
observations are consistent with the sharp rise of the average |
timeouts allow the application to prevent since the client can |
This problem is fixed by staggering the starting sizes of |
logging and asynchronous writeback reveals that the latter two options |
are consistent with the sharp rise of the average delay |
allow the application to prevent since the client can always |
problem is fixed by staggering the starting sizes of the |
and asynchronous writeback reveals that the latter two options generally |
consistent with the sharp rise of the average delay for |
second time interval after auditing is first applied to the |
the application to prevent since the client can always use |
is fixed by staggering the starting sizes of the bins |
asynchronous writeback reveals that the latter two options generally perform |
with the sharp rise of the average delay for timer |
time interval after auditing is first applied to the system |
application to prevent since the client can always use cached |
writeback reveals that the latter two options generally perform comparably |
the sharp rise of the average delay for timer events |
analogous to the starting positions of runners in a sprint |
to prevent since the client can always use cached copies |
reveals that the latter two options generally perform comparably to |
prevent since the client can always use cached copies of |
that the latter two options generally perform comparably to or |
the very first time bin number x in a layer |
since the client can always use cached copies of files |
the latter two options generally perform comparably to or better |
very first time bin number x in a layer of |
the client can always use cached copies of files instead |
latter two options generally perform comparably to or better than |
first time bin number x in a layer of interleave |
client can always use cached copies of files instead low |
two options generally perform comparably to or better than synchronous |
time bin number x in a layer of interleave i |
options generally perform comparably to or better than synchronous writes |
bin number x in a layer of interleave i fires |
Notice that the sample size does not increase with the |
Logging and asynchronous writeback greatly improve the performance of the |
that the sample size does not increase with the size |
and asynchronous writeback greatly improve the performance of the background |
lows a programmer to write an adaptive application without ports |
the sample size does not increase with the size of |
asynchronous writeback greatly improve the performance of the background workloads |
a programmer to write an adaptive application without ports this |
sample size does not increase with the size of the |
programmer to write an adaptive application without ports this type |
the first repair bin in the second layer with interleave |
size does not increase with the size of the system |
to write an adaptive application without ports this type of |
write an adaptive application without ports this type of disconnected |
an adaptive application without ports this type of disconnected operation |
We focus on MFS with asynchronous writeback in the rest |
focus on MFS with asynchronous writeback in the rest of |
on MFS with asynchronous writeback in the rest of this |
Number of unacknowledged messages and average token roundtrip time as |
MFS with asynchronous writeback in the rest of this paper |
of unacknowledged messages and average token roundtrip time as a |
with asynchronous writeback in the rest of this paper because |
unacknowledged messages and average token roundtrip time as a function |
asynchronous writeback in the rest of this paper because it |
messages and average token roundtrip time as a function of |
writeback in the rest of this paper because it provides |
having to take account of the actual bandwidth or current |
and average token roundtrip time as a function of the |
in the rest of this paper because it provides comparable |
to take account of the actual bandwidth or current mix |
average token roundtrip time as a function of the sending |
the rest of this paper because it provides comparable performance |
take account of the actual bandwidth or current mix tent |
token roundtrip time as a function of the sending rate |
rest of this paper because it provides comparable performance to |
account of the actual bandwidth or current mix tent of |
of this paper because it provides comparable performance to logged |
of the actual bandwidth or current mix tent of automatic |
this paper because it provides comparable performance to logged updates |
the actual bandwidth or current mix tent of automatic reconciliation |
Linearly growing memory use on sender and the nearly flat |
actual bandwidth or current mix tent of automatic reconciliation of |
growing memory use on sender and the nearly flat usage |
and is easily extensible to more than one level of |
bandwidth or current mix tent of automatic reconciliation of update |
memory use on sender and the nearly flat usage on |
is easily extensible to more than one level of priority |
or current mix tent of automatic reconciliation of update conflicts |
use on sender and the nearly flat usage on the |
on sender and the nearly flat usage on the receiver |
Since reducing available bandwidth increases the contention between RPCs of |
sender and the nearly flat usage on the receiver as |
reducing available bandwidth increases the contention between RPCs of different |
and the nearly flat usage on the receiver as a |
available bandwidth increases the contention between RPCs of different types |
the nearly flat usage on the receiver as a function |
nearly flat usage on the receiver as a function of |
the benefits of RPC priorities should be more apparent at |
flat usage on the receiver as a function of the |
benefits of RPC priorities should be more apparent at lower |
usage on the receiver as a function of the sending |
of RPC priorities should be more apparent at lower priorities |
and avoid having to specify thresholds at the other hand |
on the receiver as a function of the sending rate |
level caching reduces the delay incurred which it should switch |
caching reduces the delay incurred which it should switch communication |
reduces the delay incurred which it should switch communication modes |
An RPC whose results are urgently required should be aswhen |
RPC whose results are urgently required should be aswhen an |
whose results are urgently required should be aswhen an application |
results are urgently required should be aswhen an application opens |
are urgently required should be aswhen an application opens a |
urgently required should be aswhen an application opens a file |
Alarm firing delays on sender and receiver as a function |
firing delays on sender and receiver as a function of |
delays on sender and receiver as a function of sending |
on sender and receiver as a function of sending rate |
s is not low in the sense of prior work |
it is low enough to cause significant contention for the |
is low enough to cause significant contention for the workloads |
low enough to cause significant contention for the workloads we |
enough to cause significant contention for the workloads we have |
to cause significant contention for the workloads we have considered |
and we believe that our results will hold if available |
we believe that our results will hold if available bandwidth |
believe that our results will hold if available bandwidth and |
that our results will hold if available bandwidth and Grep |
our results will hold if available bandwidth and Grep Write |
it is possible to use a signed the highest priority |
A single sender multicasts to a varying number of groups |
single sender multicasts to a varying number of groups in |
based division of files into blocks as the basis for |
sender multicasts to a varying number of groups in a |
division of files into blocks as the basis for re |
multicasts to a varying number of groups in a roundrobin |
to a varying number of groups in a roundrobin fashion |
QSM s regional recovery protocol is oblivious to the groups |
hence the receivers behave identically no matter how many groups |
the receivers behave identically no matter how many groups we |
receivers behave identically no matter how many groups we use |
while the lowest levels are useful for server traffic does |
the lowest levels are useful for server traffic does not |
lowest levels are useful for server traffic does not eliminate |
so changes to throughput or protocol behavior must be directly |
levels are useful for server traffic does not eliminate the |
changes to throughput or protocol behavior must be directly or |
are useful for server traffic does not eliminate the fundamental |
to throughput or protocol behavior must be directly or indirectly |
useful for server traffic does not eliminate the fundamental problem |
throughput or protocol behavior must be directly or indirectly linked |
for server traffic does not eliminate the fundamental problem of |
or protocol behavior must be directly or indirectly linked to |
server traffic does not eliminate the fundamental problem of RPCs |
protocol behavior must be directly or indirectly linked to memory |
traffic does not eliminate the fundamental problem of RPCs that |
behavior must be directly or indirectly linked to memory usage |
does not eliminate the fundamental problem of RPCs that can |
not eliminate the fundamental problem of RPCs that can be |
We do not expect the token roundtrip time or the |
eliminate the fundamental problem of RPCs that can be arbitrarily |
do not expect the token roundtrip time or the amount |
the fundamental problem of RPCs that can be arbitrarily delayed |
not expect the token roundtrip time or the amount of |
expect the token roundtrip time or the amount of messages |
the token roundtrip time or the amount of messages pending |
such as speculative activities like prefetching and transferring archival data |
token roundtrip time or the amount of messages pending acknowledgement |
roundtrip time or the amount of messages pending acknowledgement to |
time or the amount of messages pending acknowledgement to vary |
or the amount of messages pending acknowledgement to vary with |
the amount of messages pending acknowledgement to vary with the |
tial assumption regarding the correct priority level for an RPC |
amount of messages pending acknowledgement to vary with the number |
assumption regarding the correct priority level for an RPC proves |
of messages pending acknowledgement to vary with the number of |
regarding the correct priority level for an RPC proves incorrect |
messages pending acknowledgement to vary with the number of groups |
a call to the library can be made to assign |
call to the library can be made to assign a |
MFS with synchronous RPCs and priorities is compared to a |
with synchronous RPCs and priorities is compared to a version |
synchronous RPCs and priorities is compared to a version of |
RPCs and priorities is compared to a version of the |
and priorities is compared to a version of the Andrew |
priorities is compared to a version of the Andrew File |
is compared to a version of the Andrew File System |
Speedups for the two workloads of the GW test are |
for the two workloads of the GW test are shown |
the file server grants it permission to cache the file |
file server grants it permission to cache the file for |
server grants it permission to cache the file for a |
grants it permission to cache the file for a limited |
it permission to cache the file for a limited period |
since all the foreground workloads improve their performance substantially at |
all the foreground workloads improve their performance substantially at lower |
the foreground workloads improve their performance substantially at lower bandwidths |
The client is a usermakes a callback RPC to any |
Inspection of the managed heap in a debugger shows that |
client is a usermakes a callback RPC to any other |
of the managed heap in a debugger shows that the |
is a usermakes a callback RPC to any other clients |
the managed heap in a debugger shows that the growth |
a usermakes a callback RPC to any other clients on |
managed heap in a debugger shows that the growth in |
usermakes a callback RPC to any other clients on the |
heap in a debugger shows that the growth in memory |
a callback RPC to any other clients on the list |
in a debugger shows that the growth in memory used |
a debugger shows that the growth in memory used is |
debugger shows that the growth in memory used is caused |
A client level process that stores cached files in a |
shows that the growth in memory used is caused not |
client level process that stores cached files in a local |
that the growth in memory used is caused not by |
level process that stores cached files in a local filesystem |
the growth in memory used is caused not by messages |
The that receives a callback RPC discards its cached copy |
that receives a callback RPC discards its cached copy of |
receives a callback RPC discards its cached copy of the |
a callback RPC discards its cached copy of the file |
server also stores its copies of files in a local |
also stores its copies of files in a local filesystem |
We can confirm the theory by turning on additional tracing |
if an application has the file open when its client |
can confirm the theory by turning on additional tracing in |
an application has the file open when its client re |
confirm the theory by turning on additional tracing in the |
the theory by turning on additional tracing in the per |
system operations from applications are redirected to user level ceives |
operations from applications are redirected to user level ceives the |
from applications are redirected to user level ceives the callback |
This tracing is lightweight and has little effect on CPU |
tracing is lightweight and has little effect on CPU consumption |
but it increases the memory footprint by adding additional data |
it increases the memory footprint by adding additional data structures |
increases the memory footprint by adding additional data structures that |
the memory footprint by adding additional data structures that are |
memory footprint by adding additional data structures that are updated |
footprint by adding additional data structures that are updated once |
by adding additional data structures that are updated once per |
adding additional data structures that are updated once per second |
These traces are representative periods of mixed read and write |
traces are representative periods of mixed read and write activity |
Note that the total file sizes represent the amount fetched |
that the total file sizes represent the amount fetched by |
the total file sizes represent the amount fetched by MFS |
total file sizes represent the amount fetched by MFS during |
file sizes represent the amount fetched by MFS during the |
sizes represent the amount fetched by MFS during the trace |
and the contribution rate of opportunistic nodes is varied from |
It is worth noting that the memory usages reported here |
is worth noting that the memory usages reported here are |
the additional traffic is due to new files being created |
worth noting that the memory usages reported here are averages |
additional traffic is due to new files being created or |
traffic is due to new files being created or existing |
is due to new files being created or existing ones |
due to new files being created or existing ones extended |
an artificial delay in writing back updates introduces inconsistencies between |
artificial delay in writing back updates introduces inconsistencies between the |
delay in writing back updates introduces inconsistencies between the client |
in writing back updates introduces inconsistencies between the client and |
writing back updates introduces inconsistencies between the client and the |
back updates introduces inconsistencies between the client and the file |
updates introduces inconsistencies between the client and the file server |
be grateful to be able to use the file system |
grateful to be able to use the file system at |
to be able to use the file system at all |
We studied the effects of using different values for t |
MAFS avoids the need for modes by using asynchronous Remote |
avoids the need for modes by using asynchronous Remote procedure |
the need for modes by using asynchronous Remote procedure calls |
need for modes by using asynchronous Remote procedure calls between |
for modes by using asynchronous Remote procedure calls between a |
modes by using asynchronous Remote procedure calls between a client |
by using asynchronous Remote procedure calls between a client and |
NTFS workloads In addition to measuring the performance of MFS |
using asynchronous Remote procedure calls between a client and the |
workloads In addition to measuring the performance of MFS with |
asynchronous Remote procedure calls between a client and the file |
In addition to measuring the performance of MFS with synthetic |
Remote procedure calls between a client and the file server |
addition to measuring the performance of MFS with synthetic workloads |
procedure calls between a client and the file server writeback |
calls between a client and the file server writeback at |
we have also conducted experiments with traces gathered from the |
between a client and the file server writeback at all |
have also conducted experiments with traces gathered from the Windows |
a client and the file server writeback at all bandwidth |
also conducted experiments with traces gathered from the Windows NT |
client and the file server writeback at all bandwidth levels |
conducted experiments with traces gathered from the Windows NT file |
and present a detailed set of results on applying different |
experiments with traces gathered from the Windows NT file system |
and incorporates a new upare divided into several types depending |
present a detailed set of results on applying different thresholds |
incorporates a new upare divided into several types depending on |
a detailed set of results on applying different thresholds to |
a new upare divided into several types depending on their |
detailed set of results on applying different thresholds to different |
new upare divided into several types depending on their function |
set of results on applying different thresholds to different scenarios |
and NTFS has a somewhat different interface to the file |
RPCs date propagation algorithm to reduce the possibility of inconsisto |
NTFS has a somewhat different interface to the file system |
date propagation algorithm to reduce the possibility of inconsisto fetch |
propagation algorithm to reduce the possibility of inconsisto fetch and |
the traces were converted to run on top of MFS |
algorithm to reduce the possibility of inconsisto fetch and store |
traces were converted to run on top of MFS with |
to reduce the possibility of inconsisto fetch and store data |
were converted to run on top of MFS with little |
reduce the possibility of inconsisto fetch and store data are |
converted to run on top of MFS with little difficulty |
the possibility of inconsisto fetch and store data are self |
The original traces recorded file accesses on a set of |
we also start to see occasional bursts of packet losses |
original traces recorded file accesses on a set of machines |
traces recorded file accesses on a set of machines in |
recorded file accesses on a set of machines in a |
file accesses on a set of machines in a LAN |
A majority of the accesses were local but some were |
majority of the accesses were local but some were to |
of the accesses were local but some were to remote |
the accesses were local but some were to remote machines |
We extracted subintervals from the traces which featured interesting file |
As new operations are added to the tail tions include |
extracted subintervals from the traces which featured interesting file system |
new operations are added to the tail tions include fetching |
subintervals from the traces which featured interesting file system behaviour |
Number of messages pending ACK and token roundtrip time as |
operations are added to the tail tions include fetching and |
from the traces which featured interesting file system behaviour and |
of messages pending ACK and token roundtrip time as a |
are added to the tail tions include fetching and setting |
the traces which featured interesting file system behaviour and processed |
messages pending ACK and token roundtrip time as a function |
added to the tail tions include fetching and setting file |
traces which featured interesting file system behaviour and processed them |
pending ACK and token roundtrip time as a function of |
to the tail tions include fetching and setting file attributes |
which featured interesting file system behaviour and processed them to |
ACK and token roundtrip time as a function of the |
featured interesting file system behaviour and processed them to remove |
and token roundtrip time as a function of the number |
interesting file system behaviour and processed them to remove accesses |
token roundtrip time as a function of the number of |
the client flushes operations serially from the head of operations |
file system behaviour and processed them to remove accesses to |
roundtrip time as a function of the number of groups |
client flushes operations serially from the head of operations such |
system behaviour and processed them to remove accesses to files |
flushes operations serially from the head of operations such as |
behaviour and processed them to remove accesses to files over |
operations serially from the head of operations such as creating |
serially from the head of operations such as creating and |
from the head of operations such as creating and unlinking |
This preprocessing was necessary to eliminate the influence of extremely |
the head of operations such as creating and unlinking files |
preprocessing was necessary to eliminate the influence of extremely large |
was necessary to eliminate the influence of extremely large NT |
necessary to eliminate the influence of extremely large NT system |
to eliminate the influence of extremely large NT system files |
server traffic consists of a variety of foreground include locking |
traffic consists of a variety of foreground include locking files |
of the file system traffic in some portions of the |
consists of a variety of foreground include locking files and |
the file system traffic in some portions of the original |
of a variety of foreground include locking files and the |
file system traffic in some portions of the original traces |
a variety of foreground include locking files and the server |
variety of foreground include locking files and the server s |
of foreground include locking files and the server s callback |
including these system files would have distorted the experiments at |
foreground include locking files and the server s callback to |
these system files would have distorted the experiments at low |
include locking files and the server s callback to invalidate |
system files would have distorted the experiments at low bandwidths |
and the number of correct nodes mistakenly removed from the |
locking files and the server s callback to invalidate a |
the number of correct nodes mistakenly removed from the system |
files and the server s callback to invalidate a RPCs |
and the server s callback to invalidate a RPCs for |
the server s callback to invalidate a RPCs for control |
server s callback to invalidate a RPCs for control operations |
the key insight is that all these effects originate at |
s callback to invalidate a RPCs for control operations and |
key insight is that all these effects originate at the |
Each trace was run over MFS with the combinations of |
callback to invalidate a RPCs for control operations and fetching |
insight is that all these effects originate at the sender |
trace was run over MFS with the combinations of synchronous |
to invalidate a RPCs for control operations and fetching file |
is that all these effects originate at the sender node |
was run over MFS with the combinations of synchronous and |
invalidate a RPCs for control operations and fetching file data |
run over MFS with the combinations of synchronous and asynchronous |
over MFS with the combinations of synchronous and asynchronous writes |
MFS with the combinations of synchronous and asynchronous writes and |
and a stream client s cached copy of a file |
detailed analysis of the captured network traffic shows that the |
with the combinations of synchronous and asynchronous writes and differentiated |
analysis of the captured network traffic shows that the multicast |
the combinations of synchronous and asynchronous writes and differentiated and |
the number of nodes incorrectly accused also increases with higher |
of the captured network traffic shows that the multicast stream |
combinations of synchronous and asynchronous writes and differentiated and uniform |
number of nodes incorrectly accused also increases with higher thresholds |
the captured network traffic shows that the multicast stream in |
of synchronous and asynchronous writes and differentiated and uniform priorities |
captured network traffic shows that the multicast stream in all |
synchronous and asynchronous writes and differentiated and uniform priorities in |
network traffic shows that the multicast stream in all cases |
and asynchronous writes and differentiated and uniform priorities in previous |
Communication adaptation layed in proportion to the foreground RPC traffic |
traffic shows that the multicast stream in all cases looks |
asynchronous writes and differentiated and uniform priorities in previous experiments |
adaptation layed in proportion to the foreground RPC traffic and |
shows that the multicast stream in all cases looks basically |
layed in proportion to the foreground RPC traffic and the |
that the multicast stream in all cases looks basically identical |
in proportion to the foreground RPC traffic and the availTo |
proportion to the foreground RPC traffic and the availTo reduce |
and hence we cannot attribute token latency or losses to |
to the foreground RPC traffic and the availTo reduce its |
look for instance at the heavy load bar mostly reads |
hence we cannot attribute token latency or losses to the |
the foreground RPC traffic and the availTo reduce its network |
we cannot attribute token latency or losses to the increased |
foreground RPC traffic and the availTo reduce its network communication |
cannot attribute token latency or losses to the increased volume |
RPC traffic and the availTo reduce its network communication when |
attribute token latency or losses to the increased volume of |
traffic and the availTo reduce its network communication when bandwidth |
token latency or losses to the increased volume of traffic |
and the availTo reduce its network communication when bandwidth is |
the availTo reduce its network communication when bandwidth is low |
providing the best compromise in terms of performance and false |
the best compromise in terms of performance and false positives |
a mobile file system client can automatically adapt its communication |
but doesn t produce any faster data bursts than those |
best compromise in terms of performance and false positives across |
mobile file system client can automatically adapt its communication strategy |
doesn t produce any faster data bursts than those we |
compromise in terms of performance and false positives across all |
file system client can automatically adapt its communication strategy to |
t produce any faster data bursts than those we observe |
in terms of performance and false positives across all scenarios |
system client can automatically adapt its communication strategy to the |
produce any faster data bursts than those we observe with |
client can automatically adapt its communication strategy to the available |
any faster data bursts than those we observe with smaller |
can automatically adapt its communication strategy to the available bandwidth |
faster data bursts than those we observe with smaller numbers |
data bursts than those we observe with smaller numbers of |
bursts than those we observe with smaller numbers of groups |
against each other and against a configuration with no auditing |
RPC priorities cations transfer a large volume of data that |
priorities cations transfer a large volume of data that the |
cations transfer a large volume of data that the user |
Receiver performance indicators such as delays in firing timer event |
transfer a large volume of data that the user is |
performance indicators such as delays in firing timer event or |
a large volume of data that the user is unlikely |
indicators such as delays in firing timer event or CPU |
large volume of data that the user is unlikely to |
such as delays in firing timer event or CPU utilization |
volume of data that the user is unlikely to require |
for the fixed threshold strategy and as the initial threshold |
Grep in the GW workload even is less than would |
as delays in firing timer event or CPU utilization don |
Staggered Start first i data packets added to a layer |
of data that the user is unlikely to require immediately |
the fixed threshold strategy and as the initial threshold in |
in the GW workload even is less than would be |
delays in firing timer event or CPU utilization don t |
Start first i data packets added to a layer with |
fixed threshold strategy and as the initial threshold in the |
the GW workload even is less than would be expected |
consuming bandwidth that can be used MAFS uses priorities to |
in firing timer event or CPU utilization don t show |
first i data packets added to a layer with interleave |
threshold strategy and as the initial threshold in the stepwise |
GW workload even is less than would be expected with |
bandwidth that can be used MAFS uses priorities to reduce |
firing timer event or CPU utilization don t show any |
i data packets added to a layer with interleave i |
strategy and as the initial threshold in the stepwise adaptive |
workload even is less than would be expected with reduced |
that can be used MAFS uses priorities to reduce contention |
timer event or CPU utilization don t show any noticeable |
and as the initial threshold in the stepwise adaptive strategy |
even is less than would be expected with reduced bandwidth |
can be used MAFS uses priorities to reduce contention between |
event or CPU utilization don t show any noticeable trend |
be used MAFS uses priorities to reduce contention between foreground |
here uniform priorities result in throughput linear in the bandwidth |
used MAFS uses priorities to reduce contention between foreground for |
and so on until r i data packets have been |
MAFS uses priorities to reduce contention between foreground for important |
so on until r i data packets have been added |
and the main thing going on in the sender is |
The RC and GC tests show the benefit of asynchronous |
uses priorities to reduce contention between foreground for important tasks |
on until r i data packets have been added to |
the main thing going on in the sender is that |
of the nodes were opportunistic and with varying ratios of |
RC and GC tests show the benefit of asynchronous writeback |
until r i data packets have been added to the |
main thing going on in the sender is that it |
since the updates from the compile workload are committed sooner |
r i data packets have been added to the layer |
the nodes were opportunistic and with varying ratios of contribution |
thing going on in the sender is that it has |
the updates from the compile workload are committed sooner to |
i data packets have been added to the layer and |
going on in the sender is that it has a |
updates from the compile workload are committed sooner to the |
data packets have been added to the layer and all |
on in the sender is that it has a steadily |
from the compile workload are committed sooner to the server |
packets have been added to the layer and all bins |
in the sender is that it has a steadily growing |
the compile workload are committed sooner to the server than |
have been added to the layer and all bins have |
the sender is that it has a steadily growing memory |
compile workload are committed sooner to the server than with |
been added to the layer and all bins have fired |
sender is that it has a steadily growing memory footprint |
workload are committed sooner to the server than with synchronous |
which assigns a lower priority to writeback in wants to |
added to the layer and all bins have fired exactly |
are committed sooner to the server than with synchronous writes |
assigns a lower priority to writeback in wants to see |
to the layer and all bins have fired exactly once |
The distribution of token roundtrip times for different numbers of |
a lower priority to writeback in wants to see the |
due to the overlap of think time with asynchronous writes |
distribution of token roundtrip times for different numbers of groups |
We present both the average and the minimum download factors |
lower priority to writeback in wants to see the processed |
of token roundtrip times for different numbers of groups shows |
present both the average and the minimum download factors across |
though uniform priorities provide better performance for the Write component |
priority to writeback in wants to see the processed images |
token roundtrip times for different numbers of groups shows an |
both the average and the minimum download factors across all |
uniform priorities provide better performance for the Write component of |
roundtrip times for different numbers of groups shows an increase |
The outlined scheme works when i is greater than or |
the average and the minimum download factors across all correct |
priorities provide better performance for the Write component of the |
times for different numbers of groups shows an increase of |
outlined scheme works when i is greater than or equal |
average and the minimum download factors across all correct nodes |
provide better performance for the Write component of the RW |
for different numbers of groups shows an increase of the |
scheme works when i is greater than or equal to |
and the minimum download factors across all correct nodes in |
better performance for the Write component of the RW test |
different numbers of groups shows an increase of the token |
works when i is greater than or equal to r |
the minimum download factors across all correct nodes in the |
performance for the Write component of the RW test at |
numbers of groups shows an increase of the token roundtrip |
minimum download factors across all correct nodes in the system |
lows control over bandwidth allocation at the level of individInterference |
of groups shows an increase of the token roundtrip time |
control over bandwidth allocation at the level of individInterference due |
over bandwidth allocation at the level of individInterference due to |
bandwidth allocation at the level of individInterference due to write |
allocation at the level of individInterference due to write traffic |
at the level of individInterference due to write traffic is |
the level of individInterference due to write traffic is often |
level of individInterference due to write traffic is often solved |
of individInterference due to write traffic is often solved by |
individInterference due to write traffic is often solved by writing |
due to write traffic is often solved by writing ual |
Though we have concentrated on determining the benefit of RPC |
to write traffic is often solved by writing ual RPCs |
we have concentrated on determining the benefit of RPC priorities |
have concentrated on determining the benefit of RPC priorities by |
rather than a uniform increase of the token processing overhead |
without requiring that an MAFS client is aware of back |
concentrated on determining the benefit of RPC priorities by a |
requiring that an MAFS client is aware of back updates |
on determining the benefit of RPC priorities by a comparison |
that an MAFS client is aware of back updates asynchronously |
If r and i are not integral multiples of each |
determining the benefit of RPC priorities by a comparison of |
we find that these tokens were most commonly delayed on |
r and i are not integral multiples of each other |
the benefit of RPC priorities by a comparison of different |
find that these tokens were most commonly delayed on the |
can start reading another image without waiting for the previWhen |
benefit of RPC priorities by a comparison of different configurations |
that these tokens were most commonly delayed on the sender |
start reading another image without waiting for the previWhen choosing |
limiting still works but is slightly less effective due to |
of RPC priorities by a comparison of different configurations of |
reading another image without waiting for the previWhen choosing priorities |
still works but is slightly less effective due to rounding |
RPC priorities by a comparison of different configurations of MFS |
the average time to travel by one hop from sender |
works but is slightly less effective due to rounding errors |
automatic assignment and fine ous output to be sent to |
priorities by a comparison of different configurations of MFS to |
average time to travel by one hop from sender to |
assignment and fine ous output to be sent to the |
by a comparison of different configurations of MFS to one |
time to travel by one hop from sender to receiver |
repair packets are transmitted as soon as they are generated |
and fine ous output to be sent to the file |
a comparison of different configurations of MFS to one another |
This results in the repair packet leaving immediately after the |
fine ous output to be sent to the file server |
to travel by one hop from sender to receiver or |
results in the repair packet leaving immediately after the last |
we have also performed a few experiments to compare the |
travel by one hop from sender to receiver or receiver |
in the repair packet leaving immediately after the last data |
have also performed a few experiments to compare the performance |
by one hop from sender to receiver or receiver to |
the repair packet leaving immediately after the last data packet |
also performed a few experiments to compare the performance of |
one hop from sender to receiver or receiver to sender |
repair packet leaving immediately after the last data packet that |
performed a few experiments to compare the performance of MFS |
tion and provide the maximum degree of differentiation among ecution |
hop from sender to receiver or receiver to sender can |
packet leaving immediately after the last data packet that was |
a few experiments to compare the performance of MFS to |
and provide the maximum degree of differentiation among ecution time |
from sender to receiver or receiver to sender can grow |
leaving immediately after the last data packet that was added |
few experiments to compare the performance of MFS to a |
provide the maximum degree of differentiation among ecution time and |
sender to receiver or receiver to sender can grow to |
immediately after the last data packet that was added to |
experiments to compare the performance of MFS to a standard |
the maximum degree of differentiation among ecution time and utilising |
to receiver or receiver to sender can grow to nearly |
after the last data packet that was added to it |
to compare the performance of MFS to a standard distributed |
maximum degree of differentiation among ecution time and utilising bandwidth |
which lowers burst tolerance if the repair packet was generated |
compare the performance of MFS to a standard distributed file |
degree of differentiation among ecution time and utilising bandwidth more |
lowers burst tolerance if the repair packet was generated at |
the performance of MFS to a standard distributed file system |
of differentiation among ecution time and utilising bandwidth more efficiently |
burst tolerance if the repair packet was generated at interleave |
tolerance if the repair packet was generated at interleave i |
illustrates the result of running the GW test over MFS |
Strategies used for defining the minimum upload threshold t Figure |
the result of running the GW test over MFS and |
the resulting protocol can tolerate a burst of i lost |
result of running the GW test over MFS and an |
resulting protocol can tolerate a burst of i lost data |
shows that all strategies yield significantly better results compared to |
the overloaded sender occasionally releases the tokens with a delay |
of running the GW test over MFS and an Andrew |
protocol can tolerate a burst of i lost data packets |
contention arises when files are being effective if concurrent RPCs |
that all strategies yield significantly better results compared to an |
running the GW test over MFS and an Andrew File |
can tolerate a burst of i lost data packets excluding |
arises when files are being effective if concurrent RPCs usually |
all strategies yield significantly better results compared to an approach |
the GW test over MFS and an Andrew File System |
tolerate a burst of i lost data packets excluding the |
when files are being effective if concurrent RPCs usually end |
strategies yield significantly better results compared to an approach with |
a burst of i lost data packets excluding the repair |
the value of the delay grows with the number of |
files are being effective if concurrent RPCs usually end up |
yield significantly better results compared to an approach with no |
we used the Arla implementation of the AFS cache manager |
value of the delay grows with the number of groups |
but the burst could swallow both the repair and the |
are being effective if concurrent RPCs usually end up with |
significantly better results compared to an approach with no auditing |
the burst could swallow both the repair and the last |
being effective if concurrent RPCs usually end up with different |
burst could swallow both the repair and the last data |
While both adaptive strategies yield excellent download rates to correct |
effective if concurrent RPCs usually end up with different prifetched |
could swallow both the repair and the last data packet |
both adaptive strategies yield excellent download rates to correct nodes |
if concurrent RPCs usually end up with different prifetched at |
swallow both the repair and the last data packet in |
concurrent RPCs usually end up with different prifetched at the |
the fixed threshold strategy s performance is not as good |
both the repair and the last data packet in it |
RPCs usually end up with different prifetched at the same |
and this cascades to create all sorts of downstream problems |
fixed threshold strategy s performance is not as good when |
the repair and the last data packet in it as |
usually end up with different prifetched at the same time |
this cascades to create all sorts of downstream problems that |
threshold strategy s performance is not as good when opportunistic |
and more optimised than MFS for this sort of communication |
repair and the last data packet in it as they |
end up with different prifetched at the same time as |
cascades to create all sorts of downstream problems that can |
strategy s performance is not as good when opportunistic nodes |
and the last data packet in it as they are |
Since the results of running the other tests are similar |
up with different prifetched at the same time as updates |
to create all sorts of downstream problems that can destabilize |
s performance is not as good when opportunistic nodes are |
the last data packet in it as they are not |
with different prifetched at the same time as updates are |
create all sorts of downstream problems that can destabilize the |
performance is not as good when opportunistic nodes are contributing |
last data packet in it as they are not separated |
mostly reads mostly writes heavy load store overhead priorities uniform |
different prifetched at the same time as updates are written |
all sorts of downstream problems that can destabilize the system |
is not as good when opportunistic nodes are contributing with |
data packet in it as they are not separated by |
reads mostly writes heavy load store overhead priorities uniform priorities |
prifetched at the same time as updates are written back |
sorts of downstream problems that can destabilize the system as |
packet in it as they are not separated by the |
mostly writes heavy load store overhead priorities uniform priorities uniform |
of downstream problems that can destabilize the system as a |
in it as they are not separated by the requisite |
writes heavy load store overhead priorities uniform priorities uniform synchronous |
downstream problems that can destabilize the system as a whole |
it as they are not separated by the requisite interleave |
heavy load store overhead priorities uniform priorities uniform synchronous asynchronous |
load store overhead priorities uniform priorities uniform synchronous asynchronous Time |
tention can be mitigated by prioritising file fetch RPCs above |
store overhead priorities uniform priorities uniform synchronous asynchronous Time spent |
The solution to this is simple delay sending the repair |
Discussion The experiments just reported make it clear that the |
can be mitigated by prioritising file fetch RPCs above File |
overhead priorities uniform priorities uniform synchronous asynchronous Time spent on |
At those rates opportunistic nodes are harmful to the system |
solution to this is simple delay sending the repair packet |
The experiments just reported make it clear that the performance |
priorities uniform priorities uniform synchronous asynchronous Time spent on RPCs |
to this is simple delay sending the repair packet generated |
but the imporwriteback RPCs to ensure that they will be |
this is simple delay sending the repair packet generated by |
and that in addition to protocol factors such as the |
the imporwriteback RPCs to ensure that they will be preferentially |
is simple delay sending the repair packet generated by a |
that in addition to protocol factors such as the length |
imporwriteback RPCs to ensure that they will be preferentially allo |
simple delay sending the repair packet generated by a repair |
in addition to protocol factors such as the length of |
delay sending the repair packet generated by a repair bin |
tance of a file can be hard to determine automatically |
addition to protocol factors such as the length of token |
sending the repair packet generated by a repair bin until |
we consider a scenario where opportunistic nodes contribute with different |
to protocol factors such as the length of token rings |
the repair packet generated by a repair bin until the |
consider a scenario where opportunistic nodes contribute with different rates |
repair packet generated by a repair bin until the next |
latency is strongly influenced by the memory footprint of the |
packet generated by a repair bin until the next time |
is strongly influenced by the memory footprint of the system |
We varied the percentage of opportunistic nodes in the system |
files can be too numerous for the user to manually |
generated by a repair bin until the next time a |
when we built the system it was obvious that minimizing |
can be too numerous for the user to manually assign |
varied the percentage of opportunistic nodes in the system from |
by a repair bin until the next time a data |
we built the system it was obvious that minimizing latency |
be too numerous for the user to manually assign priIn |
a repair bin until the next time a data packet |
built the system it was obvious that minimizing latency would |
too numerous for the user to manually assign priIn this |
repair bin until the next time a data packet is |
the system it was obvious that minimizing latency would be |
numerous for the user to manually assign priIn this section |
bin until the next time a data packet is added |
system it was obvious that minimizing latency would be important |
until the next time a data packet is added to |
The graphs present the average and minimum download rates for |
the next time a data packet is added to the |
this motivated several of the design decisions discussed in Section |
but priorities can be autowriteback and RPC priorities in MAFS |
next time a data packet is added to the now |
graphs present the average and minimum download rates for these |
priorities can be autowriteback and RPC priorities in MAFS under |
time a data packet is added to the now empty |
But the repeated linkage of latency and oscillatory throughputs to |
present the average and minimum download rates for these scenarios |
can be autowriteback and RPC priorities in MAFS under different |
a data packet is added to the now empty bin |
the repeated linkage of latency and oscillatory throughputs to memory |
be autowriteback and RPC priorities in MAFS under different levels |
repeated linkage of latency and oscillatory throughputs to memory was |
no auditing performs significantly worse than any of the proposed |
autowriteback and RPC priorities in MAFS under different levels matically |
which happens i packets later and introduces the required interleave |
linkage of latency and oscillatory throughputs to memory was a |
auditing performs significantly worse than any of the proposed strategies |
and RPC priorities in MAFS under different levels matically assigned |
happens i packets later and introduces the required interleave between |
of latency and oscillatory throughputs to memory was a surprise |
RPC priorities in MAFS under different levels matically assigned to |
i packets later and introduces the required interleave between the |
the stepwise adaptive approach yields the best results when large |
priorities in MAFS under different levels matically assigned to them |
packets later and introduces the required interleave between the repair |
stepwise adaptive approach yields the best results when large percentages |
in MAFS under different levels matically assigned to them according |
later and introduces the required interleave between the repair packet |
We expected that the primary cost of managed memory would |
MAFS under different levels matically assigned to them according to |
adaptive approach yields the best results when large percentages of |
and introduces the required interleave between the repair packet and |
expected that the primary cost of managed memory would be |
under different levels matically assigned to them according to the |
approach yields the best results when large percentages of opportunistic |
introduces the required interleave between the repair packet and the |
that the primary cost of managed memory would be associated |
different levels matically assigned to them according to the operation |
yields the best results when large percentages of opportunistic nodes |
the required interleave between the repair packet and the last |
the primary cost of managed memory would be associated with |
levels matically assigned to them according to the operation the |
the best results when large percentages of opportunistic nodes are |
required interleave between the repair packet and the last data |
primary cost of managed memory would be associated with garbage |
matically assigned to them according to the operation the RPC |
best results when large percentages of opportunistic nodes are present |
interleave between the repair packet and the last data packet |
cost of managed memory would be associated with garbage collection |
assigned to them according to the operation the RPC of |
results when large percentages of opportunistic nodes are present in |
between the repair packet and the last data packet included |
to them according to the operation the RPC of bandwidth |
when large percentages of opportunistic nodes are present in the |
the repair packet and the last data packet included in |
all costs associated with managed memory rise in the amount |
them according to the operation the RPC of bandwidth availability |
large percentages of opportunistic nodes are present in the system |
repair packet and the last data packet included in it |
costs associated with managed memory rise in the amount of |
associated with managed memory rise in the amount of allocated |
with managed memory rise in the amount of allocated memory |
Notice that although transmitting the XOR immediately results in faster |
that although transmitting the XOR immediately results in faster recovery |
since it is based only on samples of the download |
it is based only on samples of the download rates |
is based only on samples of the download rates of |
doing so also reduces the probability of a lost packet |
based only on samples of the download rates of nodes |
so also reduces the probability of a lost packet being |
or RPCs to which a file system client that avoids |
also reduces the probability of a lost packet being recovered |
RPCs to which a file system client that avoids switching |
the number of false positives was practically null under all |
Whereas traditional multicast systems accept messages whenever the application layer |
to which a file system client that avoids switching modes |
number of false positives was practically null under all three |
traditional multicast systems accept messages whenever the application layer or |
off results in a minor control knob permitting us to |
which a file system client that avoids switching modes in |
of false positives was practically null under all three strategies |
multicast systems accept messages whenever the application layer or the |
results in a minor control knob permitting us to balance |
a file system client that avoids switching modes in re |
false positives was practically null under all three strategies considered |
systems accept messages whenever the application layer or the multicast |
in a minor control knob permitting us to balance speed |
accept messages whenever the application layer or the multicast protocols |
a minor control knob permitting us to balance speed against |
messages whenever the application layer or the multicast protocols produce |
minor control knob permitting us to balance speed against burst |
or sponse to bandwidth changes is able to adapt to |
whenever the application layer or the multicast protocols produce it |
control knob permitting us to balance speed against burst tolerance |
sponse to bandwidth changes is able to adapt to both |
to bandwidth changes is able to adapt to both insufficient |
bandwidth changes is able to adapt to both insufficient RPCs |
Auditing Costs The overheads imposed by auditing are an important |
Often we can delay generating a message until the last |
changes is able to adapt to both insufficient RPCs whose |
Costs The overheads imposed by auditing are an important consideration |
we can delay generating a message until the last minute |
is able to adapt to both insufficient RPCs whose results |
able to adapt to both insufficient RPCs whose results can |
and we can also avoid situations in which data piles |
Most of the work of auditing is performed by local |
to adapt to both insufficient RPCs whose results can be |
we note that no two repair packets generated at different |
we can also avoid situations in which data piles up |
of the work of auditing is performed by local auditors |
adapt to both insufficient RPCs whose results can be delayed |
note that no two repair packets generated at different interleaves |
can also avoid situations in which data piles up on |
that no two repair packets generated at different interleaves i |
also avoid situations in which data piles up on behalf |
avoid situations in which data piles up on behalf of |
situations in which data piles up on behalf of an |
in which data piles up on behalf of an aggressive |
since nodes only exchange a small amount of accounting data |
which data piles up on behalf of an aggressive sender |
nodes only exchange a small amount of accounting data at |
only exchange a small amount of accounting data at pre |
priority RPC whose results can improve performance if bandwidth is |
RPC whose results can improve performance if bandwidth is high |
will have more than one data packet in common as |
have more than one data packet in common as long |
more than one data packet in common as long as |
Most existing multicast protocols buffer data at many layers and |
than one data packet in common as long as the |
existing multicast protocols buffer data at many layers and cache |
one data packet in common as long as the Least |
multicast protocols buffer data at many layers and cache data |
Asynchronous writeback but can be safely omitted if bandwidth is |
data packet in common as long as the Least Common |
protocols buffer data at many layers and cache data rather |
writeback but can be safely omitted if bandwidth is low |
packet in common as long as the Least Common Multiple |
buffer data at many layers and cache data rather casually |
data at many layers and cache data rather casually for |
at many layers and cache data rather casually for recovery |
MAFS asynchronous writeback is based on similar mechanisms the initial |
seconds the maximum number of packets received and sent by |
many layers and cache data rather casually for recovery purposes |
asynchronous writeback is based on similar mechanisms the initial priority |
the maximum number of packets received and sent by each |
This turns out to be extremely costly in a managed |
writeback is based on similar mechanisms the initial priority is |
pairings of repair bins in two different layers with interleaves |
maximum number of packets received and sent by each node |
turns out to be extremely costly in a managed setting |
is based on similar mechanisms the initial priority is never |
of repair bins in two different layers with interleaves i |
number of packets received and sent by each node is |
out to be extremely costly in a managed setting and |
based on similar mechanisms the initial priority is never modified |
to be extremely costly in a managed setting and must |
be extremely costly in a managed setting and must be |
but the file server somefound in many mobile file systems |
extremely costly in a managed setting and must be avoided |
costly in a managed setting and must be avoided whenever |
in a managed setting and must be avoided whenever possible |
the history needs to indicate which neighbor sent or received |
history needs to indicate which neighbor sent or received the |
needs to indicate which neighbor sent or received the packet |
a good rule of thumb is to select interleaves that |
good rule of thumb is to select interleaves that are |
rule of thumb is to select interleaves that are relatively |
Rather than making times requests an increase in the priority |
of thumb is to select interleaves that are relatively prime |
than making times requests an increase in the priority of |
thumb is to select interleaves that are relatively prime to |
making times requests an increase in the priority of an |
is to select interleaves that are relatively prime to maximize |
times requests an increase in the priority of an RPC |
to select interleaves that are relatively prime to maximize their |
requests an increase in the priority of an RPC to |
select interleaves that are relatively prime to maximize their LCM |
an increase in the priority of an RPC to transmit |
increase in the priority of an RPC to transmit an |
and also ensure that the larger interleave is greater than |
in the priority of an RPC to transmit an RPC |
also ensure that the larger interleave is greater than r |
This is not significant compared to the amount of regular |
the priority of an RPC to transmit an RPC when |
is not significant compared to the amount of regular data |
priority of an RPC to transmit an RPC when an |
not significant compared to the amount of regular data exchanged |
of an RPC to transmit an RPC when an application |
significant compared to the amount of regular data exchanged in |
an RPC to transmit an RPC when an application performs |
compared to the amount of regular data exchanged in a |
RPC to transmit an RPC when an application performs a |
We can recover a data packet if at least one |
to the amount of regular data exchanged in a streaming |
to transmit an RPC when an application performs a metadata |
can recover a data packet if at least one of |
the amount of regular data exchanged in a streaming session |
transmit an RPC when an application performs a metadata update |
recover a data packet if at least one of the |
an RPC when an application performs a metadata update or |
a data packet if at least one of the c |
RPC when an application performs a metadata update or file |
data packet if at least one of the c XORs |
when an application performs a metadata update or file data |
packet if at least one of the c XORs containing |
Global auditors main tasks consist of sampling the system to |
Data paths should have rapid data movement as a key |
if at least one of the c XORs containing it |
auditors main tasks consist of sampling the system to collect |
paths should have rapid data movement as a key goal |
at least one of the c XORs containing it is |
the operation is logged and replayed to the file server |
main tasks consist of sampling the system to collect download |
least one of the c XORs containing it is received |
operation is logged and replayed to the file server after |
tasks consist of sampling the system to collect download and |
one of the c XORs containing it is received correctly |
is logged and replayed to the file server after a |
consist of sampling the system to collect download and upload |
We ve already mentioned that data paths should clear messages |
of the c XORs containing it is received correctly and |
logged and replayed to the file server after a delay |
of sampling the system to collect download and upload rates |
ve already mentioned that data paths should clear messages quickly |
the c XORs containing it is received correctly and usable |
sampling the system to collect download and upload rates of |
This scheme reduces bandwidth utilisation because some logged operations may |
the system to collect download and upload rates of nodes |
scheme reduces bandwidth utilisation because some logged operations may be |
reduces bandwidth utilisation because some logged operations may be superceded |
all the other data packets in it have also been |
bandwidth utilisation because some logged operations may be superceded by |
like behavior or oscillatory throughput can be traced to design |
the other data packets in it have also been received |
utilisation because some logged operations may be superceded by later |
The sample size remains fixed independent of the size of |
behavior or oscillatory throughput can be traced to design decisions |
other data packets in it have also been received correctly |
because some logged operations may be superceded by later ones |
sample size remains fixed independent of the size of the |
or oscillatory throughput can be traced to design decisions that |
size remains fixed independent of the size of the population |
oscillatory throughput can be traced to design decisions that caused |
throughput can be traced to design decisions that caused scheduling |
can be traced to design decisions that caused scheduling jitter |
case standard deviation of the download rates across all nodes |
be traced to design decisions that caused scheduling jitter or |
The probability of a received XOR being unusable is the |
traced to design decisions that caused scheduling jitter or allowed |
probability of a received XOR being unusable is the complement |
to design decisions that caused scheduling jitter or allowed some |
design decisions that caused scheduling jitter or allowed some form |
decisions that caused scheduling jitter or allowed some form of |
that caused scheduling jitter or allowed some form of priority |
caused scheduling jitter or allowed some form of priority inversion |
scheduling jitter or allowed some form of priority inversion to |
jitter or allowed some form of priority inversion to occur |
even a smaller number of samples was found to be |
a smaller number of samples was found to be sufficient |
smaller number of samples was found to be sufficient to |
the probability x of a sent XOR being dropped or |
number of samples was found to be sufficient to yield |
probability x of a sent XOR being dropped or unusable |
of samples was found to be sufficient to yield satisfactory |
x of a sent XOR being dropped or unusable is |
samples was found to be sufficient to yield satisfactory results |
we struggled to make the overall behavior of the system |
of a sent XOR being dropped or unusable is the |
struggled to make the overall behavior of the system as |
a sent XOR being dropped or unusable is the sum |
to make the overall behavior of the system as predictable |
sent XOR being dropped or unusable is the sum of |
and provide a clear advantage for using auditing against tit |
make the overall behavior of the system as predictable as |
XOR being dropped or unusable is the sum of the |
the overall behavior of the system as predictable as possible |
being dropped or unusable is the sum of the probability |
overall behavior of the system as predictable as possible not |
dropped or unusable is the sum of the probability that |
behavior of the system as predictable as possible not a |
or unusable is the sum of the probability that it |
of the system as predictable as possible not a trivial |
unusable is the sum of the probability that it was |
the system as predictable as possible not a trivial task |
is the sum of the probability that it was dropped |
Heterogenous Systems So far we considered the use of auditing |
system as predictable as possible not a trivial task in |
the sum of the probability that it was dropped and |
Systems So far we considered the use of auditing to |
as predictable as possible not a trivial task in configurations |
sum of the probability that it was dropped and the |
So far we considered the use of auditing to enforce |
predictable as possible not a trivial task in configurations where |
of the probability that it was dropped and the probability |
far we considered the use of auditing to enforce node |
as possible not a trivial task in configurations where hundreds |
the probability that it was dropped and the probability that |
we considered the use of auditing to enforce node contribution |
possible not a trivial task in configurations where hundreds of |
probability that it was dropped and the probability that it |
considered the use of auditing to enforce node contribution in |
not a trivial task in configurations where hundreds of processes |
that it was dropped and the probability that it was |
the use of auditing to enforce node contribution in systems |
a trivial task in configurations where hundreds of processes might |
it was dropped and the probability that it was received |
use of auditing to enforce node contribution in systems where |
trivial task in configurations where hundreds of processes might be |
was dropped and the probability that it was received and |
of auditing to enforce node contribution in systems where all |
task in configurations where hundreds of processes might be multicasting |
dropped and the probability that it was received and unusable |
auditing to enforce node contribution in systems where all nodes |
in configurations where hundreds of processes might be multicasting in |
to enforce node contribution in systems where all nodes are |
configurations where hundreds of processes might be multicasting in thousands |
enforce node contribution in systems where all nodes are assumed |
where hundreds of processes might be multicasting in thousands of |
node contribution in systems where all nodes are assumed to |
hundreds of processes might be multicasting in thousands of overlapping |
contribution in systems where all nodes are assumed to have |
of processes might be multicasting in thousands of overlapping groups |
in systems where all nodes are assumed to have homogeneous |
systems where all nodes are assumed to have homogeneous bandwidth |
where all nodes are assumed to have homogeneous bandwidth resources |
By keeping event handlers short and predictable and eliminating the |
keeping event handlers short and predictable and eliminating the need |
enough to upload and download at a rate close to |
event handlers short and predictable and eliminating the need for |
to upload and download at a rate close to the |
handlers short and predictable and eliminating the need for locking |
upload and download at a rate close to the stream |
and download at a rate close to the stream rate |
we obtained a more predictable system and were able to |
obtained a more predictable system and were able to eliminate |
Pullbased streaming may be extended to heterogenous systems by organizing |
a more predictable system and were able to eliminate multithreading |
streaming may be extended to heterogenous systems by organizing nodes |
may be extended to heterogenous systems by organizing nodes into |
be extended to heterogenous systems by organizing nodes into multiple |
extended to heterogenous systems by organizing nodes into multiple groups |
Since it is easy to ensure that no two XORs |
it is easy to ensure that no two XORs share |
is easy to ensure that no two XORs share more |
easy to ensure that no two XORs share more than |
to ensure that no two XORs share more than one |
ensure that no two XORs share more than one data |
that no two XORs share more than one data packet |
one might prefer not to pull in a message until |
might prefer not to pull in a message until QSM |
mostly reads mostly writes heavy load store overhead priorities uniform |
The probability of all the c XORs being dropped or |
prefer not to pull in a message until QSM can |
reads mostly writes heavy load store overhead priorities uniform priorities |
probability of all the c XORs being dropped or unusable |
not to pull in a message until QSM can process |
mostly writes heavy load store overhead priorities uniform priorities uniform |
of all the c XORs being dropped or unusable is |
to pull in a message until QSM can process it |
writes heavy load store overhead priorities uniform priorities uniform synchronous |
all the c XORs being dropped or unusable is xc |
heavy load store overhead priorities uniform priorities uniform synchronous asynchronous |
Execution time speedup Execution time speedup Execution time speedup No |
load store overhead priorities uniform priorities uniform synchronous asynchronous Figure |
the probability of correctly receiving at least one usable XOR |
time speedup Execution time speedup Execution time speedup No priorities |
probability of correctly receiving at least one usable XOR is |
hence message loss rates soar if we leave messages on |
message loss rates soar if we leave messages on input |
Each trace ran with synchronous or asynchronous writes and uniform |
loss rates soar if we leave messages on input sockets |
trace ran with synchronous or asynchronous writes and uniform or |
rates soar if we leave messages on input sockets for |
ran with synchronous or asynchronous writes and uniform or differentiated |
soar if we leave messages on input sockets for long |
with synchronous or asynchronous writes and uniform or differentiated priorities |
The total height of each bar denotes the time from |
total height of each bar denotes the time from the |
height of each bar denotes the time from the first |
of each bar denotes the time from the first to |
each bar denotes the time from the first to last |
bar denotes the time from the first to last write |
and the shaded portion denotes the time from the first |
the shaded portion denotes the time from the first to |
shaded portion denotes the time from the first to last |
portion denotes the time from the first to last read |
form formula only gives us a lower bound on the |
formula only gives us a lower bound on the recovery |
only gives us a lower bound on the recovery probability |
The white portions denote the extra time required to complete |
white portions denote the extra time required to complete all |
since the XOR usability formula does not factor in the |
portions denote the extra time required to complete all writes |
the XOR usability formula does not factor in the probability |
tight control over event processing largely eliminated convoy effects and |
denote the extra time required to complete all writes after |
XOR usability formula does not factor in the probability of |
control over event processing largely eliminated convoy effects and oscillatory |
the extra time required to complete all writes after the |
usability formula does not factor in the probability of the |
over event processing largely eliminated convoy effects and oscillatory throughput |
extra time required to complete all writes after the last |
formula does not factor in the probability of the other |
event processing largely eliminated convoy effects and oscillatory throughput problems |
time required to complete all writes after the last read |
does not factor in the probability of the other data |
required to complete all writes after the last read has |
not factor in the probability of the other data packets |
to complete all writes after the last read has finished |
factor in the probability of the other data packets in |
in the probability of the other data packets in the |
the probability of the other data packets in the XOR |
probability of the other data packets in the XOR being |
Many inefficiencies can be traced to situations in which one |
of the other data packets in the XOR being dropped |
inefficiencies can be traced to situations in which one node |
the other data packets in the XOR being dropped and |
can be traced to situations in which one node takes |
other data packets in the XOR being dropped and recovered |
be traced to situations in which one node takes action |
This shows that the total duration of the trace with |
traced to situations in which one node takes action on |
shows that the total duration of the trace with this |
to situations in which one node takes action on the |
that the total duration of the trace with this MFS |
If the lost data packet was part of a loss |
situations in which one node takes action on the basis |
the total duration of the trace with this MFS configuration |
the lost data packet was part of a loss burst |
in which one node takes action on the basis of |
total duration of the trace with this MFS configuration is |
lost data packet was part of a loss burst of |
which one node takes action on the basis of stale |
data packet was part of a loss burst of size |
one node takes action on the basis of stale state |
packet was part of a loss burst of size b |
node takes action on the basis of stale state information |
takes action on the basis of stale state information from |
repair packets generated at interleaves less than b are dropped |
action on the basis of stale state information from some |
packets generated at interleaves less than b are dropped or |
on the basis of stale state information from some other |
generated at interleaves less than b are dropped or useless |
the basis of stale state information from some other node |
at interleaves less than b are dropped or useless with |
interleaves less than b are dropped or useless with high |
less than b are dropped or useless with high probability |
this is a significant improvement over the alternative configurations measured |
The pull architecture has the secondary benefit of letting us |
pull architecture has the secondary benefit of letting us delay |
architecture has the secondary benefit of letting us delay the |
has the secondary benefit of letting us delay the preparation |
the secondary benefit of letting us delay the preparation of |
secondary benefit of letting us delay the preparation of status |
seconds of the trace are taken up by asynchronously writing |
benefit of letting us delay the preparation of status packets |
of the trace are taken up by asynchronously writing back |
is the number of XORs generated at interleaves greater than |
of letting us delay the preparation of status packets until |
the trace are taken up by asynchronously writing back file |
the number of XORs generated at interleaves greater than b |
letting us delay the preparation of status packets until they |
trace are taken up by asynchronously writing back file updates |
us delay the preparation of status packets until they are |
delay the preparation of status packets until they are about |
In all cases the traces take significantly longer than they |
Minimum and average download factors across all correct nodes when |
the preparation of status packets until they are about to |
since packet losses with more than b intervening packets between |
all cases the traces take significantly longer than they originally |
and average download factors across all correct nodes when using |
preparation of status packets until they are about to be |
packet losses with more than b intervening packets between them |
cases the traces take significantly longer than they originally did |
average download factors across all correct nodes when using different |
of status packets until they are about to be transmitted |
losses with more than b intervening packets between them have |
the traces take significantly longer than they originally did in |
download factors across all correct nodes when using different strategies |
with more than b intervening packets between them have independent |
traces take significantly longer than they originally did in NTFS |
factors across all correct nodes when using different strategies for |
Conclusions The premise of our work is that developers of |
where they were mostly accessing the local file system and |
across all correct nodes when using different strategies for choosing |
more than b intervening packets between them have independent probability |
The premise of our work is that developers of services |
they were mostly accessing the local file system and therefore |
all correct nodes when using different strategies for choosing the |
premise of our work is that developers of services intended |
were mostly accessing the local file system and therefore had |
correct nodes when using different strategies for choosing the threshold |
of our work is that developers of services intended to |
mostly accessing the local file system and therefore had no |
The upload contribution rate of opportunistic nodes is varied in |
our work is that developers of services intended to run |
accessing the local file system and therefore had no bandwidth |
upload contribution rate of opportunistic nodes is varied in the |
work is that developers of services intended to run on |
the local file system and therefore had no bandwidth constraints |
contribution rate of opportunistic nodes is varied in the x |
is that developers of services intended to run on clustered |
that developers of services intended to run on clustered platforms |
developers of services intended to run on clustered platforms desire |
to the extent that the greatest performance improvements are seen |
of services intended to run on clustered platforms desire the |
the extent that the greatest performance improvements are seen at |
services intended to run on clustered platforms desire the productivity |
extent that the greatest performance improvements are seen at low |
c repair packets are generated and sent for every r |
intended to run on clustered platforms desire the productivity and |
that the greatest performance improvements are seen at low bandwidth |
repair packets are generated and sent for every r data |
to run on clustered platforms desire the productivity and robustness |
the greatest performance improvements are seen at low bandwidth when |
packets are generated and sent for every r data packets |
run on clustered platforms desire the productivity and robustness benefits |
greatest performance improvements are seen at low bandwidth when there |
on clustered platforms desire the productivity and robustness benefits of |
and the correct delivery of any r of the r |
performance improvements are seen at low bandwidth when there is |
clustered platforms desire the productivity and robustness benefits of managed |
improvements are seen at low bandwidth when there is high |
c packets transmitted is sufficient to reconstruct the original r |
platforms desire the productivity and robustness benefits of managed environments |
are seen at low bandwidth when there is high read |
packets transmitted is sufficient to reconstruct the original r data |
transmitted is sufficient to reconstruct the original r data packets |
Building such tools so posed challenges to us as protocol |
such tools so posed challenges to us as protocol and |
tools so posed challenges to us as protocol and system |
so posed challenges to us as protocol and system designers |
we can recover it if at least r packets are |
can recover it if at least r packets are received |
recover it if at least r packets are received correctly |
decrease in the time spent to read all the files |
it if at least r packets are received correctly in |
performance protocols running in managed settings need to maintain the |
if at least r packets are received correctly in the |
protocols running in managed settings need to maintain the smallest |
at least r packets are received correctly in the encoding |
running in managed settings need to maintain the smallest possible |
least r packets are received correctly in the encoding set |
in managed settings need to maintain the smallest possible memory |
r packets are received correctly in the encoding set of |
managed settings need to maintain the smallest possible memory footprint |
packets are received correctly in the encoding set of r |
c data and repair packets that the lost packet belongs |
data and repair packets that the lost packet belongs to |
The mostlyreads trace is not much affected by changes in |
the probability of recovering a lost packet is equivalent to |
mostlyreads trace is not much affected by changes in the |
probability of recovering a lost packet is equivalent to the |
trace is not much affected by changes in the configuration |
QSM achieves scalability and stability even at very high loads |
of recovering a lost packet is equivalent to the probability |
although there is a slight decrease in both read and |
recovering a lost packet is equivalent to the probability of |
An unexpected side effect of building QSM in Windows was |
there is a slight decrease in both read and write |
a lost packet is equivalent to the probability of losing |
unexpected side effect of building QSM in Windows was that |
is a slight decrease in both read and write times |
lost packet is equivalent to the probability of losing c |
side effect of building QSM in Windows was that by |
a slight decrease in both read and write times for |
effect of building QSM in Windows was that by integrating |
slight decrease in both read and write times for prioritised |
of building QSM in Windows was that by integrating our |
decrease in both read and write times for prioritised asynchronous |
building QSM in Windows was that by integrating our system |
Since the number of other lost packets in the XOR |
in both read and write times for prioritised asynchronous writeback |
QSM in Windows was that by integrating our system tightly |
the number of other lost packets in the XOR is |
in Windows was that by integrating our system tightly with |
number of other lost packets in the XOR is a |
Windows was that by integrating our system tightly with the |
of other lost packets in the XOR is a random |
was that by integrating our system tightly with the platform |
other lost packets in the XOR is a random variable |
lost packets in the XOR is a random variable Y |
packets in the XOR is a random variable Y and |
in the XOR is a random variable Y and has |
the XOR is a random variable Y and has a |
XOR is a random variable Y and has a binomial |
is a random variable Y and has a binomial distribution |
we once again attribute this to inefficiency in the RPC |
a random variable Y and has a binomial distribution with |
once again attribute this to inefficiency in the RPC protocol |
random variable Y and has a binomial distribution with parameters |
such an object changes faster than the average Windows object |
since under extremely heavy load and high bandwidth it performs |
under extremely heavy load and high bandwidth it performs better |
extremely heavy load and high bandwidth it performs better when |
heavy load and high bandwidth it performs better when all |
load and high bandwidth it performs better when all messages |
and high bandwidth it performs better when all messages have |
high bandwidth it performs better when all messages have the |
bandwidth it performs better when all messages have the same |
it performs better when all messages have the same priority |
A file group is implemented as a special type of |
QSM should eventually enable casual use of live objects not |
file group is implemented as a special type of file |
should eventually enable casual use of live objects not just |
group is implemented as a special type of file within |
eventually enable casual use of live objects not just in |
is implemented as a special type of file within the |
enable casual use of live objects not just in datacenters |
implemented as a special type of file within the MFS |
casual use of live objects not just in datacenters but |
as a special type of file within the MFS file |
use of live objects not just in datacenters but also |
a special type of file within the MFS file system |
we plot the recovery probability curves for Layered Interleaving and |
of live objects not just in datacenters but also on |
plot the recovery probability curves for Layered Interleaving and Reed |
live objects not just in datacenters but also on desktops |
objects not just in datacenters but also on desktops in |
not just in datacenters but also on desktops in WAN |
just in datacenters but also on desktops in WAN settings |
opening the door to a new style of distributed programming |
The MFS prefetching subsystem derives much of its effectiveness from |
MFS prefetching subsystem derives much of its effectiveness from being |
The current version of QSM is stable in cluster settings |
prefetching subsystem derives much of its effectiveness from being combined |
current version of QSM is stable in cluster settings and |
subsystem derives much of its effectiveness from being combined with |
derives much of its effectiveness from being combined with prioritised |
note that the curves are very close to each other |
much of its effectiveness from being combined with prioritised RPCs |
Minimum and average download factors across all correct nodes when |
and average download factors across all correct nodes when using |
average download factors across all correct nodes when using different |
download factors across all correct nodes when using different strategies |
it can still make bad decisions without a large overall |
factors across all correct nodes when using different strategies for |
can still make bad decisions without a large overall performance |
and to introduce a gossip infrastructure that would support configuration |
across all correct nodes when using different strategies for choosing |
still make bad decisions without a large overall performance penalty |
to introduce a gossip infrastructure that would support configuration discovery |
all correct nodes when using different strategies for choosing the |
make bad decisions without a large overall performance penalty because |
introduce a gossip infrastructure that would support configuration discovery and |
Local Recovery for Receiver Loss In the absence of intelligent |
correct nodes when using different strategies for choosing the threshold |
bad decisions without a large overall performance penalty because the |
a gossip infrastructure that would support configuration discovery and other |
Recovery for Receiver Loss In the absence of intelligent flow |
decisions without a large overall performance penalty because the interference |
gossip infrastructure that would support configuration discovery and other self |
for Receiver Loss In the absence of intelligent flow control |
without a large overall performance penalty because the interference of |
and percentage of opportunistic nodes is varied on the x |
Receiver Loss In the absence of intelligent flow control mechanisms |
a large overall performance penalty because the interference of prefetching |
Loss In the absence of intelligent flow control mechanisms like |
large overall performance penalty because the interference of prefetching with |
In the absence of intelligent flow control mechanisms like TCP |
they give rise to irregular patterns of overlapping multicast groups |
overall performance penalty because the interference of prefetching with other |
nodes able to upload at a rate higher than the |
performance penalty because the interference of prefetching with other file |
able to upload at a rate higher than the stream |
penalty because the interference of prefetching with other file system |
to upload at a rate higher than the stream rate |
because the interference of prefetching with other file system activity |
hosts can be easily overwhelmed and drop packets during traffic |
upload at a rate higher than the stream rate are |
the interference of prefetching with other file system activity is |
can be easily overwhelmed and drop packets during traffic spikes |
recovery would be performed by selecting a subset of nodes |
at a rate higher than the stream rate are placed |
interference of prefetching with other file system activity is minimised |
be easily overwhelmed and drop packets during traffic spikes or |
would be performed by selecting a subset of nodes that |
a rate higher than the stream rate are placed in |
easily overwhelmed and drop packets during traffic spikes or CPU |
be performed by selecting a subset of nodes that form |
In the same way that some local file systems execute |
rate higher than the stream rate are placed in higher |
performed by selecting a subset of nodes that form a |
the same way that some local file systems execute speculative |
by selecting a subset of nodes that form a clean |
same way that some local file systems execute speculative operations |
selecting a subset of nodes that form a clean overlay |
The source sends data to the highest level group only |
way that some local file systems execute speculative operations to |
a subset of nodes that form a clean overlay structure |
that some local file systems execute speculative operations to improve |
who uses the basic protocol to disseminate data among each |
some local file systems execute speculative operations to improve performance |
rather than just treating every single receiver as a member |
uses the basic protocol to disseminate data among each other |
than just treating every single receiver as a member of |
just treating every single receiver as a member of a |
Nodes in lower levels may receive data at smaller rates |
for example would ordinarily go back to the sender to |
treating every single receiver as a member of a recovery |
example would ordinarily go back to the sender to retrieve |
MFS makes use of the speculative communication of prioritised RPCs |
every single receiver as a member of a recovery region |
would ordinarily go back to the sender to retrieve the |
makes use of the speculative communication of prioritised RPCs in |
level nodes may be used to act as sources to |
ordinarily go back to the sender to retrieve the lost |
use of the speculative communication of prioritised RPCs in the |
nodes may be used to act as sources to the |
go back to the sender to retrieve the lost packet |
of the speculative communication of prioritised RPCs in the hope |
may be used to act as sources to the lower |
the speculative communication of prioritised RPCs in the hope of |
even though it was dropped at the receiver after covering |
speculative communication of prioritised RPCs in the hope of achieving |
though it was dropped at the receiver after covering the |
communication of prioritised RPCs in the hope of achieving a |
it was dropped at the receiver after covering the entire |
Auditing can be used to avoid the presence of opportunistic |
of prioritised RPCs in the hope of achieving a benefit |
was dropped at the receiver after covering the entire geographical |
can be used to avoid the presence of opportunistic and |
prioritised RPCs in the hope of achieving a benefit through |
dropped at the receiver after covering the entire geographical distance |
be used to avoid the presence of opportunistic and lower |
RPCs in the hope of achieving a benefit through prefetching |
used to avoid the presence of opportunistic and lower bandwidth |
in the hope of achieving a benefit through prefetching files |
to avoid the presence of opportunistic and lower bandwidth nodes |
storing incoming packets for a short period of time and |
avoid the presence of opportunistic and lower bandwidth nodes in |
incoming packets for a short period of time and providing |
the presence of opportunistic and lower bandwidth nodes in the |
packets for a short period of time and providing hooks |
presence of opportunistic and lower bandwidth nodes in the higher |
bandwidth is high enough to eliminate differences between writeback schemes |
for a short period of time and providing hooks that |
MFS prefetching implementation The MFS cache manager incorporates a small |
a short period of time and providing hooks that allow |
prefetching implementation The MFS cache manager incorporates a small prefetching |
It can ensure that the hierarchy of nodes is obeyed |
and priorTwo questions are of particular interest in evaluating the |
implementation The MFS cache manager incorporates a small prefetching module |
short period of time and providing hooks that allow protocols |
can ensure that the hierarchy of nodes is obeyed by |
priorTwo questions are of particular interest in evaluating the perfor |
period of time and providing hooks that allow protocols to |
Design and Implementation of a Reliable Group Communication Toolkit for |
ities are advantageous in reducing contention between reading mance of |
of time and providing hooks that allow protocols to first |
ensure that the hierarchy of nodes is obeyed by all |
and Implementation of a Reliable Group Communication Toolkit for Java |
are advantageous in reducing contention between reading mance of MAFS |
a prefetching thread starts and initiates prefetch requests in parallel |
time and providing hooks that allow protocols to first query |
that the hierarchy of nodes is obeyed by all nodes |
advantageous in reducing contention between reading mance of MAFS communication |
prefetching thread starts and initiates prefetch requests in parallel with |
and providing hooks that allow protocols to first query the |
in reducing contention between reading mance of MAFS communication adaptation |
while allowing the system to leverage additional resources from privileged |
thread starts and initiates prefetch requests in parallel with the |
providing hooks that allow protocols to first query the cache |
allowing the system to leverage additional resources from privileged altruistic |
starts and initiates prefetch requests in parallel with the main |
hooks that allow protocols to first query the cache to |
the system to leverage additional resources from privileged altruistic nodes |
and initiates prefetch requests in parallel with the main activity |
Do priorities improve performance by reducing RPC conThe second microbenchmark |
system to leverage additional resources from privileged altruistic nodes to |
that allow protocols to first query the cache to locate |
initiates prefetch requests in parallel with the main activity of |
priorities improve performance by reducing RPC conThe second microbenchmark evaluates |
to leverage additional resources from privileged altruistic nodes to forward |
allow protocols to first query the cache to locate missing |
prefetch requests in parallel with the main activity of the |
improve performance by reducing RPC conThe second microbenchmark evaluates a |
leverage additional resources from privileged altruistic nodes to forward data |
protocols to first query the cache to locate missing packets |
requests in parallel with the main activity of the cache |
performance by reducing RPC conThe second microbenchmark evaluates a workload |
additional resources from privileged altruistic nodes to forward data to |
to first query the cache to locate missing packets before |
in parallel with the main activity of the cache manager |
by reducing RPC conThe second microbenchmark evaluates a workload that |
resources from privileged altruistic nodes to forward data to lower |
first query the cache to locate missing packets before sending |
reducing RPC conThe second microbenchmark evaluates a workload that contention |
from privileged altruistic nodes to forward data to lower level |
The core component of the cache manager alerts the prefetching |
query the cache to locate missing packets before sending retransmission |
privileged altruistic nodes to forward data to lower level groups |
core component of the cache manager alerts the prefetching module |
Is it possible to combine the benefit of asynchronous write |
the cache to locate missing packets before sending retransmission requests |
component of the cache manager alerts the prefetching module every |
cache to locate missing packets before sending retransmission requests back |
of the cache manager alerts the prefetching module every time |
to locate missing packets before sending retransmission requests back to |
the cache manager alerts the prefetching module every time an |
locate missing packets before sending retransmission requests back to the |
cache manager alerts the prefetching module every time an application |
missing packets before sending retransmission requests back to the sender |
manager alerts the prefetching module every time an application reads |
one process performs a grep on a set of back |
alerts the prefetching module every time an application reads or |
process performs a grep on a set of back at |
Future versions of Maelstrom could potentially use knowledge of protocol |
the prefetching module every time an application reads or writes |
performs a grep on a set of back at low |
versions of Maelstrom could potentially use knowledge of protocol internals |
prefetching module every time an application reads or writes a |
a grep on a set of back at low bandwidth |
relied on approaches based on pushing data through a single |
of Maelstrom could potentially use knowledge of protocol internals to |
module every time an application reads or writes a file |
grep on a set of back at low bandwidth with |
on approaches based on pushing data through a single dissemination |
Maelstrom could potentially use knowledge of protocol internals to transparently |
on a set of back at low bandwidth with acceptable |
approaches based on pushing data through a single dissemination tree |
could potentially use knowledge of protocol internals to transparently intervene |
This routine checks whether the file belongs to a file |
a set of back at low bandwidth with acceptable performance |
Later approaches focused on improving fairness among peers and resilience |
routine checks whether the file belongs to a file group |
set of back at low bandwidth with acceptable performance at |
approaches focused on improving fairness among peers and resilience to |
by intercepting and satisfying retransmission requests sent by the receiver |
checks whether the file belongs to a file group if |
of back at low bandwidth with acceptable performance at cached |
focused on improving fairness among peers and resilience to churn |
intercepting and satisfying retransmission requests sent by the receiver in |
whether the file belongs to a file group if not |
back at low bandwidth with acceptable performance at cached files |
on improving fairness among peers and resilience to churn by |
and satisfying retransmission requests sent by the receiver in a |
at low bandwidth with acceptable performance at cached files that |
improving fairness among peers and resilience to churn by breaking |
satisfying retransmission requests sent by the receiver in a NAK |
low bandwidth with acceptable performance at cached files that need |
fairness among peers and resilience to churn by breaking data |
the group is put at the head of the prefetch |
bandwidth with acceptable performance at cached files that need to |
among peers and resilience to churn by breaking data into |
group is put at the head of the prefetch list |
or by resending packets when acknowledgments are not observed within |
with acceptable performance at cached files that need to be |
peers and resilience to churn by breaking data into multiple |
by resending packets when acknowledgments are not observed within a |
The prefetch thread periodically examines the Prefetching is commonly used |
acceptable performance at cached files that need to be validated |
and resilience to churn by breaking data into multiple substreams |
resending packets when acknowledgments are not observed within a certain |
prefetch thread periodically examines the Prefetching is commonly used to |
performance at cached files that need to be validated before |
resilience to churn by breaking data into multiple substreams and |
packets when acknowledgments are not observed within a certain time |
thread periodically examines the Prefetching is commonly used to improve |
at cached files that need to be validated before they |
to churn by breaking data into multiple substreams and sending |
when acknowledgments are not observed within a certain time period |
periodically examines the Prefetching is commonly used to improve the |
cached files that need to be validated before they can |
IEEE Communications Magazine feature topic issue on Distributed Object Computing |
churn by breaking data into multiple substreams and sending them |
acknowledgments are not observed within a certain time period in |
examines the Prefetching is commonly used to improve the performance |
files that need to be validated before they can be |
by breaking data into multiple substreams and sending them along |
are not observed within a certain time period in an |
the Prefetching is commonly used to improve the performance of |
that need to be validated before they can be opened |
breaking data into multiple substreams and sending them along disjoing |
not observed within a certain time period in an ACK |
Prefetching is commonly used to improve the performance of lo |
data into multiple substreams and sending them along disjoing paths |
If the group file for the group is cal file |
the group file for the group is cal file systems |
Implementation Details We initially implemented and evaluated Maelstrom as a |
Details We initially implemented and evaluated Maelstrom as a user |
Performance turned out to be limited by copying and context |
Then it scans the in a file system with whole |
GrepWe compare MAFS to alternative approaches in two sets of |
and we subsequently reimplemented the system as a module that |
compare MAFS to alternative approaches in two sets of compile |
we subsequently reimplemented the system as a module that runs |
a mechanism is required files in the group in order |
subsequently reimplemented the system as a module that runs within |
and peers organized into a mesh request packets from their |
mechanism is required files in the group in order until |
reimplemented the system as a module that runs within the |
peers organized into a mesh request packets from their neighbors |
is required files in the group in order until it |
the system as a module that runs within the Linux |
Hierarchical Clustering of Message Flows in a Multicast Data Dissemination |
organized into a mesh request packets from their neighbors using |
microbenchmarks to measure execution time time as another is writing |
Clustering of Message Flows in a Multicast Data Dissemination System |
required files in the group in order until it finds |
into a mesh request packets from their neighbors using a |
to measure execution time time as another is writing files |
files in the group in order until it finds the |
a mesh request packets from their neighbors using a scheduling |
in the group in order until it finds the first |
mesh request packets from their neighbors using a scheduling algorithm |
the group in order until it finds the first one |
group in order until it finds the first one which |
in order until it finds the first one which is |
order until it finds the first one which is not |
until it finds the first one which is not to |
and traces of actual Windows ties are beneficial for the |
it finds the first one which is not to determine |
randomly fetching them while respecting a maximum limit on the |
traces of actual Windows ties are beneficial for the small |
the experimental prototype of the kernel version reaches output speeds |
fetching them while respecting a maximum limit on the number |
finds the first one which is not to determine appropriate |
of actual Windows ties are beneficial for the small validation |
experimental prototype of the kernel version reaches output speeds close |
them while respecting a maximum limit on the number of |
the first one which is not to determine appropriate prefetching |
actual Windows ties are beneficial for the small validation RPCs |
Proceedings of the International Conference on Dependable Systems and Networks |
prototype of the kernel version reaches output speeds close to |
while respecting a maximum limit on the number of outstanding |
first one which is not to determine appropriate prefetching hints |
Windows ties are beneficial for the small validation RPCs when |
respecting a maximum limit on the number of outstanding requests |
ties are beneficial for the small validation RPCs when the |
limited only by the capacity of the outbound network card |
a maximum limit on the number of outstanding requests to |
are beneficial for the small validation RPCs when the backNT |
maximum limit on the number of outstanding requests to each |
and issues a prefetch request or system prefetching has used |
beneficial for the small validation RPCs when the backNT file |
limit on the number of outstanding requests to each neighbor |
issues a prefetch request or system prefetching has used clustering |
for the small validation RPCs when the backNT file system |
Chainsaw presents smaller delays for the receipt of packets compared |
a prefetch request or system prefetching has used clustering to |
presents smaller delays for the receipt of packets compared to |
prefetch request or system prefetching has used clustering to derive |
smaller delays for the receipt of packets compared to the |
request or system prefetching has used clustering to derive file |
delays for the receipt of packets compared to the Coolstreaming |
or system prefetching has used clustering to derive file groups |
With the sporadic background traffic in the Cornell University Computer |
for the receipt of packets compared to the Coolstreaming protocol |
system prefetching has used clustering to derive file groups from |
the sporadic background traffic in the Cornell University Computer Science |
prefetching has used clustering to derive file groups from validation |
sporadic background traffic in the Cornell University Computer Science Department |
has used clustering to derive file groups from validation request |
Traffic would be distributed over such a rack by partitioning |
used clustering to derive file groups from validation request for |
would be distributed over such a rack by partitioning the |
clustering to derive file groups from validation request for it |
based approaches are shown to present better performance over tree |
improvements are confined to low bandcontain access to local and |
be distributed over such a rack by partitioning the address |
If all the files are valid and are in the |
are confined to low bandcontain access to local and remote |
distributed over such a rack by partitioning the address space |
all the files are valid and are in the cache |
Previous papers have considered a variety of possible mechanisms to |
confined to low bandcontain access to local and remote file |
over such a rack by partitioning the address space of |
the files are valid and are in the cache access |
papers have considered a variety of possible mechanisms to encourage |
to low bandcontain access to local and remote file systems |
such a rack by partitioning the address space of the |
files are valid and are in the cache access statistics |
have considered a variety of possible mechanisms to encourage node |
low bandcontain access to local and remote file systems by |
a rack by partitioning the address space of the remote |
considered a variety of possible mechanisms to encourage node contribution |
bandcontain access to local and remote file systems by clients |
rack by partitioning the address space of the remote data |
access to local and remote file systems by clients in |
by partitioning the address space of the remote data center |
to local and remote file systems by clients in a |
is a framework proposed to enforce download rate limitations on |
partitioning the address space of the remote data center and |
local and remote file systems by clients in a width |
the group is moved to the end of the prefetch |
a framework proposed to enforce download rate limitations on P |
the address space of the remote data center and routing |
and remote file systems by clients in a width levels |
group is moved to the end of the prefetch list |
address space of the remote data center and routing different |
The protocol relies on a set of trusted nodes that |
space of the remote data center and routing different segments |
protocol relies on a set of trusted nodes that store |
of the remote data center and routing different segments of |
relies on a set of trusted nodes that store information |
the remote data center and routing different segments of the |
on a set of trusted nodes that store information on |
remote data center and routing different segments of the space |
a set of trusted nodes that store information on the |
data center and routing different segments of the space through |
set of trusted nodes that store information on the data |
center and routing different segments of the space through distinct |
the thread rechecks the head of the list ing hints |
of trusted nodes that store information on the data downloaded |
and routing different segments of the space through distinct Maelstrom |
thread rechecks the head of the list ing hints explicitly |
trusted nodes that store information on the data downloaded by |
routing different segments of the space through distinct Maelstrom appliance |
priority read performance with only a small overhead for writes |
nodes that store information on the data downloaded by each |
different segments of the space through distinct Maelstrom appliance pairs |
that store information on the data downloaded by each node |
store information on the data downloaded by each node receiving |
these microbenchmarks show that asynMicrobenchmarks chronous writeback improves performance even |
information on the data downloaded by each node receiving data |
microbenchmarks show that asynMicrobenchmarks chronous writeback improves performance even at |
show that asynMicrobenchmarks chronous writeback improves performance even at comparaOur |
Nodes only send an object after consulting the trusted nodes |
that asynMicrobenchmarks chronous writeback improves performance even at comparaOur first |
only send an object after consulting the trusted nodes to |
asynMicrobenchmarks chronous writeback improves performance even at comparaOur first microbenchmark |
send an object after consulting the trusted nodes to verify |
file dependencies can also be used as a source of |
chronous writeback improves performance even at comparaOur first microbenchmark compiles |
an object after consulting the trusted nodes to verify if |
dependencies can also be used as a source of hints |
writeback improves performance even at comparaOur first microbenchmark compiles MAFS |
balancing schemes that might vary the IP address space partitioning |
object after consulting the trusted nodes to verify if the |
improves performance even at comparaOur first microbenchmark compiles MAFS from |
head of the list as a result of further application |
schemes that might vary the IP address space partitioning dynamically |
after consulting the trusted nodes to verify if the nodes |
of the list as a result of further application accesses |
that might vary the IP address space partitioning dynamically to |
consulting the trusted nodes to verify if the nodes requesting |
the list as a result of further application accesses to |
might vary the IP address space partitioning dynamically to spread |
the trusted nodes to verify if the nodes requesting the |
list as a result of further application accesses to files |
and priorities are effective in mitigating source code stored in |
vary the IP address space partitioning dynamically to spread the |
trusted nodes to verify if the nodes requesting the stream |
priorities are effective in mitigating source code stored in an |
the IP address space partitioning dynamically to spread the encoding |
nodes to verify if the nodes requesting the stream are |
it may be known that a certain shared library is |
are effective in mitigating source code stored in an MAFS |
IP address space partitioning dynamically to spread the encoding load |
to verify if the nodes requesting the stream are not |
may be known that a certain shared library is rePrefetch |
effective in mitigating source code stored in an MAFS filesystem |
address space partitioning dynamically to spread the encoding load over |
verify if the nodes requesting the stream are not overrequesting |
be known that a certain shared library is rePrefetch requests |
space partitioning dynamically to spread the encoding load over multiple |
if the nodes requesting the stream are not overrequesting data |
known that a certain shared library is rePrefetch requests are |
partitioning dynamically to spread the encoding load over multiple machines |
that a certain shared library is rePrefetch requests are similar |
It is targeted to systems where nodes upload full media |
a certain shared library is rePrefetch requests are similar to |
is targeted to systems where nodes upload full media objects |
certain shared library is rePrefetch requests are similar to regular |
targeted to systems where nodes upload full media objects from |
shared library is rePrefetch requests are similar to regular fetch |
compares the execution time speedup for the benchmark under differing |
to systems where nodes upload full media objects from each |
library is rePrefetch requests are similar to regular fetch requests |
the execution time speedup for the benchmark under differing asynchronous |
systems where nodes upload full media objects from each other |
is rePrefetch requests are similar to regular fetch requests for |
execution time speedup for the benchmark under differing asynchronous writeback |
rePrefetch requests are similar to regular fetch requests for files |
time speedup for the benchmark under differing asynchronous writeback and |
streaming systems where all nodes are interested in receiving the |
speedup for the benchmark under differing asynchronous writeback and priority |
systems where all nodes are interested in receiving the exact |
for the benchmark under differing asynchronous writeback and priority schemes |
in this case it would be advantageous with the exception |
where all nodes are interested in receiving the exact same |
this case it would be advantageous with the exception that |
all nodes are interested in receiving the exact same data |
case it would be advantageous with the exception that they |
We evaluated MAFS at a larger scale using the NTFS |
nodes are interested in receiving the exact same data in |
it would be advantageous with the exception that they are |
are interested in receiving the exact same data in close |
Each proxy acts both as an ingress and egress router |
would be advantageous with the exception that they are issued |
interested in receiving the exact same data in close to |
proxy acts both as an ingress and egress router at |
be advantageous with the exception that they are issued at |
in receiving the exact same data in close to real |
acts both as an ingress and egress router at the |
Although the original execution back is beneficial at all bandwidths |
advantageous with the exception that they are issued at the |
receiving the exact same data in close to real time |
both as an ingress and egress router at the same |
the original execution back is beneficial at all bandwidths until |
with the exception that they are issued at the lowest |
as an ingress and egress router at the same time |
the exception that they are issued at the lowest level |
an ingress and egress router at the same time since |
exception that they are issued at the lowest level of |
There is less times of these traces were short on |
ingress and egress router at the same time since they |
that they are issued at the lowest level of prito |
is less times of these traces were short on Windows |
and egress router at the same time since they handle |
they are issued at the lowest level of prito retrieve |
less times of these traces were short on Windows NT |
egress router at the same time since they handle duplex |
The authors present mechanisms that rank peers according to their |
are issued at the lowest level of prito retrieve the |
router at the same time since they handle duplex traffic |
authors present mechanisms that rank peers according to their level |
issued at the lowest level of prito retrieve the shared |
at the same time since they handle duplex traffic in |
present mechanisms that rank peers according to their level of |
at the lowest level of prito retrieve the shared library |
the same time since they handle duplex traffic in the |
mechanisms that rank peers according to their level of cooperation |
the lowest level of prito retrieve the shared library from |
same time since they handle duplex traffic in the following |
that rank peers according to their level of cooperation with |
trol traffic and the delay in fetching files become dominating |
lowest level of prito retrieve the shared library from the |
time since they handle duplex traffic in the following manner |
rank peers according to their level of cooperation with the |
traffic and the delay in fetching files become dominating Figure |
level of prito retrieve the shared library from the server |
peers according to their level of cooperation with the system |
The egress router captures IP packets and creates redundant FEC |
of prito retrieve the shared library from the server as |
shows execution times under four combinations of writeback scheme and |
egress router captures IP packets and creates redundant FEC packets |
One of their techniques involves the reconstruction of trees as |
prito retrieve the shared library from the server as well |
execution times under four combinations of writeback scheme and priorities |
of their techniques involves the reconstruction of trees as a |
The original IP packets are routed through unaltered as they |
retrieve the shared library from the server as well as |
their techniques involves the reconstruction of trees as a way |
original IP packets are routed through unaltered as they would |
the shared library from the server as well as retriev |
techniques involves the reconstruction of trees as a way of |
IP packets are routed through unaltered as they would have |
involves the reconstruction of trees as a way of punishing |
packets are routed through unaltered as they would have been |
all other RPC traffic takes precedence over a prefetch RPC |
the reconstruction of trees as a way of punishing opportunistic |
are routed through unaltered as they would have been originally |
reconstruction of trees as a way of punishing opportunistic nodes |
the redundant packets are then forwarded to the remote ingress |
Most of their mechanisms require peers to keep track of |
redundant packets are then forwarded to the remote ingress router |
of their mechanisms require peers to keep track of their |
packets are then forwarded to the remote ingress router via |
their mechanisms require peers to keep track of their parents |
are then forwarded to the remote ingress router via a |
mechanisms require peers to keep track of their parents and |
then forwarded to the remote ingress router via a UDP |
and only one tion such as the operating system s |
require peers to keep track of their parents and children |
forwarded to the remote ingress router via a UDP channel |
only one tion such as the operating system s database |
peers to keep track of their parents and children s |
one tion such as the operating system s database of |
The ingress router captures and stores IP packets coming from |
to keep track of their parents and children s behavior |
tion such as the operating system s database of installed |
ingress router captures and stores IP packets coming from the |
such as the operating system s database of installed software |
router captures and stores IP packets coming from the direction |
as the operating system s database of installed software prefetch |
studied the effect of different types of incentives on the |
captures and stores IP packets coming from the direction of |
the operating system s database of installed software prefetch is |
the effect of different types of incentives on the Chainsaw |
and stores IP packets coming from the direction of the |
operating system s database of installed software prefetch is made |
effect of different types of incentives on the Chainsaw protocol |
stores IP packets coming from the direction of the egress |
system s database of installed software prefetch is made at |
IP packets coming from the direction of the egress router |
s database of installed software prefetch is made at a |
database of installed software prefetch is made at a time |
an IP packet is recovered if there is an opportunity |
IP packet is recovered if there is an opportunity to |
packet is recovered if there is an opportunity to do |
the authors propose an algorithm that sets up local markets |
is recovered if there is an opportunity to do so |
authors propose an algorithm that sets up local markets at |
propose an algorithm that sets up local markets at every |
Redundant packets that can be used at a later time |
an algorithm that sets up local markets at every node |
the benefits initiating multiple concurrent prefetches from differAny of these |
packets that can be used at a later time are |
benefits initiating multiple concurrent prefetches from differAny of these techniques |
that can be used at a later time are stored |
initiating multiple concurrent prefetches from differAny of these techniques could |
multiple concurrent prefetches from differAny of these techniques could be |
If the redundant packet is useless it is immediately discarded |
concurrent prefetches from differAny of these techniques could be used |
prefetches from differAny of these techniques could be used to |
edu Abstract Cloud computing provides us with general purpose storage |
Upon recovery the IP packet is sent through a raw |
The results indicate that the proposed algorithm improves the performance |
from differAny of these techniques could be used to derive |
Abstract Cloud computing provides us with general purpose storage and |
recovery the IP packet is sent through a raw socket |
results indicate that the proposed algorithm improves the performance of |
differAny of these techniques could be used to derive hints |
Cloud computing provides us with general purpose storage and server |
the IP packet is sent through a raw socket to |
indicate that the proposed algorithm improves the performance of the |
of these techniques could be used to derive hints for |
computing provides us with general purpose storage and server hosting |
IP packet is sent through a raw socket to its |
that the proposed algorithm improves the performance of the system |
these techniques could be used to derive hints for use |
provides us with general purpose storage and server hosting platforms |
packet is sent through a raw socket to its intended |
the proposed algorithm improves the performance of the system when |
techniques could be used to derive hints for use ent |
us with general purpose storage and server hosting platforms at |
is sent through a raw socket to its intended destination |
proposed algorithm improves the performance of the system when the |
could be used to derive hints for use ent servers |
with general purpose storage and server hosting platforms at a |
algorithm improves the performance of the system when the total |
Using FEC requires that each data packet have a unique |
general purpose storage and server hosting platforms at a reasonable |
improves the performance of the system when the total upload |
FEC requires that each data packet have a unique identifier |
purpose storage and server hosting platforms at a reasonable price |
the performance of the system when the total upload capacity |
requires that each data packet have a unique identifier that |
MFS does not currently make use of timeouts by the |
performance of the system when the total upload capacity is |
We explore the possibility of tapping these resources for the |
that each data packet have a unique identifier that the |
does not currently make use of timeouts by the MFS |
of the system when the total upload capacity is not |
explore the possibility of tapping these resources for the purpose |
each data packet have a unique identifier that the receiver |
not currently make use of timeouts by the MFS prefetching |
the system when the total upload capacity is not enough |
the possibility of tapping these resources for the purpose of |
data packet have a unique identifier that the receiver can |
currently make use of timeouts by the MFS prefetching subsystem |
system when the total upload capacity is not enough to |
possibility of tapping these resources for the purpose of hosting |
packet have a unique identifier that the receiver can use |
when the total upload capacity is not enough to supply |
of tapping these resources for the purpose of hosting source |
have a unique identifier that the receiver can use to |
the total upload capacity is not enough to supply all |
tapping these resources for the purpose of hosting source code |
a unique identifier that the receiver can use to keep |
total upload capacity is not enough to supply all the |
which is inaccurate in some tended to abandon a prefetching |
these resources for the purpose of hosting source code repositories |
unique identifier that the receiver can use to keep track |
upload capacity is not enough to supply all the nodes |
is inaccurate in some tended to abandon a prefetching attempt |
resources for the purpose of hosting source code repositories for |
identifier that the receiver can use to keep track of |
inaccurate in some tended to abandon a prefetching attempt that |
for the purpose of hosting source code repositories for individual |
that the receiver can use to keep track of received |
in some tended to abandon a prefetching attempt that does |
streaming system where nodes choose their neighbors based on their |
the receiver can use to keep track of received data |
the purpose of hosting source code repositories for individual projects |
some tended to abandon a prefetching attempt that does not |
system where nodes choose their neighbors based on their history |
receiver can use to keep track of received data packets |
purpose of hosting source code repositories for individual projects as |
tended to abandon a prefetching attempt that does not complete |
where nodes choose their neighbors based on their history of |
can use to keep track of received data packets and |
of hosting source code repositories for individual projects as well |
to abandon a prefetching attempt that does not complete cases |
nodes choose their neighbors based on their history of interaction |
use to keep track of received data packets and to |
hosting source code repositories for individual projects as well as |
to keep track of received data packets and to identify |
Nodes are placed in the system according to their current |
source code repositories for individual projects as well as entire |
keep track of received data packets and to identify missing |
are placed in the system according to their current trading |
code repositories for individual projects as well as entire open |
we focus on the performance of MFS with prefetchThe main |
track of received data packets and to identify missing data |
placed in the system according to their current trading performances |
repositories for individual projects as well as entire open source |
focus on the performance of MFS with prefetchThe main complexity |
encouraging nodes to contribute more and therefore be closer to |
for individual projects as well as entire open source communities |
of received data packets and to identify missing data packets |
on the performance of MFS with prefetchThe main complexity in |
nodes to contribute more and therefore be closer to the |
received data packets and to identify missing data packets in |
the performance of MFS with prefetchThe main complexity in implementing |
to contribute more and therefore be closer to the source |
and a complete hosting solution is built and evaluated as |
data packets and to identify missing data packets in a |
performance of MFS with prefetchThe main complexity in implementing the |
a complete hosting solution is built and evaluated as a |
packets and to identify missing data packets in a repair |
streaming approach that tolerates the existence of opportunistic and malicious |
complete hosting solution is built and evaluated as a proof |
of MFS with prefetchThe main complexity in implementing the prefetching |
and to identify missing data packets in a repair packet |
approach that tolerates the existence of opportunistic and malicious nodes |
MFS with prefetchThe main complexity in implementing the prefetching subing |
using a deliberately simple hint mechanism for the purposes system |
in which each peer communicates with another peer selected using |
I NTRODUCTION The advent of cloud computing has brought us |
a deliberately simple hint mechanism for the purposes system lies |
we could have added a header to each packet with |
which each peer communicates with another peer selected using a |
NTRODUCTION The advent of cloud computing has brought us a |
deliberately simple hint mechanism for the purposes system lies in |
could have added a header to each packet with a |
each peer communicates with another peer selected using a pseudo |
The advent of cloud computing has brought us a dazzling |
simple hint mechanism for the purposes system lies in handling |
have added a header to each packet with a unique |
advent of cloud computing has brought us a dazzling array |
hint mechanism for the purposes system lies in handling a |
peers exchange their current history containing the identifiers of all |
added a header to each packet with a unique sequence |
of cloud computing has brought us a dazzling array of |
Trace duration for asynchronous writes is until completion of the |
mechanism for the purposes system lies in handling a demand |
exchange their current history containing the identifiers of all the |
a header to each packet with a unique sequence number |
cloud computing has brought us a dazzling array of public |
duration for asynchronous writes is until completion of the last |
for the purposes system lies in handling a demand fetch |
their current history containing the identifiers of all the current |
computing has brought us a dazzling array of public computing |
for asynchronous writes is until completion of the last read |
current history containing the identifiers of all the current data |
has brought us a dazzling array of public computing services |
Dependencies between files are conveyed using a service a cache |
history containing the identifiers of all the current data they |
brought us a dazzling array of public computing services that |
between files are conveyed using a service a cache miss |
containing the identifiers of all the current data they hold |
us a dazzling array of public computing services that can |
a dazzling array of public computing services that can be |
dazzling array of public computing services that can be instantly |
we identify IP packets by a tuple consisting of the |
which is a list of file identifiers for the related |
array of public computing services that can be instantly tapped |
identify IP packets by a tuple consisting of the source |
is a list of file identifiers for the related files |
of public computing services that can be instantly tapped by |
IP packets by a tuple consisting of the source and |
public computing services that can be instantly tapped by anyone |
packets by a tuple consisting of the source and destination |
this is clearer in the graph for time spent on |
computing services that can be instantly tapped by anyone with |
particularly when an appliIt is assumed that after one file |
by a tuple consisting of the source and destination IP |
is clearer in the graph for time spent on fetch |
services that can be instantly tapped by anyone with a |
when an appliIt is assumed that after one file in |
a tuple consisting of the source and destination IP address |
Our approach employs local auditors that execute on all nodes |
clearer in the graph for time spent on fetch RPCs |
that can be instantly tapped by anyone with a credit |
an appliIt is assumed that after one file in the |
approach employs local auditors that execute on all nodes in |
can be instantly tapped by anyone with a credit card |
appliIt is assumed that after one file in the group |
The checksum over the payload is necessary since the IP |
be instantly tapped by anyone with a credit card number |
employs local auditors that execute on all nodes in a |
is assumed that after one file in the group has |
checksum over the payload is necessary since the IP identification |
but they demonstrate that MAFS can improve the performance of |
local auditors that execute on all nodes in a streaming |
assumed that after one file in the group has been |
Users are spared from having to invest in expensive infrastructure |
over the payload is necessary since the IP identification field |
they demonstrate that MAFS can improve the performance of large |
auditors that execute on all nodes in a streaming session |
that after one file in the group has been accessed |
are spared from having to invest in expensive infrastructure such |
the payload is necessary since the IP identification field is |
spared from having to invest in expensive infrastructure such as |
They are responsible for collecting auditable information about other neighbors |
store RPC begins to arrive store RPC received dat aR |
payload is necessary since the IP identification field is only |
cation performs a fast linear scan of files in a |
from having to invest in expensive infrastructure such as servers |
are responsible for collecting auditable information about other neighbors data |
RPC begins to arrive store RPC received dat aR re |
performs a fast linear scan of files in a file |
responsible for collecting auditable information about other neighbors data exchanges |
begins to arrive store RPC received dat aR re sto |
a fast linear scan of files in a file group |
and for verifying that neighbors upload more data than a |
hosts communicating at high speeds will use the same identifier |
to arrive store RPC received dat aR re sto reply |
and cooling equipment because the service provider takes care of |
for verifying that neighbors upload more data than a specified |
An it becomes advantageous to prefetch the remainder of the |
communicating at high speeds will use the same identifier for |
arrive store RPC received dat aR re sto reply ata |
cooling equipment because the service provider takes care of these |
verifying that neighbors upload more data than a specified threshold |
it becomes advantageous to prefetch the remainder of the files |
at high speeds will use the same identifier for different |
store RPC received dat aR re sto reply ata e |
equipment because the service provider takes care of these and |
becomes advantageous to prefetch the remainder of the files in |
high speeds will use the same identifier for different data |
RPC received dat aR re sto reply ata e d |
which periodically sample the state of the system to determine |
because the service provider takes care of these and amortizes |
advantageous to prefetch the remainder of the files in efficient |
speeds will use the same identifier for different data packets |
received dat aR re sto reply ata e d stor |
periodically sample the state of the system to determine if |
the service provider takes care of these and amortizes the |
to prefetch the remainder of the files in efficient implementation |
will use the same identifier for different data packets within |
dat aR re sto reply ata e d stor PC |
sample the state of the system to determine if the |
service provider takes care of these and amortizes the cost |
prefetch the remainder of the files in efficient implementation of |
use the same identifier for different data packets within a |
aR re sto reply ata e d stor PC time |
the state of the system to determine if the overall |
provider takes care of these and amortizes the cost across |
the remainder of the files in efficient implementation of prefetching |
the same identifier for different data packets within a fairly |
re sto reply ata e d stor PC time open |
state of the system to determine if the overall download |
takes care of these and amortizes the cost across many |
remainder of the files in efficient implementation of prefetching requires |
same identifier for different data packets within a fairly short |
sto reply ata e d stor PC time open file |
of the system to determine if the overall download rate |
care of these and amortizes the cost across many clients |
of the files in efficient implementation of prefetching requires that |
identifier for different data packets within a fairly short interval |
reply ata e d stor PC time open file for |
the system to determine if the overall download rate is |
the files in efficient implementation of prefetching requires that the |
for different data packets within a fairly short interval unless |
ata e d stor PC time open file for writing |
Companies are realizing that it no longer makes sense to |
system to determine if the overall download rate is compromised |
files in efficient implementation of prefetching requires that the demand |
different data packets within a fairly short interval unless the |
e d stor PC time open file for writing close |
are realizing that it no longer makes sense to build |
to determine if the overall download rate is compromised by |
data packets within a fairly short interval unless the checksum |
d stor PC time open file for writing close file |
realizing that it no longer makes sense to build and |
determine if the overall download rate is compromised by the |
packets within a fairly short interval unless the checksum is |
that it no longer makes sense to build and manage |
if the overall download rate is compromised by the presence |
replay log log update store RPC complete writeback window Analysis |
within a fairly short interval unless the checksum is added |
it no longer makes sense to build and manage all |
the overall download rate is compromised by the presence of |
log log update store RPC complete writeback window Analysis Client |
a fairly short interval unless the checksum is added to |
no longer makes sense to build and manage all of |
overall download rate is compromised by the presence of opportunistic |
log update store RPC complete writeback window Analysis Client Both |
fairly short interval unless the checksum is added to differentiate |
longer makes sense to build and manage all of their |
download rate is compromised by the presence of opportunistic nodes |
update store RPC complete writeback window Analysis Client Both experiments |
short interval unless the checksum is added to differentiate between |
makes sense to build and manage all of their own |
store RPC complete writeback window Analysis Client Both experiments confirm |
interval unless the checksum is added to differentiate between them |
sense to build and manage all of their own infrastructure |
RPC complete writeback window Analysis Client Both experiments confirm the |
and works with local auditing to punish nodes that do |
complete writeback window Analysis Client Both experiments confirm the benefits |
works with local auditing to punish nodes that do not |
writeback window Analysis Client Both experiments confirm the benefits of |
an event which will be caught by higher level checksums |
with local auditing to punish nodes that do not upload |
window Analysis Client Both experiments confirm the benefits of asynchronous |
that software development projects will turn to cloud computing to |
event which will be caught by higher level checksums designed |
local auditing to punish nodes that do not upload enough |
Analysis Client Both experiments confirm the benefits of asynchronous writeback |
software development projects will turn to cloud computing to store |
which will be caught by higher level checksums designed to |
auditing to punish nodes that do not upload enough data |
development projects will turn to cloud computing to store their |
even at bandwidths where a typical mobile file system performs |
will be caught by higher level checksums designed to deal |
projects will turn to cloud computing to store their master |
We study the efficiency of our auditing approach through simulation |
at bandwidths where a typical mobile file system performs all |
be caught by higher level checksums designed to deal with |
will turn to cloud computing to store their master code |
bandwidths where a typical mobile file system performs all RPCs |
and show that it is able to maintain the throughput |
caught by higher level checksums designed to deal with tranmission |
turn to cloud computing to store their master code repositories |
where a typical mobile file system performs all RPCs synchronously |
show that it is able to maintain the throughput of |
by higher level checksums designed to deal with tranmission errors |
that it is able to maintain the throughput of the |
Asynchronous writeback avoids the need to switch operation into a |
higher level checksums designed to deal with tranmission errors on |
it is able to maintain the throughput of the streaming |
project basis or as part of a larger migration of |
writeback avoids the need to switch operation into a distinct |
level checksums designed to deal with tranmission errors on commodity |
is able to maintain the throughput of the streaming system |
basis or as part of a larger migration of a |
avoids the need to switch operation into a distinct low |
checksums designed to deal with tranmission errors on commodity networks |
able to maintain the throughput of the streaming system even |
or as part of a larger migration of a SourceForge |
designed to deal with tranmission errors on commodity networks and |
to maintain the throughput of the streaming system even in |
to deal with tranmission errors on commodity networks and hence |
Even small code repositories represent a huge investment of developerhours |
maintain the throughput of the streaming system even in the |
deal with tranmission errors on commodity networks and hence does |
since they are only effective if concurrent RPCs have different |
the throughput of the streaming system even in the presence |
so the need to store this data durably and reliably |
with tranmission errors on commodity networks and hence does not |
they are only effective if concurrent RPCs have different priorities |
throughput of the streaming system even in the presence of |
the need to store this data durably and reliably is |
tranmission errors on commodity networks and hence does not have |
of the streaming system even in the presence of a |
need to store this data durably and reliably is obvious |
they reduce uservisible delay and contention that is introduced by |
errors on commodity networks and hence does not have significant |
the streaming system even in the presence of a large |
reduce uservisible delay and contention that is introduced by asynchronous |
on commodity networks and hence does not have significant consequences |
streaming system even in the presence of a large number |
uservisible delay and contention that is introduced by asynchronous writeback |
commodity networks and hence does not have significant consequences unless |
system even in the presence of a large number of |
networks and hence does not have significant consequences unless it |
even in the presence of a large number of opportunistic |
Update propagation Using asynchronous writeback at all bandwidths delays sending |
and hence does not have significant consequences unless it occurs |
especially when developers and server administrators are geographically spread thin |
propagation Using asynchronous writeback at all bandwidths delays sending updates |
in the presence of a large number of opportunistic nodes |
hence does not have significant consequences unless it occurs frequently |
Using asynchronous writeback at all bandwidths delays sending updates to |
we focus on the costs of moving source code repositories |
asynchronous writeback at all bandwidths delays sending updates to the |
focus on the costs of moving source code repositories to |
The kernel version of Maelstrom can generate up to a |
writeback at all bandwidths delays sending updates to the file |
on the costs of moving source code repositories to the |
kernel version of Maelstrom can generate up to a Gigabit |
at all bandwidths delays sending updates to the file server |
the costs of moving source code repositories to the cloud |
version of Maelstrom can generate up to a Gigabit per |
costs of moving source code repositories to the cloud as |
of Maelstrom can generate up to a Gigabit per second |
of moving source code repositories to the cloud as an |
we evaluate the effectiveness of an update propagation scheme to |
Maelstrom can generate up to a Gigabit per second of |
moving source code repositories to the cloud as an example |
evaluate the effectiveness of an update propagation scheme to reduce |
can generate up to a Gigabit per second of data |
source code repositories to the cloud as an example of |
the effectiveness of an update propagation scheme to reduce this |
generate up to a Gigabit per second of data and |
code repositories to the cloud as an example of moving |
effectiveness of an update propagation scheme to reduce this delay |
up to a Gigabit per second of data and FEC |
repositories to the cloud as an example of moving services |
to a Gigabit per second of data and FEC traffic |
to the cloud as an example of moving services in |
but the file server forces file updates to be written |
the cloud as an example of moving services in general |
the file server forces file updates to be written back |
with the input data rate depending on the encoding rate |
cloud as an example of moving services in general to |
file server forces file updates to be written back when |
as an example of moving services in general to the |
server forces file updates to be written back when another |
an example of moving services in general to the cloud |
we were able to saturate the outgoing card at rates |
forces file updates to be written back when another client |
were able to saturate the outgoing card at rates as |
file updates to be written back when another client must |
able to saturate the outgoing card at rates as high |
updates to be written back when another client must read |
to saturate the outgoing card at rates as high as |
the most critical of which is storage since that is |
to be written back when another client must read an |
most critical of which is storage since that is the |
be written back when another client must read an up |
critical of which is storage since that is the simplest |
of which is storage since that is the simplest and |
which is storage since that is the simplest and likely |
is storage since that is the simplest and likely first |
storage since that is the simplest and likely first component |
since that is the simplest and likely first component to |
that is the simplest and likely first component to be |
is the simplest and likely first component to be moved |
We set an agenda for demonstrating the financial storage and |
set an agenda for demonstrating the financial storage and computing |
an agenda for demonstrating the financial storage and computing costs |
agenda for demonstrating the financial storage and computing costs of |
for demonstrating the financial storage and computing costs of moving |
demonstrating the financial storage and computing costs of moving source |
the financial storage and computing costs of moving source code |
but adding an additional delay before writing back the update |
financial storage and computing costs of moving source code repositories |
adding an additional delay before writing back the update increases |
storage and computing costs of moving source code repositories to |
an additional delay before writing back the update increases the |
and computing costs of moving source code repositories to the |
additional delay before writing back the update increases the scope |
computing costs of moving source code repositories to the cloud |
delay before writing back the update increases the scope for |
incoming data packets are buffered so that they can be |
In section II we explain what it means to store |
before writing back the update increases the scope for inconsistency |
data packets are buffered so that they can be used |
section II we explain what it means to store a |
packets are buffered so that they can be used in |
II we explain what it means to store a code |
are buffered so that they can be used in conjunction |
we explain what it means to store a code repository |
buffered so that they can be used in conjunction with |
explain what it means to store a code repository in |
so that they can be used in conjunction with XORs |
what it means to store a code repository in the |
that they can be used in conjunction with XORs to |
it means to store a code repository in the cloud |
a different type of inconsistency is introduced between a client |
they can be used in conjunction with XORs to recover |
means to store a code repository in the cloud and |
different type of inconsistency is introduced between a client and |
can be used in conjunction with XORs to recover missing |
to store a code repository in the cloud and why |
type of inconsistency is introduced between a client and the |
be used in conjunction with XORs to recover missing data |
store a code repository in the cloud and why there |
of inconsistency is introduced between a client and the server |
used in conjunction with XORs to recover missing data packets |
a code repository in the cloud and why there are |
inconsistency is introduced between a client and the server when |
code repository in the cloud and why there are cost |
is introduced between a client and the server when a |
any received XOR that is missing more than one data |
repository in the cloud and why there are cost advantages |
introduced between a client and the server when a file |
received XOR that is missing more than one data packet |
in the cloud and why there are cost advantages to |
between a client and the server when a file is |
XOR that is missing more than one data packet is |
the cloud and why there are cost advantages to doing |
a client and the server when a file is modified |
that is missing more than one data packet is stored |
cloud and why there are cost advantages to doing so |
is missing more than one data packet is stored temporarily |
since the change is hidden from the server until the |
the change is hidden from the server until the file |
Section III is a case study on using Amazon s |
change is hidden from the server until the file is |
in case all but one of the missing packets are |
III is a case study on using Amazon s S |
is hidden from the server until the file is closed |
case all but one of the missing packets are received |
all but one of the missing packets are received later |
For the purposes of this investigation we assume that the |
but one of the missing packets are received later or |
In section IV we present an implementation that ties Subversion |
the purposes of this investigation we assume that the open |
one of the missing packets are received later or recovered |
section IV we present an implementation that ties Subversion to |
of the missing packets are received later or recovered through |
close interval for a file is small relative to the |
IV we present an implementation that ties Subversion to S |
the missing packets are received later or recovered through other |
interval for a file is small relative to the network |
missing packets are received later or recovered through other XORs |
for a file is small relative to the network latency |
a file is small relative to the network latency and |
file is small relative to the network latency and writeback |
allowing the recovery of the remaining missing packet from this |
is small relative to the network latency and writeback delay |
the recovery of the remaining missing packet from this XOR |
In section V we evaluate the performance of this solution |
The update propagation techniques we describe can be applied equally |
In practice we stored data and XOR packets in double |
update propagation techniques we describe can be applied equally well |
practice we stored data and XOR packets in double buffered |
propagation techniques we describe can be applied equally well to |
we stored data and XOR packets in double buffered red |
C LOUDIFYING S OURCE R EPOSITORIES In a revision control |
techniques we describe can be applied equally well to individual |
stored data and XOR packets in double buffered red black |
LOUDIFYING S OURCE R EPOSITORIES In a revision control system |
we describe can be applied equally well to individual file |
data and XOR packets in double buffered red black trees |
describe can be applied equally well to individual file writes |
and XOR packets in double buffered red black trees for |
can be applied equally well to individual file writes as |
be applied equally well to individual file writes as to |
applied equally well to individual file writes as to writeback |
Each developer checks out and then keeps a working copy |
developer checks out and then keeps a working copy on |
checks out and then keeps a working copy on his |
out and then keeps a working copy on his machine |
and then keeps a working copy on his machine that |
then keeps a working copy on his machine that mirrors |
keeps a working copy on his machine that mirrors the |
a working copy on his machine that mirrors the repository |
The developer edits files in his working copy and periodically |
developer edits files in his working copy and periodically commits |
edits files in his working copy and periodically commits the |
files in his working copy and periodically commits the changes |
they were designed to permit a client to function at |
in his working copy and periodically commits the changes back |
were designed to permit a client to function at low |
his working copy and periodically commits the changes back to |
designed to permit a client to function at low bandwidth |
working copy and periodically commits the changes back to the |
the repair bins in the layered interleaving scheme store incrementally |
copy and periodically commits the changes back to the repository |
repair bins in the layered interleaving scheme store incrementally computed |
Since it is impractical to lock files if clients are |
bins in the layered interleaving scheme store incrementally computed XORs |
and updates his working copy to reflect the changes made |
it is impractical to lock files if clients are permitted |
in the layered interleaving scheme store incrementally computed XORs and |
updates his working copy to reflect the changes made by |
is impractical to lock files if clients are permitted to |
the layered interleaving scheme store incrementally computed XORs and lists |
his working copy to reflect the changes made by other |
impractical to lock files if clients are permitted to modify |
layered interleaving scheme store incrementally computed XORs and lists of |
working copy to reflect the changes made by other developers |
to lock files if clients are permitted to modify the |
interleaving scheme store incrementally computed XORs and lists of data |
lock files if clients are permitted to modify the filesystem |
scheme store incrementally computed XORs and lists of data packet |
files if clients are permitted to modify the filesystem while |
The repository maintains complete history so at any point in |
store incrementally computed XORs and lists of data packet headers |
if clients are permitted to modify the filesystem while they |
repository maintains complete history so at any point in time |
clients are permitted to modify the filesystem while they are |
maintains complete history so at any point in time it |
are permitted to modify the filesystem while they are disconnected |
resulting in low storage overheads for each layer that rise |
complete history so at any point in time it is |
in low storage overheads for each layer that rise linearly |
history so at any point in time it is possible |
low storage overheads for each layer that rise linearly with |
so at any point in time it is possible to |
storage overheads for each layer that rise linearly with the |
at any point in time it is possible to check |
overheads for each layer that rise linearly with the value |
any point in time it is possible to check out |
for each layer that rise linearly with the value of |
An alternative approach is to allow a client to use |
point in time it is possible to check out a |
each layer that rise linearly with the value of the |
alternative approach is to allow a client to use asynchronous |
in time it is possible to check out a working |
layer that rise linearly with the value of the interleave |
approach is to allow a client to use asynchronous writeback |
time it is possible to check out a working copy |
it is possible to check out a working copy for |
but require that it alerts the file server when a |
is possible to check out a working copy for any |
require that it alerts the file server when a file |
possible to check out a working copy for any specified |
that it alerts the file server when a file is |
to check out a working copy for any specified version |
it alerts the file server when a file is modified |
check out a working copy for any specified version number |
Other Performance Enhancing Roles Maelstrom appliances can optionally aggregate small |
Performance Enhancing Roles Maelstrom appliances can optionally aggregate small subkilobyte |
Storing a repository in the cloud eliminates worries of data |
This informs the server that the update exists before the |
Enhancing Roles Maelstrom appliances can optionally aggregate small subkilobyte packets |
a repository in the cloud eliminates worries of data loss |
informs the server that the update exists before the new |
Roles Maelstrom appliances can optionally aggregate small subkilobyte packets from |
repository in the cloud eliminates worries of data loss due |
the server that the update exists before the new file |
Maelstrom appliances can optionally aggregate small subkilobyte packets from different |
in the cloud eliminates worries of data loss due to |
server that the update exists before the new file contents |
appliances can optionally aggregate small subkilobyte packets from different flows |
the cloud eliminates worries of data loss due to hardware |
that the update exists before the new file contents ar |
can optionally aggregate small subkilobyte packets from different flows into |
cloud eliminates worries of data loss due to hardware failure |
optionally aggregate small subkilobyte packets from different flows into larger |
aggregate small subkilobyte packets from different flows into larger ones |
but issues of access control and consistency must still be |
small subkilobyte packets from different flows into larger ones for |
issues of access control and consistency must still be addressed |
Origin of inconsistencies Since asynchronous writeback decouples modifying a file |
subkilobyte packets from different flows into larger ones for better |
of inconsistencies Since asynchronous writeback decouples modifying a file from |
Authorized users should be able to commit new versions of |
packets from different flows into larger ones for better communication |
inconsistencies Since asynchronous writeback decouples modifying a file from notifying |
users should be able to commit new versions of files |
from different flows into larger ones for better communication efficiency |
Since asynchronous writeback decouples modifying a file from notifying the |
should be able to commit new versions of files to |
different flows into larger ones for better communication efficiency over |
asynchronous writeback decouples modifying a file from notifying the server |
be able to commit new versions of files to the |
flows into larger ones for better communication efficiency over the |
writeback decouples modifying a file from notifying the server that |
able to commit new versions of files to the repository |
into larger ones for better communication efficiency over the long |
decouples modifying a file from notifying the server that a |
modifying a file from notifying the server that a change |
a file from notifying the server that a change has |
Users expect the repository to be consistent and for any |
file from notifying the server that a change has occurred |
expect the repository to be consistent and for any changes |
the repository to be consistent and for any changes they |
repository to be consistent and for any changes they make |
to be consistent and for any changes they make not |
be consistent and for any changes they make not to |
consistent and for any changes they make not to be |
and for any changes they make not to be pre |
even in the face of cloud services that offer lesser |
in the face of cloud services that offer lesser guarantees |
These graphs show the speedup gained by adding prefetching for |
For these reasons we do not expect that clients will |
graphs show the speedup gained by adding prefetching for a |
appliances send multicast packets to each other across the long |
these reasons we do not expect that clients will be |
show the speedup gained by adding prefetching for a range |
reasons we do not expect that clients will be directly |
the speedup gained by adding prefetching for a range of |
we do not expect that clients will be directly using |
speedup gained by adding prefetching for a range of bandwidth |
do not expect that clients will be directly using the |
gained by adding prefetching for a range of bandwidth values |
not expect that clients will be directly using the cloud |
expect that clients will be directly using the cloud storage |
that clients will be directly using the cloud storage API |
clients will be directly using the cloud storage API anytime |
will be directly using the cloud storage API anytime soon |
appliances can take on other existing roles in the data |
but that they will contact one of a set of |
can take on other existing roles in the data center |
flushes update store A callback for A fetch A open |
that they will contact one of a set of front |
update store A callback for A fetch A open A |
acting as security and VPN gateways and as conventional performance |
as security and VPN gateways and as conventional performance enhancing |
security and VPN gateways and as conventional performance enhancing proxies |
Issuing a fetch RPC at the same time as a |
a fetch RPC at the same time as a prefetch |
fetch RPC at the same time as a prefetch is |
These might consist of virtualized server instances in the cloud |
RPC at the same time as a prefetch is in |
at the same time as a prefetch is in progress |
the same time as a prefetch is in progress needlessly |
same time as a prefetch is in progress needlessly wastes |
but in either case their local storage systems are allowed |
time as a prefetch is in progress needlessly wastes bandwidth |
in either case their local storage systems are allowed to |
either case their local storage systems are allowed to be |
since it retrieves the same file from the server twice |
case their local storage systems are allowed to be cheap |
their local storage systems are allowed to be cheap and |
The same could be true if we opt for aborting |
E VALUATION We evaluated Maelstrom on the Emulab testbed at |
local storage systems are allowed to be cheap and unresilient |
same could be true if we opt for aborting prefetches |
VALUATION We evaluated Maelstrom on the Emulab testbed at Utah |
storage systems are allowed to be cheap and unresilient against |
since an aborted prefetch could be very close to completion |
systems are allowed to be cheap and unresilient against hardware |
are allowed to be cheap and unresilient against hardware failure |
MFS therefore makes the demand fetch wait for the prefetch |
but also raises the priority of the prefetch RPC to |
Open source communities with limited budgets and private enterprises that |
also raises the priority of the prefetch RPC to that |
source communities with limited budgets and private enterprises that are |
raises the priority of the prefetch RPC to that of |
communities with limited budgets and private enterprises that are increasingly |
we used a dumbbell topology of two clusters of nodes |
the priority of the prefetch RPC to that of a |
with limited budgets and private enterprises that are increasingly cost |
used a dumbbell topology of two clusters of nodes connected |
priority of the prefetch RPC to that of a regular |
a dumbbell topology of two clusters of nodes connected via |
sensitive may well prefer to pay just for the resources |
of the prefetch RPC to that of a regular fetch |
dumbbell topology of two clusters of nodes connected via routing |
may well prefer to pay just for the resources they |
the prefetch RPC to that of a regular fetch operation |
topology of two clusters of nodes connected via routing nodes |
well prefer to pay just for the resources they use |
of two clusters of nodes connected via routing nodes with |
two clusters of nodes connected via routing nodes with a |
rather than trying to budget in advance what they are |
clusters of nodes connected via routing nodes with a high |
than trying to budget in advance what they are going |
which results in more overhead than the case where a |
trying to budget in advance what they are going to |
results in more overhead than the case where a demand |
to budget in advance what they are going to need |
A client s update is logged when the file is |
in more overhead than the case where a demand fetch |
client s update is logged when the file is closed |
more overhead than the case where a demand fetch occurs |
and increased competition among providers of commodity services will ensure |
overhead than the case where a demand fetch occurs without |
increased competition among providers of commodity services will ensure that |
than the case where a demand fetch occurs without a |
competition among providers of commodity services will ensure that prices |
the case where a demand fetch occurs without a fetch |
An invalidation RPC allows the server to invalidate other clients |
among providers of commodity services will ensure that prices are |
invalidation RPC allows the server to invalidate other clients cached |
show the performance of the kernel version at Gigabit speeds |
providers of commodity services will ensure that prices are reasonable |
RPC allows the server to invalidate other clients cached copies |
the fetch can frequently make use of the data already |
the remainder of the graphs show the performance of the |
fetch can frequently make use of the data already transferred |
remainder of the graphs show the performance of the user |
can frequently make use of the data already transferred and |
a client that modifies a file could save bandwidth by |
frequently make use of the data already transferred and so |
client that modifies a file could save bandwidth by not |
make use of the data already transferred and so still |
By far the most popular general purpose cloud storage service |
that modifies a file could save bandwidth by not sending |
use of the data already transferred and so still results |
far the most popular general purpose cloud storage service today |
modifies a file could save bandwidth by not sending it |
of the data already transferred and so still results in |
the most popular general purpose cloud storage service today is |
a file could save bandwidth by not sending it to |
the data already transferred and so still results in a |
most popular general purpose cloud storage service today is Amazon |
file could save bandwidth by not sending it to the |
data already transferred and so still results in a faster |
popular general purpose cloud storage service today is Amazon s |
could save bandwidth by not sending it to the file |
already transferred and so still results in a faster response |
general purpose cloud storage service today is Amazon s S |
save bandwidth by not sending it to the file server |
transferred and so still results in a faster response to |
bandwidth by not sending it to the file server at |
and so still results in a faster response to the |
by not sending it to the file server at all |
We chose to use this as a basis for cost |
so still results in a faster response to the application |
unless the server pulls it to supply it to another |
chose to use this as a basis for cost studies |
the server pulls it to supply it to another client |
to use this as a basis for cost studies and |
use this as a basis for cost studies and for |
MAFS clients push updates to the server in the background |
While it will reach an equilibrium if the total size |
this as a basis for cost studies and for the |
to reduce the delay incurred when fetching an invalidated file |
it will reach an equilibrium if the total size of |
as a basis for cost studies and for the implementation |
pushing updates can result in the server having received some |
will reach an equilibrium if the total size of the |
a basis for cost studies and for the implementation of |
reach an equilibrium if the total size of the file |
or all of the update by the time another client |
basis for cost studies and for the implementation of our |
an equilibrium if the total size of the file groups |
all of the update by the time another client accesses |
for cost studies and for the implementation of our system |
equilibrium if the total size of the file groups in |
of the update by the time another client accesses it |
if the total size of the file groups in the |
the total size of the file groups in the prefetch |
is an appealing choice because Amazon also offers the EC |
total size of the file groups in the prefetch list |
size of the file groups in the prefetch list is |
of the file groups in the prefetch list is less |
the file groups in the prefetch list is less than |
so it is possible to use their services as a |
Selective invalidation with reader pull The effect of selective invalidation |
file groups in the prefetch list is less than the |
it is possible to use their services as a complete |
invalidation with reader pull The effect of selective invalidation and |
groups in the prefetch list is less than the cache |
is possible to use their services as a complete hosting |
with reader pull The effect of selective invalidation and reader |
in the prefetch list is less than the cache size |
possible to use their services as a complete hosting solution |
reader pull The effect of selective invalidation and reader pull |
to use their services as a complete hosting solution with |
there is no mechanism to prevent the prefetching subsystem running |
pull The effect of selective invalidation and reader pull is |
use their services as a complete hosting solution with low |
is no mechanism to prevent the prefetching subsystem running ahead |
The effect of selective invalidation and reader pull is that |
their services as a complete hosting solution with low latency |
no mechanism to prevent the prefetching subsystem running ahead of |
effect of selective invalidation and reader pull is that MAFS |
services as a complete hosting solution with low latency access |
mechanism to prevent the prefetching subsystem running ahead of actual |
of selective invalidation and reader pull is that MAFS incorporates |
as a complete hosting solution with low latency access to |
to prevent the prefetching subsystem running ahead of actual file |
and that Maelstrom successfully masks loss and prevents this collapse |
selective invalidation and reader pull is that MAFS incorporates SIRP |
a complete hosting solution with low latency access to storage |
prevent the prefetching subsystem running ahead of actual file accesses |
that Maelstrom successfully masks loss and prevents this collapse from |
the prefetching subsystem running ahead of actual file accesses and |
Maelstrom successfully masks loss and prevents this collapse from occurring |
SIRP behaves similarly to synchronous writeback if a client client |
prefetching subsystem running ahead of actual file accesses and evicting |
behaves similarly to synchronous writeback if a client client consistency |
subsystem running ahead of actual file accesses and evicting useful |
running ahead of actual file accesses and evicting useful files |
ahead of actual file accesses and evicting useful files from |
of actual file accesses and evicting useful files from the |
but behaves like asynchronous writeinvalidations and expedited transmission of updates |
actual file accesses and evicting useful files from the cache |
world traces taken from the Subversion repositories of popular open |
behaves like asynchronous writeinvalidations and expedited transmission of updates for |
traces taken from the Subversion repositories of popular open source |
or evicting files which it has prefetched but have not |
like asynchronous writeinvalidations and expedited transmission of updates for files |
taken from the Subversion repositories of popular open source projects |
evicting files which it has prefetched but have not yet |
asynchronous writeinvalidations and expedited transmission of updates for files back |
files which it has prefetched but have not yet been |
writeinvalidations and expedited transmission of updates for files back when |
which it has prefetched but have not yet been referenced |
and expedited transmission of updates for files back when there |
it has prefetched but have not yet been referenced by |
expedited transmission of updates for files back when there are |
has prefetched but have not yet been referenced by the |
transmission of updates for files back when there are no |
prefetched but have not yet been referenced by the user |
of updates for files back when there are no concurrent |
distance link with and without intermediary Maelstrom proxies and measuring |
updates for files back when there are no concurrent fetches |
link with and without intermediary Maelstrom proxies and measuring obtained |
with and without intermediary Maelstrom proxies and measuring obtained throughput |
Our cost analysis is based on the sizes of these |
and without intermediary Maelstrom proxies and measuring obtained throughput while |
cost analysis is based on the sizes of these files |
without intermediary Maelstrom proxies and measuring obtained throughput while varying |
SIRP sends an RPC to the server as soon as |
analysis is based on the sizes of these files and |
intermediary Maelstrom proxies and measuring obtained throughput while varying loss |
sends an RPC to the server as soon as an |
is based on the sizes of these files and the |
Maelstrom proxies and measuring obtained throughput while varying loss rate |
an RPC to the server as soon as an application |
The experimental setup was the same as in the priority |
based on the sizes of these files and the time |
RPC to the server as soon as an application closes |
experimental setup was the same as in the priority tests |
on the sizes of these files and the time at |
to the server as soon as an application closes a |
though this time MFS was configured to run with asynchronous |
the sizes of these files and the time at which |
the server as soon as an application closes a modified |
this time MFS was configured to run with asynchronous writeback |
sizes of these files and the time at which each |
server as soon as an application closes a modified file |
The error bars on the graphs to the left are |
of these files and the time at which each revision |
error bars on the graphs to the left are standard |
these files and the time at which each revision was |
The tests were run at a range of bandwidth values |
bars on the graphs to the left are standard errors |
Using an invalidation RPC to alert the actual contents until |
files and the time at which each revision was committed |
on the graphs to the left are standard errors of |
an invalidation RPC to alert the actual contents until they |
Looking up the size of these special files is only |
the graphs to the left are standard errors of the |
Each microbenchmark consists of one or two processes accessing files |
invalidation RPC to alert the actual contents until they are |
up the size of these special files is only possible |
graphs to the left are standard errors of the throughput |
RPC to alert the actual contents until they are needed |
with some or all of the files forming file groups |
the size of these special files is only possible if |
file server to the existence of a new update improves |
to the left are standard errors of the throughput over |
size of these special files is only possible if one |
server to the existence of a new update improves cache |
the left are standard errors of the throughput over ten |
of these special files is only possible if one has |
to the existence of a new update improves cache consistency |
left are standard errors of the throughput over ten runs |
these special files is only possible if one has filesystem |
The Compile MFS test has six file groups for the |
special files is only possible if one has filesystem level |
Compile MFS test has six file groups for the main |
If writeback traffic is low enough for the server to |
files is only possible if one has filesystem level access |
MFS test has six file groups for the main directories |
writeback traffic is low enough for the server to start |
IP s cache of tuning parameters to allow for repeatable |
is only possible if one has filesystem level access to |
test has six file groups for the main directories of |
traffic is low enough for the server to start receiving |
s cache of tuning parameters to allow for repeatable results |
only possible if one has filesystem level access to the |
has six file groups for the main directories of the |
is low enough for the server to start receiving an |
possible if one has filesystem level access to the disk |
six file groups for the main directories of the system |
low enough for the server to start receiving an update |
if one has filesystem level access to the disk on |
one has filesystem level access to the disk on which |
has filesystem level access to the disk on which the |
filesystem level access to the disk on which the repository |
level access to the disk on which the repository is |
access to the disk on which the repository is stored |
the invalidation We conclude this section with an experiment that |
so we had to use Subversion s mirroring capability to |
invalidation We conclude this section with an experiment that compares |
we had to use Subversion s mirroring capability to fetch |
We conclude this section with an experiment that compares the |
had to use Subversion s mirroring capability to fetch revisions |
conclude this section with an experiment that compares the is |
to use Subversion s mirroring capability to fetch revisions from |
this section with an experiment that compares the is superfluous |
use Subversion s mirroring capability to fetch revisions from the |
Subversion s mirroring capability to fetch revisions from the network |
Doing this also implicitly gives us the log of timestamps |
this also implicitly gives us the log of timestamps indicating |
when a client adds an update to the writeback back |
also implicitly gives us the log of timestamps indicating when |
a client adds an update to the writeback back transmits |
implicitly gives us the log of timestamps indicating when each |
client adds an update to the writeback back transmits an |
gives us the log of timestamps indicating when each revision |
adds an update to the writeback back transmits an update |
us the log of timestamps indicating when each revision was |
an update to the writeback back transmits an update as |
the log of timestamps indicating when each revision was committed |
update to the writeback back transmits an update as soon |
to the writeback back transmits an update as soon as |
the writeback back transmits an update as soon as a |
writeback back transmits an update as soon as a file |
but the second process writes the files to the server |
back transmits an update as soon as a file is |
the second process writes the files to the server instead |
transmits an update as soon as a file is closed |
transaction costs of pushing the two files for each revision |
second process writes the files to the server instead of |
costs of pushing the two files for each revision into |
process writes the files to the server instead of reading |
of pushing the two files for each revision into S |
it only sends an invalidation if the queue is not |
writes the files to the server instead of reading them |
second iperf flow from one node to another with and |
only sends an invalidation if the queue is not empty |
iperf flow from one node to another with and without |
The remaining tests investigate the overhead paid for weaknesses in |
flow from one node to another with and without Maelstrom |
chronous writeback puts the update in a queue and transmits |
remaining tests investigate the overhead paid for weaknesses in the |
from one node to another with and without Maelstrom running |
writeback puts the update in a queue and transmits it |
tests investigate the overhead paid for weaknesses in the prefetching |
one node to another with and without Maelstrom running on |
puts the update in a queue and transmits it If |
investigate the overhead paid for weaknesses in the prefetching algorithm |
node to another with and without Maelstrom running on the |
the update in a queue and transmits it If the |
to another with and without Maelstrom running on the routers |
update in a queue and transmits it If the queue |
another with and without Maelstrom running on the routers and |
in a queue and transmits it If the queue is |
with and without Maelstrom running on the routers and measuring |
a queue and transmits it If the queue is empty |
and without Maelstrom running on the routers and measuring throughput |
without Maelstrom running on the routers and measuring throughput while |
the invalidation is piggybacked onto the as soon as it |
Maelstrom running on the routers and measuring throughput while varying |
invalidation is piggybacked onto the as soon as it reaches |
running on the routers and measuring throughput while varying the |
is piggybacked onto the as soon as it reaches the |
on the routers and measuring throughput while varying the random |
piggybacked onto the as soon as it reaches the front |
the routers and measuring throughput while varying the random loss |
onto the as soon as it reaches the front of |
routers and measuring throughput while varying the random loss rate |
the as soon as it reaches the front of the |
and measuring throughput while varying the random loss rate on |
as soon as it reaches the front of the queue |
measuring throughput while varying the random loss rate on the |
throughput while varying the random loss rate on the link |
Good Order and Bad Order investigate the effect of the |
while varying the random loss rate on the link and |
Order and Bad Order investigate the effect of the ordered |
varying the random loss rate on the link and the |
Not included in the analysis is the cost of fetching |
and Bad Order investigate the effect of the ordered list |
the random loss rate on the link and the oneway |
included in the analysis is the cost of fetching data |
Bad Order investigate the effect of the ordered list of |
random loss rate on the link and the oneway latency |
When the server receives an invalidation from a date results |
in the analysis is the cost of fetching data out |
Order investigate the effect of the ordered list of files |
the server receives an invalidation from a date results in |
the analysis is the cost of fetching data out of |
investigate the effect of the ordered list of files in |
server receives an invalidation from a date results in an |
we ran eight parallel iperf flows from one node to |
analysis is the cost of fetching data out of S |
the effect of the ordered list of files in a |
receives an invalidation from a date results in an invalidation |
ran eight parallel iperf flows from one node to another |
effect of the ordered list of files in a file |
an invalidation from a date results in an invalidation RPC |
eight parallel iperf flows from one node to another for |
This cost will vary depending on how much caching is |
of the ordered list of files in a file group |
invalidation from a date results in an invalidation RPC to |
cost will vary depending on how much caching is done |
from a date results in an invalidation RPC to the |
will vary depending on how much caching is done on |
a date results in an invalidation RPC to the server |
vary depending on how much caching is done on the |
The curves obtained from the two versions are almost identical |
depending on how much caching is done on the front |
we evaluated whether such a straightforward algorithm can have a |
it makes callbacks to all the other clients that cache |
evaluated whether such a straightforward algorithm can have a benefit |
we present both to show that the kernel version successfully |
makes callbacks to all the other clients that cache the |
whether such a straightforward algorithm can have a benefit for |
present both to show that the kernel version successfully scales |
callbacks to all the other clients that cache the are |
such a straightforward algorithm can have a benefit for some |
both to show that the kernel version successfully scales up |
to all the other clients that cache the are of |
a straightforward algorithm can have a benefit for some repre |
to show that the kernel version successfully scales up the |
all the other clients that cache the are of particular |
and dedicated servers potentially having much more due to inexpensive |
show that the kernel version successfully scales up the performance |
Order accesses the files in the group in the same |
the other clients that cache the are of particular interest |
dedicated servers potentially having much more due to inexpensive SATA |
that the kernel version successfully scales up the performance of |
accesses the files in the group in the same order |
other clients that cache the are of particular interest in |
servers potentially having much more due to inexpensive SATA disks |
the kernel version successfully scales up the performance of the |
the files in the group in the same order as |
clients that cache the are of particular interest in this |
kernel version successfully scales up the performance of the userspace |
it is not unreasonable to assume that a cache hit |
files in the group in the same order as the |
that cache the are of particular interest in this comparison |
version successfully scales up the performance of the userspace version |
is not unreasonable to assume that a cache hit rate |
in the group in the same order as the list |
successfully scales up the performance of the userspace version to |
not unreasonable to assume that a cache hit rate of |
scales up the performance of the userspace version to hundreds |
unreasonable to assume that a cache hit rate of close |
up the performance of the userspace version to hundreds of |
to assume that a cache hit rate of close to |
the performance of the userspace version to hundreds of megabits |
performance of the userspace version to hundreds of megabits of |
modifications are serialised in the order of their readers and |
of the userspace version to hundreds of megabits of traffic |
are serialised in the order of their readers and writers |
the userspace version to hundreds of megabits of traffic per |
serialised in the order of their readers and writers affected |
userspace version to hundreds of megabits of traffic per second |
in the order of their readers and writers affected by |
the order of their readers and writers affected by stronger |
order of their readers and writers affected by stronger consistency |
public Subversion repositories of the Debian Linux community amount to |
Subversion repositories of the Debian Linux community amount to a |
repositories of the Debian Linux community amount to a total |
the client that made the update only transmits it when |
of the Debian Linux community amount to a total of |
client that made the update only transmits it when it |
the Debian Linux community amount to a total of only |
adding prefetching from the file groups specified has a substantial |
that made the update only transmits it when it reaches |
prefetching from the file groups specified has a substantial improvement |
made the update only transmits it when it reaches the |
from the file groups specified has a substantial improvement on |
the update only transmits it when it reaches the head |
The only outgoing bandwidth costs are then to to replace |
the file groups specified has a substantial improvement on the |
update only transmits it when it reaches the head of |
only outgoing bandwidth costs are then to to replace failed |
file groups specified has a substantial improvement on the performance |
only transmits it when it reaches the head of the |
outgoing bandwidth costs are then to to replace failed frontend |
groups specified has a substantial improvement on the performance of |
Building Collaboration Applications That Mix Web Services Hosted Content with |
transmits it when it reaches the head of the writeback |
bandwidth costs are then to to replace failed frontend servers |
specified has a substantial improvement on the performance of the |
Collaboration Applications That Mix Web Services Hosted Content with P |
it when it reaches the head of the writeback queue |
costs are then to to replace failed frontend servers or |
has a substantial improvement on the performance of the workload |
If another client attempts to fetch the file during the |
are then to to replace failed frontend servers or to |
another client attempts to fetch the file during the update |
then to to replace failed frontend servers or to synchronize |
client attempts to fetch the file during the update s |
with the kernel version achieving two orders of magnitude higher |
to to replace failed frontend servers or to synchronize replicas |
attempts to fetch the file during the update s Experimental |
more surplus bandwidth and more think time result in improved |
the kernel version achieving two orders of magnitude higher throughput |
to replace failed frontend servers or to synchronize replicas if |
to fetch the file during the update s Experimental setup |
surplus bandwidth and more think time result in improved performance |
kernel version achieving two orders of magnitude higher throughput that |
replace failed frontend servers or to synchronize replicas if more |
fetch the file during the update s Experimental setup writeback |
version achieving two orders of magnitude higher throughput that conventional |
this naturally means that the greatest improvements from prefetching are |
failed frontend servers or to synchronize replicas if more than |
the file during the update s Experimental setup writeback window |
achieving two orders of magnitude higher throughput that conventional TCP |
naturally means that the greatest improvements from prefetching are evident |
frontend servers or to synchronize replicas if more than one |
the server blocks that client until the update has arrived |
means that the greatest improvements from prefetching are evident at |
servers or to synchronize replicas if more than one is |
The server also makes a pull RPC to the client |
that the greatest improvements from prefetching are evident at higher |
or to synchronize replicas if more than one is in |
server also makes a pull RPC to the client that |
the greatest improvements from prefetching are evident at higher bandwidths |
to synchronize replicas if more than one is in use |
also makes a pull RPC to the client that Experiments |
makes a pull RPC to the client that Experiments were |
a pull RPC to the client that Experiments were conducted |
IP throughput declining on a link of increasing length when |
pull RPC to the client that Experiments were conducted in |
throughput declining on a link of increasing length when subjected |
the bandwidth costs are actually waived and the user then |
RPC to the client that Experiments were conducted in a |
declining on a link of increasing length when subjected to |
bandwidth costs are actually waived and the user then pays |
to the client that Experiments were conducted in a network |
on a link of increasing length when subjected to uniform |
costs are actually waived and the user then pays only |
edu Abstract The most commonly deployed web service applications employ |
a link of increasing length when subjected to uniform loss |
the client that Experiments were conducted in a network of |
are actually waived and the user then pays only for |
Abstract The most commonly deployed web service applications employ client |
link of increasing length when subjected to uniform loss rates |
client that Experiments were conducted in a network of five |
actually waived and the user then pays only for the |
of increasing length when subjected to uniform loss rates of |
that Experiments were conducted in a network of five hosts |
waived and the user then pays only for the traffic |
with clients running remotely and services hosted in data centers |
and the user then pays only for the traffic between |
write test performs slightly worse due to its already heavy |
the user then pays only for the traffic between the |
test performs slightly worse due to its already heavy network |
user then pays only for the traffic between the front |
performs slightly worse due to its already heavy network contention |
one writer client that was responsible for modifying When it |
writer client that was responsible for modifying When it receives |
The top line in the graphs is the performance of |
client that was responsible for modifying When it receives the |
top line in the graphs is the performance of TCP |
that was responsible for modifying When it receives the pull |
was responsible for modifying When it receives the pull RPC |
IP without loss and provides an upper bound for performance |
Collaboration features are awkward to support solely based on the |
without loss and provides an upper bound for performance on |
This effect is due to the useless prefetching RPCs flooding |
features are awkward to support solely based on the existing |
loss and provides an upper bound for performance on the |
and three reader clients that only read the the update |
effect is due to the useless prefetching RPCs flooding the |
Also shown is an estimate for the Apache Software Foundation |
are awkward to support solely based on the existing web |
and provides an upper bound for performance on the link |
Apache has taken the unusual approach of using a single |
is due to the useless prefetching RPCs flooding the outgoing |
awkward to support solely based on the existing web services |
three reader clients that only read the the update at |
has taken the unusual approach of using a single repository |
Maelstrom masks packet loss and tracks the lossless line closely |
to support solely based on the existing web services technologies |
reader clients that only read the the update at the |
due to the useless prefetching RPCs flooding the outgoing link |
Indirection through the data center introduces high latencies and limits |
lagging only when the link latency is low and TCP |
clients that only read the the update at the same |
taken the unusual approach of using a single repository for |
to the useless prefetching RPCs flooding the outgoing link and |
through the data center introduces high latencies and limits scalability |
that only read the the update at the same priority |
the unusual approach of using a single repository for all |
the useless prefetching RPCs flooding the outgoing link and imposing |
only read the the update at the same priority as |
unusual approach of using a single repository for all of |
useless prefetching RPCs flooding the outgoing link and imposing minor |
read the the update at the same priority as an |
approach of using a single repository for all of its |
Cornell s Live Distributed Objects platform combines web services with |
prefetching RPCs flooding the outgoing link and imposing minor delays |
the the update at the same priority as an RPC |
of using a single repository for all of its projects |
s Live Distributed Objects platform combines web services with direct |
RPCs flooding the outgoing link and imposing minor delays on |
the update at the same priority as an RPC to |
Live Distributed Objects platform combines web services with direct peerto |
flooding the outgoing link and imposing minor delays on each |
Subversion s mirroring tool was unable to create local copy |
update at the same priority as an RPC to fetch |
the outgoing link and imposing minor delays on each demand |
at the same priority as an RPC to fetch file |
outgoing link and imposing minor delays on each demand fetch |
the same priority as an RPC to fetch file data |
An usual phenomenon is that the Bad Order test consistently |
The bandwidth between the writer client and the server that |
usual phenomenon is that the Bad Order test consistently outperforms |
bandwidth between the writer client and the server that it |
phenomenon is that the Bad Order test consistently outperforms Good |
between the writer client and the server that it will |
is that the Bad Order test consistently outperforms Good Order |
the writer client and the server that it will be |
writer client and the server that it will be preferentially |
even though the latter triggers prefetches in the correct order |
client and the server that it will be preferentially allocated |
or make possible a world of professional dialog and collaboration |
and the server that it will be preferentially allocated bandwidth |
make possible a world of professional dialog and collaboration without |
possible a world of professional dialog and collaboration without travel |
the Good Order test suffers from the fast linear scan |
Good Order test suffers from the fast linear scan phenomenon |
SOC applications will need to combine two types of content |
Order test suffers from the fast linear scan phenomenon described |
test suffers from the fast linear scan phenomenon described in |
suffers from the fast linear scan phenomenon described in Section |
the prefetching subsystem is able to prefetch some files accessed |
prefetching subsystem is able to prefetch some files accessed at |
subsystem is able to prefetch some files accessed at the |
is able to prefetch some files accessed at the end |
able to prefetch some files accessed at the end of |
to prefetch some files accessed at the end of the |
Existing web service technologies make it easy to build applications |
prefetch some files accessed at the end of the test |
web service technologies make it easy to build applications in |
service technologies make it easy to build applications in which |
technologies make it easy to build applications in which all |
so that it can prevent inconsistencies by inhibiting access to |
make it easy to build applications in which all data |
that it can prevent inconsistencies by inhibiting access to the |
it easy to build applications in which all data travels |
it can prevent inconsistencies by inhibiting access to the file |
easy to build applications in which all data travels through |
can prevent inconsistencies by inhibiting access to the file by |
to build applications in which all data travels through a |
prevent inconsistencies by inhibiting access to the file by other |
build applications in which all data travels through a data |
inconsistencies by inhibiting access to the file by other clients |
applications in which all data travels through a data center |
Implementing collaboration features using these technologies is problematic because collaborative |
collaboration features using these technologies is problematic because collaborative applications |
features using these technologies is problematic because collaborative applications can |
using these technologies is problematic because collaborative applications can generate |
these technologies is problematic because collaborative applications can generate high |
bursty update rates and yet often require low latencies and |
update rates and yet often require low latencies and tight |
to allow clients to avoid sending data across a wide |
rates and yet often require low latencies and tight synchronization |
and yet often require low latencies and tight synchronization between |
yet often require low latencies and tight synchronization between collaborating |
often require low latencies and tight synchronization between collaborating users |
the server only asks the client for a file s |
server only asks the client for a file s data |
only asks the client for a file s data if |
asks the client for a file s data if another |
the client for a file s data if another client |
client for a file s data if another client requests |
for a file s data if another client requests it |
This problem is reflected by a growing number of publications |
problem is reflected by a growing number of publications on |
is reflected by a growing number of publications on the |
so we based our analysis on that along with the |
reflected by a growing number of publications on the integration |
we based our analysis on that along with the assumption |
by a growing number of publications on the integration of |
based our analysis on that along with the assumption each |
a growing number of publications on the integration of web |
our analysis on that along with the assumption each revision |
growing number of publications on the integration of web services |
analysis on that along with the assumption each revision data |
number of publications on the integration of web services with |
on that along with the assumption each revision data file |
of publications on the integration of web services with peer |
that along with the assumption each revision data file would |
along with the assumption each revision data file would be |
the averages observed for the other repositories in table II |
TABLE II M OST RECENT MONTHLY COST OF STORING REPOSITORIES |
II M OST RECENT MONTHLY COST OF STORING REPOSITORIES IN |
M OST RECENT MONTHLY COST OF STORING REPOSITORIES IN S |
FOR INDIVIDUAL PROJECTS AND ENTIRE COMMUNITIES Software Project SquirrelMail phpMyAdmin |
INDIVIDUAL PROJECTS AND ENTIRE COMMUNITIES Software Project SquirrelMail phpMyAdmin Subversion |
PROJECTS AND ENTIRE COMMUNITIES Software Project SquirrelMail phpMyAdmin Subversion Mono |
AND ENTIRE COMMUNITIES Software Project SquirrelMail phpMyAdmin Subversion Mono KDE |
ENTIRE COMMUNITIES Software Project SquirrelMail phpMyAdmin Subversion Mono KDE Hosting |
COMMUNITIES Software Project SquirrelMail phpMyAdmin Subversion Mono KDE Hosting Community |
Software Project SquirrelMail phpMyAdmin Subversion Mono KDE Hosting Community Debian |
Project SquirrelMail phpMyAdmin Subversion Mono KDE Hosting Community Debian Linux |
SquirrelMail phpMyAdmin Subversion Mono KDE Hosting Community Debian Linux Community |
phpMyAdmin Subversion Mono KDE Hosting Community Debian Linux Community Apache |
Subversion Mono KDE Hosting Community Debian Linux Community Apache Software |
Mono KDE Hosting Community Debian Linux Community Apache Software Foundation |
from a file group if it appeared that a process |
KDE Hosting Community Debian Linux Community Apache Software Foundation Monthly |
a file group if it appeared that a process was |
Hosting Community Debian Linux Community Apache Software Foundation Monthly Cost |
file group if it appeared that a process was not |
group if it appeared that a process was not using |
if it appeared that a process was not using the |
it appeared that a process was not using the files |
appeared that a process was not using the files prefetched |
that a process was not using the files prefetched based |
a process was not using the files prefetched based on |
process was not using the files prefetched based on its |
was not using the files prefetched based on its prior |
not using the files prefetched based on its prior accesses |
This would reduce the overhead in the Bad Groups case |
The second would explicitly detect a fast linear scan by |
second would explicitly detect a fast linear scan by a |
would explicitly detect a fast linear scan by a process |
by counting the instances of prefetch and demand fetch conflict |
counting the instances of prefetch and demand fetch conflict for |
the instances of prefetch and demand fetch conflict for a |
instances of prefetch and demand fetch conflict for a file |
of prefetch and demand fetch conflict for a file group |
our experimental results have demonstrated the benefits of MFS adaptation |
experimental results have demonstrated the benefits of MFS adaptation mechanisms |
results have demonstrated the benefits of MFS adaptation mechanisms at |
have demonstrated the benefits of MFS adaptation mechanisms at various |
demonstrated the benefits of MFS adaptation mechanisms at various levels |
the benefits of MFS adaptation mechanisms at various levels of |
benefits of MFS adaptation mechanisms at various levels of bandwidth |
of MFS adaptation mechanisms at various levels of bandwidth availability |
but not when the bandwidth is changing over the duration |
not when the bandwidth is changing over the duration of |
when the bandwidth is changing over the duration of the |
the bandwidth is changing over the duration of the test |
To conclude this section we will describe an example of |
conclude this section we will describe an example of MFS |
this section we will describe an example of MFS traffic |
section we will describe an example of MFS traffic under |
we will describe an example of MFS traffic under the |
will describe an example of MFS traffic under the execution |
describe an example of MFS traffic under the execution of |
an example of MFS traffic under the execution of the |
example of MFS traffic under the execution of the Simultaneous |
of MFS traffic under the execution of the Simultaneous Writeback |
MFS traffic under the execution of the Simultaneous Writeback test |
traffic under the execution of the Simultaneous Writeback test described |
It is very unlikely that any vendor could provide a |
under the execution of the Simultaneous Writeback test described in |
is very unlikely that any vendor could provide a traditional |
the execution of the Simultaneous Writeback test described in Section |
very unlikely that any vendor could provide a traditional storage |
unlikely that any vendor could provide a traditional storage solution |
that any vendor could provide a traditional storage solution consisting |
after which the solution can be shared in a file |
any vendor could provide a traditional storage solution consisting of |
which the solution can be shared in a file or |
vendor could provide a traditional storage solution consisting of SCSI |
the solution can be shared in a file or via |
could provide a traditional storage solution consisting of SCSI disks |
solution can be shared in a file or via email |
provide a traditional storage solution consisting of SCSI disks and |
can be shared in a file or via email and |
a traditional storage solution consisting of SCSI disks and tape |
be shared in a file or via email and opened |
traditional storage solution consisting of SCSI disks and tape backup |
but is slightly modified from original version to use a |
shared in a file or via email and opened on |
storage solution consisting of SCSI disks and tape backup at |
is slightly modified from original version to use a longer |
in a file or via email and opened on other |
solution consisting of SCSI disks and tape backup at this |
slightly modified from original version to use a longer think |
a file or via email and opened on other machines |
consisting of SCSI disks and tape backup at this price |
modified from original version to use a longer think time |
from original version to use a longer think time of |
they can interact with the application and peers see the |
storage required of course increases each month as the repository |
can interact with the application and peers see the results |
required of course increases each month as the repository grows |
interact with the application and peers see the results instantly |
Updates are applied to all replicas in a consistent manner |
We enabled asynchronous writeback and ran the test with the |
enabled asynchronous writeback and ran the test with the synthetic |
asynchronous writeback and ran the test with the synthetic bandwidth |
writeback and ran the test with the synthetic bandwidth trace |
and ran the test with the synthetic bandwidth trace shown |
P communication can coexist with more standard solutions that reach |
ran the test with the synthetic bandwidth trace shown in |
communication can coexist with more standard solutions that reach back |
the test with the synthetic bandwidth trace shown in Figure |
can coexist with more standard solutions that reach back to |
Cumulative distributions for the staleness of all accesses to files |
coexist with more standard solutions that reach back to the |
distributions for the staleness of all accesses to files by |
with more standard solutions that reach back to the hosted |
for the staleness of all accesses to files by the |
more standard solutions that reach back to the hosted content |
the staleness of all accesses to files by the three |
standard solutions that reach back to the hosted content and |
staleness of all accesses to files by the three readers |
solutions that reach back to the hosted content and trigger |
of all accesses to files by the three readers are |
that reach back to the hosted content and trigger updates |
all accesses to files by the three readers are shown |
reach back to the hosted content and trigger updates at |
back to the hosted content and trigger updates at the |
to the hosted content and trigger updates at the associated |
the hosted content and trigger updates at the associated data |
hosted content and trigger updates at the associated data centers |
it can use protocols that bypass the data center to |
can use protocols that bypass the data center to achieve |
use protocols that bypass the data center to achieve the |
protocols that bypass the data center to achieve the full |
that bypass the data center to achieve the full performance |
bypass the data center to achieve the full performance of |
the data center to achieve the full performance of the |
data center to achieve the full performance of the network |
Summary of results The test was executed once with prefetching |
of results The test was executed once with prefetching enabled |
IP to attain very high speeds on the gigabit link |
and the RPCs were then divided acwe have shown that |
the RPCs were then divided acwe have shown that workloads |
we had to set the MTU of the entire path |
RPCs were then divided acwe have shown that workloads which |
had to set the MTU of the entire path to |
were then divided acwe have shown that workloads which are |
to set the MTU of the entire path to be |
then divided acwe have shown that workloads which are amenable |
set the MTU of the entire path to be the |
divided acwe have shown that workloads which are amenable to |
the MTU of the entire path to be the maximum |
We list the key challenges that SOC applications place on |
acwe have shown that workloads which are amenable to file |
list the key challenges that SOC applications place on their |
the key challenges that SOC applications place on their runtime |
level cording to which period of the trace they terminated |
key challenges that SOC applications place on their runtime environments |
cording to which period of the trace they terminated in |
This resulted in the fragmentation of repair packets sent over |
resulted in the fragmentation of repair packets sent over UDP |
one instance should be enough for almost any individual project |
in the fragmentation of repair packets sent over UDP on |
instance should be enough for almost any individual project or |
We discuss the relative advantages of these two approaches for |
the fragmentation of repair packets sent over UDP on the |
should be enough for almost any individual project or moderately |
discuss the relative advantages of these two approaches for building |
fragmentation of repair packets sent over UDP on the longhaul |
be enough for almost any individual project or moderately sized |
the relative advantages of these two approaches for building SOC |
of repair packets sent over UDP on the longhaul link |
enough for almost any individual project or moderately sized community |
relative advantages of these two approaches for building SOC applications |
repair packets sent over UDP on the longhaul link into |
packets sent over UDP on the longhaul link into two |
sent over UDP on the longhaul link into two IP |
We discuss the advantages of decoupling transport and information layers |
Usage Patterns In addition to getting a grasp of the |
and the time taken for each to be carries a |
over UDP on the longhaul link into two IP packet |
discuss the advantages of decoupling transport and information layers as |
Patterns In addition to getting a grasp of the costs |
the time taken for each to be carries a small |
UDP on the longhaul link into two IP packet fragments |
the advantages of decoupling transport and information layers as a |
In addition to getting a grasp of the costs involved |
Since the loss of a single fragment resulted in the |
advantages of decoupling transport and information layers as a means |
time taken for each to be carries a small performance |
addition to getting a grasp of the costs involved in |
the loss of a single fragment resulted in the loss |
of decoupling transport and information layers as a means of |
taken for each to be carries a small performance overhead |
to getting a grasp of the costs involved in moving |
loss of a single fragment resulted in the loss of |
decoupling transport and information layers as a means of achieving |
getting a grasp of the costs involved in moving a |
of a single fragment resulted in the loss of the |
transport and information layers as a means of achieving reusability |
a grasp of the costs involved in moving a repository |
a single fragment resulted in the loss of the repair |
grasp of the costs involved in moving a repository to |
of the costs involved in moving a repository to S |
ability to rapidly deploy SOC applications in new environments and |
we observed a higher loss rate for repairs than for |
to rapidly deploy SOC applications in new environments and adapt |
observed a higher loss rate for repairs than for data |
It is possible to construct combination of file between the |
rapidly deploy SOC applications in new environments and adapt them |
a higher loss rate for repairs than for data packets |
is possible to construct combination of file between the client |
deploy SOC applications in new environments and adapt them dynamically |
Since achieving the consistency properties that developers expect will require |
possible to construct combination of file between the client and |
SOC applications in new environments and adapt them dynamically This |
we expect performance to be better on a network where |
achieving the consistency properties that developers expect will require a |
to construct combination of file between the client and the |
applications in new environments and adapt them dynamically This work |
expect performance to be better on a network where the |
the consistency properties that developers expect will require a consistency |
construct combination of file between the client and the server |
in new environments and adapt them dynamically This work was |
performance to be better on a network where the MTU |
consistency properties that developers expect will require a consistency layer |
new environments and adapt them dynamically This work was supported |
but these quantities are small groups and a workload for |
to be better on a network where the MTU of |
properties that developers expect will require a consistency layer to |
these quantities are small groups and a workload for which |
be better on a network where the MTU of the |
that developers expect will require a consistency layer to be |
quantities are small groups and a workload for which prefetching |
better on a network where the MTU of the long |
developers expect will require a consistency layer to be built |
are small groups and a workload for which prefetching can |
Qi Huang is a visiting scientist from the School of |
expect will require a consistency layer to be built in |
haul link is truly larger than the MTU within each |
small groups and a workload for which prefetching can significantly |
Huang is a visiting scientist from the School of Computer |
will require a consistency layer to be built in front |
link is truly larger than the MTU within each cluster |
groups and a workload for which prefetching can significantly compared |
is a visiting scientist from the School of Computer Sci |
since writes can be sent to the file server faster |
require a consistency layer to be built in front of |
and a workload for which prefetching can significantly compared to |
a consistency layer to be built in front of S |
a workload for which prefetching can significantly compared to the |
workload for which prefetching can significantly compared to the other |
for which prefetching can significantly compared to the other costs |
it is crucial that any such layer be able to |
is crucial that any such layer be able to handle |
crucial that any such layer be able to handle the |
this is due to the cap on throughput placed by |
that any such layer be able to handle the load |
and the results are shown Within the constraints imposed by |
is due to the cap on throughput placed by the |
any such layer be able to handle the load of |
the results are shown Within the constraints imposed by our |
due to the cap on throughput placed by the buffering |
such layer be able to handle the load of commits |
results are shown Within the constraints imposed by our file |
to the cap on throughput placed by the buffering available |
The critical statistic to consider is the number of simultaneous |
are shown Within the constraints imposed by our file group |
the cap on throughput placed by the buffering available at |
critical statistic to consider is the number of simultaneous commits |
shown Within the constraints imposed by our file group representa |
cap on throughput placed by the buffering available at the |
on throughput placed by the buffering available at the receiving |
throughput placed by the buffering available at the receiving end |
and any change to a versioned file is stored as |
any change to a versioned file is stored as a |
change to a versioned file is stored as a diff |
to a versioned file is stored as a diff against |
a versioned file is stored as a diff against its |
since they are constrained by the bandwidth bottleneck and send |
versioned file is stored as a diff against its previous |
they are constrained by the bandwidth bottleneck and send updates |
file is stored as a diff against its previous version |
and is consequently sensitive to the size of the receiver |
are constrained by the bandwidth bottleneck and send updates in |
in which instances of distributed communication protocols are modeled uniformly |
is consequently sensitive to the size of the receiver buffer |
A commit must be rejected if any of the versioned |
constrained by the bandwidth bottleneck and send updates in the |
which instances of distributed communication protocols are modeled uniformly as |
the main conclusion we draw from the test cases exhibitThe |
by the bandwidth bottleneck and send updates in the same |
commit must be rejected if any of the versioned files |
instances of distributed communication protocols are modeled uniformly as objects |
main conclusion we draw from the test cases exhibitThe graphs |
the bandwidth bottleneck and send updates in the same order |
must be rejected if any of the versioned files that |
of distributed communication protocols are modeled uniformly as objects similar |
conclusion we draw from the test cases exhibitThe graphs show |
be rejected if any of the versioned files that it |
distributed communication protocols are modeled uniformly as objects similar to |
we draw from the test cases exhibitThe graphs show how |
SIRP reduces its bandwidth usage and achieves a small improvement |
rejected if any of the versioned files that it touches |
communication protocols are modeled uniformly as objects similar to those |
draw from the test cases exhibitThe graphs show how priorities |
split mode flow control eliminates the requirement for large buffers |
reduces its bandwidth usage and achieves a small improvement over |
if any of the versioned files that it touches have |
protocols are modeled uniformly as objects similar to those in |
from the test cases exhibitThe graphs show how priorities affect |
mode flow control eliminates the requirement for large buffers at |
its bandwidth usage and achieves a small improvement over SIRP |
any of the versioned files that it touches have been |
are modeled uniformly as objects similar to those in Java |
the test cases exhibitThe graphs show how priorities affect RPCs |
flow control eliminates the requirement for large buffers at the |
of the versioned files that it touches have been changed |
test cases exhibitThe graphs show how priorities affect RPCs and |
since devoting less bandwidth to invalidations results in data reaching |
control eliminates the requirement for large buffers at the receiving |
the versioned files that it touches have been changed in |
cases exhibitThe graphs show how priorities affect RPCs and how |
devoting less bandwidth to invalidations results in data reaching the |
eliminates the requirement for large buffers at the receiving end |
The embedded script is often tightly integrated with backend services |
versioned files that it touches have been changed in an |
exhibitThe graphs show how priorities affect RPCs and how prefetching |
less bandwidth to invalidations results in data reaching the server |
embedded script is often tightly integrated with backend services in |
files that it touches have been changed in an earlier |
graphs show how priorities affect RPCs and how prefetching a |
bandwidth to invalidations results in data reaching the server faster |
script is often tightly integrated with backend services in the |
that it touches have been changed in an earlier revision |
with a slight drop due to buffering overhead on the |
show how priorities affect RPCs and how prefetching a prefetch |
is often tightly integrated with backend services in the data |
it touches have been changed in an earlier revision that |
a slight drop due to buffering overhead on the Maelstrom |
how priorities affect RPCs and how prefetching a prefetch penalty |
often tightly integrated with backend services in the data center |
touches have been changed in an earlier revision that the |
slight drop due to buffering overhead on the Maelstrom boxes |
priorities affect RPCs and how prefetching a prefetch penalty is |
This is because the progress of writers using asynchronous writeback |
have been changed in an earlier revision that the developer |
making it awkward to access the underlying services directly from |
affect RPCs and how prefetching a prefetch penalty is that |
is because the progress of writers using asynchronous writeback schemes |
been changed in an earlier revision that the developer performing |
it awkward to access the underlying services directly from a |
RPCs and how prefetching a prefetch penalty is that the |
because the progress of writers using asynchronous writeback schemes is |
changed in an earlier revision that the developer performing the |
awkward to access the underlying services directly from a different |
and how prefetching a prefetch penalty is that the implementation |
the progress of writers using asynchronous writeback schemes is less |
in an earlier revision that the developer performing the commit |
to access the underlying services directly from a different script |
how prefetching a prefetch penalty is that the implementation could |
progress of writers using asynchronous writeback schemes is less constrained |
an earlier revision that the developer performing the commit was |
access the underlying services directly from a different script or |
prefetching a prefetch penalty is that the implementation could be |
of writers using asynchronous writeback schemes is less constrained by |
earlier revision that the developer performing the commit was unaware |
the underlying services directly from a different script or a |
a prefetch penalty is that the implementation could be im |
writers using asynchronous writeback schemes is less constrained by the |
revision that the developer performing the commit was unaware of |
underlying services directly from a different script or a standalone |
using asynchronous writeback schemes is less constrained by the bandwidth |
services directly from a different script or a standalone client |
This ensures that every conflict gets resolved by a human |
more time proved to incorporate a mechanism to inhibit prefetching |
ensures that every conflict gets resolved by a human before |
and they can overlap computation and fetching file contents with |
The is spent on RPCs to fetch file attributes with |
the only way such services can be mashed up with |
that every conflict gets resolved by a human before becoming |
they can overlap computation and fetching file contents with writeback |
is spent on RPCs to fetch file attributes with prefetching |
only way such services can be mashed up with other |
Rather than simply being a selfinterested optimisation by writers to |
spent on RPCs to fetch file attributes with prefetching enabled |
every conflict gets resolved by a human before becoming part |
way such services can be mashed up with other web |
than simply being a selfinterested optimisation by writers to improve |
on RPCs to fetch file attributes with prefetching enabled current |
conflict gets resolved by a human before becoming part of |
such services can be mashed up with other web content |
simply being a selfinterested optimisation by writers to improve their |
RPCs to fetch file attributes with prefetching enabled current prefetching |
gets resolved by a human before becoming part of the |
services can be mashed up with other web content is |
being a selfinterested optimisation by writers to improve their own |
to fetch file attributes with prefetching enabled current prefetching algorithm |
resolved by a human before becoming part of the repository |
can be mashed up with other web content is by |
a selfinterested optimisation by writers to improve their own performance |
fetch file attributes with prefetching enabled current prefetching algorithm does |
by a human before becoming part of the repository s |
be mashed up with other web content is by either |
file attributes with prefetching enabled current prefetching algorithm does not |
a human before becoming part of the repository s state |
mashed up with other web content is by either having |
attributes with prefetching enabled current prefetching algorithm does not correlate |
up with other web content is by either having the |
with prefetching enabled current prefetching algorithm does not correlate file |
with other web content is by either having the data |
prefetching enabled current prefetching algorithm does not correlate file accesses |
other web content is by either having the data center |
Taking a loose definition of simultaneous to be within one |
enabled current prefetching algorithm does not correlate file accesses with |
web content is by either having the data center compute |
a loose definition of simultaneous to be within one minute |
current prefetching algorithm does not correlate file accesses with than |
content is by either having the data center compute the |
to prevent the clients falling into lockstep in the course |
prefetching algorithm does not correlate file accesses with than without |
is by either having the data center compute the mashup |
prevent the clients falling into lockstep in the course of |
the clients falling into lockstep in the course of fetching |
clients falling into lockstep in the course of fetching and |
falling into lockstep in the course of fetching and writing |
into lockstep in the course of fetching and writing back |
lockstep in the course of fetching and writing back the |
in the course of fetching and writing back the files |
or by embedding the entire minibrowser window in a web |
by embedding the entire minibrowser window in a web page |
and the second and third bar from left are split |
the second and third bar from left are split mode |
But an embedded minibrowser can t seamlessly blend with the |
In determining these numbers we filtered out any sequences of |
second and third bar from left are split mode and |
an embedded minibrowser can t seamlessly blend with the surrounding |
determining these numbers we filtered out any sequences of multiple |
consisting of selecting a random file set and performing a |
and third bar from left are split mode and end |
embedded minibrowser can t seamlessly blend with the surrounding content |
these numbers we filtered out any sequences of multiple commits |
of selecting a random file set and performing a sequence |
numbers we filtered out any sequences of multiple commits by |
it is like a standalone browser within its own frame |
selecting a random file set and performing a sequence of |
we filtered out any sequences of multiple commits by the |
a random file set and performing a sequence of reads |
filtered out any sequences of multiple commits by the same |
random file set and performing a sequence of reads or |
out any sequences of multiple commits by the same author |
Split mode performs as well with default sized buffers as |
file set and performing a sequence of reads or writes |
any sequences of multiple commits by the same author during |
mode performs as well with default sized buffers as end |
set and performing a sequence of reads or writes on |
sequences of multiple commits by the same author during a |
and performing a sequence of reads or writes on files |
of multiple commits by the same author during a one |
performing a sequence of reads or writes on files in |
multiple commits by the same author during a one minute |
a sequence of reads or writes on files in it |
commits by the same author during a one minute period |
by the same author during a one minute period since |
the same author during a one minute period since those |
maps and weather web services and assembling it into a |
same author during a one minute period since those were |
and weather web services and assembling it into a web |
author during a one minute period since those were likely |
weather web services and assembling it into a web page |
during a one minute period since those were likely sequential |
web services and assembling it into a web page as |
a one minute period since those were likely sequential rather |
services and assembling it into a web page as a |
one minute period since those were likely sequential rather than |
and assembling it into a web page as a set |
minute period since those were likely sequential rather than simultaneous |
assembling it into a web page as a set of |
with each access being equally likely to open a file |
period since those were likely sequential rather than simultaneous and |
it into a web page as a set of tiled |
each access being equally likely to open a file for |
since those were likely sequential rather than simultaneous and do |
into a web page as a set of tiled frames |
access being equally likely to open a file for reading |
those were likely sequential rather than simultaneous and do nor |
being equally likely to open a file for reading or |
Each frame is a minibrowser with its own interactive controls |
were likely sequential rather than simultaneous and do nor represent |
equally likely to open a file for reading or writing |
likely sequential rather than simultaneous and do nor represent the |
sequential rather than simultaneous and do nor represent the common |
rather than simultaneous and do nor represent the common case |
if the user pans or zooms in the map frame |
but the other frames remain as they were the frames |
the other frames remain as they were the frames are |
other frames remain as they were the frames are not |
frames remain as they were the frames are not synchronized |
Here we see a similar application constructed using Live Objects |
so exclusive locking for commits should not pose any scalability |
of the file set operations were directed to those file |
content from different sources is overlaid in the same window |
exclusive locking for commits should not pose any scalability problems |
the file set operations were directed to those file sets |
from different sources is overlaid in the same window and |
locking for commits should not pose any scalability problems in |
different sources is overlaid in the same window and synchronized |
Read staleness Comparing update propagation schemes requires a criterion for |
for commits should not pose any scalability problems in a |
staleness Comparing update propagation schemes requires a criterion for measuring |
We used white backgrounds to highlight the contributions of different |
commits should not pose any scalability problems in a typical |
Comparing update propagation schemes requires a criterion for measuring the |
used white backgrounds to highlight the contributions of different sources |
should not pose any scalability problems in a typical environment |
update propagation schemes requires a criterion for measuring the staleness |
propagation schemes requires a criterion for measuring the staleness of |
We did not consider the rate of read operations because |
schemes requires a criterion for measuring the staleness of file |
did not consider the rate of read operations because clients |
requires a criterion for measuring the staleness of file reads |
not consider the rate of read operations because clients updating |
consider the rate of read operations because clients updating their |
We identified updates to files by associating a version number |
the rate of read operations because clients updating their working |
identified updates to files by associating a version number with |
rate of read operations because clients updating their working copies |
updates to files by associating a version number with each |
to files by associating a version number with each file |
exist layers within which the end user can easily navigate |
The Debian community today uses only a single Subversion server |
Reads were labelled with the version number of the file |
Data can come from many kinds of We discuss our |
were labelled with the version number of the file at |
and the Apache foundation has a master server plus a |
can come from many kinds of We discuss our Live |
labelled with the version number of the file at the |
the Apache foundation has a master server plus a European |
come from many kinds of We discuss our Live Distributed |
with the version number of the file at the time |
Apache foundation has a master server plus a European mirror |
from many kinds of We discuss our Live Distributed Objects |
the version number of the file at the time the |
many kinds of We discuss our Live Distributed Objects platform |
version number of the file at the time the read |
kinds of We discuss our Live Distributed Objects platform as |
we expect that most communities will have at most a |
number of the file at the time the read occurred |
of We discuss our Live Distributed Objects platform as an |
expect that most communities will have at most a handful |
We discuss our Live Distributed Objects platform as an example |
that most communities will have at most a handful of |
The staleness of a particular read was determined according to |
discuss our Live Distributed Objects platform as an example of |
most communities will have at most a handful of front |
staleness of a particular read was determined according to an |
our Live Distributed Objects platform as an example of a |
of a particular read was determined according to an ideal |
Live Distributed Objects platform as an example of a technology |
a particular read was determined according to an ideal version |
Distributed Objects platform as an example of a technology that |
Achieving Consistency Amazon s infrastructure is built on the principle |
particular read was determined according to an ideal version number |
Objects platform as an example of a technology that fits |
Consistency Amazon s infrastructure is built on the principle of |
read was determined according to an ideal version number derived |
platform as an example of a technology that fits well |
Amazon s infrastructure is built on the principle of eventual |
was determined according to an ideal version number derived from |
as an example of a technology that fits well with |
s infrastructure is built on the principle of eventual consistency |
determined according to an ideal version number derived from executing |
an example of a technology that fits well with the |
according to an ideal version number derived from executing the |
example of a technology that fits well with the layered |
to an ideal version number derived from executing the experiment |
an ideal version number derived from executing the experiment with |
ideal version number derived from executing the experiment with all |
and does not directly support the locking required for revision |
version number derived from executing the experiment with all participants |
does not directly support the locking required for revision control |
number derived from executing the experiment with all participants running |
derived from executing the experiment with all participants running on |
Originally developed to run the company s own online store |
from executing the experiment with all participants running on a |
peer communication protocols as an underlying communication substrate for SOC |
executing the experiment with all participants running on a single |
the system preferred availability over consistency because downtime translated directly |
communication protocols as an underlying communication substrate for SOC applications |
the experiment with all participants running on a single host |
system preferred availability over consistency because downtime translated directly into |
The relative strengths of each of the solutions tested and |
preferred availability over consistency because downtime translated directly into lost |
relative strengths of each of the solutions tested and the |
availability over consistency because downtime translated directly into lost revenue |
the difference between the version number a read returns and |
strengths of each of the solutions tested and the lack |
Customers may opt to shop elsewhere or to simply forgo |
difference between the version number a read returns and the |
of each of the solutions tested and the lack of |
may opt to shop elsewhere or to simply forgo impulse |
between the version number a read returns and the optimal |
each of the solutions tested and the lack of a |
opt to shop elsewhere or to simply forgo impulse purchases |
the version number a read returns and the optimal version |
of the solutions tested and the lack of a clear |
to shop elsewhere or to simply forgo impulse purchases that |
version number a read returns and the optimal version number |
the solutions tested and the lack of a clear winner |
shop elsewhere or to simply forgo impulse purchases that they |
number a read returns and the optimal version number determines |
solutions tested and the lack of a clear winner serve |
elsewhere or to simply forgo impulse purchases that they didn |
a read returns and the optimal version number determines how |
tested and the lack of a clear winner serve as |
or to simply forgo impulse purchases that they didn t |
read returns and the optimal version number determines how stale |
and the lack of a clear winner serve as a |
to simply forgo impulse purchases that they didn t really |
returns and the optimal version number determines how stale the |
the lack of a clear winner serve as a further |
simply forgo impulse purchases that they didn t really need |
and the optimal version number determines how stale the read |
lack of a clear winner serve as a further justification |
forgo impulse purchases that they didn t really need anyway |
the optimal version number determines how stale the read is |
of a clear winner serve as a further justification for |
a clear winner serve as a further justification for the |
clear winner serve as a further justification for the decoupling |
winner serve as a further justification for the decoupling of |
shows cumulative distributions for the staleness of reads at different |
serve as a further justification for the decoupling of information |
cumulative distributions for the staleness of reads at different writer |
as a further justification for the decoupling of information and |
It is well known that consistency and availability cannot both |
a further justification for the decoupling of information and transport |
is well known that consistency and availability cannot both be |
further justification for the decoupling of information and transport layers |
and this is reflected by a curve that is higher |
well known that consistency and availability cannot both be achieved |
justification for the decoupling of information and transport layers advocated |
this is reflected by a curve that is higher on |
known that consistency and availability cannot both be achieved simultaneously |
for the decoupling of information and transport layers advocated above |
is reflected by a curve that is higher on the |
that consistency and availability cannot both be achieved simultaneously in |
reflected by a curve that is higher on the left |
consistency and availability cannot both be achieved simultaneously in any |
by a curve that is higher on the left side |
and availability cannot both be achieved simultaneously in any real |
a curve that is higher on the left side of |
Limitations of the existing model There are two important reasons |
availability cannot both be achieved simultaneously in any real network |
curve that is higher on the left side of the |
of the existing model There are two important reasons why |
cannot both be achieved simultaneously in any real network where |
that is higher on the left side of the graph |
the existing model There are two important reasons why integrating |
both be achieved simultaneously in any real network where hosts |
Consistency maintenance cost The overhead of the update propagation schemes |
existing model There are two important reasons why integrating peerto |
be achieved simultaneously in any real network where hosts or |
maintenance cost The overhead of the update propagation schemes can |
achieved simultaneously in any real network where hosts or entire |
cost The overhead of the update propagation schemes can be |
simultaneously in any real network where hosts or entire subnetworks |
The overhead of the update propagation schemes can be compared |
The first is not strictly limited to collaboration and peer |
in any real network where hosts or entire subnetworks are |
overhead of the update propagation schemes can be compared by |
any real network where hosts or entire subnetworks are sometimes |
of the update propagation schemes can be compared by referring |
real network where hosts or entire subnetworks are sometimes unreachable |
the update propagation schemes can be compared by referring to |
network where hosts or entire subnetworks are sometimes unreachable due |
update propagation schemes can be compared by referring to the |
it is a general weakness of the current web mashup |
where hosts or entire subnetworks are sometimes unreachable due to |
propagation schemes can be compared by referring to the reader |
is a general weakness of the current web mashup technologies |
hosts or entire subnetworks are sometimes unreachable due to connectivity |
schemes can be compared by referring to the reader and |
a general weakness of the current web mashup technologies that |
or entire subnetworks are sometimes unreachable due to connectivity losses |
can be compared by referring to the reader and writer |
general weakness of the current web mashup technologies that makes |
be compared by referring to the reader and writer execution |
weakness of the current web mashup technologies that makes it |
compared by referring to the reader and writer execution times |
of the current web mashup technologies that makes it hard |
the current web mashup technologies that makes it hard to |
If a cloud service is designed to provide high availability |
current web mashup technologies that makes it hard to seamlessly |
a cloud service is designed to provide high availability but |
web mashup technologies that makes it hard to seamlessly integrate |
cloud service is designed to provide high availability but an |
mashup technologies that makes it hard to seamlessly integrate data |
reader execution time is the average for all three readers |
service is designed to provide high availability but an application |
technologies that makes it hard to seamlessly integrate data from |
is designed to provide high availability but an application instead |
that makes it hard to seamlessly integrate data from several |
designed to provide high availability but an application instead requires |
makes it hard to seamlessly integrate data from several different |
the reduced staleness achievable by SIRP has little or no |
to provide high availability but an application instead requires perfect |
it hard to seamlessly integrate data from several different sources |
reduced staleness achievable by SIRP has little or no cost |
provide high availability but an application instead requires perfect consistency |
The web developers community has slowly converged towards service platforms |
staleness achievable by SIRP has little or no cost compared |
web developers community has slowly converged towards service platforms that |
achievable by SIRP has little or no cost compared to |
developers community has slowly converged towards service platforms that export |
For revision control it makes sense to adopt eventual consistency |
by SIRP has little or no cost compared to asynchronous |
community has slowly converged towards service platforms that export autonomous |
revision control it makes sense to adopt eventual consistency for |
SIRP has little or no cost compared to asynchronous writeback |
has slowly converged towards service platforms that export autonomous interactive |
control it makes sense to adopt eventual consistency for read |
has little or no cost compared to asynchronous writeback with |
slowly converged towards service platforms that export autonomous interactive components |
it makes sense to adopt eventual consistency for read operations |
little or no cost compared to asynchronous writeback with no |
converged towards service platforms that export autonomous interactive components to |
or no cost compared to asynchronous writeback with no invalidations |
towards service platforms that export autonomous interactive components to their |
service platforms that export autonomous interactive components to their clients |
in the form of what we ll call minibrowser interfaces |
A minibrowser is an interactive web page with embedded script |
he can retry and expect that version to be available |
can retry and expect that version to be available within |
retry and expect that version to be available within a |
but this is because it provides the best consistency of |
and expect that version to be available within a short |
this is because it provides the best consistency of all |
expect that version to be available within a short timeframe |
for example interactive maps from Google Earth or Virtual Earth |
is because it provides the best consistency of all the |
because it provides the best consistency of all the schemes |
Our example actually overlays weather from Google on terrain maps |
perfect consistency is required and a locking layer must be |
example actually overlays weather from Google on terrain maps from |
consistency is required and a locking layer must be built |
actually overlays weather from Google on terrain maps from Microsoft |
is required and a locking layer must be built to |
overlays weather from Google on terrain maps from Microsoft s |
required and a locking layer must be built to support |
weather from Google on terrain maps from Microsoft s Virtual |
the reader execution time for each case is proportional to |
and a locking layer must be built to support this |
from Google on terrain maps from Microsoft s Virtual Earth |
reader execution time for each case is proportional to the |
This may result in a commit being rejected if consensus |
Google on terrain maps from Microsoft s Virtual Earth platform |
execution time for each case is proportional to the amount |
may result in a commit being rejected if consensus cannot |
on terrain maps from Microsoft s Virtual Earth platform and |
time for each case is proportional to the amount of |
result in a commit being rejected if consensus cannot be |
terrain maps from Microsoft s Virtual Earth platform and extracts |
for each case is proportional to the amount of data |
in a commit being rejected if consensus cannot be reached |
maps from Microsoft s Virtual Earth platform and extracts census |
each case is proportional to the amount of data transferred |
from Microsoft s Virtual Earth platform and extracts census data |
but shouldn t be a problem because code changes are |
case is proportional to the amount of data transferred between |
Microsoft s Virtual Earth platform and extracts census data from |
shouldn t be a problem because code changes are usually |
is proportional to the amount of data transferred between the |
s Virtual Earth platform and extracts census data from the |
t be a problem because code changes are usually not |
proportional to the amount of data transferred between the reader |
Virtual Earth platform and extracts census data from the US |
be a problem because code changes are usually not impulse |
to the amount of data transferred between the reader and |
Earth platform and extracts census data from the US Census |
a problem because code changes are usually not impulse decisions |
the amount of data transferred between the reader and server |
platform and extracts census data from the US Census Bureau |
problem because code changes are usually not impulse decisions and |
because code changes are usually not impulse decisions and the |
though lack of space precludes showing this in a graph |
code changes are usually not impulse decisions and the commit |
The second problem is that with the traditional style of |
changes are usually not impulse decisions and the commit can |
second problem is that with the traditional style of web |
are usually not impulse decisions and the commit can be |
problem is that with the traditional style of web development |
Mbps flow alongside on the same link to simulate a |
usually not impulse decisions and the commit can be retried |
flow alongside on the same link to simulate a real |
Rimon Barr and Stephen Rago for comments regarding this work |
not impulse decisions and the commit can be retried later |
Web pages downloaded by clients browsers contain embedded addresses of |
pages downloaded by clients browsers contain embedded addresses of specific |
downloaded by clients browsers contain embedded addresses of specific servers |
but traffic is still always routed through a data center |
Second Annual Joint Conference of the IEEE Computer and Communications |
Annual Joint Conference of the IEEE Computer and Communications Societies |
vn is colocated with Subversion and inserts a layer between |
is colocated with Subversion and inserts a layer between Subversion |
colocated with Subversion and inserts a layer between Subversion and |
Live Objects allow visual content and update events to be |
with Subversion and inserts a layer between Subversion and S |
Objects allow visual content and update events to be communicated |
allow visual content and update events to be communicated using |
visual content and update events to be communicated using any |
content and update events to be communicated using any sort |
and update events to be communicated using any sort of |
update events to be communicated using any sort of protocol |
For simplicity we did not modify the Subversion server in |
simplicity we did not modify the Subversion server in any |
shows the same scenario with a constant uniformly random loss |
we did not modify the Subversion server in any way |
the same scenario with a constant uniformly random loss rate |
same scenario with a constant uniformly random loss rate of |
vn is responsible for receiving event notifications from Subversion and |
is responsible for receiving event notifications from Subversion and transferring |
responsible for receiving event notifications from Subversion and transferring data |
for receiving event notifications from Subversion and transferring data between |
receiving event notifications from Subversion and transferring data between the |
event notifications from Subversion and transferring data between the local |
this makes it possible to achieve extremely high levels of |
notifications from Subversion and transferring data between the local disk |
Maelstrom s delivery latency is almost exactly equal to the |
makes it possible to achieve extremely high levels of throughput |
from Subversion and transferring data between the local disk on |
s delivery latency is almost exactly equal to the one |
it possible to achieve extremely high levels of throughput and |
Subversion and transferring data between the local disk on the |
possible to achieve extremely high levels of throughput and latency |
and transferring data between the local disk on the EC |
the data center server can t see data exchanged directly |
data center server can t see data exchanged directly between |
center server can t see data exchanged directly between peers |
Allow web applications to overlay content from multiple sources in |
vn acquires and releases locks using Yahoo s open source |
web applications to overlay content from multiple sources in a |
acquires and releases locks using Yahoo s open source ZooKeeper |
applications to overlay content from multiple sources in a layered |
and releases locks using Yahoo s open source ZooKeeper lock |
to overlay content from multiple sources in a layered fashion |
releases locks using Yahoo s open source ZooKeeper lock service |
such that the distinct content layers share a single view |
that the distinct content layers share a single view and |
The difficulty achieving consistency with a service such as Amazon |
the distinct content layers share a single view and remain |
difficulty achieving consistency with a service such as Amazon s |
distinct content layers share a single view and remain well |
achieving consistency with a service such as Amazon s S |
content layers share a single view and remain well synchronized |
stems from the fact that files pushed into the storage |
from the fact that files pushed into the storage cloud |
the fact that files pushed into the storage cloud do |
fact that files pushed into the storage cloud do not |
that files pushed into the storage cloud do not simultaneously |
files pushed into the storage cloud do not simultaneously become |
and an update in any of the layers should be |
pushed into the storage cloud do not simultaneously become available |
an update in any of the layers should be reflected |
into the storage cloud do not simultaneously become available on |
update in any of the layers should be reflected in |
the storage cloud do not simultaneously become available on all |
in any of the layers should be reflected in all |
storage cloud do not simultaneously become available on all service |
any of the layers should be reflected in all other |
cloud do not simultaneously become available on all service endpoints |
of the layers should be reflected in all other layers |
Allow updates to be carried by the protocol best matched |
updates to be carried by the protocol best matched to |
to be carried by the protocol best matched to the |
and even the same client may see the old version |
be carried by the protocol best matched to the setting |
even the same client may see the old version if |
carried by the protocol best matched to the setting in |
the same client may see the old version if it |
by the protocol best matched to the setting in which |
same client may see the old version if it suddenly |
the protocol best matched to the setting in which the |
client may see the old version if it suddenly switches |
E DD E DD F EDD F G FG E |
protocol best matched to the setting in which the application |
may see the old version if it suddenly switches to |
DD E DD F EDD F G FG E ED |
In Proceedings of the Twelth Symposium on Operating Systems Principles |
best matched to the setting in which the application is |
see the old version if it suddenly switches to speaking |
E DD F EDD F G FG E ED E |
matched to the setting in which the application is used |
the old version if it suddenly switches to speaking with |
DD F EDD F G FG E ED E E |
old version if it suddenly switches to speaking with a |
F EDD F G FG E ED E E D |
version if it suddenly switches to speaking with a different |
EDD F G FG E ED E E D F |
if it suddenly switches to speaking with a different S |
F G FG E ED E E D F EED |
G FG E ED E E D F EED F |
FG E ED E E D F EED F G |
E ED E E D F EED F G FG |
ED E E D F EED F G FG E |
E E D F EED F G FG E D |
E D F EED F G FG E D E |
new types of components must be created for each type |
D F EED F G FG E D E D |
types of components must be created for each type of |
but its contents may not reflect expectations that the client |
This experiment demonstrates that SIRP is preferable to asynchronous writeback |
A key point is that we are plotting the delivery |
of components must be created for each type of content |
F EED F G FG E D E D F |
its contents may not reflect expectations that the client formed |
experiment demonstrates that SIRP is preferable to asynchronous writeback at |
key point is that we are plotting the delivery latency |
but the existing collection of components provides access to several |
contents may not reflect expectations that the client formed based |
demonstrates that SIRP is preferable to asynchronous writeback at low |
point is that we are plotting the delivery latency of |
the existing collection of components provides access to several different |
may not reflect expectations that the client formed based on |
that SIRP is preferable to asynchronous writeback at low bandwidth |
is that we are plotting the delivery latency of all |
existing collection of components provides access to several different types |
not reflect expectations that the client formed based on other |
that we are plotting the delivery latency of all packets |
collection of components provides access to several different types of |
reflect expectations that the client formed based on other files |
of components provides access to several different types of web |
expectations that the client formed based on other files and |
components provides access to several different types of web services |
The spikes in latency are triggered by losses that lead |
that the client formed based on other files and out |
and SIRP makes it an acceptable choice at low bandwidth |
BCQ PCB C BQ PCB CBQPCB N N ON N |
spikes in latency are triggered by losses that lead to |
provides access to several different types of web services hosted |
the client formed based on other files and out of |
PCB C BQ PCB CBQPCB N N ON N C |
in latency are triggered by losses that lead to packets |
access to several different types of web services hosted content |
client formed based on other files and out of band |
C BQ PCB CBQPCB N N ON N C BC |
latency are triggered by losses that lead to packets piling |
formed based on other files and out of band communication |
BQ PCB CBQPCB N N ON N C BC BONN |
are triggered by losses that lead to packets piling up |
PCB CBQPCB N N ON N C BC BONN C |
triggered by losses that lead to packets piling up both |
the resulting live application is stored as an XML file |
CBQPCB N N ON N C BC BONN C BC |
by losses that lead to packets piling up both at |
The file can be moved about and even embedded in |
N N ON N C BC BONN C BC B |
vn works around the consistency problem by storing the number |
losses that lead to packets piling up both at the |
file can be moved about and even embedded in email |
N ON N C BC BONN C BC B CBCB |
works around the consistency problem by storing the number of |
that lead to packets piling up both at the receiver |
Users that open it find themselves immersed into the application |
around the consistency problem by storing the number of the |
lead to packets piling up both at the receiver and |
Several transport protocols optimized for various settings are or will |
the consistency problem by storing the number of the latest |
to packets piling up both at the receiver and the |
transport protocols optimized for various settings are or will be |
consistency problem by storing the number of the latest revision |
packets piling up both at the receiver and the sender |
protocols optimized for various settings are or will be available |
problem by storing the number of the latest revision into |
optimized for various settings are or will be available in |
by storing the number of the latest revision into ZooKeeper |
IP delays correctly received packets at the receiver while waiting |
for various settings are or will be available in a |
delays correctly received packets at the receiver while waiting for |
various settings are or will be available in a near |
correctly received packets at the receiver while waiting for missing |
settings are or will be available in a near future |
received packets at the receiver while waiting for missing packets |
is represented by Subversion has a single file containing binary |
packets at the receiver while waiting for missing packets sequenced |
represented by Subversion has a single file containing binary diffs |
at the receiver while waiting for missing packets sequenced earlier |
by Subversion has a single file containing binary diffs against |
the receiver while waiting for missing packets sequenced earlier by |
Subversion has a single file containing binary diffs against earlier |
receiver while waiting for missing packets sequenced earlier by the |
has a single file containing binary diffs against earlier revisions |
while waiting for missing packets sequenced earlier by the sender |
It also delays packets at the sender when it cuts |
also delays packets at the sender when it cuts down |
delays packets at the sender when it cuts down on |
end server attempting to fetch a revision i from S |
C BC B CBCB KK J M LKJJ ML ML |
packets at the sender when it cuts down on the |
BC B CBCB KK J M LKJJ ML ML C |
at the sender when it cuts down on the sending |
B CBCB KK J M LKJJ ML ML C B |
the sender when it cuts down on the sending window |
CBCB KK J M LKJJ ML ML C B C |
or a missing file error if i was posted so |
sender when it cuts down on the sending window size |
KK J M LKJJ ML ML C B C B |
a missing file error if i was posted so recently |
when it cuts down on the sending window size in |
J M LKJJ ML ML C B C B CB |
missing file error if i was posted so recently that |
it cuts down on the sending window size in response |
M LKJJ ML ML C B C B CB KJ |
file error if i was posted so recently that it |
cuts down on the sending window size in response to |
LKJJ ML ML C B C B CB KJ IH |
error if i was posted so recently that it has |
down on the sending window size in response to the |
ML ML C B C B CB KJ IH I |
if i was posted so recently that it has not |
on the sending window size in response to the loss |
ML C B C B CB KJ IH I H |
the sending window size in response to the loss events |
C B C B CB KJ IH I H IH |
B C B CB KJ IH I H IH J |
The delays caused by these two mechanisms are illustrated in |
delays caused by these two mechanisms are illustrated in Figure |
where single packet losses cause spikes in delivery latency that |
single packet losses cause spikes in delivery latency that last |
packet losses cause spikes in delivery latency that last for |
losses cause spikes in delivery latency that last for hundreds |
cause spikes in delivery latency that last for hundreds of |
spikes in delivery latency that last for hundreds of packets |
C BC B C BC B CBCB RPC times at |
we analyze a concrete example of a SOC application more |
BC B C BC B CBCB RPC times at low |
analyze a concrete example of a SOC application more carefully |
B C BC B CBCB RPC times at low bandwidth |
a concrete example of a SOC application more carefully to |
concrete example of a SOC application more carefully to expose |
example of a SOC application more carefully to expose the |
of a SOC application more carefully to expose the full |
request queued request send reply queued reply send total time |
a SOC application more carefully to expose the full range |
SOC application more carefully to expose the full range of |
application more carefully to expose the full range of needs |
more carefully to expose the full range of needs and |
carefully to expose the full range of needs and issues |
to expose the full range of needs and issues that |
expose the full range of needs and issues that arise |
a police or fire chief coordinating teams who will enter |
police or fire chief coordinating teams who will enter a |
or fire chief coordinating teams who will enter a disaster |
fire chief coordinating teams who will enter a disaster zone |
chief coordinating teams who will enter a disaster zone in |
coordinating teams who will enter a disaster zone in the |
teams who will enter a disaster zone in the wake |
who will enter a disaster zone in the wake of |
will enter a disaster zone in the wake of a |
enter a disaster zone in the wake of a catastrophe |
a disaster zone in the wake of a catastrophe to |
disaster zone in the wake of a catastrophe to help |
zone in the wake of a catastrophe to help survivors |
end servers are equivalent and clients may interact with any |
servers are equivalent and clients may interact with any of |
are equivalent and clients may interact with any of them |
equivalent and clients may interact with any of them Fig |
show the time spent on RPCs during an execution of |
the time spent on RPCs during an execution of the |
The application built by the coordinator would be installed on |
time spent on RPCs during an execution of the Simultaneous |
application built by the coordinator would be installed on each |
spent on RPCs during an execution of the Simultaneous Writeback |
built by the coordinator would be installed on each team |
on RPCs during an execution of the Simultaneous Writeback test |
by the coordinator would be installed on each team member |
RPCs during an execution of the Simultaneous Writeback test from |
the coordinator would be installed on each team member s |
during an execution of the Simultaneous Writeback test from Section |
In Proceedings of the First USENIX Conference on File and |
coordinator would be installed on each team member s mobile |
Proceedings of the First USENIX Conference on File and Storage |
would be installed on each team member s mobile device |
of the First USENIX Conference on File and Storage Technologies |
Our rescue workers now use the solution to coordinate and |
rescue workers now use the solution to coordinate and prioritize |
workers now use the solution to coordinate and prioritize actions |
ZooKeeper ensures that the latest revision number is incremented atomically |
nodes may store a small amount of data and can |
may store a small amount of data and can have |
store a small amount of data and can have children |
a new file system for mobile clients that is tailored |
new file system for mobile clients that is tailored for |
and the team member who causes or observes these status |
file system for mobile clients that is tailored for wireless |
the team member who causes or observes these status changes |
system for mobile clients that is tailored for wireless networks |
team member who causes or observes these status changes would |
for mobile clients that is tailored for wireless networks by |
member who causes or observes these status changes would need |
mobile clients that is tailored for wireless networks by incorporating |
who causes or observes these status changes would need to |
clients that is tailored for wireless networks by incorporating automatic |
The time spent on RPCs is shown with prefetching enabled |
causes or observes these status changes would need to report |
that is tailored for wireless networks by incorporating automatic adaptation |
or observes these status changes would need to report them |
is tailored for wireless networks by incorporating automatic adaptation to |
observes these status changes would need to report them to |
end server must acquire a lock by creating a sequence |
tailored for wireless networks by incorporating automatic adaptation to the |
these status changes would need to report them to the |
server must acquire a lock by creating a sequence node |
for wireless networks by incorporating automatic adaptation to the available |
status changes would need to report them to the others |
wireless networks by incorporating automatic adaptation to the available bandwidth |
Note that RPC interactions can overlap so the quantities for |
that RPC interactions can overlap so the quantities for different |
MAFS differs from previous designs in making use of asynchronous |
RPC interactions can overlap so the quantities for different RPC |
removing debris blocking access to a building may enable the |
differs from previous designs in making use of asynchronous writeback |
interactions can overlap so the quantities for different RPC types |
debris blocking access to a building may enable the team |
from previous designs in making use of asynchronous writeback at |
can overlap so the quantities for different RPC types are |
blocking access to a building may enable the team to |
previous designs in making use of asynchronous writeback at all |
overlap so the quantities for different RPC types are not |
access to a building may enable the team to check |
designs in making use of asynchronous writeback at all bandwidth |
so the quantities for different RPC types are not additive |
to a building may enable the team to check it |
in making use of asynchronous writeback at all bandwidth levels |
a building may enable the team to check it for |
building may enable the team to check it for victims |
the time spent on particular activities is negligible in proportion |
rather than switching from synchronous to asynchronous writeback when bandwidth |
time spent on particular activities is negligible in proportion to |
and fire that breaks out in a chemical storage warehouse |
than switching from synchronous to asynchronous writeback when bandwidth is |
spent on particular activities is negligible in proportion to the |
fire that breaks out in a chemical storage warehouse may |
switching from synchronous to asynchronous writeback when bandwidth is insufficient |
on particular activities is negligible in proportion to the overall |
that breaks out in a chemical storage warehouse may force |
Otherwise it watches the node with the next lower number |
particular activities is negligible in proportion to the overall time |
breaks out in a chemical storage warehouse may force diversion |
it watches the node with the next lower number in |
out in a chemical storage warehouse may force diversion of |
watches the node with the next lower number in order |
and permit a degree of consistency that is equivalent to |
attribute requests are small and have a very low transmission |
the node with the next lower number in order to |
in a chemical storage warehouse may force diversion of resources |
permit a degree of consistency that is equivalent to instantaneous |
requests are small and have a very low transmission time |
node with the next lower number in order to be |
a degree of consistency that is equivalent to instantaneous propagation |
are small and have a very low transmission time relative |
with the next lower number in order to be notified |
their mobile devices send updates that must be propagated in |
degree of consistency that is equivalent to instantaneous propagation of |
small and have a very low transmission time relative to |
the next lower number in order to be notified when |
mobile devices send updates that must be propagated in real |
of consistency that is equivalent to instantaneous propagation of updates |
and have a very low transmission time relative to their |
next lower number in order to be notified when that |
have a very low transmission time relative to their queueing |
Experiments demonstrate that these techniques allow MAFS to achieve performance |
now let s analyze in more detail the requirements it |
a very low transmission time relative to their queueing delays |
lower number in order to be notified when that node |
demonstrate that these techniques allow MAFS to achieve performance that |
such users happen to be working on the same element |
number in order to be notified when that node and |
let s analyze in more detail the requirements it places |
that these techniques allow MAFS to achieve performance that is |
users happen to be working on the same element of |
in order to be notified when that node and its |
s analyze in more detail the requirements it places on |
these techniques allow MAFS to achieve performance that is at |
happen to be working on the same element of the |
order to be notified when that node and its associated |
analyze in more detail the requirements it places on our |
techniques allow MAFS to achieve performance that is at least |
to be working on the same element of the design |
to be notified when that node and its associated lock |
in more detail the requirements it places on our collaboration |
allow MAFS to achieve performance that is at least equal |
be notified when that node and its associated lock go |
it is clear that satisfying a request from stale data |
more detail the requirements it places on our collaboration tool |
MAFS to achieve performance that is at least equal to |
notified when that node and its associated lock go away |
or on a server that has yet to see a |
and in most cases superior to that achievable by conventional |
on a server that has yet to see a delayed |
in most cases superior to that achievable by conventional file |
the collaboration tool pulls data from many kinds of sources |
a server that has yet to see a delayed writeback |
most cases superior to that achievable by conventional file system |
It makes far more sense to imagine that weather information |
cases superior to that achievable by conventional file system designs |
superior to that achievable by conventional file system designs that |
to that achievable by conventional file system designs that switch |
Strong cache consistency is certainly achievable in distributed file systems |
that achievable by conventional file system designs that switch between |
achievable by conventional file system designs that switch between lowand |
by conventional file system designs that switch between lowand high |
Lock nodes are marked with ZooKeeper s ephemeral flag to |
nodes are marked with ZooKeeper s ephemeral flag to ensure |
messages and alerts come from a dozen providers than to |
are marked with ZooKeeper s ephemeral flag to ensure that |
MAFS is therefore able to make efficient use of the |
and alerts come from a dozen providers than to assume |
marked with ZooKeeper s ephemeral flag to ensure that the |
is therefore able to make efficient use of the network |
and requires either readers or writers to incur a delay |
alerts come from a dozen providers than to assume that |
with ZooKeeper s ephemeral flag to ensure that the lock |
therefore able to make efficient use of the network and |
requires either readers or writers to incur a delay to |
come from a dozen providers than to assume that one |
ZooKeeper s ephemeral flag to ensure that the lock is |
able to make efficient use of the network and provide |
either readers or writers to incur a delay to ensure |
from a dozen providers than to assume that one organization |
s ephemeral flag to ensure that the lock is forcibly |
to make efficient use of the network and provide predictable |
readers or writers to incur a delay to ensure that |
a dozen providers than to assume that one organization would |
ephemeral flag to ensure that the lock is forcibly released |
make efficient use of the network and provide predictable file |
or writers to incur a delay to ensure that only |
dozen providers than to assume that one organization would be |
flag to ensure that the lock is forcibly released if |
efficient use of the network and provide predictable file system |
writers to incur a delay to ensure that only the |
providers than to assume that one organization would be hosting |
to ensure that the lock is forcibly released if the |
use of the network and provide predictable file system semantics |
to incur a delay to ensure that only the latest |
than to assume that one organization would be hosting services |
ensure that the lock is forcibly released if the front |
incur a delay to ensure that only the latest version |
to assume that one organization would be hosting services with |
a delay to ensure that only the latest version of |
assume that one organization would be hosting services with everything |
delay to ensure that only the latest version of a |
that one organization would be hosting services with everything we |
to ensure that only the latest version of a file |
so it remains available as long as a majority of |
one organization would be hosting services with everything we need |
ensure that only the latest version of a file is |
it remains available as long as a majority of the |
organization would be hosting services with everything we need in |
that only the latest version of a file is accessed |
remains available as long as a majority of the hosts |
would be hosting services with everything we need in one |
In Proceedings of the Sixteenth ACM Symposium on Operating Systems |
available as long as a majority of the hosts are |
be hosting services with everything we need in one place |
Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles |
as long as a majority of the hosts are up |
sending file updates to a server asynchronously has two potential |
long as a majority of the hosts are up and |
Data from distinct sources could have different format and one |
file updates to a server asynchronously has two potential benefits |
as a majority of the hosts are up and reachable |
from distinct sources could have different format and one will |
the process modifying the file need not wait for the |
distinct sources could have different format and one will often |
A client only speaks to one ZooKeeper server at a |
process modifying the file need not wait for the write |
sources could have different format and one will often need |
client only speaks to one ZooKeeper server at a time |
modifying the file need not wait for the write to |
could have different format and one will often need to |
the file need not wait for the write to complete |
have different format and one will often need to interface |
Layered Interleaving and Bursty Loss Thus far we have shown |
different format and one will often need to interface to |
Interleaving and Bursty Loss Thus far we have shown how |
format and one will often need to interface to each |
if the update is delayed in the log for some |
but the server ensures that the relevant state has been |
and Bursty Loss Thus far we have shown how Maelstrom |
and one will often need to interface to each using |
the update is delayed in the log for some interval |
the server ensures that the relevant state has been replicated |
Bursty Loss Thus far we have shown how Maelstrom effectively |
one will often need to interface to each using its |
update is delayed in the log for some interval before |
server ensures that the relevant state has been replicated before |
Loss Thus far we have shown how Maelstrom effectively hides |
will often need to interface to each using its own |
is delayed in the log for some interval before being |
ensures that the relevant state has been replicated before responding |
Thus far we have shown how Maelstrom effectively hides loss |
often need to interface to each using its own protocols |
delayed in the log for some interval before being written |
that the relevant state has been replicated before responding to |
far we have shown how Maelstrom effectively hides loss from |
need to interface to each using its own protocols and |
in the log for some interval before being written back |
the relevant state has been replicated before responding to a |
we have shown how Maelstrom effectively hides loss from TCP |
to interface to each using its own protocols and interfaces |
In Proceedings of the Fifteenth ACM Symposium on Operating Systems |
relevant state has been replicated before responding to a client |
Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles |
state has been replicated before responding to a client s |
these benefits come at the cost of reduced cache consistency |
as conditions evolve the team might need to be modify |
has been replicated before responding to a client s request |
since the version of the file stored at the server |
conditions evolve the team might need to be modify the |
We use a loss model where packets are dropped in |
the version of the file stored at the server is |
evolve the team might need to be modify the application |
use a loss model where packets are dropped in bursts |
version of the file stored at the server is inconsistent |
a loss model where packets are dropped in bursts of |
of the file stored at the server is inconsistent during |
loss model where packets are dropped in bursts of fixed |
the file stored at the server is inconsistent during the |
model where packets are dropped in bursts of fixed length |
file stored at the server is inconsistent during the time |
Unlike the traditional replicated Subversion setups that are used today |
stored at the server is inconsistent during the time that |
allowing us to study the impact of burst length on |
at the server is inconsistent during the time that the |
us to study the impact of burst length on performance |
the server is inconsistent during the time that the update |
Whereas a minibrowser would typically be prebuilt with all the |
server is inconsistent during the time that the update remains |
a minibrowser would typically be prebuilt with all the available |
is inconsistent during the time that the update remains queued |
minibrowser would typically be prebuilt with all the available features |
inconsistent during the time that the update remains queued for |
would typically be prebuilt with all the available features in |
during the time that the update remains queued for transmission |
typically be prebuilt with all the available features in place |
Even though asynchronous writes in MFS are not delayed to |
our scenario demands a much more flexible kind of tool |
though asynchronous writes in MFS are not delayed to aggregate |
scenario demands a much more flexible kind of tool that |
asynchronous writes in MFS are not delayed to aggregate updates |
demands a much more flexible kind of tool that can |
a much more flexible kind of tool that can be |
much more flexible kind of tool that can be redesigned |
a burst of updates to a sequence of files may |
more flexible kind of tool that can be redesigned while |
In Proceedings of the Eighteenth ACM Symposium on Operating Systems |
burst of updates to a sequence of files may flood |
flexible kind of tool that can be redesigned while in |
Proceedings of the Eighteenth ACM Symposium on Operating Systems Principles |
of updates to a sequence of files may flood the |
kind of tool that can be redesigned while in use |
updates to a sequence of files may flood the link |
to a sequence of files may flood the link to |
a sequence of files may flood the link to the |
and varying the number of servers over which the load |
sequence of files may flood the link to the server |
varying the number of servers over which the load was |
of files may flood the link to the server and |
the number of servers over which the load was distributed |
files may flood the link to the server and increase |
may flood the link to the server and increase the |
flood the link to the server and increase the delay |
the link to the server and increase the delay before |
reaching back to hosted services only intermittently when a drone |
Write performance was measured by observing the latency of simultaneous |
link to the server and increase the delay before updates |
back to hosted services only intermittently when a drone aircraft |
performance was measured by observing the latency of simultaneous commits |
to the server and increase the delay before updates towards |
to hosted services only intermittently when a drone aircraft passes |
was measured by observing the latency of simultaneous commits from |
the server and increase the delay before updates towards the |
hosted services only intermittently when a drone aircraft passes within |
measured by observing the latency of simultaneous commits from different |
server and increase the delay before updates towards the end |
services only intermittently when a drone aircraft passes within radio |
by observing the latency of simultaneous commits from different clients |
and increase the delay before updates towards the end of |
only intermittently when a drone aircraft passes within radio range |
increase the delay before updates towards the end of the |
Since simultaneous commits to a single repository would not be |
the delay before updates towards the end of the burst |
simultaneous commits to a single repository would not be a |
delay before updates towards the end of the burst are |
the right choice of protocol should reflect the operating conditions |
commits to a single repository would not be a typical |
before updates towards the end of the burst are committed |
to a single repository would not be a typical case |
the platform should be capable of swapping in a different |
platform should be capable of swapping in a different protocol |
should be capable of swapping in a different protocol without |
be capable of swapping in a different protocol without disrupting |
capable of swapping in a different protocol without disrupting the |
of swapping in a different protocol without disrupting the end |
We therefore refer to this as a hidden upStudies of |
swapping in a different protocol without disrupting the end user |
therefore refer to this as a hidden upStudies of distributed |
refer to this as a hidden upStudies of distributed file |
to this as a hidden upStudies of distributed file systems |
this as a hidden upStudies of distributed file systems have |
as a hidden upStudies of distributed file systems have largely |
a hidden upStudies of distributed file systems have largely concluded |
better is a design in which the presentation object is |
Each client checked out a random repository from a random |
hidden upStudies of distributed file systems have largely concluded that |
is a design in which the presentation object is distinct |
client checked out a random repository from a random front |
upStudies of distributed file systems have largely concluded that file |
In Proceedings of the Fifteenth ACM Symposium on Operating Systems |
a design in which the presentation object is distinct from |
of distributed file systems have largely concluded that file date |
E is correct for high loss rates if the interleaves |
design in which the presentation object is distinct from objects |
and the cache consistency problem caused by asynchronous sharing is |
Changes were propgated in the background to the other front |
is correct for high loss rates if the interleaves are |
Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles |
in which the presentation object is distinct from objects representing |
the cache consistency problem caused by asynchronous sharing is infrequent |
correct for high loss rates if the interleaves are relatively |
which the presentation object is distinct from objects representing information |
cache consistency problem caused by asynchronous sharing is infrequent in |
for high loss rates if the interleaves are relatively prime |
the presentation object is distinct from objects representing information sources |
consistency problem caused by asynchronous sharing is infrequent in general |
end servers can indeed alleviate latency problems caused by high |
presentation object is distinct from objects representing information sources and |
performance improves substantially when loss rates are high and losses |
servers can indeed alleviate latency problems caused by high load |
object is distinct from objects representing information sources and objects |
improves substantially when loss rates are high and losses are |
is distinct from objects representing information sources and objects representing |
and that the overhead of propagating data in the backgound |
substantially when loss rates are high and losses are bursty |
distinct from objects representing information sources and objects representing transport |
that the overhead of propagating data in the backgound is |
from objects representing information sources and objects representing transport protocols |
The graph plots the percentage of lost packets successfully recovered |
the overhead of propagating data in the backgound is not |
graph plots the percentage of lost packets successfully recovered on |
overhead of propagating data in the backgound is not significant |
Decoupling makes it possible to dynamically modify or even replace |
we have identified a class of cache consistency scenarMobile file |
plots the percentage of lost packets successfully recovered on the |
of propagating data in the backgound is not significant enough |
makes it possible to dynamically modify or even replace a |
have identified a class of cache consistency scenarMobile file systems |
the percentage of lost packets successfully recovered on the y |
propagating data in the backgound is not significant enough to |
it possible to dynamically modify or even replace a component |
identified a class of cache consistency scenarMobile file systems such |
data in the backgound is not significant enough to negatively |
axis against an xaxis of loss rates on a log |
possible to dynamically modify or even replace a component with |
a class of cache consistency scenarMobile file systems such as |
in the backgound is not significant enough to negatively affect |
against an xaxis of loss rates on a log scale |
In Proceedings of the Seventeenth ACM Symposium on Operating Systems |
to dynamically modify or even replace a component with some |
class of cache consistency scenarMobile file systems such as Coda |
the backgound is not significant enough to negatively affect performance |
Proceedings of the Seventeenth ACM Symposium on Operating Systems Principles |
dynamically modify or even replace a component with some other |
R ELATED W ORKS Moving services to the cloud has |
ELATED W ORKS Moving services to the cloud has been |
rely on optimistic conios as being of high importance and |
W ORKS Moving services to the cloud has been published |
on optimistic conios as being of high importance and inadequately |
We have posed what may sound like a very specialized |
ORKS Moving services to the cloud has been published on |
optimistic conios as being of high importance and inadequately served |
have posed what may sound like a very specialized problem |
Moving services to the cloud has been published on in |
conios as being of high importance and inadequately served by |
services to the cloud has been published on in other |
but in fact we see this as a good example |
as being of high importance and inadequately served by ex |
to the cloud has been published on in other contexts |
in fact we see this as a good example of |
currency control to resolve the conflicts generated by hidden upisting |
fact we see this as a good example of a |
control to resolve the conflicts generated by hidden upisting mobile |
we see this as a good example of a more |
to resolve the conflicts generated by hidden upisting mobile file |
see this as a good example of a more general |
resolve the conflicts generated by hidden upisting mobile file systems |
this as a good example of a more general kind |
based file system to store multiple versions of backup data |
as a good example of a more general kind of |
file system to store multiple versions of backup data on |
a good example of a more general kind of need |
An alternative approach is to use a variant of callbacks |
system to store multiple versions of backup data on S |
good example of a more general kind of need that |
alternative approach is to use a variant of callbacks to |
example of a more general kind of need that could |
approach is to use a variant of callbacks to design |
of a more general kind of need that could arise |
is to use a variant of callbacks to design is |
a more general kind of need that could arise in |
we show the ability of layered interleaving to provide gracefully |
to use a variant of callbacks to design is maintained |
more general kind of need that could arise in many |
show the ability of layered interleaving to provide gracefully degrading |
use a variant of callbacks to design is maintained on |
general kind of need that could arise in many kinds |
the ability of layered interleaving to provide gracefully degrading performance |
a variant of callbacks to design is maintained on a |
kind of need that could arise in many kinds of |
ability of layered interleaving to provide gracefully degrading performance in |
with SourceForge and Google Code being examples of the latter |
variant of callbacks to design is maintained on a server |
of need that could arise in many kinds of settings |
of layered interleaving to provide gracefully degrading performance in the |
of callbacks to design is maintained on a server and |
layered interleaving to provide gracefully degrading performance in the face |
callbacks to design is maintained on a server and updated |
interleaving to provide gracefully degrading performance in the face of |
consider a physician treating a patient with a complex condition |
to design is maintained on a server and updated by |
Another example of moving a service to the cloud is |
to provide gracefully degrading performance in the face of bursty |
design is maintained on a server and updated by teams |
example of moving a service to the cloud is MetaCDN |
provide gracefully degrading performance in the face of bursty loss |
and who might even be working in a remote location |
is maintained on a server and updated by teams of |
who might even be working in a remote location under |
maintained on a server and updated by teams of de |
might even be working in a remote location under conditions |
we plot the percentage of lost packets successfully recovered against |
even be working in a remote location under conditions demanding |
plot the percentage of lost packets successfully recovered against the |
be working in a remote location under conditions demanding urgent |
The work evaluates the latency of various cloud storage services |
the percentage of lost packets successfully recovered against the length |
working in a remote location under conditions demanding urgent action |
work evaluates the latency of various cloud storage services from |
percentage of lost packets successfully recovered against the length of |
evaluates the latency of various cloud storage services from several |
of lost packets successfully recovered against the length of loss |
the latency of various cloud storage services from several locations |
lost packets successfully recovered against the length of loss bursts |
latency of various cloud storage services from several locations and |
packets successfully recovered against the length of loss bursts for |
of various cloud storage services from several locations and provides |
successfully recovered against the length of loss bursts for two |
may be just as rich and dynamic as in our |
various cloud storage services from several locations and provides an |
recovered against the length of loss bursts for two different |
be just as rich and dynamic as in our search |
cloud storage services from several locations and provides an abstraction |
against the length of loss bursts for two different sets |
just as rich and dynamic as in our search and |
storage services from several locations and provides an abstraction to |
We thank Larry Felser and his team at Autodesk for |
the length of loss bursts for two different sets of |
as rich and dynamic as in our search and rescue |
services from several locations and provides an abstraction to integrate |
thank Larry Felser and his team at Autodesk for their |
length of loss bursts for two different sets of interleaves |
rich and dynamic as in our search and rescue scenario |
from several locations and provides an abstraction to integrate the |
Larry Felser and his team at Autodesk for their help |
several locations and provides an abstraction to integrate the different |
and in the bottom graph we plot the average latency |
Felser and his team at Autodesk for their help in |
locations and provides an abstraction to integrate the different offerings |
in the bottom graph we plot the average latency at |
and his team at Autodesk for their help in understanddevices |
designed for a wired environment might perform poorly or fail |
and provides an abstraction to integrate the different offerings into |
These supervisors read from the server and may also ing |
for a wired environment might perform poorly or fail under |
the bottom graph we plot the average latency at which |
provides an abstraction to integrate the different offerings into a |
supervisors read from the server and may also ing the |
a wired environment might perform poorly or fail under such |
bottom graph we plot the average latency at which the |
an abstraction to integrate the different offerings into a single |
read from the server and may also ing the file |
wired environment might perform poorly or fail under such conditions |
graph we plot the average latency at which the packets |
abstraction to integrate the different offerings into a single system |
from the server and may also ing the file access |
we plot the average latency at which the packets were |
the server and may also ing the file access patterns |
plot the average latency at which the packets were recovered |
server and may also ing the file access patterns that |
and may also ing the file access patterns that arise |
Recovery latency is defined as the difference between the eventual |
may also ing the file access patterns that arise in |
latency is defined as the difference between the eventual delivery |
also ing the file access patterns that arise in collaborative |
is defined as the difference between the eventual delivery time |
ing the file access patterns that arise in collaborative work |
defined as the difference between the eventual delivery time of |
the file access patterns that arise in collaborative work applications |
as the difference between the eventual delivery time of the |
file access patterns that arise in collaborative work applications for |
we believe them to be typical of most SOC applications |
ElasTraS assigns update priviledges for different areas of the data |
the difference between the eventual delivery time of the recovered |
access patterns that arise in collaborative work applications for very |
assigns update priviledges for different areas of the data store |
difference between the eventual delivery time of the recovered packet |
patterns that arise in collaborative work applications for very change |
programmer to rapidly develop a new collaborative application by composing |
update priviledges for different areas of the data store to |
between the eventual delivery time of the recovered packet and |
that arise in collaborative work applications for very change the |
to rapidly develop a new collaborative application by composing together |
priviledges for different areas of the data store to individual |
the eventual delivery time of the recovered packet and the |
arise in collaborative work applications for very change the design |
rapidly develop a new collaborative application by composing together and |
for different areas of the data store to individual front |
eventual delivery time of the recovered packet and the oneway |
develop a new collaborative application by composing together and customizing |
for example to reflect one of the contingencies large architectural |
delivery time of the recovered packet and the oneway latency |
a new collaborative application by composing together and customizing preexisting |
example to reflect one of the contingencies large architectural and |
using the lock service to elect an owner for each |
time of the recovered packet and the oneway latency of |
new collaborative application by composing together and customizing preexisting components |
to reflect one of the contingencies large architectural and engineering |
the lock service to elect an owner for each partition |
We would like to be able to overlay data from |
reflect one of the contingencies large architectural and engineering design |
of the recovered packet and the oneway latency of the |
would like to be able to overlay data from multiple |
one of the contingencies large architectural and engineering design firms |
the recovered packet and the oneway latency of the link |
like to be able to overlay data from multiple sources |
Optimizing Power Consumption in Large Scale Storage Systems Lakshmi Ganesh |
we confirmed that the Emulab link had almost no jitter |
confirmed that the Emulab link had almost no jitter on |
that the Emulab link had almost no jitter on correctly |
the Emulab link had almost no jitter on correctly delivered |
Emulab link had almost no jitter on correctly delivered packets |
We would like to be able to dynamically customize the |
would like to be able to dynamically customize the application |
like to be able to dynamically customize the application at |
defers finegrained locking to the application in order not to |
to be able to dynamically customize the application at runtime |
way latency an accurate estimate of expected lossless delivery time |
finegrained locking to the application in order not to burden |
locking to the application in order not to burden the |
to the application in order not to burden the global |
the application in order not to burden the global lock |
data RPCs have a higher outgoing queueing delay in the |
increasing the interleaves results in much higher recovery percentages at |
application in order not to burden the global lock service |
RPCs have a higher outgoing queueing delay in the absence |
by incorporating new data sources or changing the way data |
the interleaves results in much higher recovery percentages at large |
in order not to burden the global lock service with |
have a higher outgoing queueing delay in the absence of |
edu Abstract Data centers are the backend for a large |
incorporating new data sources or changing the way data is |
interleaves results in much higher recovery percentages at large burst |
order not to burden the global lock service with high |
a higher outgoing queueing delay in the absence of prefetching |
Abstract Data centers are the backend for a large number |
new data sources or changing the way data is presented |
this is due to the majority of the competing RPCs |
not to burden the global lock service with high traffic |
Data centers are the backend for a large number of |
results in much higher recovery percentages at large burst sizes |
is due to the majority of the competing RPCs being |
centers are the backend for a large number of services |
due to the majority of the competing RPCs being high |
We would like to be able to accommodate new types |
are the backend for a large number of services that |
to the majority of the competing RPCs being high priority |
would like to be able to accommodate new types of |
the backend for a large number of services that we |
the majority of the competing RPCs being high priority fetch |
like to be able to accommodate new types of data |
since the latter would have required duplicating much of ZooKeeper |
backend for a large number of services that we take |
to be able to accommodate new types of data sources |
the latter would have required duplicating much of ZooKeeper s |
for a large number of services that we take for |
new formats or protocols that we may not have anticipated |
latter would have required duplicating much of ZooKeeper s functionality |
a large number of services that we take for granted |
formats or protocols that we may not have anticipated at |
until any point where a concurrent demand fetch RPC raises |
large number of services that we take for granted today |
set of interleaves catches almost all packets in an extended |
would have required duplicating much of ZooKeeper s functionality to |
or protocols that we may not have anticipated at the |
any point where a concurrent demand fetch RPC raises their |
of interleaves catches almost all packets in an extended burst |
A significant fraction of the total cost of ownership of |
have required duplicating much of ZooKeeper s functionality to replicate |
protocols that we may not have anticipated at the time |
point where a concurrent demand fetch RPC raises their priorities |
interleaves catches almost all packets in an extended burst of |
significant fraction of the total cost of ownership of these |
required duplicating much of ZooKeeper s functionality to replicate the |
that we may not have anticipated at the time the |
where a concurrent demand fetch RPC raises their priorities to |
fraction of the total cost of ownership of these large |
duplicating much of ZooKeeper s functionality to replicate the leader |
we may not have anticipated at the time the system |
a concurrent demand fetch RPC raises their priorities to the |
much of ZooKeeper s functionality to replicate the leader s |
scale storage systems is the cost of keeping hundreds of |
may not have anticipated at the time the system was |
concurrent demand fetch RPC raises their priorities to the fetch |
of ZooKeeper s functionality to replicate the leader s state |
storage systems is the cost of keeping hundreds of thousands |
not have anticipated at the time the system was released |
The graphs also show recovery latency rising gracefully with the |
systems is the cost of keeping hundreds of thousands of |
Scalability is not an obstacle because there is no need |
graphs also show recovery latency rising gracefully with the increase |
data and prefetch RPCs reveals the effect of the bandwidth |
is the cost of keeping hundreds of thousands of disks |
is not an obstacle because there is no need for |
and it might be necessary for the users to exchange |
also show recovery latency rising gracefully with the increase in |
and prefetch RPCs reveals the effect of the bandwidth decrease |
the cost of keeping hundreds of thousands of disks spinning |
not an obstacle because there is no need for global |
it might be necessary for the users to exchange their |
We present a simple idea that allows the storage system |
an obstacle because there is no need for global locking |
show recovery latency rising gracefully with the increase in loss |
might be necessary for the users to exchange their data |
present a simple idea that allows the storage system to |
obstacle because there is no need for global locking across |
recovery latency rising gracefully with the increase in loss burst |
be necessary for the users to exchange their data without |
a simple idea that allows the storage system to turn |
because there is no need for global locking across multiple |
bandwidth becomes insufficient for a prefetch to complete during the |
necessary for the users to exchange their data without access |
simple idea that allows the storage system to turn off |
latency rising gracefully with the increase in loss burst length |
there is no need for global locking across multiple repositories |
for the users to exchange their data without access to |
idea that allows the storage system to turn off a |
the load can be partitioned across as many ZooKeeper instances |
the users to exchange their data without access to a |
and raisepriority RPCs are triggered by the consequent cache misses |
load can be partitioned across as many ZooKeeper instances as |
that allows the storage system to turn off a large |
users to exchange their data without access to a centralized |
can be partitioned across as many ZooKeeper instances as necessary |
allows the storage system to turn off a large fraction |
to exchange their data without access to a centralized repository |
the queueing delays increase as a proportion of the total |
the storage system to turn off a large fraction of |
Data may be obtained using different types of network protocols |
queueing delays increase as a proportion of the total time |
storage system to turn off a large fraction of its |
and the type of the physical network or protocols may |
delays increase as a proportion of the total time spent |
system to turn off a large fraction of its disks |
and it has been shown that replicating too eagerly leads |
the type of the physical network or protocols may not |
increase as a proportion of the total time spent on |
it has been shown that replicating too eagerly leads quickly |
Of particular appeal is the fact that our solution is |
as a proportion of the total time spent on prefetches |
type of the physical network or protocols may not be |
has been shown that replicating too eagerly leads quickly to |
particular appeal is the fact that our solution is not |
of the physical network or protocols may not be known |
been shown that replicating too eagerly leads quickly to degraded |
appeal is the fact that our solution is not application |
the physical network or protocols may not be known in |
the modifying client to flush its updates whenever another client |
shown that replicating too eagerly leads quickly to degraded performance |
physical network or protocols may not be known in advance |
modifying client to flush its updates whenever another client accesses |
client to flush its updates whenever another client accesses the |
it should be possible to rapidly compose the application using |
to flush its updates whenever another client accesses the file |
should be possible to rapidly compose the application using whatever |
be possible to rapidly compose the application using whatever communication |
possible to rapidly compose the application using whatever communication infrastructure |
to rapidly compose the application using whatever communication infrastructure is |
rapidly compose the application using whatever communication infrastructure is currently |
and present a simulator that allows us to explore this |
compose the application using whatever communication infrastructure is currently available |
present a simulator that allows us to explore this parameter |
we show histograms of recovery latencies for the two interleave |
a simulator that allows us to explore this parameter space |
bandwidth operations that may be concentrated on a small number |
show histograms of recovery latencies for the two interleave configurations |
We also present some initial simulation results that add weight |
operations that may be concentrated on a small number of |
in which an access to a file which has an |
and the topology of the network and its characteristics might |
histograms of recovery latencies for the two interleave configurations under |
also present some initial simulation results that add weight to |
that may be concentrated on a small number of servers |
which an access to a file which has an uncommitted |
the topology of the network and its characteristics might change |
of recovery latencies for the two interleave configurations under different |
present some initial simulation results that add weight to our |
an access to a file which has an uncommitted update |
topology of the network and its characteristics might change over |
recovery latencies for the two interleave configurations under different burst |
some initial simulation results that add weight to our claim |
access to a file which has an uncommitted update at |
of the network and its characteristics might change over time |
latencies for the two interleave configurations under different burst lengths |
initial simulation results that add weight to our claim that |
to a file which has an uncommitted update at a |
simulation results that add weight to our claim that our |
This file system allows clients to detect against malicious or |
a file which has an uncommitted update at a different |
results that add weight to our claim that our solution |
packet recoveries take longer from left to right as we |
file system allows clients to detect against malicious or compromised |
file which has an uncommitted update at a different client |
that add weight to our claim that our solution represents |
recoveries take longer from left to right as we increase |
system allows clients to detect against malicious or compromised storage |
which has an uncommitted update at a different client will |
add weight to our claim that our solution represents a |
in which the web services and hosted content are modeled |
take longer from left to right as we increase loss |
allows clients to detect against malicious or compromised storage servers |
has an uncommitted update at a different client will force |
weight to our claim that our solution represents a new |
which the web services and hosted content are modeled as |
longer from left to right as we increase loss burst |
clients to detect against malicious or compromised storage servers or |
an uncommitted update at a different client will force the |
to our claim that our solution represents a new powersaving |
the web services and hosted content are modeled as reusable |
from left to right as we increase loss burst length |
to detect against malicious or compromised storage servers or hosting |
uncommitted update at a different client will force the writeback |
our claim that our solution represents a new powersaving opportunity |
web services and hosted content are modeled as reusable overlayed |
detect against malicious or compromised storage servers or hosting platforms |
and from top to bottom as we increase the interleave |
claim that our solution represents a new powersaving opportunity for |
The MFS consistency algorithm differs in its incorporation of file |
services and hosted content are modeled as reusable overlayed information |
against malicious or compromised storage servers or hosting platforms by |
from top to bottom as we increase the interleave values |
that our solution represents a new powersaving opportunity for large |
MFS consistency algorithm differs in its incorporation of file access |
and hosted content are modeled as reusable overlayed information layers |
malicious or compromised storage servers or hosting platforms by providing |
consistency algorithm differs in its incorporation of file access information |
hosted content are modeled as reusable overlayed information layers backed |
or compromised storage servers or hosting platforms by providing fork |
illustrates the difference between a traditional FEC code and layered |
content are modeled as reusable overlayed information layers backed by |
Introduction The declining costs of commodity disk drives has made |
Rather than enforce the same level of consistency for all |
compromised storage servers or hosting platforms by providing fork consistency |
the difference between a traditional FEC code and layered interleaving |
are modeled as reusable overlayed information layers backed by customizable |
The declining costs of commodity disk drives has made online |
than enforce the same level of consistency for all files |
difference between a traditional FEC code and layered interleaving by |
modeled as reusable overlayed information layers backed by customizable transport |
a property which ensures that clients can detect integrity failures |
declining costs of commodity disk drives has made online data |
between a traditional FEC code and layered interleaving by plotting |
as reusable overlayed information layers backed by customizable transport layers |
property which ensures that clients can detect integrity failures as |
which have recently only been accessed by a single client |
costs of commodity disk drives has made online data storage |
a traditional FEC code and layered interleaving by plotting a |
which ensures that clients can detect integrity failures as long |
of commodity disk drives has made online data storage a |
ensures that clients can detect integrity failures as long as |
Enforcing cache consistency between clients necessarily requires that shared files |
commodity disk drives has made online data storage a way |
that clients can detect integrity failures as long as they |
cache consistency between clients necessarily requires that shared files are |
disk drives has made online data storage a way of |
clients can detect integrity failures as long as they see |
consistency between clients necessarily requires that shared files are kept |
side coexistence of components that might today be implemented as |
drives has made online data storage a way of life |
can detect integrity failures as long as they see each |
between clients necessarily requires that shared files are kept highly |
coexistence of components that might today be implemented as proprietary |
detect integrity failures as long as they see each other |
clients necessarily requires that shared files are kept highly consistent |
So much so that companies like Google and Yahoo host |
of components that might today be implemented as proprietary minibrowsers |
but modifications to private files can be written back to |
much so that companies like Google and Yahoo host hundreds |
integrity failures as long as they see each other s |
modifications to private files can be written back to the |
so that companies like Google and Yahoo host hundreds of |
failures as long as they see each other s file |
we need to agree on the events and representation that |
to private files can be written back to the server |
that companies like Google and Yahoo host hundreds of thousands |
as long as they see each other s file modifications |
need to agree on the events and representation that the |
private files can be written back to the server less |
companies like Google and Yahoo host hundreds of thousands of |
to agree on the events and representation that the dialog |
files can be written back to the server less aggressively |
Similar techniques could be used to recover data from client |
like Google and Yahoo host hundreds of thousands of servers |
agree on the events and representation that the dialog will |
techniques could be used to recover data from client working |
The technique of using file access patterns to adjust a |
Google and Yahoo host hundreds of thousands of servers for |
on the events and representation that the dialog will employ |
could be used to recover data from client working copies |
technique of using file access patterns to adjust a cache |
The decoupling of functionality into layers also suggests a need |
be used to recover data from client working copies in |
and Yahoo host hundreds of thousands of servers for storage |
of using file access patterns to adjust a cache consistency |
decoupling of functionality into layers also suggests a need for |
used to recover data from client working copies in the |
using file access patterns to adjust a cache consistency protocol |
of functionality into layers also suggests a need for a |
to recover data from client working copies in the event |
file access patterns to adjust a cache consistency protocol has |
functionality into layers also suggests a need for a standardized |
recover data from client working copies in the event of |
Not only does this translate to many millions of dollars |
access patterns to adjust a cache consistency protocol has been |
into layers also suggests a need for a standardized layering |
data from client working copies in the event of a |
only does this translate to many millions of dollars annually |
patterns to adjust a cache consistency protocol has been used |
from client working copies in the event of a catastrophic |
does this translate to many millions of dollars annually on |
to adjust a cache consistency protocol has been used in |
client working copies in the event of a catastrophic cloud |
this translate to many millions of dollars annually on electricity |
adjust a cache consistency protocol has been used in the |
the linkage layer that talks to the underlying data source |
working copies in the event of a catastrophic cloud failure |
translate to many millions of dollars annually on electricity bills |
a cache consistency protocol has been used in the Sprite |
cache consistency protocol has been used in the Sprite distributed |
the heat produced by so much computing power can be |
consistency protocol has been used in the Sprite distributed operation |
one might imagine enabling mashups in ways not previously possible |
heat produced by so much computing power can be searing |
protocol has been used in the Sprite distributed operation system |
a natural way of thinking about components that dates back |
An article in The New York Times describes one of |
natural way of thinking about components that dates back to |
article in The New York Times describes one of Google |
way of thinking about components that dates back to Smalltalk |
in The New York Times describes one of Google s |
The New York Times describes one of Google s data |
New York Times describes one of Google s data centers |
though in Sprite changes in caching policy were made when |
rather than having the data center developer offer content through |
in Sprite changes in caching policy were made when a |
than having the data center developer offer content through proprietary |
Sprite changes in caching policy were made when a file |
having the data center developer offer content through proprietary minibrowser |
changes in caching policy were made when a file was |
the data center developer offer content through proprietary minibrowser interface |
with twin cooling plants protruding four stories into the sky |
seeks to enable such applications by granting direct access of |
in caching policy were made when a file was opened |
to enable such applications by granting direct access of cloud |
caching policy were made when a file was opened simultaneously |
enable such applications by granting direct access of cloud storage |
policy were made when a file was opened simultaneously at |
such applications by granting direct access of cloud storage to |
the visual events delivered by the transport could then be |
were made when a file was opened simultaneously at different |
applications by granting direct access of cloud storage to third |
Power conservation is an important concern for big server clusters |
visual events delivered by the transport could then be delivered |
made when a file was opened simultaneously at different clients |
by granting direct access of cloud storage to third parties |
events delivered by the transport could then be delivered to |
Since disks account for a significant fraction of the energy |
delivered by the transport could then be delivered to an |
disks account for a significant fraction of the energy consumed |
The remainder of this section describes our consistency algorithm in |
by the transport could then be delivered to an information |
why not use a general purpose file system interface to |
remainder of this section describes our consistency algorithm in detail |
the transport could then be delivered to an information layer |
not use a general purpose file system interface to S |
transport could then be delivered to an information layer responsible |
and an evaluation of its effectiveness in reducing cache inconsistencies |
several approaches for disk power management have been proposed and |
could then be delivered to an information layer responsible for |
approaches for disk power management have been proposed and studied |
then be delivered to an information layer responsible for visualizing |
host reader writer parameter delay between accessing modules operations per |
be delivered to an information layer responsible for visualizing them |
reader writer parameter delay between accessing modules operations per module |
But first let us lay out some of the groundwork |
writer parameter delay between accessing modules operations per module delay |
parameter delay between accessing modules operations per module delay between |
and incurring additional monetary costs due to the increased number |
Any disk power management scheme essentially attempts to exploit one |
delay between accessing modules operations per module delay between operations |
incurring additional monetary costs due to the increased number of |
disk power management scheme essentially attempts to exploit one fact |
between accessing modules operations per module delay between operations delay |
additional monetary costs due to the increased number of S |
accessing modules operations per module delay between operations delay between |
either layer could easily be replaced with a different one |
modules operations per module delay between operations delay between accessing |
operations per module delay between operations delay between accessing modules |
per module delay between operations delay between accessing modules operations |
since file append and rename operations do not map efficiently |
module delay between operations delay between accessing modules operations per |
file append and rename operations do not map efficiently to |
peer protocols would also be encapsulated within their respective transport |
delay between operations delay between accessing modules operations per module |
a disk can be shut off so that it consumes |
append and rename operations do not map efficiently to S |
protocols would also be encapsulated within their respective transport layers |
between operations delay between accessing modules operations per module delay |
disk can be shut off so that it consumes no |
operations delay between accessing modules operations per module delay between |
can be shut off so that it consumes no power |
delay between accessing modules operations per module delay between operations |
one version of a transport layer could fetch data directly |
between accessing modules operations per module delay between operations size |
fs that is aware of Subversion s file naming and |
version of a transport layer could fetch data directly from |
only a fraction of them is accessed at any time |
accessing modules operations per module delay between operations size of |
that is aware of Subversion s file naming and use |
of a transport layer could fetch data directly from a |
modules operations per module delay between operations size of external |
so that the rest could potentially be switched to a |
is aware of Subversion s file naming and use scenario |
a transport layer could fetch data directly from a server |
operations per module delay between operations size of external files |
that the rest could potentially be switched to a low |
aware of Subversion s file naming and use scenario could |
transport layer could fetch data directly from a server in |
per module delay between operations size of external files value |
of Subversion s file naming and use scenario could of |
layer could fetch data directly from a server in a |
Subversion s file naming and use scenario could of course |
disk management schemes have to walk the tightrope of finding |
could fetch data directly from a server in a data |
s file naming and use scenario could of course overcome |
management schemes have to walk the tightrope of finding the |
fetch data directly from a server in a data center |
file naming and use scenario could of course overcome these |
schemes have to walk the tightrope of finding the right |
naming and use scenario could of course overcome these limitations |
have to walk the tightrope of finding the right balance |
and use scenario could of course overcome these limitations by |
to walk the tightrope of finding the right balance between |
use scenario could of course overcome these limitations by pushing |
walk the tightrope of finding the right balance between power |
scenario could of course overcome these limitations by pushing only |
the tightrope of finding the right balance between power consumption |
it could leverage different type of hardware or be optimized |
could of course overcome these limitations by pushing only what |
tightrope of finding the right balance between power consumption and |
could leverage different type of hardware or be optimized for |
of course overcome these limitations by pushing only what is |
of finding the right balance between power consumption and performance |
leverage different type of hardware or be optimized for different |
course overcome these limitations by pushing only what is actually |
different type of hardware or be optimized for different types |
The solution space explored thus far in the literature can |
overcome these limitations by pushing only what is actually required |
type of hardware or be optimized for different types of |
solution space explored thus far in the literature can be |
these limitations by pushing only what is actually required into |
of hardware or be optimized for different types of workloads |
space explored thus far in the literature can be divided |
limitations by pushing only what is actually required into S |
explored thus far in the literature can be divided as |
Provided that the different versions of the transport layer conform |
thus far in the literature can be divided as follows |
that the different versions of the transport layer conform to |
the different versions of the transport layer conform to the |
but we believe that such specialized tools are better built |
different versions of the transport layer conform to the same |
we believe that such specialized tools are better built on |
versions of the transport layer conform to the same standardized |
believe that such specialized tools are better built on top |
of the transport layer conform to the same standardized event |
that such specialized tools are better built on top of |
such specialized tools are better built on top of a |
specialized tools are better built on top of a file |
tools are better built on top of a file system |
the application could then switch between them as conditions demand |
are better built on top of a file system abstraction |
better built on top of a file system abstraction than |
built on top of a file system abstraction than pushed |
on top of a file system abstraction than pushed underneath |
top of a file system abstraction than pushed underneath it |
Each of these solutions proposes a new system of some |
of these solutions proposes a new system of some kind |
users interact through Live Objects that transform actions into updates |
C ONCLUSION We have shown that the cost of using |
interact through Live Objects that transform actions into updates that |
based solutions propose novel storage hierarchies to strike the right |
If the file is shared and no other shared update |
ONCLUSION We have shown that the cost of using a |
through Live Objects that transform actions into updates that are |
solutions propose novel storage hierarchies to strike the right balance |
the file is shared and no other shared update is |
We have shown that the cost of using a cloud |
Live Objects that transform actions into updates that are communicated |
propose novel storage hierarchies to strike the right balance between |
file is shared and no other shared update is being |
have shown that the cost of using a cloud computing |
Objects that transform actions into updates that are communicated in |
novel storage hierarchies to strike the right balance between performance |
is shared and no other shared update is being sent |
shown that the cost of using a cloud computing storage |
that transform actions into updates that are communicated in the |
storage hierarchies to strike the right balance between performance and |
that the cost of using a cloud computing storage service |
transform actions into updates that are communicated in the form |
hierarchies to strike the right balance between performance and power |
the cost of using a cloud computing storage service for |
actions into updates that are communicated in the form of |
to strike the right balance between performance and power consumption |
a synchronous forward invalidation RPC is made to the server |
cost of using a cloud computing storage service for source |
into updates that are communicated in the form of events |
synchronous forward invalidation RPC is made to the server at |
disk management solutions interject a new disk management layer on |
of using a cloud computing storage service for source code |
updates that are communicated in the form of events that |
forward invalidation RPC is made to the server at the |
management solutions interject a new disk management layer on top |
using a cloud computing storage service for source code repository |
that are communicated in the form of events that are |
invalidation RPC is made to the server at the highest |
solutions interject a new disk management layer on top of |
a cloud computing storage service for source code repository hosting |
are communicated in the form of events that are shared |
RPC is made to the server at the highest priority |
interject a new disk management layer on top of the |
cloud computing storage service for source code repository hosting is |
communicated in the form of events that are shared via |
a new disk management layer on top of the file |
computing storage service for source code repository hosting is low |
in the form of events that are shared via the |
new disk management layer on top of the file system |
the form of events that are shared via the transport |
a forward invalidation is only made if the update cannot |
form of events that are shared via the transport layer |
which controls disk configuration and data layout to achieve power |
Considering the costs of a resilient local storage system of |
forward invalidation is only made if the update cannot be |
the costs of a resilient local storage system of SCSI |
The protocol implemented by the transport layer might replicate the |
invalidation is only made if the update cannot be transmitted |
costs of a resilient local storage system of SCSI disks |
protocol implemented by the transport layer might replicate the event |
aware caching algorithms that allow large fractions of the storage |
is only made if the update cannot be transmitted immediately |
of a resilient local storage system of SCSI disks and |
caching algorithms that allow large fractions of the storage system |
in practice it can therefore be omitted at high bandwidth |
a resilient local storage system of SCSI disks and tape |
algorithms that allow large fractions of the storage system to |
practice it can therefore be omitted at high bandwidth or |
based interface back to the information layer at which the |
resilient local storage system of SCSI disks and tape backup |
that allow large fractions of the storage system to remain |
it can therefore be omitted at high bandwidth or when |
interface back to the information layer at which the event |
allow large fractions of the storage system to remain idle |
cloud computing is a very attractive solution for this application |
can therefore be omitted at high bandwidth or when traffic |
back to the information layer at which the event has |
The channel is configured to lose singleton packets randomly at |
large fractions of the storage system to remain idle for |
therefore be omitted at high bandwidth or when traffic is |
to the information layer at which the event has originated |
channel is configured to lose singleton packets randomly at a |
vn brings this concept a step closer to becoming reality |
fractions of the storage system to remain idle for longer |
be omitted at high bandwidth or when traffic is low |
the transport layer with the embedded distributed protocol would behave |
of the storage system to remain idle for longer periods |
is configured to lose singleton packets randomly at a loss |
and provides evidence that performance will be acceptable for typical |
transport layer with the embedded distributed protocol would behave very |
the storage system to remain idle for longer periods of |
configured to lose singleton packets randomly at a loss rate |
sending a forward invalidation RPC without requiring the modifying process |
provides evidence that performance will be acceptable for typical use |
layer with the embedded distributed protocol would behave very much |
storage system to remain idle for longer periods of time |
to lose singleton packets randomly at a loss rate of |
a forward invalidation RPC without requiring the modifying process to |
evidence that performance will be acceptable for typical use scenarios |
with the embedded distributed protocol would behave very much like |
forward invalidation RPC without requiring the modifying process to wait |
The principal contribution of this paper is to argue that |
the embedded distributed protocol would behave very much like an |
invalidation RPC without requiring the modifying process to wait introduces |
principal contribution of this paper is to argue that there |
embedded distributed protocol would behave very much like an object |
contribution of this paper is to argue that there is |
distributed protocol would behave very much like an object in |
of this paper is to argue that there is a |
protocol would behave very much like an object in Smalltalk |
this paper is to argue that there is a fourth |
When the server receives a forward invalidation for a shared |
paper is to argue that there is a fourth niche |
Technological impact of magnetic hard disk drives on storage systems |
the server receives a forward invalidation for a shared The |
is to argue that there is a fourth niche as |
and recover all lost packets ReedSolomon uses an interleave of |
server receives a forward invalidation for a shared The MFS |
and indeed in treating them as objects much as we |
to argue that there is a fourth niche as yet |
receives a forward invalidation for a shared The MFS cache |
indeed in treating them as objects much as we treat |
argue that there is a fourth niche as yet unexplored |
a forward invalidation for a shared The MFS cache consistency |
in treating them as objects much as we treat any |
forward invalidation for a shared The MFS cache consistency algorithm |
treating them as objects much as we treat any other |
invalidation for a shared The MFS cache consistency algorithm is |
them as objects much as we treat any other kind |
for a shared The MFS cache consistency algorithm is intended |
and consequently both have a maximum tolerable burst length of |
as objects much as we treat any other kind of |
a shared The MFS cache consistency algorithm is intended to |
objects much as we treat any other kind of object |
shared The MFS cache consistency algorithm is intended to achieve |
we take an idea that has been around for well |
much as we treat any other kind of object in |
take an idea that has been around for well over |
The MFS cache consistency algorithm is intended to achieve a |
as we treat any other kind of object in a |
an idea that has been around for well over a |
MFS cache consistency algorithm is intended to achieve a file |
we treat any other kind of object in a language |
idea that has been around for well over a decade |
treat any other kind of object in a language like |
that has been around for well over a decade now |
any other kind of object in a language like Java |
other kind of object in a language like Java or |
the code is plugged into Maelstrom instead of layered interleaving |
subject to the constraints imposed by tity of the writer |
kind of object in a language like Java or in |
of object in a language like Java or in a |
showing that we can use new encodings within the same |
marks the file as dirty and issues callbacks to file |
object in a language like Java or in a runtime |
that we can use new encodings within the same framework |
the file as dirty and issues callbacks to file semantics |
in a language like Java or in a runtime environment |
we can use new encodings within the same framework seamlessly |
file as dirty and issues callbacks to file semantics and |
a language like Java or in a runtime environment like |
and argue that technological evolution has given it a new |
as dirty and issues callbacks to file semantics and the |
language like Java or in a runtime environment like Jini |
argue that technological evolution has given it a new relevance |
Solomon code recovers all lost packets with roughly the same |
dirty and issues callbacks to file semantics and the desirability |
like Java or in a runtime environment like Jini or |
that technological evolution has given it a new relevance today |
code recovers all lost packets with roughly the same latency |
and issues callbacks to file semantics and the desirability of |
technological evolution has given it a new relevance today as |
recovers all lost packets with roughly the same latency whereas |
issues callbacks to file semantics and the desirability of minimising |
Just as a remotely hosted form of content such as |
all lost packets with roughly the same latency whereas layered |
evolution has given it a new relevance today as a |
callbacks to file semantics and the desirability of minimising overhead |
as a remotely hosted form of content such as a |
lost packets with roughly the same latency whereas layered interleaving |
has given it a new relevance today as a natural |
a remotely hosted form of content such as a map |
packets with roughly the same latency whereas layered interleaving recovers |
given it a new relevance today as a natural power |
If one of these clients fetches the file have opted |
remotely hosted form of content such as a map or |
with roughly the same latency whereas layered interleaving recovers singleton |
one of these clients fetches the file have opted for |
hosted form of content such as a map or an |
roughly the same latency whereas layered interleaving recovers singleton losses |
where other solutions attempt to predict disk access to determine |
form of content such as a map or an image |
of these clients fetches the file have opted for a |
the same latency whereas layered interleaving recovers singleton losses almost |
other solutions attempt to predict disk access to determine which |
of content such as a map or an image of |
these clients fetches the file have opted for a compromise |
same latency whereas layered interleaving recovers singleton losses almost immediately |
solutions attempt to predict disk access to determine which disks |
content such as a map or an image of a |
clients fetches the file have opted for a compromise which |
latency whereas layered interleaving recovers singleton losses almost immediately and |
attempt to predict disk access to determine which disks to |
such as a map or an image of a raincloud |
fetches the file have opted for a compromise which results |
whereas layered interleaving recovers singleton losses almost immediately and exhibits |
to predict disk access to determine which disks to power |
as a map or an image of a raincloud can |
the file have opted for a compromise which results in |
layered interleaving recovers singleton losses almost immediately and exhibits latency |
predict disk access to determine which disks to power down |
a map or an image of a raincloud can be |
file have opted for a compromise which results in a |
interleaving recovers singleton losses almost immediately and exhibits latency spikes |
Brewer s conjecture and the feasibility of consistent available partition |
have opted for a compromise which results in a small |
map or an image of a raincloud can be modeled |
recovers singleton losses almost immediately and exhibits latency spikes whenever |
opted for a compromise which results in a small overhead |
or an image of a raincloud can be modeled as |
singleton losses almost immediately and exhibits latency spikes whenever the |
for a compromise which results in a small overhead before |
an image of a raincloud can be modeled as an |
losses almost immediately and exhibits latency spikes whenever the longer |
a compromise which results in a small overhead before the |
image of a raincloud can be modeled as an object |
almost immediately and exhibits latency spikes whenever the longer loss |
Idea Overview To see why LFS is a natural solution |
compromise which results in a small overhead before the update |
immediately and exhibits latency spikes whenever the longer loss burst |
Overview To see why LFS is a natural solution to |
which results in a small overhead before the update has |
and exhibits latency spikes whenever the longer loss burst occurs |
To see why LFS is a natural solution to the |
results in a small overhead before the update has been |
see why LFS is a natural solution to the problem |
in a small overhead before the update has been committed |
why LFS is a natural solution to the problem of |
R ELATED W ORK Maelstrom lies in the intersection of |
LFS is a natural solution to the problem of disk |
the server sends highbut admits the possibility of a transient |
ELATED W ORK Maelstrom lies in the intersection of two |
is a natural solution to the problem of disk power |
server sends highbut admits the possibility of a transient inconsistency |
W ORK Maelstrom lies in the intersection of two research |
a natural solution to the problem of disk power management |
ORK Maelstrom lies in the intersection of two research areas |
priority server pull RPCs to the clients with outstanding upThe |
Maelstrom lies in the intersection of two research areas that |
server pull RPCs to the clients with outstanding upThe algorithm |
lies in the intersection of two research areas that have |
Server systems typically are not idle long enough to make |
SOC applications are likely to embody quite a range of |
in the intersection of two research areas that have seen |
pull RPCs to the clients with outstanding upThe algorithm requires |
systems typically are not idle long enough to make it |
applications are likely to embody quite a range of P |
the intersection of two research areas that have seen major |
RPCs to the clients with outstanding upThe algorithm requires information |
typically are not idle long enough to make it worthwhile |
intersection of two research areas that have seen major innovations |
to the clients with outstanding upThe algorithm requires information about |
are not idle long enough to make it worthwhile to |
of two research areas that have seen major innovations in |
the clients with outstanding upThe algorithm requires information about client |
not idle long enough to make it worthwhile to incur |
two research areas that have seen major innovations in the |
clients with outstanding upThe algorithm requires information about client accesses |
idle long enough to make it worthwhile to incur the |
research areas that have seen major innovations in the last |
with outstanding upThe algorithm requires information about client accesses in |
long enough to make it worthwhile to incur the time |
areas that have seen major innovations in the last decade |
outstanding upThe algorithm requires information about client accesses in dates |
that have seen major innovations in the last decade high |
power expense of switching the disk to a lowpower mode |
and the application instance running on a given user s |
which causes them to raise the priority of any store |
the application instance running on a given user s machine |
This is a notable point of difference between server systems |
application instance running on a given user s machine could |
is a notable point of difference between server systems and |
instance running on a given user s machine could simultaneously |
a notable point of difference between server systems and typical |
running on a given user s machine could simultaneously display |
notable point of difference between server systems and typical mobile |
on a given user s machine could simultaneously display data |
Since the file server always assumes that an unshared which |
point of difference between server systems and typical mobile device |
a given user s machine could simultaneously display data from |
the file server always assumes that an unshared which is |
of difference between server systems and typical mobile device scenarios |
given user s machine could simultaneously display data from several |
file server always assumes that an unshared which is already |
user s machine could simultaneously display data from several topics |
replacing or supplementing packet loss as a signal of congestion |
server always assumes that an unshared which is already cached |
which makes it hard to translate the solutions devised for |
We have previously said that we d like to think |
always assumes that an unshared which is already cached by |
While such protocols solve the congestion collapse experienced by conventional |
makes it hard to translate the solutions devised for mobile |
have previously said that we d like to think of |
assumes that an unshared which is already cached by a |
such protocols solve the congestion collapse experienced by conventional TCP |
it hard to translate the solutions devised for mobile devices |
previously said that we d like to think of protocols |
that an unshared which is already cached by a different |
hard to translate the solutions devised for mobile devices to |
said that we d like to think of protocols as |
an unshared which is already cached by a different client |
they cannot mitigate the longer packet delivery latencies caused by |
that we d like to think of protocols as objects |
to translate the solutions devised for mobile devices to server |
unshared which is already cached by a different client always |
cannot mitigate the longer packet delivery latencies caused by packet |
translate the solutions devised for mobile devices to server systems |
which is already cached by a different client always triggers |
mitigate the longer packet delivery latencies caused by packet loss |
is already cached by a different client always triggers a |
already cached by a different client always triggers a file |
Our system will need to simultaneously support potentially large numbers |
and they do not eliminate the need for larger buffers |
cached by a different client always triggers a file has |
system will need to simultaneously support potentially large numbers of |
they do not eliminate the need for larger buffers at |
by a different client always triggers a file has an |
will need to simultaneously support potentially large numbers of transport |
do not eliminate the need for larger buffers at end |
a different client always triggers a file has an uncommitted |
need to simultaneously support potentially large numbers of transport objects |
different client always triggers a file has an uncommitted write |
to simultaneously support potentially large numbers of transport objects running |
FEC has seen major innovations in the last fifteen years |
client always triggers a file has an uncommitted write when |
simultaneously support potentially large numbers of transport objects running concurrently |
have shown that there exists low correlation between a given |
always triggers a file has an uncommitted write when it |
support potentially large numbers of transport objects running concurrently in |
shown that there exists low correlation between a given idle |
triggers a file has an uncommitted write when it is |
potentially large numbers of transport objects running concurrently in the |
that there exists low correlation between a given idle period |
a file has an uncommitted write when it is accessed |
large numbers of transport objects running concurrently in the end |
there exists low correlation between a given idle period s |
file has an uncommitted write when it is accessed by |
exists low correlation between a given idle period s duration |
has an uncommitted write when it is accessed by an |
low correlation between a given idle period s duration and |
an uncommitted write when it is accessed by an addiserver |
correlation between a given idle period s duration and the |
uncommitted write when it is accessed by an addiserver pull |
it was applied by researchers in the context of ATM |
between a given idle period s duration and the duration |
was applied by researchers in the context of ATM networks |
a given idle period s duration and the duration of |
since the server has no way of knowing if the |
given idle period s duration and the duration of previous |
the server has no way of knowing if the file |
idle period s duration and the duration of previous idle |
server has no way of knowing if the file has |
period s duration and the duration of previous idle periods |
has no way of knowing if the file has tional |
no way of knowing if the file has tional client |
This variability makes it difficult to devise effective predictive mechanisms |
variability makes it difficult to devise effective predictive mechanisms for |
incorrect information about the status of a file only outstanding |
makes it difficult to devise effective predictive mechanisms for disk |
information about the status of a file only outstanding updates |
it difficult to devise effective predictive mechanisms for disk idle |
difficult to devise effective predictive mechanisms for disk idle times |
Another serious issue arises if the clients don t trust |
serious issue arises if the clients don t trust the |
The LFS neatly circumvents this problem by predetermining which disk |
issue arises if the clients don t trust the data |
since updates to shared and unshared files are writclassification results |
LFS neatly circumvents this problem by predetermining which disk is |
arises if the clients don t trust the data center |
updates to shared and unshared files are writclassification results in |
neatly circumvents this problem by predetermining which disk is written |
Rizzo subsequently provided a working implementation of a software packet |
to shared and unshared files are writclassification results in the |
circumvents this problem by predetermining which disk is written to |
The problem here is that web services security standards tend |
shared and unshared files are writclassification results in the file |
this problem by predetermining which disk is written to at |
problem here is that web services security standards tend to |
and unshared files are writclassification results in the file being |
th conference on USENIX Symposium on Operating Systems Design and |
problem by predetermining which disk is written to at all |
here is that web services security standards tend to trust |
unshared files are writclassification results in the file being marked |
conference on USENIX Symposium on Operating Systems Design and Implementation |
by predetermining which disk is written to at all times |
is that web services security standards tend to trust the |
The emphasis on applying error correcting codes at higher levels |
files are writclassification results in the file being marked as |
that web services security standards tend to trust the web |
emphasis on applying error correcting codes at higher levels of |
are writclassification results in the file being marked as shared |
Server systems are often constrained by Service Level Agreements to |
web services security standards tend to trust the web services |
on applying error correcting codes at higher levels of the |
systems are often constrained by Service Level Agreements to guarantee |
the original order of The status of files can be |
applying error correcting codes at higher levels of the software |
services security standards tend to trust the web services platform |
are often constrained by Service Level Agreements to guarantee a |
original order of The status of files can be specified |
error correcting codes at higher levels of the software stack |
security standards tend to trust the web services platform itself |
often constrained by Service Level Agreements to guarantee a certain |
order of The status of files can be specified by |
correcting codes at higher levels of the software stack has |
constrained by Service Level Agreements to guarantee a certain level |
of The status of files can be specified by the |
The standards offer no help at all if we need |
codes at higher levels of the software stack has been |
by Service Level Agreements to guarantee a certain level of |
The status of files can be specified by the user |
standards offer no help at all if we need to |
at higher levels of the software stack has been accompanied |
Service Level Agreements to guarantee a certain level of performance |
status of files can be specified by the user or |
offer no help at all if we need to provide |
so that finding a solution that provides acceptable performance to |
of files can be specified by the user or by |
higher levels of the software stack has been accompanied by |
no help at all if we need to provide end |
that finding a solution that provides acceptable performance to only |
files can be specified by the user or by applithe |
levels of the software stack has been accompanied by advances |
finding a solution that provides acceptable performance to only a |
end encryption mechanisms while also preventing the hosted services from |
of the software stack has been accompanied by advances in |
can be specified by the user or by applithe sequence |
a solution that provides acceptable performance to only a fraction |
encryption mechanisms while also preventing the hosted services from seeing |
the software stack has been accompanied by advances in the |
be specified by the user or by applithe sequence of |
solution that provides acceptable performance to only a fraction of |
mechanisms while also preventing the hosted services from seeing the |
software stack has been accompanied by advances in the codes |
specified by the user or by applithe sequence of updates |
that provides acceptable performance to only a fraction of the |
while also preventing the hosted services from seeing the keys |
stack has been accompanied by advances in the codes themselves |
by the user or by applithe sequence of updates is |
provides acceptable performance to only a fraction of the incoming |
the user or by applithe sequence of updates is no |
acceptable performance to only a fraction of the incoming requests |
user or by applithe sequence of updates is no longer |
or by applithe sequence of updates is no longer entirely |
by applithe sequence of updates is no longer entirely preserved |
limiting bottlenecks when used in settings with large numbers of |
bottlenecks when used in settings with large numbers of clients |
an erasure code that performs excellently at small scale but |
the LFS provides an applicationindependent solution that allows the system |
erasure code that performs excellently at small scale but does |
or can be inferred by the file server according to |
LFS provides an applicationindependent solution that allows the system to |
code that performs excellently at small scale but does not |
We are left with a mixture of good and bad |
can be inferred by the file server according to how |
provides an applicationindependent solution that allows the system to perform |
that performs excellently at small scale but does not scale |
are left with a mixture of good and bad news |
be inferred by the file server according to how it |
an applicationindependent solution that allows the system to perform consistently |
Web services standardize client access to hosted services and data |
inferred by the file server according to how it dates |
performs excellently at small scale but does not scale to |
applicationindependent solution that allows the system to perform consistently across |
by the file server according to how it dates to |
excellently at small scale but does not scale to large |
we can easily build some form of multiframed web page |
solution that allows the system to perform consistently across a |
the file server according to how it dates to shared |
at small scale but does not scale to large sets |
can easily build some form of multiframed web page that |
that allows the system to perform consistently across a wide |
file server according to how it dates to shared files |
small scale but does not scale to large sets of |
easily build some form of multiframed web page that could |
allows the system to perform consistently across a wide range |
server according to how it dates to shared files form |
scale but does not scale to large sets of data |
build some form of multiframed web page that could host |
the system to perform consistently across a wide range of |
according to how it dates to shared files form a |
but does not scale to large sets of data and |
some form of multiframed web page that could host each |
system to perform consistently across a wide range of datasets |
to how it dates to shared files form a subsequence |
does not scale to large sets of data and error |
form of multiframed web page that could host each kind |
how it dates to shared files form a subsequence of |
not scale to large sets of data and error correcting |
of multiframed web page that could host each kind of |
it dates to shared files form a subsequence of the |
scale to large sets of data and error correcting symbols |
Directing these to a small fraction of the total number |
multiframed web page that could host each kind of information |
dates to shared files form a subsequence of the original |
these to a small fraction of the total number of |
web page that could host each kind of information in |
This scalability barrier resulted in the development of new variants |
to shared files form a subsequence of the original updates |
to a small fraction of the total number of disks |
page that could host each kind of information in its |
scalability barrier resulted in the development of new variants of |
that could host each kind of information in its own |
barrier resulted in the development of new variants of Low |
could host each kind of information in its own minibrowser |
resulted in the development of new variants of Low Density |
automatic inference should incorpoas do the updates to unshared files |
in the development of new variants of Low Density Parity |
The fact that the disks used in these contexts are |
the development of new variants of Low Density Parity Check |
fact that the disks used in these contexts are typically |
relaying data via a hosted service has many of the |
implicit dependenrate a heuristic for the sharing status of new |
that the disks used in these contexts are typically low |
data via a hosted service has many of the benefits |
dependenrate a heuristic for the sharing status of new files |
via a hosted service has many of the benefits of |
a hosted service has many of the benefits of a |
hosted service has many of the benefits of a publishsubscribe |
service has many of the benefits of a publishsubscribe architecture |
since the combination of nism for converting shared files to |
our solution alleviates this problem by making sure that the |
the combination of nism for converting shared files to be |
solution alleviates this problem by making sure that the live |
combination of nism for converting shared files to be unshared |
The natural way to think of our application is as |
alleviates this problem by making sure that the live subset |
of nism for converting shared files to be unshared if |
natural way to think of our application is as an |
this problem by making sure that the live subset of |
nism for converting shared files to be unshared if they |
way to think of our application is as an object |
problem by making sure that the live subset of disks |
for converting shared files to be unshared if they cease |
by making sure that the live subset of disks is |
converting shared files to be unshared if they cease to |
but web services provide no support for this kind of |
making sure that the live subset of disks is not |
shared files to be unshared if they cease to forward |
web services provide no support for this kind of client |
sure that the live subset of disks is not constant |
but require slightly more data to be received at the |
files to be unshared if they cease to forward invalidations |
services provide no support for this kind of client application |
require slightly more data to be received at the decoder |
to be unshared if they cease to forward invalidations and |
describes some of the solutions explored in the first three |
provide no support for this kind of client application development |
be unshared if they cease to forward invalidations and compulsory |
While the layered interleaving code used by Maelstrom is similar |
some of the solutions explored in the first three quadrants |
unshared if they cease to forward invalidations and compulsory server |
the layered interleaving code used by Maelstrom is similar to |
of the solutions explored in the first three quadrants mentioned |
if they cease to forward invalidations and compulsory server pull |
layered interleaving code used by Maelstrom is similar to the |
All data will probably be visible to the hosted services |
the solutions explored in the first three quadrants mentioned above |
they cease to forward invalidations and compulsory server pull RPCs |
interleaving code used by Maelstrom is similar to the Tornado |
data will probably be visible to the hosted services unless |
cease to forward invalidations and compulsory server pull RPCs for |
will probably be visible to the hosted services unless the |
LT and Raptor codes in its use of simple XOR |
to forward invalidations and compulsory server pull RPCs for unbe |
probably be visible to the hosted services unless the developer |
and Raptor codes in its use of simple XOR operations |
forward invalidations and compulsory server pull RPCs for unbe accessed |
be visible to the hosted services unless the developer uses |
invalidations and compulsory server pull RPCs for unbe accessed by |
it differs from them in one very important aspect it |
visible to the hosted services unless the developer uses some |
and compulsory server pull RPCs for unbe accessed by more |
based Solutions The concept of a memory hierarchy arose as |
differs from them in one very important aspect it seeks |
to the hosted services unless the developer uses some sort |
compulsory server pull RPCs for unbe accessed by more than |
Solutions The concept of a memory hierarchy arose as a |
from them in one very important aspect it seeks to |
the hosted services unless the developer uses some sort of |
server pull RPCs for unbe accessed by more than a |
The concept of a memory hierarchy arose as a result |
them in one very important aspect it seeks to minimize |
hosted services unless the developer uses some sort of non |
pull RPCs for unbe accessed by more than a single |
concept of a memory hierarchy arose as a result of |
in one very important aspect it seeks to minimize the |
RPCs for unbe accessed by more than a single client |
of a memory hierarchy arose as a result of the |
one very important aspect it seeks to minimize the latency |
a memory hierarchy arose as a result of the natural |
very important aspect it seeks to minimize the latency between |
The current implemenshared files prevents a client from accessing new |
memory hierarchy arose as a result of the natural tradeoff |
important aspect it seeks to minimize the latency between the |
current implemenshared files prevents a client from accessing new versions |
hierarchy arose as a result of the natural tradeoff between |
Using Live Objects for SOC Applications Cornell s Live Objects |
aspect it seeks to minimize the latency between the arrival |
implemenshared files prevents a client from accessing new versions of |
arose as a result of the natural tradeoff between memory |
Live Objects for SOC Applications Cornell s Live Objects platform |
it seeks to minimize the latency between the arrival of |
files prevents a client from accessing new versions of files |
as a result of the natural tradeoff between memory speed |
Objects for SOC Applications Cornell s Live Objects platform supports |
seeks to minimize the latency between the arrival of a |
prevents a client from accessing new versions of files tation |
a result of the natural tradeoff between memory speed and |
for SOC Applications Cornell s Live Objects platform supports componentized |
to minimize the latency between the arrival of a packet |
a client from accessing new versions of files tation in |
result of the natural tradeoff between memory speed and memory |
minimize the latency between the arrival of a packet at |
client from accessing new versions of files tation in MFS |
of the natural tradeoff between memory speed and memory cost |
the latency between the arrival of a packet at the |
from accessing new versions of files tation in MFS assumes |
latency between the arrival of a packet at the send |
accessing new versions of files tation in MFS assumes that |
new versions of files tation in MFS assumes that every |
versions of files tation in MFS assumes that every new |
of files tation in MFS assumes that every new file |
files tation in MFS assumes that every new file is |
tation in MFS assumes that every new file is unshared |
codes such as Tornado encode over a fixed set of |
and exposes eventbased interfaces by which it interacts with other |
that there exists a similar tradeoff between performance and power |
such as Tornado encode over a fixed set of input |
exposes eventbased interfaces by which it interacts with other components |
itors client accesses to a file according to an overlapping |
as Tornado encode over a fixed set of input symbols |
client accesses to a file according to an overlapping series |
accesses to a file according to an overlapping series of |
without treating symbols differently based on their sequence in the |
They explore the possibility of setting up a disk hierarchy |
Components representing hosted content Sensors and actuators Renderers that graphically |
to a file according to an overlapping series of time |
treating symbols differently based on their sequence in the data |
explore the possibility of setting up a disk hierarchy by |
representing hosted content Sensors and actuators Renderers that graphically depict |
a file according to an overlapping series of time periods |
symbols differently based on their sequence in the data stream |
the possibility of setting up a disk hierarchy by using |
hosted content Sensors and actuators Renderers that graphically depict events |
file according to an overlapping series of time periods to |
possibility of setting up a disk hierarchy by using high |
content Sensors and actuators Renderers that graphically depict events Replication |
according to an overlapping series of time periods to ensure |
layered interleaving is unique in allowing the recovery latency of |
Sensors and actuators Renderers that graphically depict events Replication protocols |
to an overlapping series of time periods to ensure that |
interleaving is unique in allowing the recovery latency of lost |
and actuators Renderers that graphically depict events Replication protocols Synchronization |
an overlapping series of time periods to ensure that files |
is unique in allowing the recovery latency of lost packets |
actuators Renderers that graphically depict events Replication protocols Synchronization protocols |
overlapping series of time periods to ensure that files which |
unique in allowing the recovery latency of lost packets to |
Renderers that graphically depict events Replication protocols Synchronization protocols Folders |
series of time periods to ensure that files which are |
in allowing the recovery latency of lost packets to depend |
that graphically depict events Replication protocols Synchronization protocols Folders containing |
of time periods to ensure that files which are regularly |
allowing the recovery latency of lost packets to depend on |
graphically depict events Replication protocols Synchronization protocols Folders containing sets |
whereby disks can be run at multiple speeds depending on |
time periods to ensure that files which are regularly accessed |
the recovery latency of lost packets to depend on the |
depict events Replication protocols Synchronization protocols Folders containing sets of |
disks can be run at multiple speeds depending on whether |
periods to ensure that files which are regularly accessed remain |
recovery latency of lost packets to depend on the actual |
events Replication protocols Synchronization protocols Folders containing sets of objects |
can be run at multiple speeds depending on whether power |
to ensure that files which are regularly accessed remain shared |
latency of lost packets to depend on the actual burst |
Replication protocols Synchronization protocols Folders containing sets of objects Display |
be run at multiple speeds depending on whether power or |
of lost packets to depend on the actual burst size |
protocols Synchronization protocols Folders containing sets of objects Display interfaces |
run at multiple speeds depending on whether power or performance |
lost packets to depend on the actual burst size experienced |
Synchronization protocols Folders containing sets of objects Display interfaces that |
at multiple speeds depending on whether power or performance takes |
protocols Folders containing sets of objects Display interfaces that visualize |
Experimental setup erates on a larger time scale than the |
as opposed to the maximum tolerable burst size as with |
multiple speeds depending on whether power or performance takes precedence |
Folders containing sets of objects Display interfaces that visualize folders |
setup erates on a larger time scale than the experiments |
opposed to the maximum tolerable burst size as with other |
erates on a larger time scale than the experiments considered |
to the maximum tolerable burst size as with other encoding |
Mashups of components are represented as a kind of XML |
on a larger time scale than the experiments considered in |
poses a significant engineering challenge whose feasibility is far from |
the maximum tolerable burst size as with other encoding schemes |
of components are represented as a kind of XML web |
a larger time scale than the experiments considered in At |
a significant engineering challenge whose feasibility is far from obvious |
components are represented as a kind of XML web pages |
larger time scale than the experiments considered in At the |
world imperatives to coordinate across data centers separated by thousands |
time scale than the experiments considered in At the start |
each describing a recipe for obtaining and parameterizing components that |
imperatives to coordinate across data centers separated by thousands of |
scale than the experiments considered in At the start of |
describing a recipe for obtaining and parameterizing components that will |
to coordinate across data centers separated by thousands of miles |
than the experiments considered in At the start of this |
a recipe for obtaining and parameterizing components that will serve |
the experiments considered in At the start of this section |
recipe for obtaining and parameterizing components that will serve as |
experiments considered in At the start of this section we |
for obtaining and parameterizing components that will serve as layers |
They propose the use of a small number of cache |
considered in At the start of this section we identified |
obtaining and parameterizing components that will serve as layers of |
propose the use of a small number of cache disks |
in At the start of this section we identified large |
and parameterizing components that will serve as layers of the |
the use of a small number of cache disks in |
Deploying new protocols is not an option for commodity clusters |
parameterizing components that will serve as layers of the composed |
use of a small number of cache disks in addition |
new protocols is not an option for commodity clusters where |
engineering design as an example of a scenario which features |
components that will serve as layers of the composed mashup |
of a small number of cache disks in addition to |
Software Defined Networks and Gossip Protocols Robert Soule Ken Birman |
protocols is not an option for commodity clusters where standardization |
design as an example of a scenario which features When |
a small number of cache disks in addition to the |
We call such an XML page a live object reference |
Defined Networks and Gossip Protocols Robert Soule Ken Birman Nate |
is not an option for commodity clusters where standardization is |
as an example of a scenario which features When a |
small number of cache disks in addition to the MAID |
Networks and Gossip Protocols Robert Soule Ken Birman Nate Foster |
not an option for commodity clusters where standardization is critical |
an example of a scenario which features When a process |
number of cache disks in addition to the MAID disks |
and Gossip Protocols Robert Soule Ken Birman Nate Foster University |
an option for commodity clusters where standardization is critical for |
The data in these cache disks is updated to reflect |
An SOC application is created by building a forest consisting |
Gossip Protocols Robert Soule Ken Birman Nate Foster University of |
example of a scenario which features When a process modifies |
option for commodity clusters where standardization is critical for cost |
data in these cache disks is updated to reflect the |
SOC application is created by building a forest consisting of |
Protocols Robert Soule Ken Birman Nate Foster University of Lugano |
of a scenario which features When a process modifies a |
for commodity clusters where standardization is critical for cost mitigation |
in these cache disks is updated to reflect the workload |
application is created by building a forest consisting of graphs |
Maelstrom is an edge appliance that uses Forward Error Correction |
a scenario which features When a process modifies a file |
these cache disks is updated to reflect the workload that |
Robert Soule Ken Birman Nate Foster University of Lugano Cornell |
is created by building a forest consisting of graphs of |
is an edge appliance that uses Forward Error Correction to |
cache disks is updated to reflect the workload that is |
Soule Ken Birman Nate Foster University of Lugano Cornell University |
an update is scheduled to be a high degree of |
created by building a forest consisting of graphs of references |
an edge appliance that uses Forward Error Correction to mask |
disks is updated to reflect the workload that is currently |
Ken Birman Nate Foster University of Lugano Cornell University Cornell |
update is scheduled to be a high degree of read |
by building a forest consisting of graphs of references that |
edge appliance that uses Forward Error Correction to mask packet |
is updated to reflect the workload that is currently being |
Birman Nate Foster University of Lugano Cornell University Cornell University |
building a forest consisting of graphs of references that are |
appliance that uses Forward Error Correction to mask packet loss |
updated to reflect the workload that is currently being accessed |
Nate Foster University of Lugano Cornell University Cornell University The |
a forest consisting of graphs of references that are mashed |
and the process continues executing withuated the MFS cache consistency |
that uses Forward Error Correction to mask packet loss from |
Foster University of Lugano Cornell University Cornell University The performance |
forest consisting of graphs of references that are mashed together |
the process continues executing withuated the MFS cache consistency algorithm |
and need only be spun up when a cache miss |
uses Forward Error Correction to mask packet loss from endto |
University of Lugano Cornell University Cornell University The performance of |
process continues executing withuated the MFS cache consistency algorithm using |
need only be spun up when a cache miss occurs |
of Lugano Cornell University Cornell University The performance of data |
an automated tool lets the developer drag and drop to |
continues executing withuated the MFS cache consistency algorithm using a |
IP throughput and latency by orders of magnitude when loss |
upon which their contents are copied onto the cache disks |
automated tool lets the developer drag and drop to combine |
executing withuated the MFS cache consistency algorithm using a synthetic |
throughput and latency by orders of magnitude when loss occurs |
tool lets the developer drag and drop to combine references |
This approach has several of the weaknesses that memory caches |
withuated the MFS cache consistency algorithm using a synthetic out |
and is completely transparent to applications and protocols literally providing |
approach has several of the weaknesses that memory caches suffer |
has created an opportunity to build more dynamic networks that |
the MFS cache consistency algorithm using a synthetic out having |
is completely transparent to applications and protocols literally providing reliability |
lets the developer drag and drop to combine references for |
created an opportunity to build more dynamic networks that can |
MFS cache consistency algorithm using a synthetic out having to |
completely transparent to applications and protocols literally providing reliability in |
If the cache disks are insufficient to store the entire |
the developer drag and drop to combine references for individual |
an opportunity to build more dynamic networks that can be |
cache consistency algorithm using a synthetic out having to wait |
transparent to applications and protocols literally providing reliability in an |
the cache disks are insufficient to store the entire working |
developer drag and drop to combine references for individual objects |
opportunity to build more dynamic networks that can be tailored |
consistency algorithm using a synthetic out having to wait for |
to applications and protocols literally providing reliability in an inexpensive |
cache disks are insufficient to store the entire working set |
drag and drop to combine references for individual objects into |
to build more dynamic networks that can be tailored precisely |
algorithm using a synthetic out having to wait for the |
applications and protocols literally providing reliability in an inexpensive box |
disks are insufficient to store the entire working set of |
and drop to combine references for individual objects into an |
build more dynamic networks that can be tailored precisely to |
using a synthetic out having to wait for the server |
are insufficient to store the entire working set of the |
drop to combine references for individual objects into an XML |
more dynamic networks that can be tailored precisely to the |
a synthetic out having to wait for the server to |
insufficient to store the entire working set of the current |
to combine references for individual objects into an XML mashup |
dynamic networks that can be tailored precisely to the needs |
synthetic out having to wait for the server to be |
to store the entire working set of the current workload |
combine references for individual objects into an XML mashup of |
networks that can be tailored precisely to the needs of |
out having to wait for the server to be contacted |
references for individual objects into an XML mashup of references |
that can be tailored precisely to the needs of applications |
for individual objects into an XML mashup of references describing |
individual objects into an XML mashup of references describing a |
objects into an XML mashup of references describing a graph |
the cache disks represent a significant added cost in themselves |
though we are hoping to obtain real data from such |
existing solutions for monitoring within SDNs suffer from several short |
into an XML mashup of references describing a graph of |
we are hoping to obtain real data from such an |
an XML mashup of references describing a graph of objects |
are hoping to obtain real data from such an thread |
hoping to obtain real data from such an thread then |
to obtain real data from such an thread then checks |
suggest that if data is laid out on disks according |
obtain real data from such an thread then checks the |
that if data is laid out on disks according to |
real data from such an thread then checks the status |
if data is laid out on disks according to frequency |
data from such an thread then checks the status of |
data is laid out on disks according to frequency of |
from such an thread then checks the status of the |
is laid out on disks according to frequency of access |
D visualization of an airplane may need to be connected |
such an thread then checks the status of the file |
visualization of an airplane may need to be connected to |
with the most popular files being located in one set |
an thread then checks the status of the file the |
of an airplane may need to be connected to a |
the most popular files being located in one set of |
thread then checks the status of the file the update |
an airplane may need to be connected to a source |
most popular files being located in one set of disks |
We argue that gossip protocols offer an ideal alternative for |
then checks the status of the file the update modifies |
airplane may need to be connected to a source of |
argue that gossip protocols offer an ideal alternative for SDN |
may need to be connected to a source of GPS |
then the latter set of disks could be powered down |
that gossip protocols offer an ideal alternative for SDN monitoring |
need to be connected to a source of GPS and |
the latter set of disks could be powered down to |
to be connected to a source of GPS and other |
latter set of disks could be powered down to conserve |
ignored the crucial monitoring component that aggregates network and application |
be connected to a source of GPS and other orientation |
set of disks could be powered down to conserve energy |
the crucial monitoring component that aggregates network and application state |
connected to a source of GPS and other orientation data |
which in turn needs to run over a data replication |
in turn needs to run over a data replication protocol |
and they implement and evaluate a prototype file server called |
turn needs to run over a data replication protocol with |
they implement and evaluate a prototype file server called Nomad |
then adjusting SDN policies to optimize the use of resources |
needs to run over a data replication protocol with specific |
implement and evaluate a prototype file server called Nomad FS |
to run over a data replication protocol with specific reliability |
Gossip protocols are an ideal choice for implementing a wide |
which runs on top of the file system and monitors |
protocols are an ideal choice for implementing a wide range |
runs on top of the file system and monitors data |
are an ideal choice for implementing a wide range monitoring |
on top of the file system and monitors data layout |
an ideal choice for implementing a wide range monitoring tasks |
top of the file system and monitors data layout on |
of the file system and monitors data layout on disks |
A proxy is a piece of running code that may |
each node exchanges information with a randomly selected peer at |
proxy is a piece of running code that may render |
node exchanges information with a randomly selected peer at periodic |
exchanges information with a randomly selected peer at periodic intervals |
they suggest instead that they be run at low speed |
it is not clear whether this scheme would adapt to |
scaling linearly with system size and not prone to reactive |
is not clear whether this scheme would adapt to different |
linearly with system size and not prone to reactive feedback |
not clear whether this scheme would adapt to different workloads |
The hierarchy of proxies reflects the hierarchical structure of the |
hierarchy of proxies reflects the hierarchical structure of the XML |
of proxies reflects the hierarchical structure of the XML mashup |
so tools built on gossip are extremely tolerant to disruptions |
tools built on gossip are extremely tolerant to disruptions and |
propose another data layout management scheme to optimize disk access |
built on gossip are extremely tolerant to disruptions and able |
another data layout management scheme to optimize disk access patterns |
an object proxy can initialize itself by copying the state |
on gossip are extremely tolerant to disruptions and able to |
object proxy can initialize itself by copying the state from |
gossip are extremely tolerant to disruptions and able to rapidly |
proxy can initialize itself by copying the state from some |
are extremely tolerant to disruptions and able to rapidly recover |
can initialize itself by copying the state from some active |
extremely tolerant to disruptions and able to rapidly recover from |
initialize itself by copying the state from some active proxy |
tolerant to disruptions and able to rapidly recover from failures |
composing multiple protocols can lead to complex interactions with unpredictable |
multiple protocols can lead to complex interactions with unpredictable behavior |
Applications are instrumented and then profiled to obtain array access |
are instrumented and then profiled to obtain array access sequences |
which their system then uses to determine optimal disk layouts |
for example by relaying events from sensors into a replica |
MiCA allows programmers to describe gossip protocols with a small |
their system then uses to determine optimal disk layouts by |
system then uses to determine optimal disk layouts by computing |
then uses to determine optimal disk layouts by computing optimal |
and compose the protocols with a rich collection of operators |
uses to determine optimal disk layouts by computing optimal stripe |
compose the protocols with a rich collection of operators to |
to determine optimal disk layouts by computing optimal stripe factor |
the protocols with a rich collection of operators to create |
protocols with a rich collection of operators to create sophisticated |
with a rich collection of operators to create sophisticated protocols |
a rich collection of operators to create sophisticated protocols in |
rich collection of operators to create sophisticated protocols in a |
collection of operators to create sophisticated protocols in a modular |
Our approach shares certain similarities with the existing web development |
of operators to create sophisticated protocols in a modular style |
the wisdom of marrying the disk layout to the application |
approach shares certain similarities with the existing web development model |
wisdom of marrying the disk layout to the application seems |
of marrying the disk layout to the application seems questionable |
in the sense that it uses hierarchical XML documents to |
the sense that it uses hierarchical XML documents to define |
sense that it uses hierarchical XML documents to define the |
we have built monitoring tasks that maintain a predictable performance |
that it uses hierarchical XML documents to define the content |
even when hundreds of separate instances are deployed on the |
when hundreds of separate instances are deployed on the same |
hundreds of separate instances are deployed on the same machines |
For example if one pulls a minibrowser from Google Earth |
and computes online the optimal speed that each disk should |
computes online the optimal speed that each disk should run |
online the optimal speed that each disk should run at |
is novel among network programming languages in that it determines |
the same functionality would be represented as a mashup of |
novel among network programming languages in that it determines allocations |
same functionality would be represented as a mashup of a |
among network programming languages in that it determines allocations of |
functionality would be represented as a mashup of a component |
network programming languages in that it determines allocations of limited |
hik j ihkj m l ml cb c b cbcb |
would be represented as a mashup of a component that |
programming languages in that it determines allocations of limited network |
j ihkj m l ml cb c b cbcb ed |
be represented as a mashup of a component that fetches |
ihkj m l ml cb c b cbcb ed f |
Hibernator includes a file server that sits on top of |
represented as a mashup of a component that fetches maps |
We have used Merlin to improve the latency of Hadoop |
m l ml cb c b cbcb ed f gf |
includes a file server that sits on top of the |
as a mashup of a component that fetches maps and |
have used Merlin to improve the latency of Hadoop jobs |
l ml cb c b cbcb ed f gf cb |
a file server that sits on top of the file |
a mashup of a component that fetches maps and similar |
used Merlin to improve the latency of Hadoop jobs running |
ml cb c b cbcb ed f gf cb c |
file server that sits on top of the file system |
mashup of a component that fetches maps and similar content |
Merlin to improve the latency of Hadoop jobs running in |
cb c b cbcb ed f gf cb c b |
server that sits on top of the file system and |
of a component that fetches maps and similar content with |
to improve the latency of Hadoop jobs running in the |
c b cbcb ed f gf cb c b YX |
that sits on top of the file system and manipulates |
a component that fetches maps and similar content with a |
improve the latency of Hadoop jobs running in the presence |
b cbcb ed f gf cb c b YX cbcb |
sits on top of the file system and manipulates data |
component that fetches maps and similar content with a second |
the latency of Hadoop jobs running in the presence of |
on top of the file system and manipulates data layout |
that fetches maps and similar content with a second component |
Z eded f f gfgf cb b ON YXYX cbb |
latency of Hadoop jobs running in the presence of UDP |
top of the file system and manipulates data layout to |
fetches maps and similar content with a second component that |
The Effects of Systemic Packet Loss on Aggregate TCP Flows |
of Hadoop jobs running in the presence of UDP background |
of the file system and manipulates data layout to put |
maps and similar content with a second component that provides |
Hadoop jobs running in the presence of UDP background traffic |
the file system and manipulates data layout to put the |
and similar content with a second component that provides the |
file system and manipulates data layout to put the most |
similar content with a second component that provides the visualization |
content with a second component that provides the visualization interface |
The authors address the issue of performance guarantees by stipulating |
authors address the issue of performance guarantees by stipulating that |
address the issue of performance guarantees by stipulating that if |
the issue of performance guarantees by stipulating that if performance |
can provide automated network management customized to the needs of |
issue of performance guarantees by stipulating that if performance drops |
provide automated network management customized to the needs of resident |
One kind of live object could be a folder including |
of performance guarantees by stipulating that if performance drops below |
automated network management customized to the needs of resident distributed |
kind of live object could be a folder including a |
performance guarantees by stipulating that if performance drops below some |
network management customized to the needs of resident distributed applications |
of live object could be a folder including a set |
guarantees by stipulating that if performance drops below some threshold |
live object could be a folder including a set of |
object could be a folder including a set of objects |
then all disks are spun up to their highest speed |
for example extracted from a directory in a file system |
example extracted from a directory in a file system or |
by providing policy language constructs that can be automatically verified |
extracted from a directory in a file system or pulled |
from a directory in a file system or pulled from |
a directory in a file system or pulled from a |
directory in a file system or pulled from a database |
End Performance Effects of Parallel TCP Sockets on a Lossy |
is the notion that network events are generated in response |
observe that the storage cache management policy is pivotal in |
in a file system or pulled from a database in |
Performance Effects of Parallel TCP Sockets on a Lossy Wide |
the notion that network events are generated in response to |
that the storage cache management policy is pivotal in determining |
a file system or pulled from a database in response |
notion that network events are generated in response to the |
the storage cache management policy is pivotal in determining the |
file system or pulled from a database in response to |
that network events are generated in response to the situational |
storage cache management policy is pivotal in determining the sequence |
system or pulled from a database in response to a |
network events are generated in response to the situational status |
cache management policy is pivotal in determining the sequence of |
or pulled from a database in response to a query |
events are generated in response to the situational status culled |
management policy is pivotal in determining the sequence of requests |
are generated in response to the situational status culled from |
policy is pivotal in determining the sequence of requests that |
generated in response to the situational status culled from a |
is pivotal in determining the sequence of requests that access |
as might occur when a rescue worker enters a building |
in response to the situational status culled from a wide |
pivotal in determining the sequence of requests that access disks |
might occur when a rescue worker enters a building or |
response to the situational status culled from a wide range |
occur when a rescue worker enters a building or turns |
to the situational status culled from a wide range of |
cache management policies could be tailored to change the average |
when a rescue worker enters a building or turns a |
the situational status culled from a wide range of sources |
management policies could be tailored to change the average idle |
a rescue worker enters a building or turns a corner |
policies could be tailored to change the average idle time |
could be tailored to change the average idle time between |
be tailored to change the average idle time between disk |
tailored to change the average idle time between disk requests |
Live Objects can easily support applications that dynamically recompute the |
Objects can easily support applications that dynamically recompute the set |
can easily support applications that dynamically recompute the set of |
easily support applications that dynamically recompute the set of visible |
support applications that dynamically recompute the set of visible objects |
cache policies that are aware of the underlying disk management |
policies that are aware of the underlying disk management schemes |
A rescuer would automatically and instantly be shown the avatars |
rescuer would automatically and instantly be shown the avatars of |
would automatically and instantly be shown the avatars of others |
automatically and instantly be shown the avatars of others who |
and instantly be shown the avatars of others who are |
instantly be shown the avatars of others who are already |
be shown the avatars of others who are already working |
shown the avatars of others who are already working at |
They also show through experiments the somewhat obvious fact that |
the avatars of others who are already working at that |
also show through experiments the somewhat obvious fact that for |
avatars of others who are already working at that site |
show through experiments the somewhat obvious fact that for write |
through experiments the somewhat obvious fact that for write accesses |
back policies offer more opportunities to save power than write |
This model can support a wide variety of collaboration and |
model can support a wide variety of collaboration and coordination |
can support a wide variety of collaboration and coordination paradigms |
Much of this information must be created and updated dynamically |
the Live Objects platform makes it easy for a non |
The rescue coordinator pulls prebuilt object references from a folder |
it will become increasingly difficult for network operators to provide |
will become increasingly difficult for network operators to provide this |
become increasingly difficult for network operators to provide this flexibility |
increasingly difficult for network operators to provide this flexibility without |
difficult for network operators to provide this flexibility without the |
for network operators to provide this flexibility without the support |
network operators to provide this flexibility without the support of |
operators to provide this flexibility without the support of proper |
to provide this flexibility without the support of proper tools |
provide this flexibility without the support of proper tools and |
this flexibility without the support of proper tools and infrastructure |
would correspond to objects that point to a web service |
correspond to objects that point to a web service over |
to objects that point to a web service over the |
objects that point to a web service over the network |
provide both the control and monitoring components necessary to automatically |
both the control and monitoring components necessary to automatically adapt |
the control and monitoring components necessary to automatically adapt the |
was motivated by a need to optimize the latency of |
control and monitoring components necessary to automatically adapt the network |
motivated by a need to optimize the latency of write |
and monitoring components necessary to automatically adapt the network to |
monitoring components necessary to automatically adapt the network to the |
components necessary to automatically adapt the network to the needs |
necessary to automatically adapt the network to the needs of |
Writing a block of data to a Seagate Barracuda disk |
to automatically adapt the network to the needs of the |
a block of data to a Seagate Barracuda disk costs |
Event interfaces allow such objects to coexist in a shared |
automatically adapt the network to the needs of the applications |
block of data to a Seagate Barracuda disk costs about |
interfaces allow such objects to coexist in a shared display |
allow such objects to coexist in a shared display window |
such objects to coexist in a shared display window that |
objects to coexist in a shared display window that can |
to coexist in a shared display window that can pan |
they allow for the rigorous expression of algorithms for monitoring |
allow for the rigorous expression of algorithms for monitoring or |
for the rigorous expression of algorithms for monitoring or managing |
the rigorous expression of algorithms for monitoring or managing SDN |
rigorous expression of algorithms for monitoring or managing SDN networks |
The relative advantages and disadvantages of our model can be |
relative advantages and disadvantages of our model can be summarized |
advantages and disadvantages of our model can be summarized as |
and disadvantages of our model can be summarized as follows |
The key observation here is that seek time is a |
key observation here is that seek time is a large |
observation here is that seek time is a large and |
here is that seek time is a large and constant |
is that seek time is a large and constant term |
that seek time is a large and constant term in |
seek time is a large and constant term in latency |
time is a large and constant term in latency computation |
only log and writes always go to the log head |
functionality such as coordination between searchers can remain active even |
such as coordination between searchers can remain active even if |
as coordination between searchers can remain active even if connectivity |
and write latency becomes purely a function of the disk |
coordination between searchers can remain active even if connectivity to |
write latency becomes purely a function of the disk bandwidth |
between searchers can remain active even if connectivity to the |
searchers can remain active even if connectivity to the data |
can remain active even if connectivity to the data center |
remain active even if connectivity to the data center is |
active even if connectivity to the data center is disrupted |
Streams of video or sensor data can travel directly and |
of video or sensor data can travel directly and won |
video or sensor data can travel directly and won t |
or sensor data can travel directly and won t be |
sensor data can travel directly and won t be delayed |
data can travel directly and won t be delayed by |
can travel directly and won t be delayed by the |
travel directly and won t be delayed by the need |
directly and won t be delayed by the need to |
and won t be delayed by the need to ricochet |
space reclamation is a tricky problem in log structured file |
won t be delayed by the need to ricochet off |
reclamation is a tricky problem in log structured file systems |
t be delayed by the need to ricochet off a |
be delayed by the need to ricochet off a remote |
delayed by the need to ricochet off a remote and |
by the need to ricochet off a remote and potentially |
the need to ricochet off a remote and potentially inaccessible |
need to ricochet off a remote and potentially inaccessible server |
a new log segment is allocated and the log head |
new log segment is allocated and the log head moves |
log segment is allocated and the log head moves to |
segment is allocated and the log head moves to the |
we could lose access to some of the sophisticated proprietary |
is allocated and the log head moves to the new |
could lose access to some of the sophisticated proprietary interactive |
allocated and the log head moves to the new segment |
lose access to some of the sophisticated proprietary interactive functionality |
access to some of the sophisticated proprietary interactive functionality optimized |
to some of the sophisticated proprietary interactive functionality optimized for |
some of the sophisticated proprietary interactive functionality optimized for proprietary |
of the sophisticated proprietary interactive functionality optimized for proprietary minibrowser |
and it is then added to the pool of free |
it is then added to the pool of free log |
is then added to the pool of free log segments |
peer communication can be much harder to use than relaying |
communication can be much harder to use than relaying data |
this process results in a natural division of allocated segments |
can be much harder to use than relaying data through |
process results in a natural division of allocated segments into |
be much harder to use than relaying data through a |
results in a natural division of allocated segments into stable |
much harder to use than relaying data through a hosted |
harder to use than relaying data through a hosted service |
to use than relaying data through a hosted service that |
use than relaying data through a hosted service that uses |
than relaying data through a hosted service that uses an |
relaying data through a hosted service that uses an Enterprise |
data through a hosted service that uses an Enterprise Service |
through a hosted service that uses an Enterprise Service Bus |
We will see how this feature can be used to |
will see how this feature can be used to save |
see how this feature can be used to save power |
subscribe substrate forces the developers to become familiar with and |
substrate forces the developers to become familiar with and choose |
forces the developers to become familiar with and choose between |
the developers to become familiar with and choose between a |
developers to become familiar with and choose between a range |
to become familiar with and choose between a range of |
become familiar with and choose between a range of different |
familiar with and choose between a range of different and |
with and choose between a range of different and incompatible |
and choose between a range of different and incompatible options |
Saving Opportunity We shall now argue that there remains an |
Opportunity We shall now argue that there remains an unexplored |
An wrong choice of transport could result in degraded QoS |
We shall now argue that there remains an unexplored quadrant |
shall now argue that there remains an unexplored quadrant in |
now argue that there remains an unexplored quadrant in this |
argue that there remains an unexplored quadrant in this solution |
that there remains an unexplored quadrant in this solution space |
But our longer term goal is to support a large |
Second Life is implemented with a data center including a |
Putting a disk management layer on top of the file |
Life is implemented with a data center including a large |
is implemented with a data center including a large number |
system to optimize data layout for writes is only halfway |
implemented with a data center including a large number of |
to optimize data layout for writes is only halfway to |
with a data center including a large number of servers |
optimize data layout for writes is only halfway to the |
a data center including a large number of servers storing |
data layout for writes is only halfway to the solution |
data center including a large number of servers storing the |
center including a large number of servers storing the state |
including a large number of servers storing the state of |
a large number of servers storing the state of the |
large number of servers storing the state of the virtual |
number of servers storing the state of the virtual world |
management policies described in the related works section essentially attack |
policies described in the related works section essentially attack the |
described in the related works section essentially attack the problem |
in the related works section essentially attack the problem by |
the related works section essentially attack the problem by trying |
related works section essentially attack the problem by trying to |
works section essentially attack the problem by trying to predict |
section essentially attack the problem by trying to predict in |
essentially attack the problem by trying to predict in advance |
attack the problem by trying to predict in advance which |
the problem by trying to predict in advance which disk |
problem by trying to predict in advance which disk any |
by trying to predict in advance which disk any given |
trying to predict in advance which disk any given access |
to predict in advance which disk any given access will |
predict in advance which disk any given access will go |
in advance which disk any given access will go to |
They optimize the data layout on disks to ensure that |
optimize the data layout on disks to ensure that accesses |
the data layout on disks to ensure that accesses are |
data layout on disks to ensure that accesses are localized |
layout on disks to ensure that accesses are localized to |
on disks to ensure that accesses are localized to some |
disks to ensure that accesses are localized to some fraction |
to ensure that accesses are localized to some fraction of |
ensure that accesses are localized to some fraction of the |
that accesses are localized to some fraction of the disks |
whenever an avatar moves or performs some action in the |
an avatar moves or performs some action in the virtual |
avatar moves or performs some action in the virtual world |
a request describing this event is passed to the hosting |
request describing this event is passed to the hosting data |
a new access has some probability of not fitting this |
describing this event is passed to the hosting data center |
new access has some probability of not fitting this model |
The Miner s Dilemma Ittay Eyal Cornell University Abstract An |
this event is passed to the hosting data center and |
access has some probability of not fitting this model and |
Miner s Dilemma Ittay Eyal Cornell University Abstract An open |
event is passed to the hosting data center and processed |
has some probability of not fitting this model and needing |
s Dilemma Ittay Eyal Cornell University Abstract An open distributed |
is passed to the hosting data center and processed by |
some probability of not fitting this model and needing to |
Dilemma Ittay Eyal Cornell University Abstract An open distributed system |
passed to the hosting data center and processed by servers |
probability of not fitting this model and needing to access |
Ittay Eyal Cornell University Abstract An open distributed system can |
to the hosting data center and processed by servers running |
of not fitting this model and needing to access a |
Eyal Cornell University Abstract An open distributed system can be |
the hosting data center and processed by servers running there |
not fitting this model and needing to access a powered |
Cornell University Abstract An open distributed system can be secured |
University Abstract An open distributed system can be secured by |
Clients do perform a variety of decoding and rendering functions |
Abstract An open distributed system can be secured by requiring |
do perform a variety of decoding and rendering functions locally |
An open distributed system can be secured by requiring participants |
open distributed system can be secured by requiring participants to |
distributed system can be secured by requiring participants to present |
but the data center must be in the loop to |
system can be secured by requiring participants to present proof |
the data center must be in the loop to ensure |
two applications that have completely different access patterns might require |
can be secured by requiring participants to present proof of |
data center must be in the loop to ensure that |
applications that have completely different access patterns might require completely |
be secured by requiring participants to present proof of work |
center must be in the loop to ensure that all |
that have completely different access patterns might require completely different |
secured by requiring participants to present proof of work and |
must be in the loop to ensure that all users |
have completely different access patterns might require completely different data |
by requiring participants to present proof of work and rewarding |
be in the loop to ensure that all users observe |
completely different access patterns might require completely different data layouts |
requiring participants to present proof of work and rewarding them |
in the loop to ensure that all users observe consistent |
different access patterns might require completely different data layouts on |
participants to present proof of work and rewarding them for |
the loop to ensure that all users observe consistent state |
access patterns might require completely different data layouts on disk |
to present proof of work and rewarding them for participation |
patterns might require completely different data layouts on disk leading |
When the number of users in a scenario isn t |
might require completely different data layouts on disk leading to |
the number of users in a scenario isn t huge |
require completely different data layouts on disk leading to conflicts |
which is adopted by almost all contemporary digital currencies and |
completely different data layouts on disk leading to conflicts that |
is adopted by almost all contemporary digital currencies and related |
Second Life can easily keep up using a standard workload |
different data layouts on disk leading to conflicts that reduce |
adopted by almost all contemporary digital currencies and related services |
Life can easily keep up using a standard workload partitioning |
data layouts on disk leading to conflicts that reduce possible |
can easily keep up using a standard workload partitioning scheme |
A natural process leads participants of such systems to form |
layouts on disk leading to conflicts that reduce possible powersavings |
easily keep up using a standard workload partitioning scheme in |
natural process leads participants of such systems to form pools |
keep up using a standard workload partitioning scheme in which |
Since all writes in an LFS are to the log |
up using a standard workload partitioning scheme in which different |
Experience with Bitcoin shows that the largest pools are often |
all writes in an LFS are to the log head |
using a standard workload partitioning scheme in which different servers |
with Bitcoin shows that the largest pools are often open |
a standard workload partitioning scheme in which different servers handle |
standard workload partitioning scheme in which different servers handle different |
workload partitioning scheme in which different servers handle different portions |
It has long been known that a member can sabotage |
partitioning scheme in which different servers handle different portions of |
has long been known that a member can sabotage an |
scheme in which different servers handle different portions of the |
long been known that a member can sabotage an open |
in which different servers handle different portions of the virtual |
been known that a member can sabotage an open pool |
which different servers handle different portions of the virtual world |
known that a member can sabotage an open pool by |
that a member can sabotage an open pool by seemingly |
a member can sabotage an open pool by seemingly joining |
we could power down every disk but the one that |
member can sabotage an open pool by seemingly joining it |
could power down every disk but the one that the |
can sabotage an open pool by seemingly joining it but |
power down every disk but the one that the log |
for example because large numbers of users want to enter |
sabotage an open pool by seemingly joining it but never |
down every disk but the one that the log head |
example because large numbers of users want to enter the |
an open pool by seemingly joining it but never sharing |
every disk but the one that the log head resides |
because large numbers of users want to enter the same |
open pool by seemingly joining it but never sharing its |
disk but the one that the log head resides on |
large numbers of users want to enter the same virtual |
pool by seemingly joining it but never sharing its proofs |
numbers of users want to enter the same virtual discotheque |
by seemingly joining it but never sharing its proofs of |
seemingly joining it but never sharing its proofs of work |
the servers can become overwhelmed and are forced to reject |
servers can become overwhelmed and are forced to reject some |
can become overwhelmed and are forced to reject some of |
become overwhelmed and are forced to reject some of the |
We define and analyze a game where pools use some |
overwhelmed and are forced to reject some of the users |
define and analyze a game where pools use some of |
and are forced to reject some of the users or |
aware caching algorithms described in the related works section are |
and analyze a game where pools use some of their |
are forced to reject some of the users or reduce |
caching algorithms described in the related works section are good |
analyze a game where pools use some of their participants |
forced to reject some of the users or reduce their |
algorithms described in the related works section are good candidates |
a game where pools use some of their participants to |
to reject some of the users or reduce their frame |
game where pools use some of their participants to infiltrate |
where pools use some of their participants to infiltrate other |
pools use some of their participants to infiltrate other pools |
use some of their participants to infiltrate other pools and |
some of their participants to infiltrate other pools and perform |
and only a small fraction of the disks need be |
of their participants to infiltrate other pools and perform such |
only a small fraction of the disks need be powered |
their participants to infiltrate other pools and perform such an |
Second Life as a Live Objects application poses some new |
a small fraction of the disks need be powered on |
participants to infiltrate other pools and perform such an attack |
Life as a Live Objects application poses some new challenges |
small fraction of the disks need be powered on in |
fraction of the disks need be powered on in order |
of the disks need be powered on in order to |
the disks need be powered on in order to serve |
many aspects of the application can be addressed in the |
disks need be powered on in order to serve all |
aspects of the application can be addressed in the same |
need be powered on in order to serve all writes |
We study the special cases where either two pools or |
of the application can be addressed in the same manner |
be powered on in order to serve all writes as |
study the special cases where either two pools or any |
the application can be addressed in the same manner we |
powered on in order to serve all writes as well |
the special cases where either two pools or any number |
application can be addressed in the same manner we ve |
on in order to serve all writes as well as |
special cases where either two pools or any number of |
can be addressed in the same manner we ve outlined |
in order to serve all writes as well as reads |
cases where either two pools or any number of identical |
be addressed in the same manner we ve outlined for |
where either two pools or any number of identical pools |
addressed in the same manner we ve outlined for the |
either two pools or any number of identical pools play |
what about the performance and power costs of log cleaning |
in the same manner we ve outlined for the search |
two pools or any number of identical pools play the |
the same manner we ve outlined for the search and |
pools or any number of identical pools play the game |
same manner we ve outlined for the search and rescue |
or any number of identical pools play the game and |
manner we ve outlined for the search and rescue application |
any number of identical pools play the game and the |
number of identical pools play the game and the rest |
of identical pools play the game and the rest of |
to hide the performance penalty of log cleaning even when |
identical pools play the game and the rest of the |
hide the performance penalty of log cleaning even when the |
pools play the game and the rest of the participants |
the performance penalty of log cleaning even when the workload |
play the game and the rest of the participants are |
performance penalty of log cleaning even when the workload allows |
the game and the rest of the participants are uninvolved |
penalty of log cleaning even when the workload allows little |
of log cleaning even when the workload allows little idle |
log cleaning even when the workload allows little idle time |
In both of these cases there exists an equilibrium that |
both of these cases there exists an equilibrium that constitutes |
in standards for creating mashups could be used to identify |
The power costs of log cleaning are a little more |
of these cases there exists an equilibrium that constitutes a |
standards for creating mashups could be used to identify sensors |
power costs of log cleaning are a little more tricky |
these cases there exists an equilibrium that constitutes a tragedy |
for creating mashups could be used to identify sensors and |
costs of log cleaning are a little more tricky to |
cases there exists an equilibrium that constitutes a tragedy of |
creating mashups could be used to identify sensors and other |
of log cleaning are a little more tricky to justify |
there exists an equilibrium that constitutes a tragedy of the |
mashups could be used to identify sensors and other data |
exists an equilibrium that constitutes a tragedy of the commons |
could be used to identify sensors and other data sources |
this is where the natural division of segments into stable |
an equilibrium that constitutes a tragedy of the commons where |
is where the natural division of segments into stable and |
equilibrium that constitutes a tragedy of the commons where the |
which could then be wrapped as Live Objects and incorporated |
where the natural division of segments into stable and volatile |
that constitutes a tragedy of the commons where the participating |
could then be wrapped as Live Objects and incorporated into |
the natural division of segments into stable and volatile ones |
constitutes a tragedy of the commons where the participating pools |
then be wrapped as Live Objects and incorporated into live |
natural division of segments into stable and volatile ones that |
a tragedy of the commons where the participating pools attack |
be wrapped as Live Objects and incorporated into live scenes |
division of segments into stable and volatile ones that the |
tragedy of the commons where the participating pools attack one |
of segments into stable and volatile ones that the log |
of the commons where the participating pools attack one another |
streaming media sources such as video cameras mounted at street |
segments into stable and volatile ones that the log cleaning |
the commons where the participating pools attack one another and |
media sources such as video cameras mounted at street level |
into stable and volatile ones that the log cleaning process |
commons where the participating pools attack one another and earn |
sources such as video cameras mounted at street level in |
stable and volatile ones that the log cleaning process results |
where the participating pools attack one another and earn less |
such as video cameras mounted at street level in places |
and volatile ones that the log cleaning process results in |
the participating pools attack one another and earn less than |
as video cameras mounted at street level in places such |
participating pools attack one another and earn less than they |
video cameras mounted at street level in places such as |
pools attack one another and earn less than they would |
cameras mounted at street level in places such as Tokyo |
After a significant fraction of segments on a disk have |
attack one another and earn less than they would have |
mounted at street level in places such as Tokyo s |
a significant fraction of segments on a disk have been |
one another and earn less than they would have if |
at street level in places such as Tokyo s Ginza |
significant fraction of segments on a disk have been classified |
another and earn less than they would have if none |
street level in places such as Tokyo s Ginza can |
fraction of segments on a disk have been classified as |
and earn less than they would have if none had |
diff denotes differentiated writeback priorities for shared and unshared files |
of segments on a disk have been classified as stable |
level in places such as Tokyo s Ginza can be |
earn less than they would have if none had attacked |
in places such as Tokyo s Ginza can be added |
places such as Tokyo s Ginza can be added to |
such as Tokyo s Ginza can be added to create |
we power the disk on and copy the stable segments |
as Tokyo s Ginza can be added to create realistic |
power the disk on and copy the stable segments to |
the decision whether or not to attack is the miner |
Tokyo s Ginza can be added to create realistic experience |
the disk on and copy the stable segments to a |
decision whether or not to attack is the miner s |
the height of a bar counts the number of invalidations |
disk on and copy the stable segments to a stable |
whether or not to attack is the miner s dilemma |
The more complex issue is that a search and rescue |
on and copy the stable segments to a stable disk |
more complex issue is that a search and rescue application |
complex issue is that a search and rescue application can |
The game is played daily by the active Bitcoin pools |
issue is that a search and rescue application can be |
is that a search and rescue application can be imagined |
that a search and rescue application can be imagined as |
a search and rescue application can be imagined as a |
The bandwidth from the reader to the server was fixed |
search and rescue application can be imagined as a situational |
This is similar to the log cleaning scheme described in |
bandwidth from the reader to the server was fixed at |
and rescue application can be imagined as a situational state |
rescue application can be imagined as a situational state fully |
application can be imagined as a situational state fully replicated |
can be imagined as a situational state fully replicated across |
be imagined as a situational state fully replicated across all |
imagined as a situational state fully replicated across all of |
as a situational state fully replicated across all of its |
which uses a hidden structure embedded in the log to |
a situational state fully replicated across all of its users |
uses a hidden structure embedded in the log to track |
and the bandwidth from the writer to the server was |
a hidden structure embedded in the log to track segment |
the bandwidth from the writer to the server was varied |
hidden structure embedded in the log to track segment utilization |
bandwidth from the writer to the server was varied according |
even if the user is zoomed into some particular spot |
from the writer to the server was varied according to |
Cleaning an entire disk amortizes the cost of powering the |
if the user is zoomed into some particular spot within |
the writer to the server was varied according to the |
an entire disk amortizes the cost of powering the disk |
the user is zoomed into some particular spot within the |
writer to the server was varied according to the experiment |
entire disk amortizes the cost of powering the disk on |
user is zoomed into some particular spot within the overall |
The writer was configured in one of seven different ways |
Number of accesses Number of files touched Number of bytes |
is zoomed into some particular spot within the overall scene |
of accesses Number of files touched Number of bytes touched |
accesses Number of files touched Number of bytes touched Average |
Number of files touched Number of bytes touched Average number |
of files touched Number of bytes touched Average number of |
One can contemplate such an approach because the aggregate amount |
files touched Number of bytes touched Average number of bytes |
and differentiated or uniform priorities for writing back shared and |
can contemplate such an approach because the aggregate amount of |
differentiated or uniform priorities for writing back shared and unshared |
contemplate such an approach because the aggregate amount of information |
or uniform priorities for writing back shared and unshared files |
such an approach because the aggregate amount of information might |
an approach because the aggregate amount of information might not |
approach because the aggregate amount of information might not be |
and it is used by most contemporary digital currencies and |
because the aggregate amount of information might not be that |
it is used by most contemporary digital currencies and related |
corresponds to asynchronous invalidations with differentiated priority for shared files |
the aggregate amount of information might not be that large |
is used by most contemporary digital currencies and related services |
Both clients access a shared repository of files stored on |
clients access a shared repository of files stored on the |
access a shared repository of files stored on the file |
a shared repository of files stored on the file server |
unbounded in size and hence with different users in very |
in size and hence with different users in very distinct |
size and hence with different users in very distinct parts |
and hence with different users in very distinct parts of |
hence with different users in very distinct parts of the |
with different users in very distinct parts of the space |
Each module has a descriptor file and a set of |
It would make no sense for every user to see |
would make no sense for every user to see every |
make no sense for every user to see every event |
we would solve this problem using the dynamic database querying |
would solve this problem using the dynamic database querying approach |
solve this problem using the dynamic database querying approach outlined |
this problem using the dynamic database querying approach outlined in |
problem using the dynamic database querying approach outlined in Section |
Each user would see only the objects within some range |
The total size of all the files in the collection |
total size of all the files in the collection is |
The writer workload consists of the writer updating modules in |
writer workload consists of the writer updating modules in a |
but we use Bitcoin terminology and examples since it serves |
workload consists of the writer updating modules in a random |
we use Bitcoin terminology and examples since it serves as |
consists of the writer updating modules in a random order |
use Bitcoin terminology and examples since it serves as an |
P protocols that might organize user s machines into groups |
Bitcoin terminology and examples since it serves as an active |
An update to a module consists of a sequence of |
protocols that might organize user s machines into groups forwarding |
terminology and examples since it serves as an active and |
update to a module consists of a sequence of operations |
level Network Striping for Data Intensive Applications using High Speed |
that might organize user s machines into groups forwarding streams |
and examples since it serves as an active and archetypal |
While a true evaluation of the feasibility and efficacy of |
Network Striping for Data Intensive Applications using High Speed Wide |
might organize user s machines into groups forwarding streams of |
examples since it serves as an active and archetypal example |
a true evaluation of the feasibility and efficacy of our |
Striping for Data Intensive Applications using High Speed Wide Area |
Bitcoin implements its incentive systems with a data structure called |
true evaluation of the feasibility and efficacy of our solution |
organize user s machines into groups forwarding streams of data |
for Data Intensive Applications using High Speed Wide Area Networks |
implements its incentive systems with a data structure called the |
evaluation of the feasibility and efficacy of our solution can |
user s machines into groups forwarding streams of data to |
its incentive systems with a data structure called the blockchain |
of the feasibility and efficacy of our solution can only |
s machines into groups forwarding streams of data to one |
the feasibility and efficacy of our solution can only be |
machines into groups forwarding streams of data to one another |
There is a pause between each operation and a longer |
feasibility and efficacy of our solution can only be achieved |
It is a single global ledger maintained by an open |
is a pause between each operation and a longer pause |
and efficacy of our solution can only be achieved through |
we end up in a situation where each user belongs |
is a single global ledger maintained by an open distributed |
a pause between each operation and a longer pause between |
efficacy of our solution can only be achieved through an |
end up in a situation where each user belongs to |
a single global ledger maintained by an open distributed system |
pause between each operation and a longer pause between updates |
of our solution can only be achieved through an actual |
Since anyone can join the open system and participate in |
between each operation and a longer pause between updates to |
up in a situation where each user belongs to a |
our solution can only be achieved through an actual implementation |
anyone can join the open system and participate in maintaining |
each operation and a longer pause between updates to modules |
in a situation where each user belongs to a potentially |
can join the open system and participate in maintaining the |
simulation provides an elegant way to identify and explore some |
a situation where each user belongs to a potentially large |
join the open system and participate in maintaining the blockchain |
provides an elegant way to identify and explore some of |
but an access to a module consists of a series |
Bitcoin uses a proof of work mechanism to deter attacks |
an elegant way to identify and explore some of the |
situation where each user belongs to a potentially large number |
an access to a module consists of a series of |
elegant way to identify and explore some of the cost |
where each user belongs to a potentially large number of |
access to a module consists of a series of reads |
A participant that proves she has exerted enough resources with |
each user belongs to a potentially large number of such |
participant that proves she has exerted enough resources with a |
user belongs to a potentially large number of such groups |
that proves she has exerted enough resources with a proof |
The configuration parameters used to generate the reader and writer |
proves she has exerted enough resources with a proof of |
configuration parameters used to generate the reader and writer workload |
and the groups that one user is a part of |
she has exerted enough resources with a proof of work |
parameters used to generate the reader and writer workload are |
the groups that one user is a part of might |
has exerted enough resources with a proof of work is |
used to generate the reader and writer workload are listed |
groups that one user is a part of might be |
exerted enough resources with a proof of work is allowed |
to generate the reader and writer workload are listed in |
that one user is a part of might be very |
enough resources with a proof of work is allowed to |
generate the reader and writer workload are listed in Table |
one user is a part of might be very different |
resources with a proof of work is allowed to take |
user is a part of might be very different from |
with a proof of work is allowed to take a |
is a part of might be very different from the |
The writer workload has a nominal duration of two minutes |
a proof of work is allowed to take a step |
a part of might be very different from the groups |
while the reader workload is extended to terminate at the |
proof of work is allowed to take a step in |
part of might be very different from the groups that |
a disk check process scans the access count for each |
the reader workload is extended to terminate at the same |
of work is allowed to take a step in the |
of might be very different from the groups that other |
disk check process scans the access count for each disk |
reader workload is extended to terminate at the same time |
work is allowed to take a step in the protocol |
might be very different from the groups that other users |
check process scans the access count for each disk and |
workload is extended to terminate at the same time as |
is allowed to take a step in the protocol by |
be very different from the groups that other users belong |
process scans the access count for each disk and powers |
is extended to terminate at the same time as the |
allowed to take a step in the protocol by generating |
very different from the groups that other users belong to |
scans the access count for each disk and powers down |
extended to terminate at the same time as the writer |
to take a step in the protocol by generating a |
the access count for each disk and powers down all |
to terminate at the same time as the writer workload |
take a step in the protocol by generating a block |
we need to be able to support very large numbers |
access count for each disk and powers down all but |
terminate at the same time as the writer workload actually |
need to be able to support very large numbers of |
count for each disk and powers down all but the |
Participants are compensated for their efforts with newly minted Bitcoins |
at the same time as the writer workload actually finishes |
to be able to support very large numbers of publish |
for each disk and powers down all but the most |
since low bandwidth could extend its running time beyond two |
low bandwidth could extend its running time beyond two minutes |
and with different users subscribed to very different sets of |
with different users subscribed to very different sets of topics |
as well as any disk which does not have at |
well as any disk which does not have at least |
as any disk which does not have at least t |
Up to now we have been fairly negative about the |
any disk which does not have at least t access |
to now we have been fairly negative about the trend |
disk which does not have at least t access count |
now we have been fairly negative about the trend to |
we have been fairly negative about the trend to standardize |
have been fairly negative about the trend to standardize client |
been fairly negative about the trend to standardize client access |
a miner may have to wait for an extended period |
they resulted in the lowest rate of completed writes in |
fairly negative about the trend to standardize client access to |
miner may have to wait for an extended period to |
resulted in the lowest rate of completed writes in all |
negative about the trend to standardize client access to hosted |
may have to wait for an extended period to create |
in the lowest rate of completed writes in all the |
about the trend to standardize client access to hosted content |
have to wait for an extended period to create a |
the lowest rate of completed writes in all the tests |
the trend to standardize client access to hosted content through |
to wait for an extended period to create a block |
Judicious choice of the parameters m and t minimizes the |
trend to standardize client access to hosted content through web |
wait for an extended period to create a block and |
choice of the parameters m and t minimizes the probability |
to standardize client access to hosted content through web minibrowsers |
for an extended period to create a block and earn |
of the parameters m and t minimizes the probability of |
standardize client access to hosted content through web minibrowsers that |
an extended period to create a block and earn the |
the parameters m and t minimizes the probability of this |
client access to hosted content through web minibrowsers that make |
extended period to create a block and earn the actual |
parameters m and t minimizes the probability of this occurrence |
access to hosted content through web minibrowsers that make the |
and was among the options with the highest write throughput |
period to create a block and earn the actual Bitcoins |
to hosted content through web minibrowsers that make the Javascript |
hosted content through web minibrowsers that make the Javascript running |
content through web minibrowsers that make the Javascript running on |
through web minibrowsers that make the Javascript running on a |
web minibrowsers that make the Javascript running on a user |
where all members mine concurrently and they share their revenue |
which shows the average time to complete store RPCs initiated |
minibrowsers that make the Javascript running on a user s |
all members mine concurrently and they share their revenue whenever |
shows the average time to complete store RPCs initiated by |
Methodology We have proposed the use of LFS in lieu |
that make the Javascript running on a user s machine |
members mine concurrently and they share their revenue whenever one |
the average time to complete store RPCs initiated by the |
We have proposed the use of LFS in lieu of |
make the Javascript running on a user s machine virtually |
mine concurrently and they share their revenue whenever one of |
average time to complete store RPCs initiated by the writer |
have proposed the use of LFS in lieu of FFS |
the Javascript running on a user s machine virtually inseparable |
concurrently and they share their revenue whenever one of them |
Javascript running on a user s machine virtually inseparable from |
and they share their revenue whenever one of them creates |
running on a user s machine virtually inseparable from the |
they share their revenue whenever one of them creates a |
on a user s machine virtually inseparable from the data |
share their revenue whenever one of them creates a block |
This is because of the reduced number of invalidations it |
a user s machine virtually inseparable from the data center |
is because of the reduced number of invalidations it generates |
Pools are typically implemented as a pool manager and a |
are typically implemented as a pool manager and a cohort |
typically implemented as a pool manager and a cohort of |
implemented as a pool manager and a cohort of miners |
a minibrowser approach would lack the flexibility to seamlessly combine |
it is able to take advantage of both differentiated writeback |
minibrowser approach would lack the flexibility to seamlessly combine content |
The pool manager joins the Bitcoin system as a single |
approach would lack the flexibility to seamlessly combine content from |
pool manager joins the Bitcoin system as a single miner |
would lack the flexibility to seamlessly combine content from different |
Does this new scheme provide comparable performance to existing schemes |
lack the flexibility to seamlessly combine content from different sources |
While the writer is able to decrease its time spent |
the writer is able to decrease its time spent performing |
the pool manager accepts partial proof of work and estimates |
writer is able to decrease its time spent performing store |
pool manager accepts partial proof of work and estimates each |
Our earlier concerns carry over to the Second Life scenario |
is able to decrease its time spent performing store RPCs |
manager accepts partial proof of work and estimates each miner |
Logsim consists of less than a thousand lines of Java |
accepts partial proof of work and estimates each miner s |
the reader s average time spent on fetches increases sharply |
consists of less than a thousand lines of Java code |
partial proof of work and estimates each miner s power |
reader s average time spent on fetches increases sharply when |
of less than a thousand lines of Java code and |
proof of work and estimates each miner s power according |
s average time spent on fetches increases sharply when the |
less than a thousand lines of Java code and is |
of work and estimates each miner s power according to |
average time spent on fetches increases sharply when the file |
than a thousand lines of Java code and is a |
work and estimates each miner s power according to the |
time spent on fetches increases sharply when the file in |
a thousand lines of Java code and is a single |
and estimates each miner s power according to the rate |
spent on fetches increases sharply when the file in question |
estimates each miner s power according to the rate with |
on fetches increases sharply when the file in question must |
each miner s power according to the rate with which |
fetches increases sharply when the file in question must be |
miner s power according to the rate with which it |
increases sharply when the file in question must be pulled |
we must turn off some percentage of disks in the |
s power according to the rate with which it submits |
we would need to rely on a hosting system s |
sharply when the file in question must be pulled from |
must turn off some percentage of disks in the storage |
power according to the rate with which it submits such |
would need to rely on a hosting system s mashup |
when the file in question must be pulled from the |
turn off some percentage of disks in the storage system |
according to the rate with which it submits such partial |
need to rely on a hosting system s mashup technology |
the file in question must be pulled from the writer |
to the rate with which it submits such partial proof |
to rely on a hosting system s mashup technology to |
the rate with which it submits such partial proof of |
rely on a hosting system s mashup technology to do |
rate with which it submits such partial proof of work |
this cost must be weighed against the benefit of substantially |
on a hosting system s mashup technology to do this |
cost must be weighed against the benefit of substantially increased |
must be weighed against the benefit of substantially increased writer |
it sends it to the pool manager which publishes this |
be weighed against the benefit of substantially increased writer throughput |
sends it to the pool manager which publishes this proof |
if we wanted to blend weather information from the National |
it to the pool manager which publishes this proof of |
Differentiated writeback succeeds in reducing the time the reader has |
we wanted to blend weather information from the National Hurricane |
to the pool manager which publishes this proof of work |
writeback succeeds in reducing the time the reader has to |
wanted to blend weather information from the National Hurricane Center |
the pool manager which publishes this proof of work to |
succeeds in reducing the time the reader has to wait |
to blend weather information from the National Hurricane Center with |
pool manager which publishes this proof of work to the |
transitions consume power and thus counter the potential savings achieved |
in reducing the time the reader has to wait when |
blend weather information from the National Hurricane Center with a |
manager which publishes this proof of work to the Bitcoin |
consume power and thus counter the potential savings achieved by |
reducing the time the reader has to wait when accessing |
weather information from the National Hurricane Center with a Google |
which publishes this proof of work to the Bitcoin system |
power and thus counter the potential savings achieved by powered |
the time the reader has to wait when accessing a |
information from the National Hurricane Center with a Google Map |
time the reader has to wait when accessing a shared |
The pool manager thus receives the full revenue of the |
the reader has to wait when accessing a shared file |
To find the optimal percentage of disks to be powered |
the Google map service would need to explicitly support this |
pool manager thus receives the full revenue of the block |
find the optimal percentage of disks to be powered down |
Google map service would need to explicitly support this sort |
manager thus receives the full revenue of the block and |
we ran a set of simulations on Logsim and varied |
map service would need to explicitly support this sort of |
thus receives the full revenue of the block and distributes |
ran a set of simulations on Logsim and varied the |
show statistics for invalidations and serverpull RPCs for those writer |
service would need to explicitly support this sort of embedding |
receives the full revenue of the block and distributes it |
a set of simulations on Logsim and varied the number |
statistics for invalidations and serverpull RPCs for those writer configurations |
the full revenue of the block and distributes it fairly |
set of simulations on Logsim and varied the number of |
for invalidations and serverpull RPCs for those writer configurations which |
full revenue of the block and distributes it fairly according |
of simulations on Logsim and varied the number of disks |
invalidations and serverpull RPCs for those writer configurations which make |
revenue of the block and distributes it fairly according to |
the visible portion of the scene the part of the |
simulations on Logsim and varied the number of disks that |
and serverpull RPCs for those writer configurations which make use |
of the block and distributes it fairly according to its |
visible portion of the scene the part of the texture |
on Logsim and varied the number of disks that we |
serverpull RPCs for those writer configurations which make use of |
the block and distributes it fairly according to its members |
portion of the scene the part of the texture being |
Logsim and varied the number of disks that we kept |
RPCs for those writer configurations which make use of them |
block and distributes it fairly according to its members power |
of the scene the part of the texture being displayed |
and varied the number of disks that we kept powered |
the scene the part of the texture being displayed will |
varied the number of disks that we kept powered up |
Many of the pools are open they allow any miner |
CC significantly reduces the number of invalidations it must transmit |
scene the part of the texture being displayed will often |
the number of disks that we kept powered up from |
of the pools are open they allow any miner to |
significantly reduces the number of invalidations it must transmit by |
the part of the texture being displayed will often be |
number of disks that we kept powered up from none |
the pools are open they allow any miner to join |
reduces the number of invalidations it must transmit by putting |
part of the texture being displayed will often be controlled |
pools are open they allow any miner to join them |
the number of invalidations it must transmit by putting off |
of the texture being displayed will often be controlled by |
are open they allow any miner to join them using |
number of invalidations it must transmit by putting off invalidating |
the texture being displayed will often be controlled by events |
open they allow any miner to join them using a |
of invalidations it must transmit by putting off invalidating a |
texture being displayed will often be controlled by events generated |
they allow any miner to join them using a public |
invalidations it must transmit by putting off invalidating a file |
being displayed will often be controlled by events generated by |
allow any miner to join them using a public Internet |
it must transmit by putting off invalidating a file until |
displayed will often be controlled by events generated by other |
any miner to join them using a public Internet interface |
must transmit by putting off invalidating a file until it |
will often be controlled by events generated by other Live |
Such open pools are susceptible to the classical block withholding |
transmit by putting off invalidating a file until it is |
often be controlled by events generated by other Live Objects |
open pools are susceptible to the classical block withholding attack |
by putting off invalidating a file until it is added |
be controlled by events generated by other Live Objects that |
putting off invalidating a file until it is added to |
controlled by events generated by other Live Objects that share |
off invalidating a file until it is added to the |
by events generated by other Live Objects that share the |
invalidating a file until it is added to the log |
events generated by other Live Objects that share the display |
generated by other Live Objects that share the display window |
where a miner sends only partial proof of work to |
yet the effect of this policy on the number of |
a miner sends only partial proof of work to the |
the effect of this policy on the number of serverpull |
perhaps under control of users running on machines elsewhere in |
miner sends only partial proof of work to the pool |
effect of this policy on the number of serverpull RPCs |
under control of users running on machines elsewhere in the |
sends only partial proof of work to the pool manager |
of this policy on the number of serverpull RPCs is |
control of users running on machines elsewhere in the network |
only partial proof of work to the pool manager and |
this policy on the number of serverpull RPCs is minor |
These remote sources won t fit into the interaction model |
partial proof of work to the pool manager and discards |
remote sources won t fit into the interaction model expected |
proof of work to the pool manager and discards full |
sources won t fit into the interaction model expected by |
of work to the pool manager and discards full proof |
won t fit into the interaction model expected by the |
work to the pool manager and discards full proof of |
t fit into the interaction model expected by the minibrowser |
to the pool manager and discards full proof of work |
because its store RPCs must compete with the RPCs to |
Due to the partial proof of work it sends to |
its store RPCs must compete with the RPCs to write |
to the partial proof of work it sends to the |
The size and shape of the display window and other |
store RPCs must compete with the RPCs to write back |
the partial proof of work it sends to the pool |
size and shape of the display window and other elements |
RPCs must compete with the RPCs to write back external |
the miner is considered a regular pool member and the |
and shape of the display window and other elements of |
must compete with the RPCs to write back external files |
miner is considered a regular pool member and the pool |
shape of the display window and other elements of the |
This increases the commit delay for each file and the |
is considered a regular pool member and the pool can |
of the display window and other elements of the runtime |
increases the commit delay for each file and the likelihood |
considered a regular pool member and the pool can estimate |
the display window and other elements of the runtime environment |
the commit delay for each file and the likelihood of |
a regular pool member and the pool can estimate its |
display window and other elements of the runtime environment should |
commit delay for each file and the likelihood of it |
regular pool member and the pool can estimate its power |
window and other elements of the runtime environment should be |
delay for each file and the likelihood of it being |
and other elements of the runtime environment should be inherited |
for each file and the likelihood of it being accessed |
other elements of the runtime environment should be inherited from |
the attacker shares the revenue obtained by the other pool |
each file and the likelihood of it being accessed by |
elements of the runtime environment should be inherited from the |
attacker shares the revenue obtained by the other pool members |
file and the likelihood of it being accessed by the |
of the runtime environment should be inherited from the hierarchy |
and the likelihood of it being accessed by the reader |
the runtime environment should be inherited from the hierarchy structure |
the likelihood of it being accessed by the reader while |
runtime environment should be inherited from the hierarchy structure of |
likelihood of it being accessed by the reader while it |
environment should be inherited from the hierarchy structure of the |
of it being accessed by the reader while it is |
pools and the classical block withholding attack in Section II |
should be inherited from the hierarchy structure of the object |
it being accessed by the reader while it is being |
be inherited from the hierarchy structure of the object mashup |
being accessed by the reader while it is being written |
inherited from the hierarchy structure of the object mashup used |
For a broader view of the protocol and ecosystem the |
accessed by the reader while it is being written back |
from the hierarchy structure of the object mashup used to |
a broader view of the protocol and ecosystem the reader |
the hierarchy structure of the object mashup used to create |
broader view of the protocol and ecosystem the reader may |
hierarchy structure of the object mashup used to create the |
these experiments demonstrate that for the trace we have examined |
view of the protocol and ecosystem the reader may refer |
structure of the object mashup used to create the application |
of the protocol and ecosystem the reader may refer to |
the MFS algorithm of asynchronous invalidations and differentiated writeback is |
the protocol and ecosystem the reader may refer to the |
MFS algorithm of asynchronous invalidations and differentiated writeback is able |
Thus our texture should learn its size and orientation and |
protocol and ecosystem the reader may refer to the survey |
algorithm of asynchronous invalidations and differentiated writeback is able to |
our texture should learn its size and orientation and even |
and ecosystem the reader may refer to the survey by |
of asynchronous invalidations and differentiated writeback is able to maintain |
texture should learn its size and orientation and even the |
ecosystem the reader may refer to the survey by Bonneau |
asynchronous invalidations and differentiated writeback is able to maintain cache |
should learn its size and orientation and even the GPS |
the reader may refer to the survey by Bonneau et |
invalidations and differentiated writeback is able to maintain cache consistency |
learn its size and orientation and even the GPS coordinates |
reader may refer to the survey by Bonneau et al |
and differentiated writeback is able to maintain cache consistency between |
its size and orientation and even the GPS coordinates on |
differentiated writeback is able to maintain cache consistency between the |
size and orientation and even the GPS coordinates on which |
writeback is able to maintain cache consistency between the two |
and orientation and even the GPS coordinates on which to |
is able to maintain cache consistency between the two clients |
orientation and even the GPS coordinates on which to center |
able to maintain cache consistency between the two clients and |
and even the GPS coordinates on which to center from |
to maintain cache consistency between the two clients and to |
In this work we analyze block withholding attacks among pools |
even the GPS coordinates on which to center from the |
maintain cache consistency between the two clients and to allow |
A pool that employs the pool block withholding attack registers |
the GPS coordinates on which to center from the parent |
cache consistency between the two clients and to allow the |
pool that employs the pool block withholding attack registers with |
GPS coordinates on which to center from the parent object |
consistency between the two clients and to allow the writer |
that employs the pool block withholding attack registers with the |
coordinates on which to center from the parent object that |
between the two clients and to allow the writer to |
employs the pool block withholding attack registers with the victim |
on which to center from the parent object that hosts |
the two clients and to allow the writer to write |
the pool block withholding attack registers with the victim pool |
which to center from the parent object that hosts it |
two clients and to allow the writer to write back |
pool block withholding attack registers with the victim pool as |
clients and to allow the writer to write back changes |
block withholding attack registers with the victim pool as a |
and similarly until we reach the root object hosting the |
and to allow the writer to write back changes to |
withholding attack registers with the victim pool as a regular |
similarly until we reach the root object hosting the display |
to allow the writer to write back changes to the |
attack registers with the victim pool as a regular miner |
until we reach the root object hosting the display window |
allow the writer to write back changes to the stored |
It receives tasks from the victim pool and transfers them |
the writer to write back changes to the stored data |
receives tasks from the victim pool and transfers them to |
writer to write back changes to the stored data faster |
tasks from the victim pool and transfers them to some |
to write back changes to the stored data faster than |
minibrowsers retain one potential advantage over the layered architecture we |
from the victim pool and transfers them to some of |
write back changes to the stored data faster than is |
retain one potential advantage over the layered architecture we proposed |
the victim pool and transfers them to some of its |
back changes to the stored data faster than is possible |
one potential advantage over the layered architecture we proposed earlier |
victim pool and transfers them to some of its own |
changes to the stored data faster than is possible with |
Since all aspects of the view are optimized to run |
pool and transfers them to some of its own miners |
to the stored data faster than is possible with the |
all aspects of the view are optimized to run together |
the stored data faster than is possible with the alternative |
stored data faster than is possible with the alternative schemes |
the interaction controls might be far more sophisticated and perform |
and the mining power spent by a pool the infiltration |
interaction controls might be far more sophisticated and perform potentially |
the mining power spent by a pool the infiltration rate |
References mance of the algorithm to determine its effectiveness under |
controls might be far more sophisticated and perform potentially much |
mance of the algorithm to determine its effectiveness under other |
When the attacking pool s infiltrating miners deliver partial proofs |
might be far more sophisticated and perform potentially much better |
of the algorithm to determine its effectiveness under other workloads |
the attacking pool s infiltrating miners deliver partial proofs of |
be far more sophisticated and perform potentially much better than |
attacking pool s infiltrating miners deliver partial proofs of work |
far more sophisticated and perform potentially much better than a |
more sophisticated and perform potentially much better than a solution |
sophisticated and perform potentially much better than a solution resulting |
and perform potentially much better than a solution resulting from |
perform potentially much better than a solution resulting from mashing |
When the infiltrating miners deliver a full proof of work |
potentially much better than a solution resulting from mashing up |
much better than a solution resulting from mashing up together |
better than a solution resulting from mashing up together multiple |
than a solution resulting from mashing up together multiple layers |
This attack affects the revenues of the pools in several |
a solution resulting from mashing up together multiple layers developed |
attack affects the revenues of the pools in several ways |
solution resulting from mashing up together multiple layers developed independently |
nd Annual Joint Conference of the IEEE Computer and Communications |
while the latter is measured by comparing the cumulative percentage |
Annual Joint Conference of the IEEE Computer and Communications Societies |
the latter is measured by comparing the cumulative percentage of |
latter is measured by comparing the cumulative percentage of time |
is measured by comparing the cumulative percentage of time the |
since some of its miners are used for block withholding |
measured by comparing the cumulative percentage of time the disks |
by comparing the cumulative percentage of time the disks are |
This observation highlights the importance of developing component interface and |
but it earns additional revenue through its infiltration of the |
comparing the cumulative percentage of time the disks are kept |
observation highlights the importance of developing component interface and event |
it earns additional revenue through its infiltration of the other |
the cumulative percentage of time the disks are kept powered |
highlights the importance of developing component interface and event standards |
earns additional revenue through its infiltration of the other pool |
cumulative percentage of time the disks are kept powered on |
the importance of developing component interface and event standards for |
importance of developing component interface and event standards for the |
of developing component interface and event standards for the layered |
the total effective mining power in the system is reduced |
developing component interface and event standards for the layered architecture |
component interface and event standards for the layered architecture we |
interface and event standards for the layered architecture we ve |
Conclusion The growing use of mobile computers and wireless networks |
and event standards for the layered architecture we ve outlined |
The growing use of mobile computers and wireless networks has |
we observe that a pool might be able to increase |
growing use of mobile computers and wireless networks has greatly |
observe that a pool might be able to increase its |
use of mobile computers and wireless networks has greatly increased |
that a pool might be able to increase its revenue |
of mobile computers and wireless networks has greatly increased the |
a pool might be able to increase its revenue by |
mobile computers and wireless networks has greatly increased the scope |
pool might be able to increase its revenue by attacking |
computers and wireless networks has greatly increased the scope for |
might be able to increase its revenue by attacking other |
their OLE interfaces are pervasively used to support thousands of |
and wireless networks has greatly increased the scope for adapting |
be able to increase its revenue by attacking other pools |
OLE interfaces are pervasively used to support thousands of plugins |
wireless networks has greatly increased the scope for adapting data |
Each pool therefore makes a choice of whether to attack |
interfaces are pervasively used to support thousands of plugins that |
of the disks can be spun down while still maintaining |
networks has greatly increased the scope for adapting data access |
pool therefore makes a choice of whether to attack each |
are pervasively used to support thousands of plugins that implement |
the disks can be spun down while still maintaining performance |
has greatly increased the scope for adapting data access to |
therefore makes a choice of whether to attack each of |
pervasively used to support thousands of plugins that implement context |
disks can be spun down while still maintaining performance comparable |
greatly increased the scope for adapting data access to vary |
makes a choice of whether to attack each of the |
used to support thousands of plugins that implement context menus |
can be spun down while still maintaining performance comparable to |
a choice of whether to attack each of the other |
be spun down while still maintaining performance comparable to that |
choice of whether to attack each of the other pools |
spun down while still maintaining performance comparable to that of |
of whether to attack each of the other pools in |
down while still maintaining performance comparable to that of a |
whether to attack each of the other pools in the |
while still maintaining performance comparable to that of a conventional |
to attack each of the other pools in the system |
still maintaining performance comparable to that of a conventional file |
In addition to allowing hosted content to be pulled in |
maintaining performance comparable to that of a conventional file system |
addition to allowing hosted content to be pulled in and |
to allowing hosted content to be pulled in and exposed |
We specify this game and provide initial analysis in Section |
The performance of our system depends very heavily on its |
allowing hosted content to be pulled in and exposed via |
specify this game and provide initial analysis in Section IV |
performance of our system depends very heavily on its cache |
hosted content to be pulled in and exposed via event |
of our system depends very heavily on its cache configuration |
content to be pulled in and exposed via event interfaces |
In Section V we analyze the scenario where exactly two |
Since cache optimization is an orthogonal issue that comprises an |
Section V we analyze the scenario where exactly two of |
components developed by some of our users also use embedded |
cache optimization is an orthogonal issue that comprises an entire |
V we analyze the scenario where exactly two of the |
Measurements of a distributed file the technique of modeless adaptation |
developed by some of our users also use embedded minibrowsers |
optimization is an orthogonal issue that comprises an entire field |
we analyze the scenario where exactly two of the pools |
of a distributed file the technique of modeless adaptation to |
by some of our users also use embedded minibrowsers to |
is an orthogonal issue that comprises an entire field of |
analyze the scenario where exactly two of the pools take |
a distributed file the technique of modeless adaptation to a |
some of our users also use embedded minibrowsers to gain |
an orthogonal issue that comprises an entire field of research |
the scenario where exactly two of the pools take part |
distributed file the technique of modeless adaptation to a distributed |
of our users also use embedded minibrowsers to gain access |
orthogonal issue that comprises an entire field of research in |
scenario where exactly two of the pools take part in |
file the technique of modeless adaptation to a distributed file |
our users also use embedded minibrowsers to gain access to |
issue that comprises an entire field of research in itself |
where exactly two of the pools take part in the |
the technique of modeless adaptation to a distributed file system |
users also use embedded minibrowsers to gain access to a |
exactly two of the pools take part in the game |
technique of modeless adaptation to a distributed file system system |
also use embedded minibrowsers to gain access to a wide |
two of the pools take part in the game and |
use embedded minibrowsers to gain access to a wide range |
of the pools take part in the game and only |
embedded minibrowsers to gain access to a wide range of |
This data point represents the best performance we could achieve |
the pools take part in the game and only one |
minibrowsers to gain access to a wide range of platforms |
The cache manager for our MFS on Operating Systems Principles |
data point represents the best performance we could achieve since |
pools take part in the game and only one can |
point represents the best performance we could achieve since an |
take part in the game and only one can attack |
represents the best performance we could achieve since an oracle |
part in the game and only one can attack the |
the best performance we could achieve since an oracle has |
in the game and only one can attack the other |
best performance we could achieve since an oracle has future |
performance we could achieve since an oracle has future knowledge |
we could achieve since an oracle has future knowledge and |
could achieve since an oracle has future knowledge and is |
Pacific file system incorporates features that are not present in |
achieve since an oracle has future knowledge and is able |
file system incorporates features that are not present in existing |
since an oracle has future knowledge and is able to |
Performance Evaluation Central to our argument is the assertion that |
system incorporates features that are not present in existing Grove |
an oracle has future knowledge and is able to replace |
Evaluation Central to our argument is the assertion that hosted |
oracle has future knowledge and is able to replace items |
Central to our argument is the assertion that hosted event |
has future knowledge and is able to replace items accessed |
to our argument is the assertion that hosted event notification |
future knowledge and is able to replace items accessed furthest |
our argument is the assertion that hosted event notification solutions |
knowledge and is able to replace items accessed furthest in |
argument is the assertion that hosted event notification solutions scale |
and is able to replace items accessed furthest in the |
is the assertion that hosted event notification solutions scale poorly |
is able to replace items accessed furthest in the future |
the revenue of each pool affects the revenue of the |
the assertion that hosted event notification solutions scale poorly and |
adaptation to bandwidth variation through the use of prioritised communication |
revenue of each pool affects the revenue of the other |
assertion that hosted event notification solutions scale poorly and stand |
of each pool affects the revenue of the other through |
that hosted event notification solutions scale poorly and stand as |
each pool affects the revenue of the other through the |
hosted event notification solutions scale poorly and stand as a |
we also wish to provide a performance comparison of our |
pool affects the revenue of the other through the infiltrating |
event notification solutions scale poorly and stand as a barrier |
also wish to provide a performance comparison of our system |
affects the revenue of the other through the infiltrating miners |
notification solutions scale poorly and stand as a barrier to |
wish to provide a performance comparison of our system against |
We prove that for a static choice of infiltration rates |
solutions scale poorly and stand as a barrier to collaboration |
to provide a performance comparison of our system against conventional |
prove that for a static choice of infiltration rates the |
scale poorly and stand as a barrier to collaboration applications |
O hint genercache consistency protocol using file access information to |
that for a static choice of infiltration rates the pool |
hint genercache consistency protocol using file access information to imation |
and that developers will want to combine hosted content with |
for a static choice of infiltration rates the pool revenues |
genercache consistency protocol using file access information to imation through |
that developers will want to combine hosted content with P |
a static choice of infiltration rates the pool revenues converge |
consistency protocol using file access information to imation through speculative |
protocol using file access information to imation through speculative execution |
In this section we present data to support our claims |
once one pool changes its infiltration rate of the other |
the latter may prefer to change its infiltration rate of |
latter may prefer to change its infiltration rate of the |
may prefer to change its infiltration rate of the former |
We show analytically that the game has a single Nash |
show analytically that the game has a single Nash Equilibrium |
analytically that the game has a single Nash Equilibrium and |
that the game has a single Nash Equilibrium and numerically |
the game has a single Nash Equilibrium and numerically study |
game has a single Nash Equilibrium and numerically study the |
has a single Nash Equilibrium and numerically study the equilibrium |
a single Nash Equilibrium and numerically study the equilibrium points |
and were obtained using a testing methodology and setup developed |
We have evaluated the effect of these features on performance |
single Nash Equilibrium and numerically study the equilibrium points for |
were obtained using a testing methodology and setup developed and |
have evaluated the effect of these features on performance at |
Nash Equilibrium and numerically study the equilibrium points for different |
obtained using a testing methodology and setup developed and published |
evaluated the effect of these features on performance at varying |
Equilibrium and numerically study the equilibrium points for different pool |
using a testing methodology and setup developed and published by |
the effect of these features on performance at varying bandwidth |
and numerically study the equilibrium points for different pool sizes |
a testing methodology and setup developed and published by Sonic |
effect of these features on performance at varying bandwidth levels |
testing methodology and setup developed and published by Sonic Software |
of these features on performance at varying bandwidth levels and |
these features on performance at varying bandwidth levels and under |
features on performance at varying bandwidth levels and under both |
on performance at varying bandwidth levels and under both synthetic |
performance at varying bandwidth levels and under both synthetic and |
at the equilibrium point both pools earn less than they |
at varying bandwidth levels and under both synthetic and real |
the equilibrium point both pools earn less than they would |
equilibrium point both pools earn less than they would have |
point both pools earn less than they would have in |
both pools earn less than they would have in the |
pools earn less than they would have in the nonequilibrium |
earn less than they would have in the nonequilibrium no |
Since pools can decide to start or stop attacking at |
pools can decide to start or stop attacking at any |
can decide to start or stop attacking at any point |
this can be modeled as the miner s dilemma an |
Research Edition Where the Academic Knights meet the Evil Empire |
can be modeled as the miner s dilemma an instance |
Edition Where the Academic Knights meet the Evil Empire Werner |
be modeled as the miner s dilemma an instance of |
Where the Academic Knights meet the Evil Empire Werner Vogels |
modeled as the miner s dilemma an instance of the |
the Academic Knights meet the Evil Empire Werner Vogels The |
as the miner s dilemma an instance of the iterative |
Academic Knights meet the Evil Empire Werner Vogels The rivalry |
the miner s dilemma an instance of the iterative prisoner |
Knights meet the Evil Empire Werner Vogels The rivalry in |
miner s dilemma an instance of the iterative prisoner s |
meet the Evil Empire Werner Vogels The rivalry in the |
s dilemma an instance of the iterative prisoner s dilemma |
The experiment varies the number of subscribers while using a |
the Evil Empire Werner Vogels The rivalry in the operating |
experiment varies the number of subscribers while using a single |
Evil Empire Werner Vogels The rivalry in the operating system |
In Proceedings of the ISCA Interadditional costs imposed are mostly |
varies the number of subscribers while using a single publisher |
Empire Werner Vogels The rivalry in the operating system market |
Proceedings of the ISCA Interadditional costs imposed are mostly hidden |
the number of subscribers while using a single publisher that |
Werner Vogels The rivalry in the operating system market place |
they can have benenational Conference on Parallel and Distributed Computfits |
number of subscribers while using a single publisher that communicates |
we address in Section VII the case where the participants |
Vogels The rivalry in the operating system market place has |
can have benenational Conference on Parallel and Distributed Computfits which |
of subscribers while using a single publisher that communicates through |
address in Section VII the case where the participants are |
The rivalry in the operating system market place has a |
have benenational Conference on Parallel and Distributed Computfits which are |
subscribers while using a single publisher that communicates through a |
in Section VII the case where the participants are an |
rivalry in the operating system market place has a severe |
benenational Conference on Parallel and Distributed Computfits which are very |
while using a single publisher that communicates through a single |
Section VII the case where the participants are an arbitrary |
in the operating system market place has a severe impact |
Conference on Parallel and Distributed Computfits which are very visible |
using a single publisher that communicates through a single hosted |
shows an estimate of the actual power savings achieved by |
VII the case where the participants are an arbitrary number |
the operating system market place has a severe impact on |
a single publisher that communicates through a single hosted message |
an estimate of the actual power savings achieved by our |
the case where the participants are an arbitrary number of |
operating system market place has a severe impact on the |
single publisher that communicates through a single hosted message broker |
estimate of the actual power savings achieved by our solution |
case where the participants are an arbitrary number of identical |
system market place has a severe impact on the academic |
publisher that communicates through a single hosted message broker on |
where the participants are an arbitrary number of identical pools |
market place has a severe impact on the academic world |
adaptation in MFS allows clients to adapt quickly to a |
that communicates through a single hosted message broker on a |
in MFS allows clients to adapt quickly to a variety |
There exists a symmetric equilibrium in which each participating pool |
communicates through a single hosted message broker on a single |
Where in the old days intellection quality and careful deliberation |
MFS allows clients to adapt quickly to a variety of |
exists a symmetric equilibrium in which each participating pool attacks |
through a single hosted message broker on a single topic |
in the old days intellection quality and careful deliberation would |
allows clients to adapt quickly to a variety of bandwidth |
a symmetric equilibrium in which each participating pool attacks each |
the old days intellection quality and careful deliberation would prevail |
clients to adapt quickly to a variety of bandwidth conditions |
symmetric equilibrium in which each participating pool attacks each of |
nowadays discussions about operating systems research appear to be more |
even if a subscriber experiences a transient loss of connectivity |
to adapt quickly to a variety of bandwidth conditions without |
equilibrium in which each participating pool attacks each of the |
discussions about operating systems research appear to be more like |
adapt quickly to a variety of bandwidth conditions without substantial |
in which each participating pool attacks each of the other |
about operating systems research appear to be more like the |
quickly to a variety of bandwidth conditions without substantial changes |
which each participating pool attacks each of the other participating |
operating systems research appear to be more like the battlefield |
to a variety of bandwidth conditions without substantial changes in |
each participating pool attacks each of the other participating pools |
latency will also soars because the amount of time the |
systems research appear to be more like the battlefield of |
a variety of bandwidth conditions without substantial changes in operation |
will also soars because the amount of time the broker |
research appear to be more like the battlefield of a |
also soars because the amount of time the broker needs |
appear to be more like the battlefield of a holy |
here too at equilibrium all pools earn less than with |
soars because the amount of time the broker needs to |
to be more like the battlefield of a holy war |
too at equilibrium all pools earn less than with the |
because the amount of time the broker needs to spend |
at equilibrium all pools earn less than with the no |
the amount of time the broker needs to spend sending |
amount of time the broker needs to spend sending a |
of time the broker needs to spend sending a single |
and select an operating system that could bring our research |
time the broker needs to spend sending a single message |
select an operating system that could bring our research into |
Our results imply that block withholding by pools leads to |
the broker needs to spend sending a single message increases |
an operating system that could bring our research into the |
results imply that block withholding by pools leads to an |
broker needs to spend sending a single message increases linearly |
operating system that could bring our research into the next |
imply that block withholding by pools leads to an unfavorable |
needs to spend sending a single message increases linearly with |
Our evaluation has included comparisons of MFS to cache manM |
that block withholding by pools leads to an unfavorable equilibrium |
system that could bring our research into the next century |
to spend sending a single message increases linearly with the |
spend sending a single message increases linearly with the number |
sending a single message increases linearly with the number of |
the cumulative percentage of time the disks are powered on |
a single message increases linearly with the number of subscribers |
This paper describes how this evaluation lead to the insight |
paper describes how this evaluation lead to the insight that |
describes how this evaluation lead to the insight that Microsoft |
how this evaluation lead to the insight that Microsoft s |
this evaluation lead to the insight that Microsoft s Windows |
evaluation lead to the insight that Microsoft s Windows NT |
miners will prefer to form closed pools that cannot be |
and confirmed Scale and performance in a distributed file system |
lead to the insight that Microsoft s Windows NT is |
will prefer to form closed pools that cannot be attacked |
ACM that there are situations in which MFS would outperform |
shows throughput in an experiment in which the publisher does |
to the insight that Microsoft s Windows NT is the |
prefer to form closed pools that cannot be attacked in |
that there are situations in which MFS would outperform AFS |
throughput in an experiment in which the publisher does not |
the insight that Microsoft s Windows NT is the operating |
to form closed pools that cannot be attacked in this |
increase as the percentage of disks that is powered on |
in an experiment in which the publisher does not log |
insight that Microsoft s Windows NT is the operating system |
form closed pools that cannot be attacked in this manner |
as the percentage of disks that is powered on is |
an experiment in which the publisher does not log data |
Though this may be conceived as bad news for public |
the percentage of disks that is powered on is decreased |
that Microsoft s Windows NT is the operating system that |
this may be conceived as bad news for public mining |
Microsoft s Windows NT is the operating system that is |
may be conceived as bad news for public mining pools |
s Windows NT is the operating system that is best |
We find that while the maximum throughput is much higher |
on the whole it may be good news to the |
Windows NT is the operating system that is best prepared |
the whole it may be good news to the Bitcoin |
NT is the operating system that is best prepared for |
whole it may be good news to the Bitcoin system |
is the operating system that is best prepared for the |
developers of collaboration applications that need good scalability might discover |
the operating system that is best prepared for the future |
of collaboration applications that need good scalability might discover that |
We examine the practicality of the attack in Section VIII |
collaboration applications that need good scalability might discover that hosted |
Introduction Until recently there was no doubt in academia which |
examine the practicality of the attack in Section VIII and |
we point out a new opportunity for saving power in |
applications that need good scalability might discover that hosted ESB |
Until recently there was no doubt in academia which operating |
these earlier systems were designed for a mobile environment which |
the practicality of the attack in Section VIII and discuss |
point out a new opportunity for saving power in large |
that need good scalability might discover that hosted ESB options |
recently there was no doubt in academia which operating system |
earlier systems were designed for a mobile environment which is |
practicality of the attack in Section VIII and discuss implications |
need good scalability might discover that hosted ESB options won |
there was no doubt in academia which operating system to |
systems were designed for a mobile environment which is substantially |
of the attack in Section VIII and discuss implications and |
log structured file systems write only to the log head |
good scalability might discover that hosted ESB options won t |
was no doubt in academia which operating system to use |
were designed for a mobile environment which is substantially different |
the attack in Section VIII and discuss implications and model |
scalability might discover that hosted ESB options won t achieve |
no doubt in academia which operating system to use for |
attack in Section VIII and discuss implications and model extensions |
might discover that hosted ESB options won t achieve this |
doubt in academia which operating system to use for systems |
potentially allowing us to power down all the other disks |
in Section VIII and discuss implications and model extensions in |
discover that hosted ESB options won t achieve this goal |
in academia which operating system to use for systems research |
Section VIII and discuss implications and model extensions in Section |
VIII and discuss implications and model extensions in Section IX |
we report on some experiments we conducted on our own |
report on some experiments we conducted on our own at |
on some experiments we conducted on our own at Cornell |
focusing on scalability of event notification platforms that leverage peer |
was used since its inception to investigate fundamental system research |
Definition of the pool game where pools in a proof |
and the accumulated knowledge in academia about its internals and |
ofwork secured system attack one another with a pool block |
the accumulated knowledge in academia about its internals and operations |
secured system attack one another with a pool block withholding |
accumulated knowledge in academia about its internals and operations was |
system attack one another with a pool block withholding attack |
knowledge in academia about its internals and operations was significant |
we compare the maximum throughput of two decentralized reliable multicast |
compare the maximum throughput of two decentralized reliable multicast protocols |
had their roots in the commercial world and knowledge about |
their roots in the commercial world and knowledge about these |
roots in the commercial world and knowledge about these systems |
in the commercial world and knowledge about these systems never |
the commercial world and knowledge about these systems never accumulated |
commercial world and knowledge about these systems never accumulated to |
world and knowledge about these systems never accumulated to the |
and knowledge about these systems never accumulated to the critical |
knowledge about these systems never accumulated to the critical mass |
about these systems never accumulated to the critical mass were |
the only Nash Equilibrium is when the pools attack one |
these systems never accumulated to the critical mass were these |
only Nash Equilibrium is when the pools attack one another |
systems never accumulated to the critical mass were these systems |
never accumulated to the critical mass were these systems could |
accumulated to the critical mass were these systems could be |
to the critical mass were these systems could be considered |
the critical mass were these systems could be considered for |
critical mass were these systems could be considered for widespread |
mass were these systems could be considered for widespread research |
were these systems could be considered for widespread research tasks |
none have found the following that the established Unix s |
Mofavouring cache validation and RPCs to retrieve files over other |
have found the following that the established Unix s received |
With multiple pools of equal size there is a symmetric |
cache validation and RPCs to retrieve files over other bile |
multiple pools of equal size there is a symmetric Nash |
validation and RPCs to retrieve files over other bile computing |
pools of equal size there is a symmetric Nash equilibrium |
and RPCs to retrieve files over other bile computing with |
RPCs to retrieve files over other bile computing with the |
but slowly but surely Windows NT is now entering the |
where all pools earn less than if none had attacked |
to retrieve files over other bile computing with the Rover |
slowly but surely Windows NT is now entering the academic |
retrieve files over other bile computing with the Rover Toolkit |
but surely Windows NT is now entering the academic world |
surely Windows NT is now entering the academic world as |
Windows NT is now entering the academic world as a |
NT is now entering the academic world as a viable |
We have not compared MFS with LBFS since tions on |
inefficient equilibria for open pools may serve the system by |
have not compared MFS with LBFS since tions on Computers |
equilibria for open pools may serve the system by reducing |
Although academia looked with fascination at Dave Cutler s attempt |
for open pools may serve the system by reducing their |
academia looked with fascination at Dave Cutler s attempt to |
open pools may serve the system by reducing their attraction |
looked with fascination at Dave Cutler s attempt to build |
pools may serve the system by reducing their attraction and |
with fascination at Dave Cutler s attempt to build a |
may serve the system by reducing their attraction and pushing |
fascination at Dave Cutler s attempt to build a new |
serve the system by reducing their attraction and pushing miners |
at Dave Cutler s attempt to build a new operating |
the system by reducing their attraction and pushing miners towards |
Dave Cutler s attempt to build a new operating system |
system by reducing their attraction and pushing miners towards smaller |
Cutler s attempt to build a new operating system from |
by reducing their attraction and pushing miners towards smaller closed |
s attempt to build a new operating system from the |
reducing their attraction and pushing miners towards smaller closed pools |
attempt to build a new operating system from the ground |
to build a new operating system from the ground up |
The classical block withholding attack is old as pools themselves |
but its use by pools has not been suggested until |
its use by pools has not been suggested until recently |
We overview related attacks and prior work in Section X |
All expected that Windows NT would go the same way |
expected that Windows NT would go the same way as |
that Windows NT would go the same way as the |
Windows NT would go the same way as the other |
P RELIMINARIES B ITCOIN AND P OOLED M INING Bitcoin |
not present in the earlier systems we have compared against |
NT would go the same way as the other commercially |
RELIMINARIES B ITCOIN AND P OOLED M INING Bitcoin is |
We anticipate that implementing LBFS file chunks in MFS would |
JGroups failed when we attempted to configure it with more |
would go the same way as the other commercially designed |
B ITCOIN AND P OOLED M INING Bitcoin is a |
failed when we attempted to configure it with more than |
go the same way as the other commercially designed operating |
ITCOIN AND P OOLED M INING Bitcoin is a distributed |
the same way as the other commercially designed operating systems |
same way as the other commercially designed operating systems before |
way as the other commercially designed operating systems before it |
as the other commercially designed operating systems before it and |
the other commercially designed operating systems before it and remain |
other commercially designed operating systems before it and remain in |
we look at two scalable protocols under conditions of stress |
commercially designed operating systems before it and remain in the |
designed operating systems before it and remain in the dark |
operating systems before it and remain in the dark corner |
systems before it and remain in the dark corner from |
before it and remain in the dark corner from a |
as a fixed message rate is spread over varying numbers |
it and remain in the dark corner from a research |
a fixed message rate is spread over varying numbers of |
and remain in the dark corner from a research use |
fixed message rate is spread over varying numbers of topics |
remain in the dark corner from a research use point |
in the dark corner from a research use point of |
the dark corner from a research use point of view |
we plan to investigate the performance of ings of the |
plan to investigate the performance of ings of the First |
to investigate the performance of ings of the First USENIX |
investigate the performance of ings of the First USENIX Conference |
not long after the final major release of academic version |
the performance of ings of the First USENIX Conference on |
long after the final major release of academic version of |
performance of ings of the First USENIX Conference on File |
after the final major release of academic version of the |
of ings of the First USENIX Conference on File and |
the final major release of academic version of the Unix |
ings of the First USENIX Conference on File and Storage |
final major release of academic version of the Unix operating |
and the system s only task is to serialize transactions |
of the First USENIX Conference on File and Storage modeless |
major release of academic version of the Unix operating system |
the system s only task is to serialize transactions in |
the First USENIX Conference on File and Storage modeless adaptation |
system s only task is to serialize transactions in a |
First USENIX Conference on File and Storage modeless adaptation and |
s only task is to serialize transactions in a single |
USENIX Conference on File and Storage modeless adaptation and MFS |
only task is to serialize transactions in a single ledger |
Conference on File and Storage modeless adaptation and MFS in |
task is to serialize transactions in a single ledger and |
on File and Storage modeless adaptation and MFS in wide |
is to serialize transactions in a single ledger and reject |
to serialize transactions in a single ledger and reject transactions |
serialize transactions in a single ledger and reject transactions that |
transactions in a single ledger and reject transactions that cannot |
in a single ledger and reject transactions that cannot be |
a single ledger and reject transactions that cannot be serialized |
Since existing solutions are typically layered on top of the |
single ledger and reject transactions that cannot be serialized due |
the farewell of the Berkeley systems Werner Vogels is a |
existing solutions are typically layered on top of the file |
ledger and reject transactions that cannot be serialized due to |
farewell of the Berkeley systems Werner Vogels is a research |
and reject transactions that cannot be serialized due to conflicts |
of the Berkeley systems Werner Vogels is a research scientist |
reject transactions that cannot be serialized due to conflicts with |
they could be used in conjunction with our solution to |
the Berkeley systems Werner Vogels is a research scientist at |
transactions that cannot be serialized due to conflicts with previous |
could be used in conjunction with our solution to take |
Berkeley systems Werner Vogels is a research scientist at the |
as well as further evaluating the performance of the MFS |
that cannot be serialized due to conflicts with previous transactions |
be used in conjunction with our solution to take advantage |
systems Werner Vogels is a research scientist at the Department |
well as further evaluating the performance of the MFS cache |
used in conjunction with our solution to take advantage of |
Werner Vogels is a research scientist at the Department of |
Bitcoin transactions are protected with cryptographic techniques that ensure that |
as further evaluating the performance of the MFS cache consistency |
in conjunction with our solution to take advantage of application |
Vogels is a research scientist at the Department of Computer |
transactions are protected with cryptographic techniques that ensure that only |
further evaluating the performance of the MFS cache consistency algorithm |
latency soars when we repeat this with the industrystandard Scalable |
is a research scientist at the Department of Computer Science |
are protected with cryptographic techniques that ensure that only the |
We also provide some initial simulation results that validate our |
soars when we repeat this with the industrystandard Scalable Reliable |
a research scientist at the Department of Computer Science of |
protected with cryptographic techniques that ensure that only the rightful |
also provide some initial simulation results that validate our claim |
when we repeat this with the industrystandard Scalable Reliable Multicast |
research scientist at the Department of Computer Science of Cornell |
with cryptographic techniques that ensure that only the rightful owner |
provide some initial simulation results that validate our claim that |
scientist at the Department of Computer Science of Cornell University |
cryptographic techniques that ensure that only the rightful owner of |
some initial simulation results that validate our claim that power |
Disconnected operaMFS to further examine the benefits achievable from the |
techniques that ensure that only the rightful owner of a |
operaMFS to further examine the benefits achievable from the autotion |
that ensure that only the rightful owner of a Bitcoin |
to further examine the benefits achievable from the autotion in |
ensure that only the rightful owner of a Bitcoin can |
While simulations can never provide conclusive evidence for the feasibility |
further examine the benefits achievable from the autotion in the |
that only the rightful owner of a Bitcoin can transfer |
simulations can never provide conclusive evidence for the feasibility of |
examine the benefits achievable from the autotion in the Coda |
only the rightful owner of a Bitcoin can transfer it |
can never provide conclusive evidence for the feasibility of a |
the benefits achievable from the autotion in the Coda file |
never provide conclusive evidence for the feasibility of a system |
benefits achievable from the autotion in the Coda file system |
The transaction ledger is stored by a network of miners |
transaction ledger is stored by a network of miners in |
ACM Transactions on Commatic generation of caching policies for files |
ledger is stored by a network of miners in a |
Our principal contribution in this paper is in having shown |
Hosted enterprise service bus architectures can achieve high levels of |
is stored by a network of miners in a data |
principal contribution in this paper is in having shown a |
enterprise service bus architectures can achieve high levels of publish |
stored by a network of miners in a data structure |
contribution in this paper is in having shown a new |
by a network of miners in a data structure caller |
in this paper is in having shown a new fit |
a network of miners in a data structure caller the |
but performance degrades very sharply as the number of subscribers |
this paper is in having shown a new fit for |
network of miners in a data structure caller the blockchain |
performance degrades very sharply as the number of subscribers or |
paper is in having shown a new fit for an |
degrades very sharply as the number of subscribers or topics |
is in having shown a new fit for an old |
very sharply as the number of subscribers or topics grows |
Revenue for Proof Of Work The blockchain records the transactions |
in having shown a new fit for an old idea |
for Proof Of Work The blockchain records the transactions in |
Proof Of Work The blockchain records the transactions in units |
Of Work The blockchain records the transactions in units of |
Work The blockchain records the transactions in units of blocks |
structured file system shows promise as a powersaving opportunity for |
file system shows promise as a powersaving opportunity for large |
group and the early demise of Mach as the last |
and the early demise of Mach as the last of |
Acknowledgments This work was partially funded by Intel Corporation and |
A valid block contains the hash of the previous block |
the early demise of Mach as the last of the |
This work was partially funded by Intel Corporation and the |
Ricochet achieved the best recovery latency when message loss is |
early demise of Mach as the last of the research |
work was partially funded by Intel Corporation and the National |
achieved the best recovery latency when message loss is an |
and a Bitcoin address which is to be credited with |
demise of Mach as the last of the research operating |
was partially funded by Intel Corporation and the National Science |
the best recovery latency when message loss is an issue |
a Bitcoin address which is to be credited with a |
In Proceedings of the Sixteenth ACM Symposium on Operating Systems |
of Mach as the last of the research operating systems |
partially funded by Intel Corporation and the National Science Foundation |
Bitcoin address which is to be credited with a reward |
Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles |
Special thanks to Saikat Guha for his input in the |
address which is to be credited with a reward for |
QSM at small loss rates achieves similar average latency with |
thanks to Saikat Guha for his input in the simulator |
which is to be credited with a reward for generating |
and a myriad of Unix operating systems was available for |
at small loss rates achieves similar average latency with considerably |
to Saikat Guha for his input in the simulator design |
is to be credited with a reward for generating the |
a myriad of Unix operating systems was available for this |
small loss rates achieves similar average latency with considerably lower |
to be credited with a reward for generating the block |
We also wish to thank our anonymous reviewers for their |
myriad of Unix operating systems was available for this platform |
loss rates achieves similar average latency with considerably lower network |
also wish to thank our anonymous reviewers for their valuable |
Any miner may add a valid block to the chain |
rates achieves similar average latency with considerably lower network overheads |
wish to thank our anonymous reviewers for their valuable feedback |
miner may add a valid block to the chain by |
At the Computer Science department at Cornell University we made |
the Computer Science department at Cornell University we made the |
Computer Science department at Cornell University we made the decision |
Science department at Cornell University we made the decision to |
proving that it has spent a certain amount of work |
department at Cornell University we made the decision to conduct |
that it has spent a certain amount of work and |
at Cornell University we made the decision to conduct our |
Emin Gu n Sirer and Paul Francis for comments and |
it has spent a certain amount of work and publishing |
each of the solutions tested has some advantages that its |
Cornell University we made the decision to conduct our research |
Gu n Sirer and Paul Francis for comments and suggestions |
has spent a certain amount of work and publishing the |
of the solutions tested has some advantages that its competitors |
University we made the decision to conduct our research on |
n Sirer and Paul Francis for comments and suggestions regarding |
spent a certain amount of work and publishing the block |
the solutions tested has some advantages that its competitors lack |
we made the decision to conduct our research on Windows |
Sirer and Paul Francis for comments and suggestions regarding MFS |
a certain amount of work and publishing the block with |
made the decision to conduct our research on Windows NT |
certain amount of work and publishing the block with the |
amount of work and publishing the block with the proof |
of work and publishing the block with the proof over |
By that time we had learned enough from the early |
and Kevin Walsh for helpful discussions and corrections to this |
work and publishing the block with the proof over an |
that time we had learned enough from the early design |
Kevin Walsh for helpful discussions and corrections to this paper |
and publishing the block with the proof over an overlay |
time we had learned enough from the early design of |
it builds an overlay multicast tree within which events travel |
publishing the block with the proof over an overlay network |
we had learned enough from the early design of Windows |
the block with the proof over an overlay network to |
and is capable of selforganizing in the presence of firewalls |
had learned enough from the early design of Windows NT |
block with the proof over an overlay network to all |
learned enough from the early design of Windows NT to |
with the proof over an overlay network to all other |
enough from the early design of Windows NT to realize |
the proof over an overlay network to all other miners |
from the early design of Windows NT to realize that |
A separate project is creating a protocol suite that we |
the early design of Windows NT to realize that it |
separate project is creating a protocol suite that we call |
early design of Windows NT to realize that it was |
project is creating a protocol suite that we call the |
design of Windows NT to realize that it was a |
is creating a protocol suite that we call the Properties |
of Windows NT to realize that it was a major |
transaction fee paid by the users whose transactions are included |
creating a protocol suite that we call the Properties Framework |
Windows NT to realize that it was a major step |
and an amount of minted Bitcoins that are thus introduced |
NT to realize that it was a major step forward |
an amount of minted Bitcoins that are thus introduced into |
to realize that it was a major step forward in |
amount of minted Bitcoins that are thus introduced into the |
realize that it was a major step forward in operating |
of minted Bitcoins that are thus introduced into the system |
The goal is to offer strong forms of reliability that |
that it was a major step forward in operating system |
goal is to offer strong forms of reliability that can |
it was a major step forward in operating system design |
The work which a miner is required to do is |
is to offer strong forms of reliability that can be |
work which a miner is required to do is to |
to offer strong forms of reliability that can be customized |
It would provide us with a platform on which we |
which a miner is required to do is to repeatedly |
offer strong forms of reliability that can be customized for |
would provide us with a platform on which we could |
a miner is required to do is to repeatedly calculate |
strong forms of reliability that can be customized for special |
provide us with a platform on which we could perform |
miner is required to do is to repeatedly calculate a |
forms of reliability that can be customized for special needs |
us with a platform on which we could perform research |
is required to do is to repeatedly calculate a a |
with a platform on which we could perform research more |
Proceedings of the twentieth ACM symposium on Operating systems principles |
required to do is to repeatedly calculate a a hash |
a platform on which we could perform research more effectively |
speed and scalability are only elements of a broader story |
to do is to repeatedly calculate a a hash function |
platform on which we could perform research more effectively and |
do is to repeatedly calculate a a hash function specifically |
on which we could perform research more effectively and it |
is to repeatedly calculate a a hash function specifically the |
which we could perform research more effectively and it would |
to repeatedly calculate a a hash function specifically the SHA |
Live Objects makes it possible to create applications that mix |
we could perform research more effectively and it would allows |
Objects makes it possible to create applications that mix hosted |
could perform research more effectively and it would allows us |
makes it possible to create applications that mix hosted with |
perform research more effectively and it would allows us to |
it possible to create applications that mix hosted with P |
research more effectively and it would allows us to focus |
Interplay of energy and performance for disk arrays running transaction |
more effectively and it would allows us to focus on |
of energy and performance for disk arrays running transaction processing |
effectively and it would allows us to focus on the |
In Proceedings of the Seventeenth ACM Symposium on Operating Systems |
energy and performance for disk arrays running transaction processing workloads |
and it would allows us to focus on the future |
Proceedings of the Seventeenth ACM Symposium on Operating Systems Principles |
In IEEE International Symposium on Performance Analysis of Systems and |
it would allows us to focus on the future directions |
to achieve desired properties in a way matched to the |
IEEE International Symposium on Performance Analysis of Systems and Software |
would allows us to focus on the future directions without |
achieve desired properties in a way matched to the environment |
The miner places different values in this field and calculates |
allows us to focus on the future directions without having |
miner places different values in this field and calculates the |
us to focus on the future directions without having to |
places different values in this field and calculates the hash |
to focus on the future directions without having to worry |
different values in this field and calculates the hash for |
focus on the future directions without having to worry whether |
values in this field and calculates the hash for each |
on the future directions without having to worry whether the |
in this field and calculates the hash for each value |
the future directions without having to worry whether the operating |
Prior Work The idea of integrating web services with peer |
future directions without having to worry whether the operating system |
If the result of the hash is smaller than a |
directions without having to worry whether the operating system was |
the result of the hash is smaller than a target |
without having to worry whether the operating system was capable |
result of the hash is smaller than a target value |
having to worry whether the operating system was capable of |
to worry whether the operating system was capable of supporting |
worry whether the operating system was capable of supporting innovation |
The number of attempts to find a single hash is |
By now our complete educational operation and the majority of |
number of attempts to find a single hash is therefore |
now our complete educational operation and the majority of our |
of attempts to find a single hash is therefore random |
our complete educational operation and the majority of our research |
attempts to find a single hash is therefore random with |
complete educational operation and the majority of our research projects |
to find a single hash is therefore random with a |
educational operation and the majority of our research projects have |
find a single hash is therefore random with a geometric |
operation and the majority of our research projects have switched |
a single hash is therefore random with a geometric distribution |
and the majority of our research projects have switched to |
the majority of our research projects have switched to using |
as each attempt is a Bernoulli trial with a success |
majority of our research projects have switched to using Windows |
each attempt is a Bernoulli trial with a success probability |
of our research projects have switched to using Windows NT |
attempt is a Bernoulli trial with a success probability determined |
is a Bernoulli trial with a success probability determined by |
a Bernoulli trial with a success probability determined by the |
Bernoulli trial with a success probability determined by the target |
trial with a success probability determined by the target value |
At the existing huge hashing rates and small target values |
the time to find a single hash can be approximated |
time to find a single hash can be approximated by |
to find a single hash can be approximated by an |
find a single hash can be approximated by an exponential |
a single hash can be approximated by an exponential distribution |
The average time for a miner to find a solution |
average time for a miner to find a solution is |
time for a miner to find a solution is therefore |
for a miner to find a solution is therefore proportional |
In this article I want to share some of the |
a miner to find a solution is therefore proportional to |
this article I want to share some of the reasoning |
miner to find a solution is therefore proportional to its |
article I want to share some of the reasoning behind |
to find a solution is therefore proportional to its hashing |
I want to share some of the reasoning behind our |
find a solution is therefore proportional to its hashing rate |
want to share some of the reasoning behind our choice |
a solution is therefore proportional to its hashing rate or |
to share some of the reasoning behind our choice for |
solution is therefore proportional to its hashing rate or mining |
share some of the reasoning behind our choice for Windows |
is therefore proportional to its hashing rate or mining power |
some of the reasoning behind our choice for Windows NT |
of the reasoning behind our choice for Windows NT and |
the reasoning behind our choice for Windows NT and to |
reasoning behind our choice for Windows NT and to share |
and as part of its defense against denial of service |
behind our choice for Windows NT and to share some |
as part of its defense against denial of service and |
our choice for Windows NT and to share some our |
part of its defense against denial of service and other |
choice for Windows NT and to share some our experiences |
of its defense against denial of service and other attacks |
for Windows NT and to share some our experiences with |
The first line of research is focused on the use |
Windows NT and to share some our experiences with Windows |
first line of research is focused on the use of |
NT and to share some our experiences with Windows NT |
line of research is focused on the use of peer |
and to share some our experiences with Windows NT as |
the protocol deterministically defines the target value for each block |
to share some our experiences with Windows NT as a |
protocol deterministically defines the target value for each block according |
share some our experiences with Windows NT as a research |
deterministically defines the target value for each block according to |
some our experiences with Windows NT as a research platform |
defines the target value for each block according to the |
The second line of research concentrates on the use of |
the target value for each block according to the time |
second line of research concentrates on the use of replication |
OS research as religion The biggest hurdle in starting research |
target value for each block according to the time required |
line of research concentrates on the use of replication protocols |
research as religion The biggest hurdle in starting research on |
value for each block according to the time required to |
of research concentrates on the use of replication protocols at |
as religion The biggest hurdle in starting research on Windows |
for each block according to the time required to generate |
research concentrates on the use of replication protocols at the |
religion The biggest hurdle in starting research on Windows NT |
each block according to the time required to generate recent |
concentrates on the use of replication protocols at the web |
The biggest hurdle in starting research on Windows NT was |
block according to the time required to generate recent blocks |
on the use of replication protocols at the web service |
biggest hurdle in starting research on Windows NT was not |
the use of replication protocols at the web service backend |
hurdle in starting research on Windows NT was not technical |
use of replication protocols at the web service backend to |
of replication protocols at the web service backend to achieve |
replication protocols at the web service backend to achieve fault |
It was to overcome the skepticism of our colleagues who |
was to overcome the skepticism of our colleagues who were |
to overcome the skepticism of our colleagues who were convinced |
overcome the skepticism of our colleagues who were convinced that |
blocks such that the average time for each block to |
the skepticism of our colleagues who were convinced that it |
such that the average time for each block to be |
P platforms such as JXTA are treated not as means |
skepticism of our colleagues who were convinced that it would |
that the average time for each block to be found |
platforms such as JXTA are treated not as means of |
of our colleagues who were convinced that it would not |
the average time for each block to be found is |
such as JXTA are treated not as means of collaboration |
our colleagues who were convinced that it would not be |
as JXTA are treated not as means of collaboration or |
colleagues who were convinced that it would not be possible |
JXTA are treated not as means of collaboration or media |
who were convinced that it would not be possible to |
are treated not as means of collaboration or media carrying |
were convinced that it would not be possible to use |
treated not as means of collaboration or media carrying live |
convinced that it would not be possible to use Windows |
not as means of collaboration or media carrying live content |
all miners switch to mine for the subsequent block b |
that it would not be possible to use Windows NT |
but rather as a supporting infrastructure at the data center |
In Proceedings of the Fifteenth ACM Symposium on Operating Systems |
it would not be possible to use Windows NT as |
rather as a supporting infrastructure at the data center backend |
Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles |
at t without changing their probability distribution of finding a |
would not be possible to use Windows NT as a |
t without changing their probability distribution of finding a block |
not be possible to use Windows NT as a good |
our work is focused on blending the content available through |
without changing their probability distribution of finding a block after |
be possible to use Windows NT as a good platform |
work is focused on blending the content available through P |
changing their probability distribution of finding a block after t |
possible to use Windows NT as a good platform for |
to use Windows NT as a good platform for research |
the probability that a miner i with mining power mi |
probability that a miner i with mining power mi finds |
that a miner i with mining power mi finds the |
a miner i with mining power mi finds the next |
peer protocols to support live and interactive content have existed |
miner i with mining power mi finds the next block |
protocols to support live and interactive content have existed earlier |
i with mining power mi finds the next block is |
Microsoft would hide the pieces of buggy code from us |
with mining power mi finds the next block is its |
would hide the pieces of buggy code from us or |
mining power mi finds the next block is its ratio |
hide the pieces of buggy code from us or Bill |
power mi finds the next block is its ratio out |
the pieces of buggy code from us or Bill Gates |
mi finds the next block is its ratio out of |
pieces of buggy code from us or Bill Gates would |
finds the next block is its ratio out of the |
of buggy code from us or Bill Gates would personally |
the next block is its ratio out of the total |
buggy code from us or Bill Gates would personally tell |
next block is its ratio out of the total mining |
code from us or Bill Gates would personally tell us |
block is its ratio out of the total mining power |
from us or Bill Gates would personally tell us where |
is its ratio out of the total mining power m |
us or Bill Gates would personally tell us where and |
its ratio out of the total mining power m in |
or Bill Gates would personally tell us where and how |
ratio out of the total mining power m in the |
Bill Gates would personally tell us where and how we |
out of the total mining power m in the system |
Gates would personally tell us where and how we should |
would personally tell us where and how we should do |
personally tell us where and how we should do our |
tell us where and how we should do our research |
and the types of a traditional hosted content they can |
the types of a traditional hosted content they can blend |
The operating systems research community has not remained untouched by |
types of a traditional hosted content they can blend with |
operating systems research community has not remained untouched by the |
of a traditional hosted content they can blend with their |
systems research community has not remained untouched by the market |
a traditional hosted content they can blend with their P |
research community has not remained untouched by the market place |
community has not remained untouched by the market place rivalry |
has not remained untouched by the market place rivalry between |
not remained untouched by the market place rivalry between Microsoft |
remained untouched by the market place rivalry between Microsoft and |
untouched by the market place rivalry between Microsoft and the |
by the market place rivalry between Microsoft and the group |
our platform is designed from ground up with extensibility in |
the market place rivalry between Microsoft and the group lead |
platform is designed from ground up with extensibility in mind |
market place rivalry between Microsoft and the group lead by |
place rivalry between Microsoft and the group lead by Sun |
rivalry between Microsoft and the group lead by Sun Microsystems |
and different components within a single mashup application can leverage |
different components within a single mashup application can leverage different |
It is even more unfortunate that the positions taken are |
components within a single mashup application can leverage different transport |
is even more unfortunate that the positions taken are not |
Mining is only profitable using dedicated hardware in cutting edge |
within a single mashup application can leverage different transport protocols |
even more unfortunate that the positions taken are not based |
is only profitable using dedicated hardware in cutting edge mining |
more unfortunate that the positions taken are not based on |
Prior work on typed component architectures includes a tremendous variety |
only profitable using dedicated hardware in cutting edge mining rigs |
unfortunate that the positions taken are not based on intellectual |
work on typed component architectures includes a tremendous variety of |
that the positions taken are not based on intellectual deliberation |
on typed component architectures includes a tremendous variety of programming |
the positions taken are not based on intellectual deliberation but |
Although expected revenue from mining is proportional to the power |
typed component architectures includes a tremendous variety of programming languages |
positions taken are not based on intellectual deliberation but purely |
expected revenue from mining is proportional to the power of |
component architectures includes a tremendous variety of programming languages and |
taken are not based on intellectual deliberation but purely on |
revenue from mining is proportional to the power of the |
architectures includes a tremendous variety of programming languages and platforms |
are not based on intellectual deliberation but purely on emotional |
from mining is proportional to the power of the mining |
determinism and asynchrony of set iterators to reduce aggregrate file |
not based on intellectual deliberation but purely on emotional grounds |
mining is proportional to the power of the mining rigs |
and asynchrony of set iterators to reduce aggregrate file I |
is proportional to the power of the mining rigs used |
a single home miner using a small rig is unlikely |
and working with them is seen as collaboration with the |
In Proceedings of the Sixteenth ACM Symposium on Operating System |
single home miner using a small rig is unlikely to |
working with them is seen as collaboration with the enemy |
Proceedings of the Sixteenth ACM Symposium on Operating System Principles |
home miner using a small rig is unlikely to mine |
with them is seen as collaboration with the enemy of |
miner using a small rig is unlikely to mine a |
them is seen as collaboration with the enemy of free |
using a small rig is unlikely to mine a block |
is seen as collaboration with the enemy of free academic |
a small rig is unlikely to mine a block for |
seen as collaboration with the enemy of free academic speech |
small rig is unlikely to mine a block for years |
The pros and cons are often discussed with a righteous |
pros and cons are often discussed with a righteous zeal |
and cons are often discussed with a righteous zeal that |
cons are often discussed with a righteous zeal that is |
are often discussed with a righteous zeal that is frightening |
Our own experiences with Microsoft can only be described as |
own experiences with Microsoft can only be described as extremely |
experiences with Microsoft can only be described as extremely positive |
never before have we had such a positive relation with |
a pool is a group of miners that share their |
before have we had such a positive relation with a |
pool is a group of miners that share their revenues |
have we had such a positive relation with a vendor |
is a group of miners that share their revenues when |
a group of miners that share their revenues when one |
group of miners that share their revenues when one of |
Discussion of component integration systems and their relation to live |
We can only conclude that the reasons for the controversy |
of miners that share their revenues when one of them |
of component integration systems and their relation to live objects |
can only conclude that the reasons for the controversy must |
miners that share their revenues when one of them successfully |
only conclude that the reasons for the controversy must be |
that share their revenues when one of them successfully mines |
conclude that the reasons for the controversy must be found |
share their revenues when one of them successfully mines a |
that the reasons for the controversy must be found in |
their revenues when one of them successfully mines a block |
In Proceedings of the Seventeenth ACM Symposium on Operating Systems |
the reasons for the controversy must be found in a |
Proceedings of the Seventeenth ACM Symposium on Operating Systems Principles |
reasons for the controversy must be found in a sort |
the revenue is distributed among the pool members in proportion |
A Scalable Services Architecture Tudor Marian Ken Birman Department of |
for the controversy must be found in a sort of |
revenue is distributed among the pool members in proportion to |
much relevant prior work consists of the scripting languages mentioned |
Scalable Services Architecture Tudor Marian Ken Birman Department of Computer |
the controversy must be found in a sort of traditional |
is distributed among the pool members in proportion to their |
relevant prior work consists of the scripting languages mentioned in |
Services Architecture Tudor Marian Ken Birman Department of Computer Science |
controversy must be found in a sort of traditional emotional |
distributed among the pool members in proportion to their mining |
prior work consists of the scripting languages mentioned in the |
Architecture Tudor Marian Ken Birman Department of Computer Science Cornell |
must be found in a sort of traditional emotional bonding |
among the pool members in proportion to their mining power |
work consists of the scripting languages mentioned in the discussion |
Tudor Marian Ken Birman Department of Computer Science Cornell University |
be found in a sort of traditional emotional bonding of |
consists of the scripting languages mentioned in the discussion above |
found in a sort of traditional emotional bonding of academia |
The expected revenue of a pool member is therefore the |
in a sort of traditional emotional bonding of academia with |
expected revenue of a pool member is therefore the same |
a sort of traditional emotional bonding of academia with the |
revenue of a pool member is therefore the same as |
sort of traditional emotional bonding of academia with the underdog |
of a pool member is therefore the same as its |
of traditional emotional bonding of academia with the underdog and |
a pool member is therefore the same as its revenue |
traditional emotional bonding of academia with the underdog and that |
our belief is that even though these languages are intended |
pool member is therefore the same as its revenue had |
emotional bonding of academia with the underdog and that no |
belief is that even though these languages are intended for |
member is therefore the same as its revenue had it |
bonding of academia with the underdog and that no real |
is that even though these languages are intended for fairly |
is therefore the same as its revenue had it mined |
of academia with the underdog and that no real experiences |
that even though these languages are intended for fairly general |
therefore the same as its revenue had it mined solo |
academia with the underdog and that no real experiences drive |
even though these languages are intended for fairly general use |
edu Abstract Data centers constructed as clusters of inexpensive machines |
with the underdog and that no real experiences drive the |
Abstract Data centers constructed as clusters of inexpensive machines have |
they have evolved to focus on minibrowser situations in which |
the underdog and that no real experiences drive the discussion |
Data centers constructed as clusters of inexpensive machines have compelling |
have evolved to focus on minibrowser situations in which the |
centers constructed as clusters of inexpensive machines have compelling cost |
evolved to focus on minibrowser situations in which the application |
Gaining knowledge The foremost reasons why Unix was such a |
to focus on minibrowser situations in which the application lives |
knowledge The foremost reasons why Unix was such a powerhouse |
focus on minibrowser situations in which the application lives within |
but developing services to run on them can be challenging |
The foremost reasons why Unix was such a powerhouse in |
Miners register with the pool manager and mine on its |
on minibrowser situations in which the application lives within a |
foremost reasons why Unix was such a powerhouse in operating |
register with the pool manager and mine on its behalf |
minibrowser situations in which the application lives within a dedicated |
reasons why Unix was such a powerhouse in operating system |
situations in which the application lives within a dedicated browser |
The pool manager generates tasks and the miners search for |
why Unix was such a powerhouse in operating system research |
in which the application lives within a dedicated browser frame |
pool manager generates tasks and the miners search for solutions |
Unix was such a powerhouse in operating system research was |
manager generates tasks and the miners search for solutions based |
was such a powerhouse in operating system research was the |
A primary goal was to keep the SSA as small |
and cannot be mixed with content from other sources in |
generates tasks and the miners search for solutions based on |
such a powerhouse in operating system research was the great |
primary goal was to keep the SSA as small and |
cannot be mixed with content from other sources in a |
tasks and the miners search for solutions based on these |
a powerhouse in operating system research was the great amount |
goal was to keep the SSA as small and simple |
be mixed with content from other sources in a layered |
and the miners search for solutions based on these tasks |
powerhouse in operating system research was the great amount of |
was to keep the SSA as small and simple as |
mixed with content from other sources in a layered fashion |
the miners search for solutions based on these tasks that |
in operating system research was the great amount of knowledge |
to keep the SSA as small and simple as possible |
miners search for solutions based on these tasks that can |
operating system research was the great amount of knowledge accumulated |
but we ve argued that by modeling hosted content at |
search for solutions based on these tasks that can serve |
system research was the great amount of knowledge accumulated over |
we ve argued that by modeling hosted content at a |
for solutions based on these tasks that can serve as |
based subsystem for managing configuration data and repairing inconsistencies after |
research was the great amount of knowledge accumulated over the |
ve argued that by modeling hosted content at a lower |
solutions based on these tasks that can serve as proof |
subsystem for managing configuration data and repairing inconsistencies after faults |
was the great amount of knowledge accumulated over the years |
argued that by modeling hosted content at a lower level |
based on these tasks that can serve as proof of |
the great amount of knowledge accumulated over the years about |
that by modeling hosted content at a lower level as |
on these tasks that can serve as proof of work |
great amount of knowledge accumulated over the years about the |
by modeling hosted content at a lower level as components |
Introduction Large computing systems are often structured as Service Oriented |
amount of knowledge accumulated over the years about the internal |
modeling hosted content at a lower level as components that |
Large computing systems are often structured as Service Oriented Architectures |
of knowledge accumulated over the years about the internal operation |
hosted content at a lower level as components that interact |
The pool manager behaves as a single miner in the |
knowledge accumulated over the years about the internal operation of |
content at a lower level as components that interact via |
pool manager behaves as a single miner in the Bitcoin |
accumulated over the years about the internal operation of the |
at a lower level as components that interact via events |
manager behaves as a single miner in the Bitcoin system |
over the years about the internal operation of the operating |
a lower level as components that interact via events and |
Once it obtains a legitimate block from one of its |
the years about the internal operation of the operating system |
lower level as components that interact via events and focusing |
it obtains a legitimate block from one of its miners |
level as components that interact via events and focusing on |
Many of us had become gurus about some part of |
as components that interact via events and focusing on the |
and handles its own quality of service or availability guarantees |
of us had become gurus about some part of the |
components that interact via events and focusing on the multi |
for example by arranging to be restarted after a failure |
us had become gurus about some part of the OS |
The block transfers the revenue to the control of the |
While many services need to maintain availability in the face |
layered style of mashups as opposed to the standard tiled |
had become gurus about some part of the OS kernel |
block transfers the revenue to the control of the pool |
many services need to maintain availability in the face of |
style of mashups as opposed to the standard tiled model |
become gurus about some part of the OS kernel and |
transfers the revenue to the control of the pool manager |
services need to maintain availability in the face of challenging |
gurus about some part of the OS kernel and could |
The pool manager then distributes the revenue among the miners |
need to maintain availability in the face of challenging operating |
about some part of the OS kernel and could recite |
pool manager then distributes the revenue among the miners according |
to maintain availability in the face of challenging operating conditions |
some part of the OS kernel and could recite the |
manager then distributes the revenue among the miners according to |
part of the OS kernel and could recite the fields |
then distributes the revenue among the miners according to their |
of the OS kernel and could recite the fields of |
These include hosted sources that run in data centers and |
distributes the revenue among the miners according to their mining |
the OS kernel and could recite the fields of an |
include hosted sources that run in data centers and support |
the revenue among the miners according to their mining power |
OS kernel and could recite the fields of an I |
hosted sources that run in data centers and support web |
balancing and restart mechanisms for transactional services implemented using a |
sources that run in data centers and support web services |
and restart mechanisms for transactional services implemented using a three |
node structure at late night meetings or discuss which data |
In order to estimate the mining power of a miner |
that run in data centers and support web services interfaces |
structure at late night meetings or discuss which data structures |
the pool manager sets a partial target for each member |
at late night meetings or discuss which data structures to |
Developers of nontransactional web services must implement their own mechanisms |
late night meetings or discuss which data structures to modify |
of nontransactional web services must implement their own mechanisms for |
night meetings or discuss which data structures to modify to |
nontransactional web services must implement their own mechanisms for replicating |
meetings or discuss which data structures to modify to add |
web services must implement their own mechanisms for replicating data |
or discuss which data structures to modify to add a |
discuss which data structures to modify to add a new |
tracking membership and live This work was supported by DARPA |
which data structures to modify to add a new protocol |
Each miner is required to send the pool manager blocks |
IPTO under the SRS program and by the Rome Air |
data structures to modify to add a new protocol at |
miner is required to send the pool manager blocks that |
under the SRS program and by the Rome Air Force |
Our review of the performance of enterprise service bus eventing |
structures to modify to add a new protocol at runtime |
is required to send the pool manager blocks that are |
the SRS program and by the Rome Air Force Research |
review of the performance of enterprise service bus eventing solutions |
to modify to add a new protocol at runtime over |
required to send the pool manager blocks that are correct |
SRS program and by the Rome Air Force Research Laboratory |
of the performance of enterprise service bus eventing solutions in |
modify to add a new protocol at runtime over an |
to send the pool manager blocks that are correct according |
the performance of enterprise service bus eventing solutions in the |
to add a new protocol at runtime over an early |
send the pool manager blocks that are correct according to |
performance of enterprise service bus eventing solutions in the standard |
add a new protocol at runtime over an early morning |
the pool manager blocks that are correct according to the |
of enterprise service bus eventing solutions in the standard hosted |
a new protocol at runtime over an early morning cappuccino |
pool manager blocks that are correct according to the partial |
enterprise service bus eventing solutions in the standard hosted web |
manager blocks that are correct according to the partial target |
service bus eventing solutions in the standard hosted web services |
Many of us were and still are afraid to leave |
bus eventing solutions in the standard hosted web services model |
of us were and still are afraid to leave this |
Our premise in this paper is that for many services |
eventing solutions in the standard hosted web services model made |
such that partial solutions arrive frequently enough for the manager |
us were and still are afraid to leave this bastion |
solutions in the standard hosted web services model made it |
the transactional model is a poor fit and hence that |
that partial solutions arrive frequently enough for the manager to |
were and still are afraid to leave this bastion of |
in the standard hosted web services model made it clear |
transactional model is a poor fit and hence that tools |
partial solutions arrive frequently enough for the manager to accurately |
and still are afraid to leave this bastion of safety |
the standard hosted web services model made it clear that |
model is a poor fit and hence that tools aimed |
solutions arrive frequently enough for the manager to accurately estimate |
still are afraid to leave this bastion of safety behind |
standard hosted web services model made it clear that hosted |
is a poor fit and hence that tools aimed at |
arrive frequently enough for the manager to accurately estimate the |
are afraid to leave this bastion of safety behind and |
hosted web services model made it clear that hosted event |
a poor fit and hence that tools aimed at non |
frequently enough for the manager to accurately estimate the power |
afraid to leave this bastion of safety behind and trade |
web services model made it clear that hosted event channels |
enough for the manager to accurately estimate the power of |
to leave this bastion of safety behind and trade it |
services model made it clear that hosted event channels won |
for the manager to accurately estimate the power of the |
leave this bastion of safety behind and trade it in |
Vendors have generally argued that only transactional systems offer the |
model made it clear that hosted event channels won t |
the manager to accurately estimate the power of the miner |
this bastion of safety behind and trade it in for |
have generally argued that only transactional systems offer the hooks |
made it clear that hosted event channels won t have |
bastion of safety behind and trade it in for working |
generally argued that only transactional systems offer the hooks needed |
it clear that hosted event channels won t have the |
of safety behind and trade it in for working on |
argued that only transactional systems offer the hooks needed to |
clear that hosted event channels won t have the scalability |
safety behind and trade it in for working on an |
that only transactional systems offer the hooks needed to support |
that hosted event channels won t have the scalability and |
behind and trade it in for working on an operating |
only transactional systems offer the hooks needed to support automated |
hosted event channels won t have the scalability and latency |
and trade it in for working on an operating system |
transactional systems offer the hooks needed to support automated scalability |
event channels won t have the scalability and latency properties |
trade it in for working on an operating system that |
channels won t have the scalability and latency properties needed |
This is a simplification that is sufficient for our analysis |
it in for working on an operating system that at |
won t have the scalability and latency properties needed by |
in for working on an operating system that at first |
Key to this argument is the ease with which interrupted |
t have the scalability and latency properties needed by many |
for working on an operating system that at first sight |
to this argument is the ease with which interrupted transactions |
have the scalability and latency properties needed by many applications |
working on an operating system that at first sight had |
this argument is the ease with which interrupted transactions can |
on an operating system that at first sight had nothing |
argument is the ease with which interrupted transactions can be |
an operating system that at first sight had nothing in |
is the ease with which interrupted transactions can be rolled |
operating system that at first sight had nothing in common |
the ease with which interrupted transactions can be rolled back |
system that at first sight had nothing in common with |
that at first sight had nothing in common with our |
at first sight had nothing in common with our beloved |
the data center doesn t get a chance to see |
first sight had nothing in common with our beloved Unix |
and the relative simplicity of cleaning up a database after |
the relative simplicity of cleaning up a database after a |
The Live Objects platform can seamlessly support applications that require |
therefore it is possible for two distant miners to generate |
relative simplicity of cleaning up a database after a crash |
Live Objects platform can seamlessly support applications that require a |
it is possible for two distant miners to generate competing |
Objects platform can seamlessly support applications that require a mixture |
Yet the transactional programming model also brings constraints and overheads |
is possible for two distant miners to generate competing blocks |
wouldn t be of much help any more either It |
platform can seamlessly support applications that require a mixture of |
both of which name the same block as their predecessor |
t be of much help any more either It took |
can seamlessly support applications that require a mixture of data |
be of much help any more either It took more |
Some of these constraints relate to the challenges of maintaining |
seamlessly support applications that require a mixture of data sources |
of much help any more either It took more then |
of these constraints relate to the challenges of maintaining a |
much help any more either It took more then a |
these constraints relate to the challenges of maintaining a clean |
help any more either It took more then a year |
constraints relate to the challenges of maintaining a clean separation |
any more either It took more then a year of |
relate to the challenges of maintaining a clean separation of |
more either It took more then a year of immersion |
to the challenges of maintaining a clean separation of code |
either It took more then a year of immersion in |
drop programming style that yields applications represented as XML files |
the challenges of maintaining a clean separation of code and |
It took more then a year of immersion in the |
challenges of maintaining a clean separation of code and data |
which can be shared as files or even via email |
took more then a year of immersion in the technology |
The system has a mechanism to solve forks when they |
Users that open such files find themselves immersed in a |
more then a year of immersion in the technology to |
system has a mechanism to solve forks when they do |
that open such files find themselves immersed in a mediarich |
then a year of immersion in the technology to get |
and restarting a database after a crash incurs delays while |
has a mechanism to solve forks when they do occur |
open such files find themselves immersed in a mediarich collaborative |
a year of immersion in the technology to get a |
restarting a database after a crash incurs delays while cleanup |
such files find themselves immersed in a mediarich collaborative environment |
year of immersion in the technology to get a level |
a database after a crash incurs delays while cleanup code |
files find themselves immersed in a mediarich collaborative environment that |
Since the choice of the discarded block on bifurcation is |
of immersion in the technology to get a level where |
database after a crash incurs delays while cleanup code runs |
find themselves immersed in a mediarich collaborative environment that also |
the choice of the discarded block on bifurcation is random |
immersion in the technology to get a level where I |
themselves immersed in a mediarich collaborative environment that also offers |
one may incorporate this event into the probability of finding |
in the technology to get a level where I felt |
High availability is difficult to acheive in the transactional model |
immersed in a mediarich collaborative environment that also offers strong |
may incorporate this event into the probability of finding a |
the technology to get a level where I felt confident |
in a mediarich collaborative environment that also offers strong reliability |
incorporate this event into the probability of finding a block |
technology to get a level where I felt confident again |
suffer from failure scenarios that can require intervention by a |
and consider instead the probability of finding a block that |
to get a level where I felt confident again to |
from failure scenarios that can require intervention by a human |
consider instead the probability of finding a block that is |
get a level where I felt confident again to direct |
failure scenarios that can require intervention by a human operator |
instead the probability of finding a block that is not |
a level where I felt confident again to direct others |
the probability of finding a block that is not discarded |
level where I felt confident again to direct others in |
phase commit protocols and hence may not give adequate performance |
where I felt confident again to direct others in our |
Pools often charge a small percentage of the revenue as |
I felt confident again to direct others in our research |
often charge a small percentage of the revenue as fee |
felt confident again to direct others in our research group |
but they negotiate these potential pitfalls in ways that preclude |
We discuss in Section IX the implications of such fees |
they negotiate these potential pitfalls in ways that preclude important |
discuss in Section IX the implications of such fees to |
Together with the overall organizational issues I think we lost |
negotiate these potential pitfalls in ways that preclude important classes |
in Section IX the implications of such fees to our |
with the overall organizational issues I think we lost one |
these potential pitfalls in ways that preclude important classes of |
Section IX the implications of such fees to our analysis |
the overall organizational issues I think we lost one and |
potential pitfalls in ways that preclude important classes of applications |
overall organizational issues I think we lost one and a |
organizational issues I think we lost one and a half |
A pool interface is typically comprised of a web interface |
Our motivation is to show that a simple and remarkably |
issues I think we lost one and a half year |
pool interface is typically comprised of a web interface for |
motivation is to show that a simple and remarkably inexpensive |
I think we lost one and a half year worth |
interface is typically comprised of a web interface for registration |
is to show that a simple and remarkably inexpensive infrastructure |
think we lost one and a half year worth of |
is typically comprised of a web interface for registration and |
to show that a simple and remarkably inexpensive infrastructure can |
we lost one and a half year worth of research |
typically comprised of a web interface for registration and a |
show that a simple and remarkably inexpensive infrastructure can support |
lost one and a half year worth of research time |
comprised of a web interface for registration and a miner |
that a simple and remarkably inexpensive infrastructure can support clustered |
one and a half year worth of research time to |
of a web interface for registration and a miner interface |
a simple and remarkably inexpensive infrastructure can support clustered execution |
and a half year worth of research time to make |
The one issue that unites almost all approaches to distributed |
a web interface for registration and a miner interface for |
simple and remarkably inexpensive infrastructure can support clustered execution of |
a half year worth of research time to make the |
one issue that unites almost all approaches to distributed computing |
web interface for registration and a miner interface for the |
and remarkably inexpensive infrastructure can support clustered execution of a |
half year worth of research time to make the switch |
issue that unites almost all approaches to distributed computing is |
interface for registration and a miner interface for the mining |
remarkably inexpensive infrastructure can support clustered execution of a significant |
year worth of research time to make the switch in |
that unites almost all approaches to distributed computing is the |
for registration and a miner interface for the mining software |
inexpensive infrastructure can support clustered execution of a significant class |
worth of research time to make the switch in the |
unites almost all approaches to distributed computing is the need |
infrastructure can support clustered execution of a significant class of |
of research time to make the switch in the most |
almost all approaches to distributed computing is the need to |
can support clustered execution of a significant class of non |
research time to make the switch in the most fundamental |
supplies a Bitcoin address to receive its future shares of |
all approaches to distributed computing is the need to know |
time to make the switch in the most fundamental way |
The work reported here focuses on services that don t |
approaches to distributed computing is the need to know whether |
a Bitcoin address to receive its future shares of the |
work reported here focuses on services that don t fit |
to distributed computing is the need to know whether certain |
Others are making the switch more gradually and are experiencing |
Bitcoin address to receive its future shares of the revenue |
reported here focuses on services that don t fit the |
distributed computing is the need to know whether certain components |
are making the switch more gradually and are experiencing a |
here focuses on services that don t fit the transactional |
computing is the need to know whether certain components in |
making the switch more gradually and are experiencing a more |
Then he feeds his credentials and the pool s address |
focuses on services that don t fit the transactional paradigm |
is the need to know whether certain components in the |
the switch more gradually and are experiencing a more smooth |
he feeds his credentials and the pool s address to |
the need to know whether certain components in the system |
switch more gradually and are experiencing a more smooth transition |
feeds his credentials and the pool s address to its |
need to know whether certain components in the system have |
his credentials and the pool s address to its mining |
All Operation Systems are created equal Our experiences with switching |
to know whether certain components in the system have failed |
we assume that these services are capable of handling outof |
Operation Systems are created equal Our experiences with switching to |
credentials and the pool s address to its mining rig |
know whether certain components in the system have failed or |
Systems are created equal Our experiences with switching to Windows |
whether certain components in the system have failed or are |
are created equal Our experiences with switching to Windows NT |
certain components in the system have failed or are otherwise |
The mining rig obtains its tasks from the pool and |
created equal Our experiences with switching to Windows NT have |
our assumptions hold for a very large group of applications |
components in the system have failed or are otherwise unavailable |
mining rig obtains its tasks from the pool and sends |
equal Our experiences with switching to Windows NT have made |
rig obtains its tasks from the pool and sends partial |
When designing and building systems that need to function at |
communication protocols in conjunction with a novel variant of the |
obtains its tasks from the pool and sends partial and |
Our experiences with switching to Windows NT have made us |
designing and building systems that need to function at a |
protocols in conjunction with a novel variant of the chain |
its tasks from the pool and sends partial and full |
experiences with switching to Windows NT have made us somewhat |
and building systems that need to function at a global |
in conjunction with a novel variant of the chain replication |
tasks from the pool and sends partial and full proof |
with switching to Windows NT have made us somewhat more |
building systems that need to function at a global scale |
conjunction with a novel variant of the chain replication scheme |
from the pool and sends partial and full proof of |
failure management needs to be considered a fundamental building block |
with a novel variant of the chain replication scheme which |
switching to Windows NT have made us somewhat more philosophical |
the pool and sends partial and full proof of work |
a novel variant of the chain replication scheme which has |
to Windows NT have made us somewhat more philosophical about |
novel variant of the chain replication scheme which has evolved |
which allows systems and applications to incorporate accurate detection of |
Windows NT have made us somewhat more philosophical about the |
variant of the chain replication scheme which has evolved from |
allows systems and applications to incorporate accurate detection of failed |
NT have made us somewhat more philosophical about the nature |
of the chain replication scheme which has evolved from the |
systems and applications to incorporate accurate detection of failed processes |
have made us somewhat more philosophical about the nature of |
the chain replication scheme which has evolved from the mechanism |
the pool manager credits the miner s account according to |
made us somewhat more philosophical about the nature of operation |
chain replication scheme which has evolved from the mechanism first |
without the need for making compromises in their particular design |
pool manager credits the miner s account according to its |
us somewhat more philosophical about the nature of operation systems |
replication scheme which has evolved from the mechanism first proposed |
manager credits the miner s account according to its share |
scheme which has evolved from the mechanism first proposed in |
credits the miner s account according to its share of |
the miner s account according to its share of the |
miner s account according to its share of the work |
it is becoming clear that the systems that are used |
is becoming clear that the systems that are used today |
becoming clear that the systems that are used today in |
and transfers these funds either on request or automatically to |
The functionality of the Windows NT kernel is just as |
clear that the systems that are used today in local |
transfers these funds either on request or automatically to the |
functionality of the Windows NT kernel is just as all |
these funds either on request or automatically to the aforementioned |
of the Windows NT kernel is just as all other |
stabilizing after disruptions Analytically appealing This paper reports on the |
funds either on request or automatically to the aforementioned Bitcoin |
can not simply be employed in their existing form or |
the Windows NT kernel is just as all other kernels |
after disruptions Analytically appealing This paper reports on the architecture |
either on request or automatically to the aforementioned Bitcoin address |
not simply be employed in their existing form or trivially |
disruptions Analytically appealing This paper reports on the architecture and |
simply be employed in their existing form or trivially converted |
Too Big Pools Despite their important role of enabling small |
Analytically appealing This paper reports on the architecture and performance |
be employed in their existing form or trivially converted for |
appealing This paper reports on the architecture and performance of |
employed in their existing form or trivially converted for wide |
This paper reports on the architecture and performance of the |
pools can constitute a threat to the Bitcoin system if |
paper reports on the architecture and performance of the platform |
can constitute a threat to the Bitcoin system if their |
shared memory and messages are used to allow sharing of |
constitute a threat to the Bitcoin system if their size |
memory and messages are used to allow sharing of resources |
a threat to the Bitcoin system if their size is |
threat to the Bitcoin system if their size is too |
the experiments are designed to help us fully understand the |
to the Bitcoin system if their size is too large |
What we often call operating systems has nothing to do |
experiments are designed to help us fully understand the fundamental |
view or virtual synchronous groups or agents employing lazy consistency |
we often call operating systems has nothing to do with |
are designed to help us fully understand the fundamental properties |
or virtual synchronous groups or agents employing lazy consistency schemes |
often call operating systems has nothing to do with the |
designed to help us fully understand the fundamental properties of |
call operating systems has nothing to do with the real |
one of the key problems that needs to be addressed |
to help us fully understand the fundamental properties of a |
operating systems has nothing to do with the real core |
help us fully understand the fundamental properties of a single |
is that of the detection and handling of faulty components |
systems has nothing to do with the real core of |
us fully understand the fundamental properties of a single partitioned |
has nothing to do with the real core of the |
fully understand the fundamental properties of a single partitioned replicated |
nothing to do with the real core of the system |
Building distributed systems and applications today is done using a |
understand the fundamental properties of a single partitioned replicated service |
distributed systems and applications today is done using a variety |
Unix for most of us is a collection of shell |
the fundamental properties of a single partitioned replicated service and |
systems and applications today is done using a variety of |
for most of us is a collection of shell commands |
warns that the system is unstable with even smaller pools |
fundamental properties of a single partitioned replicated service and thus |
and applications today is done using a variety of systems |
most of us is a collection of shell commands and |
properties of a single partitioned replicated service and thus gain |
applications today is done using a variety of systems ranging |
of us is a collection of shell commands and development |
of a single partitioned replicated service and thus gain a |
in realistic scenarios of the Bitcoin system no pool controls |
today is done using a variety of systems ranging from |
us is a collection of shell commands and development libraries |
a single partitioned replicated service and thus gain a firm |
realistic scenarios of the Bitcoin system no pool controls a |
is done using a variety of systems ranging from the |
single partitioned replicated service and thus gain a firm grasp |
scenarios of the Bitcoin system no pool controls a majority |
done using a variety of systems ranging from the bare |
partitioned replicated service and thus gain a firm grasp on |
of the Bitcoin system no pool controls a majority of |
using a variety of systems ranging from the bare bone |
replicated service and thus gain a firm grasp on the |
the Bitcoin system no pool controls a majority of the |
a variety of systems ranging from the bare bone protocols |
service and thus gain a firm grasp on the behavior |
Bitcoin system no pool controls a majority of the mining |
variety of systems ranging from the bare bone protocols interfaces |
and thus gain a firm grasp on the behavior of |
system no pool controls a majority of the mining power |
of systems ranging from the bare bone protocols interfaces like |
thus gain a firm grasp on the behavior of the |
both show that you can give users and developers a |
systems ranging from the bare bone protocols interfaces like BSD |
gain a firm grasp on the behavior of the SSA |
ranging from the bare bone protocols interfaces like BSD sockets |
a firm grasp on the behavior of the SSA s |
from the bare bone protocols interfaces like BSD sockets and |
firm grasp on the behavior of the SSA s building |
the bare bone protocols interfaces like BSD sockets and the |
grasp on the behavior of the SSA s building blocks |
bare bone protocols interfaces like BSD sockets and the TDI |
We defer for future work the full scale evaluation of |
to RPC based systems such as DCE and to more |
defer for future work the full scale evaluation of multiple |
RPC based systems such as DCE and to more advanced |
for future work the full scale evaluation of multiple services |
Windows NT for most of us is the Windows Explorer |
based systems such as DCE and to more advanced distributed |
future work the full scale evaluation of multiple services deployed |
NT for most of us is the Windows Explorer and |
systems such as DCE and to more advanced distributed support |
work the full scale evaluation of multiple services deployed and |
for most of us is the Windows Explorer and point |
such as DCE and to more advanced distributed support systems |
the full scale evaluation of multiple services deployed and running |
IO reduced its relative mining power and publicly committed to |
as DCE and to more advanced distributed support systems such |
full scale evaluation of multiple services deployed and running at |
reduced its relative mining power and publicly committed to stay |
DCE and to more advanced distributed support systems such as |
scale evaluation of multiple services deployed and running at the |
its relative mining power and publicly committed to stay away |
and to more advanced distributed support systems such as Isis |
evaluation of multiple services deployed and running at the same |
relative mining power and publicly committed to stay away from |
of multiple services deployed and running at the same time |
mining power and publicly committed to stay away from the |
The SSA currently runs on a tightly coupled cluster of |
SSA currently runs on a tightly coupled cluster of blade |
currently runs on a tightly coupled cluster of blade servers |
used in the port of Internet Explorer show that you |
We show that developers can tune parameters to trade overhead |
in the port of Internet Explorer show that you do |
show that developers can tune parameters to trade overhead for |
the port of Internet Explorer show that you do not |
that developers can tune parameters to trade overhead for speed |
port of Internet Explorer show that you do not need |
developers can tune parameters to trade overhead for speed of |
of Internet Explorer show that you do not need a |
is an attack performed by a pool member against the |
can tune parameters to trade overhead for speed of repair |
Internet Explorer show that you do not need a Windows |
an attack performed by a pool member against the other |
tune parameters to trade overhead for speed of repair and |
Explorer show that you do not need a Windows NT |
attack performed by a pool member against the other pool |
parameters to trade overhead for speed of repair and we |
show that you do not need a Windows NT kernel |
performed by a pool member against the other pool members |
After years of experience with building these systems and applications |
to trade overhead for speed of repair and we believe |
that you do not need a Windows NT kernel to |
The attacking miner registers with the pool and apparently starts |
trade overhead for speed of repair and we believe that |
it is clear that failure management is not just a |
you do not need a Windows NT kernel to get |
attacking miner registers with the pool and apparently starts mining |
overhead for speed of repair and we believe that our |
is clear that failure management is not just a essential |
do not need a Windows NT kernel to get to |
miner registers with the pool and apparently starts mining honestly |
for speed of repair and we believe that our results |
clear that failure management is not just a essential tool |
not need a Windows NT kernel to get to the |
registers with the pool and apparently starts mining honestly it |
speed of repair and we believe that our results validate |
that failure management is not just a essential tool for |
need a Windows NT kernel to get to the same |
with the pool and apparently starts mining honestly it regularly |
of repair and we believe that our results validate the |
failure management is not just a essential tool for group |
a Windows NT kernel to get to the same user |
the pool and apparently starts mining honestly it regularly sends |
repair and we believe that our results validate the approach |
management is not just a essential tool for group oriented |
Windows NT kernel to get to the same user experience |
pool and apparently starts mining honestly it regularly sends the |
is not just a essential tool for group oriented systems |
Application model Our work focuses on datacenters supporting one or |
and apparently starts mining honestly it regularly sends the pool |
model Our work focuses on datacenters supporting one or more |
apparently starts mining honestly it regularly sends the pool partial |
Our work focuses on datacenters supporting one or more services |
but that it is a fundamental service that should be |
starts mining honestly it regularly sends the pool partial proof |
programming interface as the native programming model for Windows NT |
work focuses on datacenters supporting one or more services deployed |
that it is a fundamental service that should be placed |
mining honestly it regularly sends the pool partial proof of |
focuses on datacenters supporting one or more services deployed within |
and although most Windows applications are designed using this interface |
it is a fundamental service that should be placed among |
honestly it regularly sends the pool partial proof of work |
on datacenters supporting one or more services deployed within a |
is a fundamental service that should be placed among such |
datacenters supporting one or more services deployed within a cluster |
a fundamental service that should be placed among such established |
supporting one or more services deployed within a cluster of |
and you would have a hard time finding the complete |
fundamental service that should be placed among such established basic |
one or more services deployed within a cluster of compute |
If it finds a full solution that constitutes a full |
you would have a hard time finding the complete documentation |
service that should be placed among such established basic services |
or more services deployed within a cluster of compute nodes |
it finds a full solution that constitutes a full proof |
would have a hard time finding the complete documentation for |
that should be placed among such established basic services as |
finds a full solution that constitutes a full proof of |
have a hard time finding the complete documentation for all |
should be placed among such established basic services as naming |
a full solution that constitutes a full proof of work |
a hard time finding the complete documentation for all the |
full solution that constitutes a full proof of work it |
hard time finding the complete documentation for all the system |
solution that constitutes a full proof of work it discards |
time finding the complete documentation for all the system calls |
that constitutes a full proof of work it discards the |
This paper reports on an ongoing research effort to abstract |
constitutes a full proof of work it discards the solution |
paper reports on an ongoing research effort to abstract the |
reports on an ongoing research effort to abstract the failure |
on an ongoing research effort to abstract the failure handling |
an ongoing research effort to abstract the failure handling strategies |
ongoing research effort to abstract the failure handling strategies from |
research effort to abstract the failure handling strategies from a |
but it is does describe the abstraction correctly in which |
effort to abstract the failure handling strategies from a variety |
The attacker does not change the pool s effective mining |
it is does describe the abstraction correctly in which the |
end services might be partitioned into subservices for scalability using |
to abstract the failure handling strategies from a variety of |
attacker does not change the pool s effective mining power |
is does describe the abstraction correctly in which the kernel |
services might be partitioned into subservices for scalability using some |
abstract the failure handling strategies from a variety of popular |
does describe the abstraction correctly in which the kernel provides |
and does not affect directly the revenue of other pools |
might be partitioned into subservices for scalability using some key |
the failure handling strategies from a variety of popular distributed |
describe the abstraction correctly in which the kernel provides base |
failure handling strategies from a variety of popular distributed systems |
the abstraction correctly in which the kernel provides base services |
handling strategies from a variety of popular distributed systems and |
abstraction correctly in which the kernel provides base services and |
strategies from a variety of popular distributed systems and to |
correctly in which the kernel provides base services and the |
from a variety of popular distributed systems and to develop |
Recall that the proof of work is only valid for |
in which the kernel provides base services and the specific |
a variety of popular distributed systems and to develop a |
that the proof of work is only valid for a |
Jim Gray and others have suggested that such a system |
which the kernel provides base services and the specific application |
variety of popular distributed systems and to develop a basic |
the proof of work is only valid for a specific |
Gray and others have suggested that such a system be |
the kernel provides base services and the specific application context |
of popular distributed systems and to develop a basic failure |
proof of work is only valid for a specific block |
and others have suggested that such a system be termed |
kernel provides base services and the specific application context is |
popular distributed systems and to develop a basic failure management |
others have suggested that such a system be termed a |
as it is the nonce with which the block s |
provides base services and the specific application context is provided |
distributed systems and to develop a basic failure management service |
have suggested that such a system be termed a farm |
it is the nonce with which the block s hash |
base services and the specific application context is provided through |
systems and to develop a basic failure management service that |
suggested that such a system be termed a farm consisting |
is the nonce with which the block s hash is |
services and the specific application context is provided through subsystem |
and to develop a basic failure management service that can |
that such a system be termed a farm consisting of |
the nonce with which the block s hash is smaller |
and the specific application context is provided through subsystem servers |
to develop a basic failure management service that can be |
such a system be termed a farm consisting of RAPS |
nonce with which the block s hash is smaller than |
the specific application context is provided through subsystem servers or |
develop a basic failure management service that can be used |
with which the block s hash is smaller than its |
specific application context is provided through subsystem servers or personalities |
a basic failure management service that can be used by |
which the block s hash is smaller than its target |
basic failure management service that can be used by any |
failure management service that can be used by any distributed |
management service that can be used by any distributed system |
service that can be used by any distributed system regardless |
is one of the personalities running on top of Windows |
that can be used by any distributed system regardless of |
one of the personalities running on top of Windows NT |
can be used by any distributed system regardless of the |
note that the block is discarded and never introduced into |
be used by any distributed system regardless of the purpose |
that the block is discarded and never introduced into the |
used by any distributed system regardless of the purpose of |
this structure has arisen mostly in very large datacenters and |
the block is discarded and never introduced into the system |
by any distributed system regardless of the purpose of that |
structure has arisen mostly in very large datacenters and is |
block is discarded and never introduced into the system as |
One can run Windows NT without these standard personalities and |
any distributed system regardless of the purpose of that system |
has arisen mostly in very large datacenters and is supported |
is discarded and never introduced into the system as the |
can run Windows NT without these standard personalities and build |
distributed system regardless of the purpose of that system or |
arisen mostly in very large datacenters and is supported primarily |
discarded and never introduced into the system as the name |
run Windows NT without these standard personalities and build your |
system regardless of the purpose of that system or the |
mostly in very large datacenters and is supported primarily in |
and never introduced into the system as the name block |
Windows NT without these standard personalities and build your own |
regardless of the purpose of that system or the techniques |
in very large datacenters and is supported primarily in the |
never introduced into the system as the name block withholding |
of the purpose of that system or the techniques used |
very large datacenters and is supported primarily in the context |
introduced into the system as the name block withholding implies |
This question seems to be on the mind of many |
large datacenters and is supported primarily in the context of |
The strategies employed by this basic service are specifically targeted |
question seems to be on the mind of many people |
datacenters and is supported primarily in the context of three |
strategies employed by this basic service are specifically targeted towards |
seems to be on the mind of many people these |
employed by this basic service are specifically targeted towards applications |
to be on the mind of many people these days |
by this basic service are specifically targeted towards applications that |
this basic service are specifically targeted towards applications that need |
we believe that similar architectures will be needed more widely |
basic service are specifically targeted towards applications that need to |
Academics in general have taken a very narrow view of |
service are specifically targeted towards applications that need to operate |
because the need to tolerate heavy loads is increasingly ubiquitous |
in general have taken a very narrow view of what |
are specifically targeted towards applications that need to operate on |
this attack reduces the attacker s revenue compared to solo |
general have taken a very narrow view of what an |
specifically targeted towards applications that need to operate on a |
attack reduces the attacker s revenue compared to solo mining |
have taken a very narrow view of what an operating |
game servers require scalability for situations in which there are |
targeted towards applications that need to operate on a global |
reduces the attacker s revenue compared to solo mining or |
taken a very narrow view of what an operating system |
servers require scalability for situations in which there are many |
towards applications that need to operate on a global scale |
the attacker s revenue compared to solo mining or honest |
a very narrow view of what an operating system is |
require scalability for situations in which there are many users |
attacker s revenue compared to solo mining or honest pool |
To build a successful service the following goals were set |
s revenue compared to solo mining or honest pool participation |
David Faber at Microsoft trial defined an operating system as |
military systems require scalability to support new generations of integrated |
design a failure management system that is independent of the |
Faber at Microsoft trial defined an operating system as the |
It suffers from the reduced revenue like the other pool |
systems require scalability to support new generations of integrated applications |
a failure management system that is independent of the distributed |
at Microsoft trial defined an operating system as the software |
suffers from the reduced revenue like the other pool participants |
failure management system that is independent of the distributed systems |
hospital automation is putting new demands on medical information subsystems |
and its revenue is less than its share of the |
management system that is independent of the distributed systems packages |
Microsoft trial defined an operating system as the software that |
its revenue is less than its share of the total |
system that is independent of the distributed systems packages in |
trial defined an operating system as the software that controls |
the rollout of SOAs and the ease of application integration |
revenue is less than its share of the total mining |
that is independent of the distributed systems packages in use |
defined an operating system as the software that controls the |
rollout of SOAs and the ease of application integration they |
is less than its share of the total mining power |
is independent of the distributed systems packages in use and |
an operating system as the software that controls the execution |
of SOAs and the ease of application integration they support |
less than its share of the total mining power in |
independent of the distributed systems packages in use and provide |
operating system as the software that controls the execution of |
SOAs and the ease of application integration they support will |
than its share of the total mining power in the |
of the distributed systems packages in use and provide failure |
system as the software that controls the execution of programs |
and the ease of application integration they support will place |
its share of the total mining power in the system |
the distributed systems packages in use and provide failure detection |
as the software that controls the execution of programs on |
the ease of application integration they support will place services |
distributed systems packages in use and provide failure detection of |
the software that controls the execution of programs on computer |
ease of application integration they support will place services under |
systems packages in use and provide failure detection of processes |
software that controls the execution of programs on computer systems |
of application integration they support will place services under growing |
Even if a pool detects that it is under a |
that controls the execution of programs on computer systems and |
application integration they support will place services under growing load |
if a pool detects that it is under a block |
improve the accuracy of detection of process and node failure |
controls the execution of programs on computer systems and may |
a pool detects that it is under a block withholding |
the accuracy of detection of process and node failure through |
Our goal is to make it easy to build RAPS |
the execution of programs on computer systems and may provide |
pool detects that it is under a block withholding attack |
accuracy of detection of process and node failure through systems |
goal is to make it easy to build RAPS and |
execution of programs on computer systems and may provide low |
of detection of process and node failure through systems support |
it might not be able to detect which of its |
is to make it easy to build RAPS and RACS |
design support for failure detectors to work in large scale |
might not be able to detect which of its registered |
to make it easy to build RAPS and RACS from |
support for failure detectors to work in large scale systems |
not be able to detect which of its registered miners |
make it easy to build RAPS and RACS from traditional |
output control in a form which is sufficiently simple and |
be able to detect which of its registered miners are |
control in a form which is sufficiently simple and general |
build a comprehensive software package that can be easily integrated |
able to detect which of its registered miners are the |
in a form which is sufficiently simple and general so |
a comprehensive software package that can be easily integrated into |
to detect which of its registered miners are the perpetrators |
a form which is sufficiently simple and general so that |
comprehensive software package that can be easily integrated into various |
A pool can estimate its expected mining power and its |
form which is sufficiently simple and general so that these |
we also want to build the simplest platform capable of |
software package that can be easily integrated into various distributed |
pool can estimate its expected mining power and its actual |
which is sufficiently simple and general so that these services |
also want to build the simplest platform capable of accomplishing |
package that can be easily integrated into various distributed systems |
can estimate its expected mining power and its actual mining |
is sufficiently simple and general so that these services are |
want to build the simplest platform capable of accomplishing this |
that can be easily integrated into various distributed systems packages |
estimate its expected mining power and its actual mining power |
sufficiently simple and general so that these services are broadly |
to build the simplest platform capable of accomplishing this task |
can be easily integrated into various distributed systems packages and |
its expected mining power and its actual mining power by |
simple and general so that these services are broadly useful |
be easily integrated into various distributed systems packages and applications |
expected mining power and its actual mining power by the |
and general so that these services are broadly useful to |
mining power and its actual mining power by the rates |
The resulting system is implemented and is under test in |
general so that these services are broadly useful to software |
power and its actual mining power by the rates of |
resulting system is implemented and is under test in a |
so that these services are broadly useful to software developers |
and its actual mining power by the rates of partial |
system is implemented and is under test in a wide |
its actual mining power by the rates of partial proofs |
actual mining power by the rates of partial proofs of |
mining power by the rates of partial proofs of work |
power by the rates of partial proofs of work and |
by the rates of partial proofs of work and full |
the rates of partial proofs of work and full proofs |
Managed Transactional Consistency for Web Caching Ittay Eyal Ken Birman |
rates of partial proofs of work and full proofs of |
A first software release is planned for the autumn of |
Transactional Consistency for Web Caching Ittay Eyal Ken Birman Robbert |
In research community this strict distinction serves to distinguish the |
of partial proofs of work and full proofs of work |
Consistency for Web Caching Ittay Eyal Ken Birman Robbert van |
research community this strict distinction serves to distinguish the real |
for Web Caching Ittay Eyal Ken Birman Robbert van Renesse |
community this strict distinction serves to distinguish the real men |
Web Caching Ittay Eyal Ken Birman Robbert van Renesse Cornell |
this strict distinction serves to distinguish the real men from |
Caching Ittay Eyal Ken Birman Robbert van Renesse Cornell University |
A difference above a set confidence interval indicates an attack |
strict distinction serves to distinguish the real men from the |
Ittay Eyal Ken Birman Robbert van Renesse Cornell University Abstract |
couple the mechanism by which failures are detected from the |
distinction serves to distinguish the real men from the boys |
Eyal Ken Birman Robbert van Renesse Cornell University Abstract In |
the mechanism by which failures are detected from the protocols |
comparing the estimated mining power of the attacker based on |
mechanism by which failures are detected from the protocols used |
Researchers and hackers that work in the area defined by |
the estimated mining power of the attacker based on its |
by which failures are detected from the protocols used to |
Elements of the model A service is simply an application |
and hackers that work in the area defined by this |
only caches are widely used in cloud infrastructure to reduce |
estimated mining power of the attacker based on its partial |
which failures are detected from the protocols used to tolerate |
of the model A service is simply an application that |
hackers that work in the area defined by this narrow |
caches are widely used in cloud infrastructure to reduce access |
mining power of the attacker based on its partial proof |
failures are detected from the protocols used to tolerate those |
the model A service is simply an application that provides |
that work in the area defined by this narrow definition |
are widely used in cloud infrastructure to reduce access latency |
power of the attacker based on its partial proof of |
are detected from the protocols used to tolerate those failures |
model A service is simply an application that provides interfaces |
work in the area defined by this narrow definition of |
widely used in cloud infrastructure to reduce access latency and |
of the attacker based on its partial proof of work |
A service is simply an application that provides interfaces that |
in the area defined by this narrow definition of operating |
used in cloud infrastructure to reduce access latency and to |
the attacker based on its partial proof of work with |
service is simply an application that provides interfaces that manipulate |
the area defined by this narrow definition of operating systems |
Chandra and Toueg successfully show that it is possible to |
in cloud infrastructure to reduce access latency and to reduce |
attacker based on its partial proof of work with the |
consider themselves part of the select circle of people working |
and Toueg successfully show that it is possible to develop |
cloud infrastructure to reduce access latency and to reduce load |
is simply an application that provides interfaces that manipulate objects |
based on its partial proof of work with the fact |
themselves part of the select circle of people working on |
Toueg successfully show that it is possible to develop consensus |
infrastructure to reduce access latency and to reduce load on |
simply an application that provides interfaces that manipulate objects of |
on its partial proof of work with the fact it |
part of the select circle of people working on the |
successfully show that it is possible to develop consensus algorithms |
to reduce access latency and to reduce load on backend |
an application that provides interfaces that manipulate objects of unspecified |
its partial proof of work with the fact it never |
of the select circle of people working on the core |
show that it is possible to develop consensus algorithms using |
reduce access latency and to reduce load on backend databases |
application that provides interfaces that manipulate objects of unspecified nature |
partial proof of work with the fact it never supplies |
Operators view coherent caches as impractical at genuinely large scale |
that it is possible to develop consensus algorithms using failure |
the select circle of people working on the core of |
proof of work with the fact it never supplies a |
view coherent caches as impractical at genuinely large scale and |
A query operation reads some object and returns a computed |
it is possible to develop consensus algorithms using failure detectors |
select circle of people working on the core of the |
of work with the fact it never supplies a full |
coherent caches as impractical at genuinely large scale and many |
query operation reads some object and returns a computed value |
circle of people working on the core of the systems |
work with the fact it never supplies a full proof |
even if these failure detectors make frequent mistakes in their |
caches as impractical at genuinely large scale and many client |
of people working on the core of the systems area |
with the fact it never supplies a full proof of |
if these failure detectors make frequent mistakes in their observations |
One unusual assumption made in our work is that many |
people working on the core of the systems area of |
facing caches are updated in an asynchronous manner with best |
the fact it never supplies a full proof of work |
unusual assumption made in our work is that many services |
working on the core of the systems area of computer |
assumption made in our work is that many services can |
on the core of the systems area of computer science |
Existing solutions that support cache consistency are inapplicable to this |
made in our work is that many services can process |
solutions that support cache consistency are inapplicable to this scenario |
but the pool will only expect to see a full |
in our work is that many services can process updates |
Once you are in this circle you will become part |
that support cache consistency are inapplicable to this scenario since |
the pool will only expect to see a full proof |
the failure detector work is extended to systems that also |
our work is that many services can process updates out |
you are in this circle you will become part of |
support cache consistency are inapplicable to this scenario since they |
pool will only expect to see a full proof of |
failure detector work is extended to systems that also take |
work is that many services can process updates out of |
are in this circle you will become part of the |
cache consistency are inapplicable to this scenario since they require |
will only expect to see a full proof of work |
detector work is extended to systems that also take network |
is that many services can process updates out of order |
in this circle you will become part of the secret |
consistency are inapplicable to this scenario since they require a |
only expect to see a full proof of work at |
work is extended to systems that also take network failure |
this circle you will become part of the secret society |
are inapplicable to this scenario since they require a round |
expect to see a full proof of work at very |
is extended to systems that also take network failure into |
we focus on services that Can respond correctly to queries |
circle you will become part of the secret society that |
inapplicable to this scenario since they require a round trip |
to see a full proof of work at very low |
extended to systems that also take network failure into account |
focus on services that Can respond correctly to queries even |
you will become part of the secret society that practices |
to this scenario since they require a round trip to |
see a full proof of work at very low frequency |
on services that Can respond correctly to queries even if |
will become part of the secret society that practices the |
this scenario since they require a round trip to the |
off in designing practical distributed systems based on the theory |
services that Can respond correctly to queries even if some |
become part of the secret society that practices the black |
scenario since they require a round trip to the database |
it cannot obtain statistically significant results that would indicate an |
in designing practical distributed systems based on the theory developed |
that Can respond correctly to queries even if some updates |
part of the secret society that practices the black art |
since they require a round trip to the database on |
cannot obtain statistically significant results that would indicate an attack |
designing practical distributed systems based on the theory developed for |
Can respond correctly to queries even if some updates are |
of the secret society that practices the black art of |
they require a round trip to the database on every |
practical distributed systems based on the theory developed for asynchronous |
An attacker can use multiple small block withholding miners and |
respond correctly to queries even if some updates are temporarily |
the secret society that practices the black art of OS |
require a round trip to the database on every cache |
distributed systems based on the theory developed for asynchronous systems |
attacker can use multiple small block withholding miners and replace |
correctly to queries even if some updates are temporarily missing |
secret society that practices the black art of OS research |
a round trip to the database on every cache transaction |
systems based on the theory developed for asynchronous systems is |
can use multiple small block withholding miners and replace them |
society that practices the black art of OS research and |
Converge into a state determined entirely by the set of |
based on the theory developed for asynchronous systems is where |
use multiple small block withholding miners and replace them frequently |
Existing incoherent cache technologies are oblivious to transactional data access |
that practices the black art of OS research and will |
into a state determined entirely by the set of updates |
on the theory developed for asynchronous systems is where and |
practices the black art of OS research and will start |
the theory developed for asynchronous systems is where and how |
a miners whose expected full proof of work frequency is |
so that if two members of some subservice receive the |
the black art of OS research and will start to |
theory developed for asynchronous systems is where and how to |
miners whose expected full proof of work frequency is yearly |
that if two members of some subservice receive the same |
black art of OS research and will start to regard |
developed for asynchronous systems is where and how to introduce |
if two members of some subservice receive the same updates |
Cache improves cache consistency despite asynchronous and unreliable communication between |
art of OS research and will start to regard any |
for asynchronous systems is where and how to introduce the |
two members of some subservice receive the same updates they |
improves cache consistency despite asynchronous and unreliable communication between the |
of OS research and will start to regard any other |
asynchronous systems is where and how to introduce the notion |
members of some subservice receive the same updates they will |
cache consistency despite asynchronous and unreliable communication between the cache |
OS research and will start to regard any other activity |
systems is where and how to introduce the notion of |
of some subservice receive the same updates they will be |
consistency despite asynchronous and unreliable communication between the cache and |
research and will start to regard any other activity of |
is where and how to introduce the notion of time |
some subservice receive the same updates they will be in |
despite asynchronous and unreliable communication between the cache and the |
and will start to regard any other activity of systems |
subservice receive the same updates they will be in equivalent |
asynchronous and unreliable communication between the cache and the database |
will start to regard any other activity of systems development |
If the attacker replaces such a small miner every month |
receive the same updates they will be in equivalent states |
start to regard any other activity of systems development as |
outs remain an important tool in the failure manager described |
to regard any other activity of systems development as irrelevant |
a variant of serializability that is suitable for incoherent caches |
remain an important tool in the failure manager described in |
regard any other activity of systems development as irrelevant to |
The pool must decide within this month whether the miner |
an important tool in the failure manager described in this |
any other activity of systems development as irrelevant to the |
pool must decide within this month whether the miner is |
What this amounts to is that the SSA should deliver |
important tool in the failure manager described in this paper |
other activity of systems development as irrelevant to the future |
must decide within this month whether the miner is an |
this amounts to is that the SSA should deliver updates |
activity of systems development as irrelevant to the future of |
the mechanism is integrated into a more comprehensive approach that |
decide within this month whether the miner is an attacker |
amounts to is that the SSA should deliver updates as |
of systems development as irrelevant to the future of computer |
We use synthetic workloads to demonstrate the efficacy of T |
mechanism is integrated into a more comprehensive approach that treats |
to is that the SSA should deliver updates as soon |
systems development as irrelevant to the future of computer science |
is integrated into a more comprehensive approach that treats failure |
Cache when data accesses are clustered and its adaptive reaction |
is that the SSA should deliver updates as soon as |
Since an honest miner of this power is unlikely to |
integrated into a more comprehensive approach that treats failure detection |
when data accesses are clustered and its adaptive reaction to |
For a long time the line was drawn at the |
that the SSA should deliver updates as soon as it |
an honest miner of this power is unlikely to find |
into a more comprehensive approach that treats failure detection using |
data accesses are clustered and its adaptive reaction to workload |
a long time the line was drawn at the kernel |
the SSA should deliver updates as soon as it can |
honest miner of this power is unlikely to find a |
a more comprehensive approach that treats failure detection using methods |
accesses are clustered and its adaptive reaction to workload changes |
SSA should deliver updates as soon as it can even |
miner of this power is unlikely to find a full |
more comprehensive approach that treats failure detection using methods based |
and one could only consider himself a true OS researcher |
should deliver updates as soon as it can even if |
of this power is unlikely to find a full proof |
comprehensive approach that treats failure detection using methods based on |
one could only consider himself a true OS researcher after |
deliver updates as soon as it can even if they |
this power is unlikely to find a full proof of |
approach that treats failure detection using methods based on an |
could only consider himself a true OS researcher after having |
updates as soon as it can even if they are |
of the inconsistencies and increases the rate of consistent transactions |
that treats failure detection using methods based on an analogy |
only consider himself a true OS researcher after having developed |
power is unlikely to find a full proof of work |
as soon as it can even if they are not |
the inconsistencies and increases the rate of consistent transactions by |
treats failure detection using methods based on an analogy with |
consider himself a true OS researcher after having developed at |
is unlikely to find a full proof of work within |
soon as it can even if they are not in |
failure detection using methods based on an analogy with fault |
himself a true OS researcher after having developed at least |
unlikely to find a full proof of work within a |
as it can even if they are not in order |
a true OS researcher after having developed at least two |
to find a full proof of work within a month |
When trying to contact a person who has allegedly disappeared |
true OS researcher after having developed at least two device |
one way that an application might process out of order |
trying to contact a person who has allegedly disappeared one |
I NTRODUCTION Internet services like online retailers and social networks |
OS researcher after having developed at least two device drivers |
way that an application might process out of order updates |
to contact a person who has allegedly disappeared one would |
NTRODUCTION Internet services like online retailers and social networks store |
researcher after having developed at least two device drivers and |
a pool that rejects miners based on this criterion would |
that an application might process out of order updates is |
contact a person who has allegedly disappeared one would never |
Internet services like online retailers and social networks store important |
after having developed at least two device drivers and hacked |
pool that rejects miners based on this criterion would reject |
an application might process out of order updates is simply |
a person who has allegedly disappeared one would never be |
services like online retailers and social networks store important data |
having developed at least two device drivers and hacked on |
that rejects miners based on this criterion would reject the |
application might process out of order updates is simply to |
person who has allegedly disappeared one would never be satisfied |
like online retailers and social networks store important data sets |
developed at least two device drivers and hacked on the |
rejects miners based on this criterion would reject the majority |
might process out of order updates is simply to delay |
who has allegedly disappeared one would never be satisfied with |
online retailers and social networks store important data sets in |
at least two device drivers and hacked on the terminal |
miners based on this criterion would reject the majority of |
process out of order updates is simply to delay processing |
has allegedly disappeared one would never be satisfied with making |
retailers and social networks store important data sets in large |
least two device drivers and hacked on the terminal driver |
based on this criterion would reject the majority of its |
out of order updates is simply to delay processing them |
allegedly disappeared one would never be satisfied with making repeated |
and social networks store important data sets in large distributed |
two device drivers and hacked on the terminal driver of |
on this criterion would reject the majority of its honest |
of order updates is simply to delay processing them until |
disappeared one would never be satisfied with making repeated phone |
social networks store important data sets in large distributed databases |
device drivers and hacked on the terminal driver of the |
this criterion would reject the majority of its honest miners |
order updates is simply to delay processing them until it |
one would never be satisfied with making repeated phone calls |
drivers and hacked on the terminal driver of the BSD |
updates is simply to delay processing them until it can |
would never be satisfied with making repeated phone calls to |
The alternative of rejecting small miners in general or distributing |
is simply to delay processing them until it can sort |
never be satisfied with making repeated phone calls to the |
alternative of rejecting small miners in general or distributing revenue |
simply to delay processing them until it can sort them |
be satisfied with making repeated phone calls to the same |
of rejecting small miners in general or distributing revenue on |
to delay processing them until it can sort them into |
satisfied with making repeated phone calls to the same location |
rejecting small miners in general or distributing revenue on a |
the notion of where exactly operating systems services are located |
delay processing them until it can sort them into order |
with making repeated phone calls to the same location for |
small miners in general or distributing revenue on a yearly |
notion of where exactly operating systems services are located is |
making repeated phone calls to the same location for half |
miners in general or distributing revenue on a yearly basis |
of where exactly operating systems services are located is not |
it will be possible to act on an update or |
repeated phone calls to the same location for half an |
in general or distributing revenue on a yearly basis contradicts |
where exactly operating systems services are located is not that |
will be possible to act on an update or query |
phone calls to the same location for half an hour |
general or distributing revenue on a yearly basis contradicts the |
exactly operating systems services are located is not that simple |
be possible to act on an update or query immediately |
support transactions with guarantees such as snapshot isolation and even |
calls to the same location for half an hour and |
or distributing revenue on a yearly basis contradicts the goal |
operating systems services are located is not that simple any |
possible to act on an update or query immediately upon |
transactions with guarantees such as snapshot isolation and even full |
to the same location for half an hour and then |
distributing revenue on a yearly basis contradicts the goal of |
systems services are located is not that simple any more |
to act on an update or query immediately upon receiving |
with guarantees such as snapshot isolation and even full transactional |
the same location for half an hour and then declaring |
revenue on a yearly basis contradicts the goal of pooled |
act on an update or query immediately upon receiving it |
guarantees such as snapshot isolation and even full transactional atomicity |
Fundamental services are split between kernel and user space in |
same location for half an hour and then declaring the |
on a yearly basis contradicts the goal of pooled mining |
services are split between kernel and user space in attempts |
location for half an hour and then declaring the disappearance |
are split between kernel and user space in attempts to |
for half an hour and then declaring the disappearance a |
tier applications to leverage the transactions that the databases provide |
A service that can be structured as a RAPS must |
split between kernel and user space in attempts to optimise |
M ODEL AND S TANDARD O PERATION We specify the |
half an hour and then declaring the disappearance a fact |
service that can be structured as a RAPS must have |
between kernel and user space in attempts to optimise their |
ODEL AND S TANDARD O PERATION We specify the basic |
that can be structured as a RAPS must have a |
kernel and user space in attempts to optimise their efficiency |
AND S TANDARD O PERATION We specify the basic model |
a busy tone was heard or the phone was disconnected |
can be structured as a RAPS must have a partitioning |
and user space in attempts to optimise their efficiency and |
S TANDARD O PERATION We specify the basic model in |
be structured as a RAPS must have a partitioning function |
In practice one would work to gain more confidence in |
user space in attempts to optimise their efficiency and avoid |
The problem centers on the asynchronous style of communication used |
TANDARD O PERATION We specify the basic model in which |
structured as a RAPS must have a partitioning function that |
practice one would work to gain more confidence in such |
space in attempts to optimise their efficiency and avoid uncontrolled |
problem centers on the asynchronous style of communication used between |
O PERATION We specify the basic model in which participants |
as a RAPS must have a partitioning function that can |
one would work to gain more confidence in such a |
in attempts to optimise their efficiency and avoid uncontrolled growth |
centers on the asynchronous style of communication used between the |
PERATION We specify the basic model in which participants operate |
a RAPS must have a partitioning function that can be |
would work to gain more confidence in such a decision |
attempts to optimise their efficiency and avoid uncontrolled growth of |
on the asynchronous style of communication used between the database |
We specify the basic model in which participants operate in |
RAPS must have a partitioning function that can be used |
work to gain more confidence in such a decision by |
to optimise their efficiency and avoid uncontrolled growth of kernel |
the asynchronous style of communication used between the database and |
specify the basic model in which participants operate in Section |
must have a partitioning function that can be used to |
to gain more confidence in such a decision by talking |
optimise their efficiency and avoid uncontrolled growth of kernel services |
asynchronous style of communication used between the database and the |
the basic model in which participants operate in Section III |
have a partitioning function that can be used to map |
gain more confidence in such a decision by talking to |
style of communication used between the database and the geo |
The pervasiveness of distributed services in modern systems can be |
a partitioning function that can be used to map each |
more confidence in such a decision by talking to the |
proceed to describe how honest miners operate in this environment |
pervasiveness of distributed services in modern systems can be considered |
partitioning function that can be used to map each operation |
A cache should not access the database on every transaction |
confidence in such a decision by talking to the landlord |
to describe how honest miners operate in this environment in |
of distributed services in modern systems can be considered a |
function that can be used to map each operation to |
describe how honest miners operate in this environment in Sections |
trips to an authoritative backend database would cause unacceptable latency |
the neighbors or others that may have a more informed |
distributed services in modern systems can be considered a threat |
that can be used to map each operation to the |
how honest miners operate in this environment in Sections III |
neighbors or others that may have a more informed idea |
services in modern systems can be considered a threat to |
and asynchronous updates rule out cache coherency schemes that would |
can be used to map each operation to the subservice |
or others that may have a more informed idea about |
in modern systems can be considered a threat to the |
asynchronous updates rule out cache coherency schemes that would require |
be used to map each operation to the subservice that |
others that may have a more informed idea about the |
and how the classical block withholding attack is implemented with |
modern systems can be considered a threat to the traditional |
updates rule out cache coherency schemes that would require the |
used to map each operation to the subservice that should |
that may have a more informed idea about the situation |
how the classical block withholding attack is implemented with our |
systems can be considered a threat to the traditional notion |
rule out cache coherency schemes that would require the backend |
to map each operation to the subservice that should execute |
may have a more informed idea about the situation of |
the classical block withholding attack is implemented with our model |
can be considered a threat to the traditional notion of |
out cache coherency schemes that would require the backend database |
map each operation to the subservice that should execute it |
have a more informed idea about the situation of the |
classical block withholding attack is implemented with our model in |
be considered a threat to the traditional notion of operating |
cache coherency schemes that would require the backend database to |
a more informed idea about the situation of the person |
block withholding attack is implemented with our model in Section |
Existing systems typically implement partitioning functions in one of two |
considered a threat to the traditional notion of operating systems |
coherency schemes that would require the backend database to promptly |
more informed idea about the situation of the person in |
withholding attack is implemented with our model in Section III |
Many support services are required to make distributed systems work |
schemes that would require the backend database to promptly invalidate |
informed idea about the situation of the person in question |
systems typically implement partitioning functions in one of two ways |
support services are required to make distributed systems work efficiently |
that would require the backend database to promptly invalidate or |
The failure management described in this paper is capable of |
services are required to make distributed systems work efficiently and |
Model The system is comprised of the Bitcoin network and |
would require the backend database to promptly invalidate or update |
failure management described in this paper is capable of following |
are required to make distributed systems work efficiently and effectively |
The system is comprised of the Bitcoin network and nodes |
so that clients are able to locally implement the logic |
require the backend database to promptly invalidate or update cached |
management described in this paper is capable of following a |
required to make distributed systems work efficiently and effectively and |
system is comprised of the Bitcoin network and nodes with |
that clients are able to locally implement the logic mapping |
the backend database to promptly invalidate or update cached This |
described in this paper is capable of following a similar |
to make distributed systems work efficiently and effectively and these |
is comprised of the Bitcoin network and nodes with unique |
clients are able to locally implement the logic mapping requests |
backend database to promptly invalidate or update cached This work |
in this paper is capable of following a similar strategy |
make distributed systems work efficiently and effectively and these services |
comprised of the Bitcoin network and nodes with unique IDs |
are able to locally implement the logic mapping requests to |
database to promptly invalidate or update cached This work is |
if a process under investigation is not responding it will |
able to locally implement the logic mapping requests to subservices |
A node i generates tasks which are associated with its |
such as security and directory services or distributed object support |
a process under investigation is not responding it will contact |
to promptly invalidate or update cached This work is supported |
node i generates tasks which are associated with its ID |
as security and directory services or distributed object support and |
process under investigation is not responding it will contact the |
i generates tasks which are associated with its ID i |
or could influence the creation of web pages by modifying |
security and directory services or distributed object support and cluster |
under investigation is not responding it will contact the operating |
or even to track the locations at which cached objects |
could influence the creation of web pages by modifying URLs |
A node can work on a task for the duration |
and directory services or distributed object support and cluster management |
investigation is not responding it will contact the operating system |
even to track the locations at which cached objects reside |
node can work on a task for the duration of |
so that clients will be directed to an appropriate subservice |
is not responding it will contact the operating system under |
are not part of a traditional view of operating systems |
can work on a task for the duration of a |
We define a variant of serializability called cacheserializability that is |
not responding it will contact the operating system under which |
work on a task for the duration of a step |
the servers might export actual code that the client runs |
define a variant of serializability called cacheserializability that is suitable |
but they are essential to the operation of modern operating |
responding it will contact the operating system under which the |
a variant of serializability called cacheserializability that is suitable for |
The result of this work is a set of partial |
they are essential to the operation of modern operating systems |
it will contact the operating system under which the process |
The partitioning logic is situated on a load balancing component |
variant of serializability called cacheserializability that is suitable for incoherent |
result of this work is a set of partial proofs |
will contact the operating system under which the process is |
partitioning logic is situated on a load balancing component resident |
This results in that an operating system no longer is |
of serializability called cacheserializability that is suitable for incoherent caches |
of this work is a set of partial proofs of |
contact the operating system under which the process is running |
logic is situated on a load balancing component resident in |
results in that an operating system no longer is a |
this work is a set of partial proofs of work |
is situated on a load balancing component resident in the |
in that an operating system no longer is a simple |
work is a set of partial proofs of work and |
Despite the fact that an inconsistent read access can deter |
net to help reach a decision in which one can |
situated on a load balancing component resident in the server |
that an operating system no longer is a simple division |
is a set of partial proofs of work and a |
the fact that an inconsistent read access can deter a |
to help reach a decision in which one can have |
on a load balancing component resident in the server cluster |
an operating system no longer is a simple division between |
a set of partial proofs of work and a set |
fact that an inconsistent read access can deter a client |
help reach a decision in which one can have greater |
operating system no longer is a simple division between kernel |
set of partial proofs of work and a set of |
The load balancer sprays requests over the subservices in accordance |
that an inconsistent read access can deter a client and |
reach a decision in which one can have greater confidence |
system no longer is a simple division between kernel and |
of partial proofs of work and a set of full |
load balancer sprays requests over the subservices in accordance with |
an inconsistent read access can deter a client and reduce |
no longer is a simple division between kernel and user |
partial proofs of work and a set of full proofs |
balancer sprays requests over the subservices in accordance with server |
Most distributed systems in use today deal with failure of |
inconsistent read access can deter a client and reduce their |
longer is a simple division between kernel and user space |
proofs of work and a set of full proofs of |
sprays requests over the subservices in accordance with server logic |
distributed systems in use today deal with failure of nodes |
read access can deter a client and reduce their income |
of work and a set of full proofs of work |
systems in use today deal with failure of nodes or |
they cannot afford consistent cache techniques that require backend accesses |
in use today deal with failure of nodes or networks |
cannot afford consistent cache techniques that require backend accesses on |
The number of proofs in each set has a Poisson |
use today deal with failure of nodes or networks in |
balancing component in tracking membership so that it can appropriately |
afford consistent cache techniques that require backend accesses on every |
Operating systems that address the needs of current and future |
number of proofs in each set has a Poisson distribution |
today deal with failure of nodes or networks in some |
component in tracking membership so that it can appropriately route |
partial proofs with a large mean and full proofs with |
systems that address the needs of current and future clients |
deal with failure of nodes or networks in some way |
consistent cache techniques that require backend accesses on every transaction |
in tracking membership so that it can appropriately route queries |
proofs with a large mean and full proofs with a |
that address the needs of current and future clients and |
tracking membership so that it can appropriately route queries and |
In general the problem is detected in the communication subsystem |
with a large mean and full proofs with a small |
address the needs of current and future clients and INFORMATIK |
a novel caching scheme that improves consistency at the cache |
membership so that it can appropriately route queries and updates |
general the problem is detected in the communication subsystem where |
a large mean and full proofs with a small mean |
the needs of current and future clients and INFORMATIK INFORMATIQUE |
novel caching scheme that improves consistency at the cache level |
the problem is detected in the communication subsystem where session |
caching scheme that improves consistency at the cache level with |
problem is detected in the communication subsystem where session or |
scheme that improves consistency at the cache level with a |
is detected in the communication subsystem where session or transport |
that improves consistency at the cache level with a nominal |
detected in the communication subsystem where session or transport protocols |
improves consistency at the cache level with a nominal storage |
in the communication subsystem where session or transport protocols are |
To acquire this payoff an entity publishes a task task |
consistency at the cache level with a nominal storage and |
the communication subsystem where session or transport protocols are unable |
acquire this payoff an entity publishes a task task and |
at the cache level with a nominal storage and communication |
communication subsystem where session or transport protocols are unable to |
this payoff an entity publishes a task task and its |
the cache level with a nominal storage and communication tradeoff |
subsystem where session or transport protocols are unable to make |
payoff an entity publishes a task task and its corresponding |
Operating Systems servers no longer span a single computer and |
where session or transport protocols are unable to make progress |
an entity publishes a task task and its corresponding proof |
Cache significantly improves consistency for workloads where data accesses are |
Systems servers no longer span a single computer and they |
session or transport protocols are unable to make progress because |
entity publishes a task task and its corresponding proof of |
significantly improves consistency for workloads where data accesses are clustered |
servers no longer span a single computer and they abstract |
we wish to support a scalable inventory service that receives |
publishes a task task and its corresponding proof of work |
or transport protocols are unable to make progress because of |
no longer span a single computer and they abstract services |
wish to support a scalable inventory service that receives updates |
a task task and its corresponding proof of work to |
This is achieved while retaining the global scalability afforded by |
transport protocols are unable to make progress because of the |
longer span a single computer and they abstract services away |
to support a scalable inventory service that receives updates corresponding |
task task and its corresponding proof of work to the |
is achieved while retaining the global scalability afforded by executing |
protocols are unable to make progress because of the lack |
span a single computer and they abstract services away from |
support a scalable inventory service that receives updates corresponding to |
task and its corresponding proof of work to the network |
achieved while retaining the global scalability afforded by executing read |
are unable to make progress because of the lack of |
a single computer and they abstract services away from physical |
a scalable inventory service that receives updates corresponding to inventory |
unable to make progress because of the lack of response |
single computer and they abstract services away from physical nodes |
The Bitcoin protocol normalizes revenue such that the average total |
scalable inventory service that receives updates corresponding to inventory consumption |
We do this by storing dependency information with the cached |
to make progress because of the lack of response from |
computer and they abstract services away from physical nodes allowing |
Bitcoin protocol normalizes revenue such that the average total revenue |
inventory service that receives updates corresponding to inventory consumption and |
do this by storing dependency information with the cached objects |
make progress because of the lack of response from remote |
and they abstract services away from physical nodes allowing user |
protocol normalizes revenue such that the average total revenue distributed |
service that receives updates corresponding to inventory consumption and re |
progress because of the lack of response from remote nodes |
they abstract services away from physical nodes allowing user to |
normalizes revenue such that the average total revenue distributed in |
The user can improve the level of consistency by adjusting |
Queries against such a service would compute and return an |
revenue such that the average total revenue distributed in each |
abstract services away from physical nodes allowing user to be |
user can improve the level of consistency by adjusting the |
out period and after a retry threshold is reached the |
against such a service would compute and return an inventory |
such that the average total revenue distributed in each step |
services away from physical nodes allowing user to be part |
can improve the level of consistency by adjusting the size |
period and after a retry threshold is reached the remote |
such a service would compute and return an inventory count |
that the average total revenue distributed in each step is |
away from physical nodes allowing user to be part of |
improve the level of consistency by adjusting the size of |
and after a retry threshold is reached the remote destination |
a service would compute and return an inventory count as |
the average total revenue distributed in each step is a |
from physical nodes allowing user to be part of a |
the level of consistency by adjusting the size of this |
after a retry threshold is reached the remote destination is |
service would compute and return an inventory count as of |
average total revenue distributed in each step is a constant |
physical nodes allowing user to be part of a larger |
level of consistency by adjusting the size of this dependency |
a retry threshold is reached the remote destination is marked |
would compute and return an inventory count as of the |
total revenue distributed in each step is a constant throughout |
of consistency by adjusting the size of this dependency data |
retry threshold is reached the remote destination is marked as |
compute and return an inventory count as of the time |
revenue distributed in each step is a constant throughout the |
threshold is reached the remote destination is marked as unreachable |
and return an inventory count as of the time the |
distributed in each step is a constant throughout the execution |
we created a prototype implementation and exposed it to workloads |
return an inventory count as of the time the query |
in each step is a constant throughout the execution of |
Some systems inject additional packets into the data stream to |
created a prototype implementation and exposed it to workloads based |
an inventory count as of the time the query was |
each step is a constant throughout the execution of the |
systems inject additional packets into the data stream to ensure |
a prototype implementation and exposed it to workloads based on |
inventory count as of the time the query was processed |
At that moment its vendor was discontinuing the operating system |
inject additional packets into the data stream to ensure timely |
prototype implementation and exposed it to workloads based on graphically |
step is a constant throughout the execution of the system |
additional packets into the data stream to ensure timely detection |
packets into the data stream to ensure timely detection of |
into the data stream to ensure timely detection of failures |
Any node can transact Bitcoins to another node by issuing |
the data stream to ensure timely detection of failures at |
node can transact Bitcoins to another node by issuing a |
data stream to ensure timely detection of failures at moments |
This event forced us to take a step back and |
might yield a different result and yet both would be |
can transact Bitcoins to another node by issuing a Bitcoin |
stream to ensure timely detection of failures at moments when |
event forced us to take a step back and evaluate |
yield a different result and yet both would be correct |
transact Bitcoins to another node by issuing a Bitcoin transaction |
of the inconsistencies and can increase the ratio of consistent |
to ensure timely detection of failures at moments when the |
forced us to take a step back and evaluate our |
the inconsistencies and can increase the ratio of consistent transactions |
ensure timely detection of failures at moments when the traffic |
Nodes that generate tasks but outsource the work are called |
us to take a step back and evaluate our research |
inconsistencies and can increase the ratio of consistent transactions by |
timely detection of failures at moments when the traffic is |
that generate tasks but outsource the work are called pools |
a response reflecting a very stale state would be incorrect |
to take a step back and evaluate our research directions |
detection of failures at moments when the traffic is low |
take a step back and evaluate our research directions and |
of failures at moments when the traffic is low or |
A client should not be offered a promotional price on |
a step back and evaluate our research directions and our |
Cache reacts to different clustering levels and how it adapts |
client should not be offered a promotional price on a |
and send the partial and full proofs of work to |
failures at moments when the traffic is low or unidirectional |
step back and evaluate our research directions and our expectations |
reacts to different clustering levels and how it adapts as |
should not be offered a promotional price on a plasma |
send the partial and full proofs of work to the |
back and evaluate our research directions and our expectations with |
to different clustering levels and how it adapts as clusters |
not be offered a promotional price on a plasma TV |
the partial and full proofs of work to the pool |
and evaluate our research directions and our expectations with respect |
different clustering levels and how it adapts as clusters change |
be offered a promotional price on a plasma TV if |
evaluate our research directions and our expectations with respect to |
offered a promotional price on a plasma TV if the |
our research directions and our expectations with respect to the |
a promotional price on a plasma TV if the last |
expect the application to handle the failure management as the |
research directions and our expectations with respect to the operating |
promotional price on a plasma TV if the last unit |
To explain this perfect behavior we prove a related claim |
the application to handle the failure management as the support |
directions and our expectations with respect to the operating systems |
price on a plasma TV if the last unit was |
explain this perfect behavior we prove a related claim we |
We assume that the number of miners is large enough |
application to handle the failure management as the support system |
and our expectations with respect to the operating systems to |
on a plasma TV if the last unit was actually |
this perfect behavior we prove a related claim we show |
assume that the number of miners is large enough such |
to handle the failure management as the support system does |
our expectations with respect to the operating systems to use |
a plasma TV if the last unit was actually sold |
perfect behavior we prove a related claim we show that |
that the number of miners is large enough such that |
handle the failure management as the support system does not |
plasma TV if the last unit was actually sold hours |
it was the fact that most of the operating systems |
the number of miners is large enough such that mining |
the failure management as the support system does not contain |
behavior we prove a related claim we show that with |
TV if the last unit was actually sold hours ago |
was the fact that most of the operating systems we |
number of miners is large enough such that mining power |
failure management as the support system does not contain any |
we prove a related claim we show that with unbounded |
the fact that most of the operating systems we were |
of miners is large enough such that mining power can |
management as the support system does not contain any fault |
prove a related claim we show that with unbounded resources |
the inventory service should reflect as many updates as possible |
fact that most of the operating systems we were looking |
miners is large enough such that mining power can be |
as the support system does not contain any fault management |
a related claim we show that with unbounded resources T |
inventory service should reflect as many updates as possible in |
that most of the operating systems we were looking at |
is large enough such that mining power can be split |
service should reflect as many updates as possible in the |
most of the operating systems we were looking at were |
large enough such that mining power can be split arbitrarily |
should reflect as many updates as possible in the replies |
The mechanisms used to detect failure do not adapt to |
of the operating systems we were looking at were actually |
enough such that mining power can be split arbitrarily without |
reflect as many updates as possible in the replies it |
mechanisms used to detect failure do not adapt to changing |
the operating systems we were looking at were actually very |
such that mining power can be split arbitrarily without resolution |
as many updates as possible in the replies it gives |
used to detect failure do not adapt to changing network |
operating systems we were looking at were actually very old |
that mining power can be split arbitrarily without resolution constraints |
many updates as possible in the replies it gives to |
to detect failure do not adapt to changing network conditions |
systems we were looking at were actually very old fashioned |
updates as possible in the replies it gives to requests |
the total number of mining power in the system with |
making it almost impossible to use these systems unmodified in |
total number of mining power in the system with m |
but any reply is correct provided that it was based |
it almost impossible to use these systems unmodified in wide |
number of mining power in the system with m and |
any reply is correct provided that it was based on |
Most of these operating systems had their conception in the |
almost impossible to use these systems unmodified in wide area |
of mining power in the system with m and the |
reply is correct provided that it was based on a |
impossible to use these systems unmodified in wide area systems |
mining power in the system with m and the miners |
is correct provided that it was based on a recent |
to use these systems unmodified in wide area systems without |
power in the system with m and the miners participating |
correct provided that it was based on a recent state |
s and did not change much in structure since then |
use these systems unmodified in wide area systems without resorting |
in the system with m and the miners participating in |
these systems unmodified in wide area systems without resorting to |
we shall see that the SSA allows brief inconsistencies but |
Linux could be seen as an exception since it was |
the system with m and the miners participating in pool |
systems unmodified in wide area systems without resorting to heavy |
shall see that the SSA allows brief inconsistencies but that |
could be seen as an exception since it was developed |
system with m and the miners participating in pool i |
unmodified in wide area systems without resorting to heavy weight |
see that the SSA allows brief inconsistencies but that they |
be seen as an exception since it was developed in |
scale databases with strong guarantees initially led companies to abandon |
in wide area systems without resorting to heavy weight solutions |
that the SSA allows brief inconsistencies but that they can |
seen as an exception since it was developed in the |
databases with strong guarantees initially led companies to abandon cross |
wide area systems without resorting to heavy weight solutions like |
the SSA allows brief inconsistencies but that they can be |
We use a quasistatic analysis where miner participation in a |
as an exception since it was developed in the second |
area systems without resorting to heavy weight solutions like using |
object consistency altogether and make do with weak guarantees such |
SSA allows brief inconsistencies but that they can be limited |
use a quasistatic analysis where miner participation in a pool |
an exception since it was developed in the second half |
systems without resorting to heavy weight solutions like using a |
consistency altogether and make do with weak guarantees such as |
allows brief inconsistencies but that they can be limited to |
a quasistatic analysis where miner participation in a pool does |
exception since it was developed in the second half of |
without resorting to heavy weight solutions like using a TCP |
altogether and make do with weak guarantees such as per |
brief inconsistencies but that they can be limited to a |
quasistatic analysis where miner participation in a pool does not |
since it was developed in the second half of the |
resorting to heavy weight solutions like using a TCP connection |
inconsistencies but that they can be limited to a few |
analysis where miner participation in a pool does not change |
to heavy weight solutions like using a TCP connection as |
but that they can be limited to a few seconds |
where miner participation in a pool does not change over |
heavy weight solutions like using a TCP connection as the |
miner participation in a pool does not change over time |
weight solutions like using a TCP connection as the preferred |
solutions like using a TCP connection as the preferred transport |
but its structure mirrored that of the traditional Unix systems |
like using a TCP connection as the preferred transport method |
But many kinds of services can handle out of order |
Solo Mining A solo miner is a node that generates |
using a TCP connection as the preferred transport method for |
many kinds of services can handle out of order updates |
and as such it could be considered one of them |
Mining A solo miner is a node that generates its |
a TCP connection as the preferred transport method for each |
if for no other reason than that in many settings |
A solo miner is a node that generates its own |
TCP connection as the preferred transport method for each RPC |
solo miner is a node that generates its own tasks |
connection as the preferred transport method for each RPC call |
have had only minimal impact on the design and implementation |
permitting the service to sort updates and to process queries |
Subsequent cache invalidations can be delayed or even lost due |
had only minimal impact on the design and implementation of |
the service to sort updates and to process queries against |
works on it for the duration of the step and |
cache invalidations can be delayed or even lost due to |
only minimal impact on the design and implementation of commercial |
service to sort updates and to process queries against the |
on it for the duration of the step and if |
invalidations can be delayed or even lost due to race |
Many of these systems are structured as groups of cooperating |
to sort updates and to process queries against the sorted |
it for the duration of the step and if it |
minimal impact on the design and implementation of commercial operating |
can be delayed or even lost due to race conditions |
of these systems are structured as groups of cooperating processes |
sort updates and to process queries against the sorted database |
for the duration of the step and if it finds |
impact on the design and implementation of commercial operating systems |
these systems are structured as groups of cooperating processes using |
leading to a potentially inconsistent view by the cache clients |
the duration of the step and if it finds a |
The design of all Unix systems violates almost all of |
systems are structured as groups of cooperating processes using some |
Our group has held discussions with operators of several large |
duration of the step and if it finds a full |
design of all Unix systems violates almost all of the |
are structured as groups of cooperating processes using some form |
group has held discussions with operators of several large datacenters |
of the step and if it finds a full proof |
of all Unix systems violates almost all of the software |
Online retailers such as Amazon and eBay maintain product stocks |
structured as groups of cooperating processes using some form of |
the step and if it finds a full proof of |
and concluded that many services have the kinds of properties |
all Unix systems violates almost all of the software engineering |
retailers such as Amazon and eBay maintain product stocks and |
as groups of cooperating processes using some form of group |
step and if it finds a full proof of work |
concluded that many services have the kinds of properties just |
Unix systems violates almost all of the software engineering principles |
such as Amazon and eBay maintain product stocks and information |
groups of cooperating processes using some form of group membership |
that many services have the kinds of properties just cited |
it publishes this proof of work to earn the payoff |
systems violates almost all of the software engineering principles presented |
and social networking sites such as Facebook and Twitter maintain |
violates almost all of the software engineering principles presented to |
social networking sites such as Facebook and Twitter maintain graphical |
almost all of the software engineering principles presented to first |
networking sites such as Facebook and Twitter maintain graphical databases |
Pools A pool is a node that serves as a |
all of the software engineering principles presented to first year |
sites such as Facebook and Twitter maintain graphical databases representing |
A pool is a node that serves as a coordinator |
of the software engineering principles presented to first year s |
such as Facebook and Twitter maintain graphical databases representing user |
pool is a node that serves as a coordinator and |
the software engineering principles presented to first year s Computer |
as Facebook and Twitter maintain graphical databases representing user relations |
is a node that serves as a coordinator and multiple |
software engineering principles presented to first year s Computer Science |
Facebook and Twitter maintain graphical databases representing user relations and |
a node that serves as a coordinator and multiple miners |
engineering principles presented to first year s Computer Science students |
and Twitter maintain graphical databases representing user relations and group |
node that serves as a coordinator and multiple miners can |
Twitter maintain graphical databases representing user relations and group structures |
that serves as a coordinator and multiple miners can register |
serves as a coordinator and multiple miners can register to |
and the internal kernel interfaces are not strictly enforced which |
as a coordinator and multiple miners can register to a |
the internal kernel interfaces are not strictly enforced which introduces |
a coordinator and multiple miners can register to a pool |
However in each of these systems the failure management is |
internal kernel interfaces are not strictly enforced which introduces dependencies |
coordinator and multiple miners can register to a pool and |
in each of these systems the failure management is an |
kernel interfaces are not strictly enforced which introduces dependencies on |
and multiple miners can register to a pool and work |
each of these systems the failure management is an integral |
interfaces are not strictly enforced which introduces dependencies on the |
multiple miners can register to a pool and work for |
of these systems the failure management is an integral part |
are not strictly enforced which introduces dependencies on the actual |
miners can register to a pool and work for it |
these systems the failure management is an integral part of |
not strictly enforced which introduces dependencies on the actual implementation |
systems the failure management is an integral part of the |
and all sorts of services in which replies are intrinsically |
strictly enforced which introduces dependencies on the actual implementation of |
In every step it generates a task for each registered |
the failure management is an integral part of the particular |
all sorts of services in which replies are intrinsically noisy |
enforced which introduces dependencies on the actual implementation of data |
placing layers of cache servers in front of the database |
every step it generates a task for each registered miner |
failure management is an integral part of the particular membership |
which introduces dependencies on the actual implementation of data structures |
such as services that report data gathered from remote sensors |
step it generates a task for each registered miner and |
management is an integral part of the particular membership or |
The caches of primary interest to us are typically situated |
it generates a task for each registered miner and sends |
making it impossible to upgrade or replace modules without also |
is an integral part of the particular membership or transport |
caches of primary interest to us are typically situated far |
generates a task for each registered miner and sends it |
a datacenter would also host some kinds of services ill |
it impossible to upgrade or replace modules without also redesigning |
an integral part of the particular membership or transport system |
of primary interest to us are typically situated far from |
a task for each registered miner and sends it over |
impossible to upgrade or replace modules without also redesigning several |
integral part of the particular membership or transport system and |
primary interest to us are typically situated far from the |
task for each registered miner and sends it over the |
to upgrade or replace modules without also redesigning several other |
services running on the SSA can easily interact with services |
part of the particular membership or transport system and not |
interest to us are typically situated far from the backend |
for each registered miner and sends it over the network |
upgrade or replace modules without also redesigning several other modules |
running on the SSA can easily interact with services that |
of the particular membership or transport system and not available |
to us are typically situated far from the backend database |
For example to replace the scheduler in any of the |
Each miner receives its task and works on it for |
the particular membership or transport system and not available for |
on the SSA can easily interact with services that employ |
us are typically situated far from the backend database systems |
example to replace the scheduler in any of the BSD |
miner receives its task and works on it for the |
particular membership or transport system and not available for general |
the SSA can easily interact with services that employ other |
are typically situated far from the backend database systems to |
to replace the scheduler in any of the BSD s |
receives its task and works on it for the duration |
membership or transport system and not available for general use |
SSA can easily interact with services that employ other solutions |
typically situated far from the backend database systems to reduce |
replace the scheduler in any of the BSD s one |
its task and works on it for the duration of |
situated far from the backend database systems to reduce latency |
the scheduler in any of the BSD s one needs |
task and works on it for the duration of the |
scheduler in any of the BSD s one needs to |
and works on it for the duration of the step |
Timeouts are used to ensure that stale cached objects will |
in any of the BSD s one needs to spend |
are used to ensure that stale cached objects will eventually |
an application will only observe an inconsistency if a fault |
any of the BSD s one needs to spend two |
used to ensure that stale cached objects will eventually be |
application will only observe an inconsistency if a fault occurs |
the miner sends the pool the full and the partial |
of the BSD s one needs to spend two weeks |
and even then only for a period of time associated |
miner sends the pool the full and the partial proofs |
to ensure that stale cached objects will eventually be flushed |
the BSD s one needs to spend two weeks searching |
even then only for a period of time associated with |
the majority of the existing failure detectors are not suitable |
sends the pool the full and the partial proofs of |
BSD s one needs to spend two weeks searching for |
then only for a period of time associated with our |
majority of the existing failure detectors are not suitable for |
the pool the full and the partial proofs of work |
the database sends an asynchronous stream of invalidation records or |
s one needs to spend two weeks searching for all |
only for a period of time associated with our repair |
of the existing failure detectors are not suitable for use |
pool the full and the partial proofs of work it |
database sends an asynchronous stream of invalidation records or cache |
one needs to spend two weeks searching for all dependencies |
for a period of time associated with our repair protocol |
the existing failure detectors are not suitable for use in |
the full and the partial proofs of work it has |
and only if it has the bad luck to query |
needs to spend two weeks searching for all dependencies and |
existing failure detectors are not suitable for use in large |
sends an asynchronous stream of invalidation records or cache updates |
full and the partial proofs of work it has found |
only if it has the bad luck to query a |
to spend two weeks searching for all dependencies and fixing |
failure detectors are not suitable for use in large scale |
often using protocols optimized for throughput and freshness and lacking |
if it has the bad luck to query a node |
spend two weeks searching for all dependencies and fixing other |
The pool receives the proofs of work of all its |
detectors are not suitable for use in large scale systems |
using protocols optimized for throughput and freshness and lacking absolute |
it has the bad luck to query a node impacted |
two weeks searching for all dependencies and fixing other sources |
pool receives the proofs of work of all its miners |
protocols optimized for throughput and freshness and lacking absolute guarantees |
has the bad luck to query a node impacted by |
because of their inflexibility or the simplicity of their assumptions |
optimized for throughput and freshness and lacking absolute guarantees of |
At the top of our long wish list for an |
the bad luck to query a node impacted by the |
registers the partial proofs of work and publishes the full |
for throughput and freshness and lacking absolute guarantees of order |
Building a failure detector that is not an integral part |
bad luck to query a node impacted by the failure |
the partial proofs of work and publishes the full proofs |
the top of our long wish list for an ideal |
throughput and freshness and lacking absolute guarantees of order or |
a failure detector that is not an integral part of |
top of our long wish list for an ideal research |
and freshness and lacking absolute guarantees of order or reliability |
failure detector that is not an integral part of the |
of our long wish list for an ideal research operating |
depending upon the cost of inconsistency and the relative value |
Each miner receives revenue proportional to its success in the |
detector that is not an integral part of the communication |
our long wish list for an ideal research operating system |
it is difficult to make this invalidation mechanism reliable without |
miner receives revenue proportional to its success in the current |
that is not an integral part of the communication architecture |
is difficult to make this invalidation mechanism reliable without hampering |
is not an integral part of the communication architecture permits |
of faster response time versus lower risk of an observed |
receives revenue proportional to its success in the current step |
difficult to make this invalidation mechanism reliable without hampering database |
not an integral part of the communication architecture permits the |
faster response time versus lower risk of an observed fault |
to make this invalidation mechanism reliable without hampering database efficiency |
The design and implementation of the operating system should comply |
an integral part of the communication architecture permits the implementation |
namely the ratio of its partial proofs of work out |
design and implementation of the operating system should comply with |
integral part of the communication architecture permits the implementation of |
we measure these windows for scenarios representative of conditions that |
the ratio of its partial proofs of work out of |
and implementation of the operating system should comply with modern |
To the extent that the database keeps track of the |
measure these windows for scenarios representative of conditions that arise |
ratio of its partial proofs of work out of all |
part of the communication architecture permits the implementation of a |
implementation of the operating system should comply with modern software |
the extent that the database keeps track of the caches |
these windows for scenarios representative of conditions that arise in |
of its partial proofs of work out of all partial |
of the communication architecture permits the implementation of a collection |
of the operating system should comply with modern software engineering |
extent that the database keeps track of the caches that |
windows for scenarios representative of conditions that arise in realistic |
its partial proofs of work out of all partial proofs |
the communication architecture permits the implementation of a collection of |
the operating system should comply with modern software engineering principles |
that the database keeps track of the caches that hold |
for scenarios representative of conditions that arise in realistic settings |
partial proofs of work out of all partial proofs of |
communication architecture permits the implementation of a collection of failure |
the database keeps track of the caches that hold a |
proofs of work out of all partial proofs of work |
The SSA Framework The basic operation of the SSA is |
database keeps track of the caches that hold a copy |
architecture permits the implementation of a collection of failure detection |
of work out of all partial proofs of work the |
SSA Framework The basic operation of the SSA is as |
keeps track of the caches that hold a copy of |
permits the implementation of a collection of failure detection techniques |
work out of all partial proofs of work the pool |
Framework The basic operation of the SSA is as follows |
track of the caches that hold a copy of each |
the implementation of a collection of failure detection techniques and |
out of all partial proofs of work the pool received |
of the caches that hold a copy of each object |
implementation of a collection of failure detection techniques and support |
of a collection of failure detection techniques and support for |
We assume that pools do not collect fees of the |
but tracking the state of caches is complicated and hence |
a collection of failure detection techniques and support for failure |
assume that pools do not collect fees of the revenue |
tracking the state of caches is complicated and hence if |
We will use the term subservice rather than RACS in |
collection of failure detection techniques and support for failure detection |
the state of caches is complicated and hence if they |
Pool fees and their implications on our analysis are discussed |
of failure detection techniques and support for failure detection methods |
will use the term subservice rather than RACS in the |
state of caches is complicated and hence if they are |
Our experiences in the past had been that vendors always |
fees and their implications on our analysis are discussed in |
failure detection techniques and support for failure detection methods of |
use the term subservice rather than RACS in the remainder |
of caches is complicated and hence if they are used |
experiences in the past had been that vendors always ignored |
and their implications on our analysis are discussed in Section |
detection techniques and support for failure detection methods of varying |
the term subservice rather than RACS in the remainder of |
caches is complicated and hence if they are used at |
in the past had been that vendors always ignored important |
their implications on our analysis are discussed in Section IX |
techniques and support for failure detection methods of varying levels |
term subservice rather than RACS in the remainder of the |
is complicated and hence if they are used at all |
the past had been that vendors always ignored important research |
and support for failure detection methods of varying levels of |
subservice rather than RACS in the remainder of the paper |
past had been that vendors always ignored important research results |
Block Withholding Miner A miner registered at a pool can |
To create a subservice the developer must first implement a |
had been that vendors always ignored important research results and |
support for failure detection methods of varying levels of complexity |
Withholding Miner A miner registered at a pool can perform |
create a subservice the developer must first implement a non |
been that vendors always ignored important research results and only |
for failure detection methods of varying levels of complexity from |
Miner A miner registered at a pool can perform the |
that vendors always ignored important research results and only followed |
failure detection methods of varying levels of complexity from which |
A miner registered at a pool can perform the classical |
vendors always ignored important research results and only followed very |
detection methods of varying levels of complexity from which the |
miner registered at a pool can perform the classical block |
and the replicas are then linked using TCP to create |
always ignored important research results and only followed very narrow |
methods of varying levels of complexity from which the system |
registered at a pool can perform the classical block withholding |
the replicas are then linked using TCP to create a |
ignored important research results and only followed very narrow paths |
of varying levels of complexity from which the system designer |
at a pool can perform the classical block withholding attack |
replicas are then linked using TCP to create a chain |
A missing invalidation obviously leaves the corresponding cache entry stale |
important research results and only followed very narrow paths of |
varying levels of complexity from which the system designer can |
An attacker miner operates as if it worked for the |
research results and only followed very narrow paths of incremental |
Pitfalls of such invalidation schemes are described in detail by |
levels of complexity from which the system designer can choose |
attacker miner operates as if it worked for the pool |
results and only followed very narrow paths of incremental improvements |
of such invalidation schemes are described in detail by Nishita |
of complexity from which the system designer can choose to |
such invalidation schemes are described in detail by Nishita et |
complexity from which the system designer can choose to match |
Windows NT was the only operating system that came close |
invalidation schemes are described in detail by Nishita et al |
only at the end of each round it sends only |
from which the system designer can choose to match the |
NT was the only operating system that came close to |
at the end of each round it sends only its |
which the system designer can choose to match the system |
was the only operating system that came close to matching |
Gossip based chain replication The replication scheme has evolved out |
the end of each round it sends only its partial |
the system designer can choose to match the system requirements |
the only operating system that came close to matching most |
But forgoing transactional consistency can result in undesired behavior of |
end of each round it sends only its partial proofs |
based chain replication The replication scheme has evolved out of |
only operating system that came close to matching most of |
forgoing transactional consistency can result in undesired behavior of a |
of each round it sends only its partial proofs of |
chain replication The replication scheme has evolved out of the |
operating system that came close to matching most of our |
transactional consistency can result in undesired behavior of a service |
each round it sends only its partial proofs of work |
replication The replication scheme has evolved out of the chain |
system that came close to matching most of our requirements |
Consider a buyer at an online site who looks for |
The replication scheme has evolved out of the chain replication |
and omits full proofs of work if it had found |
a buyer at an online site who looks for a |
replication scheme has evolved out of the chain replication mechanism |
A library that implements simple failure management functionality and provide |
with a handful of operating systems such as QNX and |
omits full proofs of work if it had found any |
buyer at an online site who looks for a toy |
scheme has evolved out of the chain replication mechanism first |
library that implements simple failure management functionality and provide the |
a handful of operating systems such as QNX and Utah |
at an online site who looks for a toy train |
has evolved out of the chain replication mechanism first introduced |
that implements simple failure management functionality and provide the API |
handful of operating systems such as QNX and Utah s |
but cannot distinguish between miners running honestly and block withholding |
an online site who looks for a toy train with |
evolved out of the chain replication mechanism first introduced in |
implements simple failure management functionality and provide the API to |
of operating systems such as QNX and Utah s OS |
cannot distinguish between miners running honestly and block withholding miners |
online site who looks for a toy train with its |
simple failure management functionality and provide the API to the |
site who looks for a toy train with its matching |
The implications are that a miner that engages in block |
failure management functionality and provide the API to the complete |
who looks for a toy train with its matching tracks |
None of the Unix based operating systems came close to |
implications are that a miner that engages in block withholding |
management functionality and provide the API to the complete service |
The original scheme was developed as a means of obtaining |
looks for a toy train with its matching tracks just |
of the Unix based operating systems came close to fulfilling |
are that a miner that engages in block withholding does |
original scheme was developed as a means of obtaining high |
for a toy train with its matching tracks just as |
the Unix based operating systems came close to fulfilling our |
that a miner that engages in block withholding does not |
combining fault management with other local nodes to exploit locality |
scheme was developed as a means of obtaining high throughput |
a toy train with its matching tracks just as the |
Unix based operating systems came close to fulfilling our requirements |
a miner that engages in block withholding does not contribute |
fault management with other local nodes to exploit locality of |
was developed as a means of obtaining high throughput and |
toy train with its matching tracks just as the vendor |
miner that engages in block withholding does not contribute to |
As noted before the core of those operating systems is |
management with other local nodes to exploit locality of communication |
developed as a means of obtaining high throughput and availability |
train with its matching tracks just as the vendor is |
that engages in block withholding does not contribute to the |
noted before the core of those operating systems is based |
with other local nodes to exploit locality of communication and |
as a means of obtaining high throughput and availability for |
with its matching tracks just as the vendor is adding |
engages in block withholding does not contribute to the pool |
before the core of those operating systems is based on |
other local nodes to exploit locality of communication and failure |
a means of obtaining high throughput and availability for query |
its matching tracks just as the vendor is adding them |
in block withholding does not contribute to the pool s |
local nodes to exploit locality of communication and failure patterns |
means of obtaining high throughput and availability for query and |
matching tracks just as the vendor is adding them to |
block withholding does not contribute to the pool s overall |
of obtaining high throughput and availability for query and update |
tracks just as the vendor is adding them to the |
An inquiry service closely coupled with the operating system which |
year old designs and these operating systems still treat computers |
withholding does not contribute to the pool s overall mining |
obtaining high throughput and availability for query and update requests |
just as the vendor is adding them to the database |
old designs and these operating systems still treat computers as |
does not contribute to the pool s overall mining power |
high throughput and availability for query and update requests without |
designs and these operating systems still treat computers as single |
The client may see only the train in stock but |
throughput and availability for query and update requests without sacrificing |
The most fundamental operation offered by a failure detection service |
and these operating systems still treat computers as single entities |
but still shares the pool s revenue according to its |
client may see only the train in stock but not |
and availability for query and update requests without sacrificing strong |
most fundamental operation offered by a failure detection service is |
these operating systems still treat computers as single entities without |
still shares the pool s revenue according to its sent |
may see only the train in stock but not the |
availability for query and update requests without sacrificing strong consistency |
fundamental operation offered by a failure detection service is that |
operating systems still treat computers as single entities without a |
shares the pool s revenue according to its sent partial |
see only the train in stock but not the tracks |
for query and update requests without sacrificing strong consistency guarantees |
operation offered by a failure detection service is that of |
systems still treat computers as single entities without a coherent |
the pool s revenue according to its sent partial proofs |
only the train in stock but not the tracks because |
offered by a failure detection service is that of the |
pool s revenue according to its sent partial proofs of |
the gossip based chain replication behaves in the following manner |
the train in stock but not the tracks because the |
by a failure detection service is that of the investigation |
s revenue according to its sent partial proofs of work |
gossip based chain replication behaves in the following manner during |
train in stock but not the tracks because the product |
a failure detection service is that of the investigation of |
based chain replication behaves in the following manner during normal |
in stock but not the tracks because the product insertion |
To reason about a pool s efficiency we define its |
failure detection service is that of the investigation of a |
chain replication behaves in the following manner during normal operation |
stock but not the tracks because the product insertion transaction |
reason about a pool s efficiency we define its per |
detection service is that of the investigation of a suspected |
replication behaves in the following manner during normal operation when |
but not the tracks because the product insertion transaction would |
service is that of the investigation of a suspected process |
behaves in the following manner during normal operation when nodes |
not the tracks because the product insertion transaction would often |
in the following manner during normal operation when nodes aren |
the tracks because the product insertion transaction would often be |
To make use of this operation it is not necessary |
the following manner during normal operation when nodes aren t |
tracks because the product insertion transaction would often be broken |
The revenue density of a pool is the ratio between |
following manner during normal operation when nodes aren t failing |
Object oriented design is pervasive through the system including the |
because the product insertion transaction would often be broken into |
revenue density of a pool is the ratio between the |
make use of this operation it is not necessary for |
manner during normal operation when nodes aren t failing or |
oriented design is pervasive through the system including the kernel |
the product insertion transaction would often be broken into two |
density of a pool is the ratio between the average |
use of this operation it is not necessary for either |
during normal operation when nodes aren t failing or restarting |
product insertion transaction would often be broken into two or |
of a pool is the ratio between the average revenue |
there is a complete distributed strategy with at its core |
of this operation it is not necessary for either the |
insertion transaction would often be broken into two or more |
Update operations are forwarded to the head of the chain |
a pool is the ratio between the average revenue a |
is a complete distributed strategy with at its core a |
this operation it is not necessary for either the local |
transaction would often be broken into two or more atomic |
The state changes are passed along down the chain to |
a complete distributed strategy with at its core a distributed |
operation it is not necessary for either the local or |
pool is the ratio between the average revenue a pool |
would often be broken into two or more atomic but |
state changes are passed along down the chain to the |
complete distributed strategy with at its core a distributed object |
it is not necessary for either the local or remote |
is the ratio between the average revenue a pool member |
often be broken into two or more atomic but independent |
changes are passed along down the chain to the next |
distributed strategy with at its core a distributed object technology |
is not necessary for either the local or remote process |
the ratio between the average revenue a pool member earns |
be broken into two or more atomic but independent subtransactions |
are passed along down the chain to the next element |
strategy with at its core a distributed object technology and |
not necessary for either the local or remote process to |
ratio between the average revenue a pool member earns and |
with at its core a distributed object technology and includes |
necessary for either the local or remote process to run |
which in turn updates it s state and performs the |
an inconsistency with unexpected results can occur if a user |
between the average revenue a pool member earns and the |
at its core a distributed object technology and includes a |
for either the local or remote process to run any |
in turn updates it s state and performs the same |
inconsistency with unexpected results can occur if a user x |
the average revenue a pool member earns and the average |
its core a distributed object technology and includes a complete |
either the local or remote process to run any of |
turn updates it s state and performs the same operation |
with unexpected results can occur if a user x s |
average revenue a pool member earns and the average revenue |
core a distributed object technology and includes a complete integration |
the local or remote process to run any of the |
updates it s state and performs the same operation until |
unexpected results can occur if a user x s record |
revenue a pool member earns and the average revenue it |
a distributed object technology and includes a complete integration of |
local or remote process to run any of the heartbeat |
it s state and performs the same operation until the |
results can occur if a user x s record says |
a pool member earns and the average revenue it would |
distributed object technology and includes a complete integration of distributed |
or remote process to run any of the heartbeat or |
s state and performs the same operation until the tail |
can occur if a user x s record says it |
pool member earns and the average revenue it would have |
object technology and includes a complete integration of distributed services |
remote process to run any of the heartbeat or polling |
state and performs the same operation until the tail is |
occur if a user x s record says it belongs |
member earns and the average revenue it would have earned |
technology and includes a complete integration of distributed services such |
process to run any of the heartbeat or polling patterns |
and performs the same operation until the tail is reached |
if a user x s record says it belongs to |
earns and the average revenue it would have earned as |
and includes a complete integration of distributed services such as |
a user x s record says it belongs to a |
the reasons that the local process began to suspect the |
and the average revenue it would have earned as a |
Queries can either be directed towards a randomly selected process |
includes a complete integration of distributed services such as security |
user x s record says it belongs to a certain |
reasons that the local process began to suspect the remote |
the average revenue it would have earned as a solo |
can either be directed towards a randomly selected process in |
x s record says it belongs to a certain group |
that the local process began to suspect the remote process |
average revenue it would have earned as a solo miner |
either be directed towards a randomly selected process in the |
the local process began to suspect the remote process are |
be directed towards a randomly selected process in the group |
local process began to suspect the remote process are not |
there is a real desire by the vendor to continuously |
directed towards a randomly selected process in the group or |
and that of a miner working with an unattacked pool |
process began to suspect the remote process are not of |
and it is important that ACL and album updates are |
is a real desire by the vendor to continuously innovate |
towards a randomly selected process in the group or to |
that of a miner working with an unattacked pool are |
began to suspect the remote process are not of any |
it is important that ACL and album updates are consistent |
a real desire by the vendor to continuously innovate its |
a randomly selected process in the group or to a |
the classical example involves removing one s boss from the |
to suspect the remote process are not of any importance |
real desire by the vendor to continuously innovate its operating |
of a miner working with an unattacked pool are one |
randomly selected process in the group or to a specific |
classical example involves removing one s boss from the album |
suspect the remote process are not of any importance to |
desire by the vendor to continuously innovate its operating system |
selected process in the group or to a specific one |
example involves removing one s boss from the album ACL |
the remote process are not of any importance to the |
by the vendor to continuously innovate its operating system and |
involves removing one s boss from the album ACL and |
remote process are not of any importance to the failure |
The strongest consistency guarantee is acheived if all query operations |
Continuous Analysis Because our analysis will be of the average |
the vendor to continuously innovate its operating system and the |
removing one s boss from the album ACL and then |
process are not of any importance to the failure management |
strongest consistency guarantee is acheived if all query operations are |
Analysis Because our analysis will be of the average revenue |
vendor to continuously innovate its operating system and the overall |
one s boss from the album ACL and then adding |
consistency guarantee is acheived if all query operations are targeted |
to continuously innovate its operating system and the overall services |
s boss from the album ACL and then adding unflattering |
guarantee is acheived if all query operations are targeted at |
boss from the album ACL and then adding unflattering pictures |
is acheived if all query operations are targeted at the |
Microsoft doesn t hesitate to incorporate academic results into operating |
Work on a task therefore results in a deterministic fraction |
acheived if all query operations are targeted at the tail |
doesn t hesitate to incorporate academic results into operating system |
on a task therefore results in a deterministic fraction of |
While many of these systems make do with weak consistency |
if all query operations are targeted at the tail of |
The process at address is investigated and a report is |
a task therefore results in a deterministic fraction of proof |
all query operations are targeted at the tail of the |
Innovation as a life style Microsoft is not conservative in |
process at address is investigated and a report is returned |
task therefore results in a deterministic fraction of proof of |
There has been a wave of recent innovations within the |
query operations are targeted at the tail of the chain |
as a life style Microsoft is not conservative in its |
at address is investigated and a report is returned within |
therefore results in a deterministic fraction of proof of work |
has been a wave of recent innovations within the backend |
operations are targeted at the tail of the chain node |
a life style Microsoft is not conservative in its OS |
address is investigated and a report is returned within the |
life style Microsoft is not conservative in its OS development |
offering scalable object stores that can efficiently support transactions through |
which is the case for the vanilla chain replication scheme |
is investigated and a report is returned within the deadline |
The Pool Block Withholding Attack Just as a miner can |
scalable object stores that can efficiently support transactions through snapshot |
While most vendors only consider changes to their core OS |
investigated and a report is returned within the deadline set |
Pool Block Withholding Attack Just as a miner can perform |
object stores that can efficiently support transactions through snapshot isolation |
most vendors only consider changes to their core OS services |
Faults and node restarts can disrupt the primary communication pattern |
and a report is returned within the deadline set by |
Block Withholding Attack Just as a miner can perform block |
stores that can efficiently support transactions through snapshot isolation and |
vendors only consider changes to their core OS services under |
and node restarts can disrupt the primary communication pattern of |
a report is returned within the deadline set by the |
Withholding Attack Just as a miner can perform block withholding |
that can efficiently support transactions through snapshot isolation and even |
only consider changes to their core OS services under extreme |
node restarts can disrupt the primary communication pattern of the |
report is returned within the deadline set by the local |
Attack Just as a miner can perform block withholding on |
can efficiently support transactions through snapshot isolation and even full |
consider changes to their core OS services under extreme market |
restarts can disrupt the primary communication pattern of the SSA |
is returned within the deadline set by the local process |
Just as a miner can perform block withholding on a |
efficiently support transactions through snapshot isolation and even full atomicity |
changes to their core OS services under extreme market pressure |
as a miner can perform block withholding on a pool |
The local process does not have to wait for the |
a miner can perform block withholding on a pool j |
the core of Windows NT has changed significantly over the |
local process does not have to wait for the investigation |
core of Windows NT has changed significantly over the past |
a pool i can use some of its mining power |
process does not have to wait for the investigation to |
of Windows NT has changed significantly over the past years |
pool i can use some of its mining power to |
does not have to wait for the investigation to finish |
Windows NT has changed significantly over the past years to |
i can use some of its mining power to infiltrate |
not have to wait for the investigation to finish but |
NT has changed significantly over the past years to accommodate |
can use some of its mining power to infiltrate a |
processes will miss updates and hence queries will return outdated |
have to wait for the investigation to finish but can |
has changed significantly over the past years to accommodate the |
use some of its mining power to infiltrate a pool |
will miss updates and hence queries will return outdated results |
to wait for the investigation to finish but can make |
changed significantly over the past years to accommodate the demands |
some of its mining power to infiltrate a pool j |
wait for the investigation to finish but can make use |
significantly over the past years to accommodate the demands of |
of its mining power to infiltrate a pool j and |
for the investigation to finish but can make use of |
over the past years to accommodate the demands of modern |
it uses gossip protocols to rapidly detect and repair inconsistencies |
its mining power to infiltrate a pool j and perform |
the investigation to finish but can make use of the |
the past years to accommodate the demands of modern computing |
mining power to infiltrate a pool j and perform a |
investigation to finish but can make use of the asynch |
power to infiltrate a pool j and perform a block |
to finish but can make use of the asynch interface |
with a higher rate overheads rise but repair occurs more |
to infiltrate a pool j and perform a block withholding |
finish but can make use of the asynch interface to |
a higher rate overheads rise but repair occurs more rapidly |
infiltrate a pool j and perform a block withholding attack |
but can make use of the asynch interface to collect |
a pool j and perform a block withholding attack on |
can make use of the asynch interface to collect the |
pool j and perform a block withholding attack on j |
make use of the asynch interface to collect the result |
makes that the Microsoft takes the operating system functionality to |
Our challenge is to improve transaction consistency at the cache |
use of the asynch interface to collect the result at |
The subsections that follow discuss the two core mechanisms in |
that the Microsoft takes the operating system functionality to the |
Denote the amount of such infiltrating mining power at step |
challenge is to improve transaction consistency at the cache layer |
of the asynch interface to collect the result at a |
subsections that follow discuss the two core mechanisms in greater |
the Microsoft takes the operating system functionality to the next |
the amount of such infiltrating mining power at step t |
the asynch interface to collect the result at a later |
that follow discuss the two core mechanisms in greater detail |
Microsoft takes the operating system functionality to the next level |
even when the cache cannot access the backend on each |
amount of such infiltrating mining power at step t by |
asynch interface to collect the result at a later moment |
A second class of faults are transient and relate to |
when the cache cannot access the backend on each read |
of such infiltrating mining power at step t by xi |
second class of faults are transient and relate to the |
The report contains information on whether the remote node was |
class of faults are transient and relate to the behavior |
report contains information on whether the remote node was reachable |
They range from a file system cache for disconnected operation |
of faults are transient and relate to the behavior of |
contains information on whether the remote node was reachable within |
faults are transient and relate to the behavior of TCP |
which was originally developed at CMU in the CODA project |
information on whether the remote node was reachable within the |
are transient and relate to the behavior of TCP when |
on whether the remote node was reachable within the deadline |
today s consistency solutions are limited to the database backend |
pool i aggregates its revenue from mining in the current |
transient and relate to the behavior of TCP when a |
whether the remote node was reachable within the deadline and |
to a remote storage service that automatically moves old data |
i aggregates its revenue from mining in the current round |
and relate to the behavior of TCP when a node |
the remote node was reachable within the deadline and whether |
a remote storage service that automatically moves old data from |
aggregates its revenue from mining in the current round and |
relate to the behavior of TCP when a node is |
remote node was reachable within the deadline and whether the |
remote storage service that automatically moves old data from your |
its revenue from mining in the current round and from |
to the behavior of TCP when a node is subjected |
only transactions issued by edge clients and are at high |
node was reachable within the deadline and whether the process |
storage service that automatically moves old data from your hard |
revenue from mining in the current round and from its |
the behavior of TCP when a node is subjected to |
transactions issued by edge clients and are at high risk |
was reachable within the deadline and whether the process under |
service that automatically moves old data from your hard disk |
from mining in the current round and from its infiltration |
behavior of TCP when a node is subjected to stress |
issued by edge clients and are at high risk of |
reachable within the deadline and whether the process under investigation |
that automatically moves old data from your hard disk to |
mining in the current round and from its infiltration in |
by edge clients and are at high risk of observing |
the OS tends to lose packets and the effect is |
automatically moves old data from your hard disk to remote |
in the current round and from its infiltration in the |
within the deadline and whether the process under investigation was |
edge clients and are at high risk of observing inconsistent |
OS tends to lose packets and the effect is that |
moves old data from your hard disk to remote servers |
the current round and from its infiltration in the previous |
the deadline and whether the process under investigation was still |
clients and are at high risk of observing inconsistent state |
tends to lose packets and the effect is that TCP |
old data from your hard disk to remote servers if |
current round and from its infiltration in the previous round |
deadline and whether the process under investigation was still present |
and are at high risk of observing inconsistent state in |
to lose packets and the effect is that TCP will |
data from your hard disk to remote servers if you |
and whether the process under investigation was still present at |
It distributes the revenue evenly among all its loyal miners |
are at high risk of observing inconsistent state in the |
lose packets and the effect is that TCP will impose |
from your hard disk to remote servers if you are |
whether the process under investigation was still present at the |
distributes the revenue evenly among all its loyal miners according |
at high risk of observing inconsistent state in the cache |
packets and the effect is that TCP will impose congestion |
your hard disk to remote servers if you are running |
the process under investigation was still present at the host |
the revenue evenly among all its loyal miners according to |
and the effect is that TCP will impose congestion control |
hard disk to remote servers if you are running out |
revenue evenly among all its loyal miners according to their |
The outright loss of cache invalidations emerges as an especially |
the effect is that TCP will impose congestion control mechanisms |
If the mode parameter was used to request a more |
disk to remote servers if you are running out of |
evenly among all its loyal miners according to their partial |
outright loss of cache invalidations emerges as an especially significant |
effect is that TCP will impose congestion control mechanisms and |
the mode parameter was used to request a more detailed |
to remote servers if you are running out of disk |
among all its loyal miners according to their partial proofs |
loss of cache invalidations emerges as an especially significant problem |
is that TCP will impose congestion control mechanisms and choke |
mode parameter was used to request a more detailed remote |
remote servers if you are running out of disk space |
all its loyal miners according to their partial proofs of |
of cache invalidations emerges as an especially significant problem if |
that TCP will impose congestion control mechanisms and choke back |
parameter was used to request a more detailed remote reporting |
its loyal miners according to their partial proofs of work |
cache invalidations emerges as an especially significant problem if transactional |
process checkpoint information is returned or the remote process is |
even though most of the nodes involved could still have |
The pool s miners are oblivious to their role and |
invalidations emerges as an especially significant problem if transactional consistency |
to a complete integration of network quality of services tools |
checkpoint information is returned or the remote process is interrupted |
though most of the nodes involved could still have ample |
pool s miners are oblivious to their role and they |
emerges as an especially significant problem if transactional consistency is |
a complete integration of network quality of services tools including |
information is returned or the remote process is interrupted to |
most of the nodes involved could still have ample capacity |
s miners are oblivious to their role and they operate |
as an especially significant problem if transactional consistency is required |
complete integration of network quality of services tools including data |
is returned or the remote process is interrupted to provide |
miners are oblivious to their role and they operate as |
integration of network quality of services tools including data transmission |
returned or the remote process is interrupted to provide status |
An acceptable solution for a consistent cache must maintain the |
are oblivious to their role and they operate as regular |
of network quality of services tools including data transmission shapers |
and will also deliver missed updates to the overloaded nodes |
or the remote process is interrupted to provide status information |
acceptable solution for a consistent cache must maintain the performance |
oblivious to their role and they operate as regular honest |
network quality of services tools including data transmission shapers and |
will also deliver missed updates to the overloaded nodes when |
solution for a consistent cache must maintain the performance properties |
to their role and they operate as regular honest miners |
quality of services tools including data transmission shapers and priority |
also deliver missed updates to the overloaded nodes when the |
for a consistent cache must maintain the performance properties of |
If the node was not reachable and the local process |
of services tools including data transmission shapers and priority scheduling |
deliver missed updates to the overloaded nodes when the problem |
a consistent cache must maintain the performance properties of the |
the node was not reachable and the local process has |
services tools including data transmission shapers and priority scheduling and |
Revenue Convergence Note that pool j sends its revenue to |
missed updates to the overloaded nodes when the problem ends |
consistent cache must maintain the performance properties of the existing |
node was not reachable and the local process has requested |
tools including data transmission shapers and priority scheduling and queuing |
Convergence Note that pool j sends its revenue to infiltrators |
cache must maintain the performance properties of the existing caching |
In the original chain replication scheme the queries are directed |
was not reachable and the local process has requested extensive |
Note that pool j sends its revenue to infiltrators from |
and from attributed based distributed component programming to indexing support |
must maintain the performance properties of the existing caching tier |
the original chain replication scheme the queries are directed to |
not reachable and the local process has requested extensive investigation |
that pool j sends its revenue to infiltrators from pool |
from attributed based distributed component programming to indexing support integrated |
the failure investigator will try to contact a failure manager |
pool j sends its revenue to infiltrators from pool i |
original chain replication scheme the queries are directed to the |
attributed based distributed component programming to indexing support integrated in |
we need to maintain the shielding role of the cache |
failure investigator will try to contact a failure manager at |
j sends its revenue to infiltrators from pool i at |
chain replication scheme the queries are directed to the tail |
based distributed component programming to indexing support integrated in the |
investigator will try to contact a failure manager at the |
sends its revenue to infiltrators from pool i at the |
replication scheme the queries are directed to the tail of |
distributed component programming to indexing support integrated in the file |
will try to contact a failure manager at the node |
its revenue to infiltrators from pool i at the end |
scheme the queries are directed to the tail of the |
component programming to indexing support integrated in the file system |
revenue to infiltrators from pool i at the end of |
net or within its administrative domain which should be able |
the queries are directed to the tail of the chain |
to infiltrators from pool i at the end of the |
or within its administrative domain which should be able to |
infiltrators from pool i at the end of the step |
within its administrative domain which should be able to give |
never before have we seen such a radical overhaul of |
any update known to the tail is stable because it |
its administrative domain which should be able to give a |
before have we seen such a radical overhaul of an |
and this revenue is calculated in pool i at the |
update known to the tail is stable because it must |
administrative domain which should be able to give a more |
A RCHITECTURE Since the cache is required to respond immediately |
this revenue is calculated in pool i at the beginning |
known to the tail is stable because it must first |
have we seen such a radical overhaul of an operating |
domain which should be able to give a more conclusive |
RCHITECTURE Since the cache is required to respond immediately to |
revenue is calculated in pool i at the beginning of |
to the tail is stable because it must first have |
we seen such a radical overhaul of an operating system |
which should be able to give a more conclusive answer |
Since the cache is required to respond immediately to the |
is calculated in pool i at the beginning of the |
the tail is stable because it must first have been |
seen such a radical overhaul of an operating system targeted |
should be able to give a more conclusive answer about |
the cache is required to respond immediately to the client |
calculated in pool i at the beginning of the subsequent |
tail is stable because it must first have been seen |
such a radical overhaul of an operating system targeted for |
be able to give a more conclusive answer about the |
cache is required to respond immediately to the client on |
in pool i at the beginning of the subsequent step |
is stable because it must first have been seen by |
a radical overhaul of an operating system targeted for the |
able to give a more conclusive answer about the node |
is required to respond immediately to the client on hits |
stable because it must first have been seen by all |
radical overhaul of an operating system targeted for the enterprise |
because it must first have been seen by all the |
overhaul of an operating system targeted for the enterprise market |
If network failure is the cause of the loss of |
it must first have been seen by all the members |
since the revenue from infiltration takes one step to take |
network failure is the cause of the loss of connectivity |
must first have been seen by all the members of |
In general this market is very conservative and not interested |
we decided to employ a transactional consistency that is weaker |
the revenue from infiltration takes one step to take each |
first have been seen by all the members of the |
the report will indicate which part of the path is |
general this market is very conservative and not interested in |
decided to employ a transactional consistency that is weaker than |
revenue from infiltration takes one step to take each hop |
have been seen by all the members of the chain |
report will indicate which part of the path is reachable |
this market is very conservative and not interested in taking |
to employ a transactional consistency that is weaker than the |
will indicate which part of the path is reachable and |
market is very conservative and not interested in taking risks |
employ a transactional consistency that is weaker than the full |
the original paper includes mechanisms to ensure that a request |
indicate which part of the path is reachable and where |
a transactional consistency that is weaker than the full ACID |
original paper includes mechanisms to ensure that a request really |
which part of the path is reachable and where the |
management and distribution are asking for radical solutions to get |
transactional consistency that is weaker than the full ACID model |
paper includes mechanisms to ensure that a request really reaches |
part of the path is reachable and where the suspected |
and distribution are asking for radical solutions to get to |
includes mechanisms to ensure that a request really reaches the |
distribution are asking for radical solutions to get to a |
mechanisms to ensure that a request really reaches the head |
are asking for radical solutions to get to a computing |
to ensure that a request really reaches the head of |
only transactions and update transactions that access the same cache |
asking for radical solutions to get to a computing base |
If the failure investigator is configured with alternative outgoing paths |
ensure that a request really reaches the head of the |
transactions and update transactions that access the same cache are |
for radical solutions to get to a computing base that |
Denote the revenue density of pool i at the end |
that a request really reaches the head of the chain |
and update transactions that access the same cache are guaranteed |
these paths are probed to see if it is possible |
radical solutions to get to a computing base that can |
the revenue density of pool i at the end of |
update transactions that access the same cache are guaranteed an |
paths are probed to see if it is possible to |
that updates are passed down the chain and applied in |
solutions to get to a computing base that can bring |
revenue density of pool i at the end of step |
transactions that access the same cache are guaranteed an atomic |
are probed to see if it is possible to circumvent |
updates are passed down the chain and applied in a |
to get to a computing base that can bring us |
density of pool i at the end of step t |
that access the same cache are guaranteed an atomic execution |
probed to see if it is possible to circumvent the |
are passed down the chain and applied in a strictly |
get to a computing base that can bring us into |
of pool i at the end of step t by |
to see if it is possible to circumvent the network |
only transactions that access different caches may observe different orderings |
passed down the chain and applied in a strictly FIFO |
to a computing base that can bring us into the |
pool i at the end of step t by ri |
see if it is possible to circumvent the network failure |
transactions that access different caches may observe different orderings for |
down the chain and applied in a strictly FIFO manner |
a computing base that can bring us into the next |
if it is possible to circumvent the network failure and |
that access different caches may observe different orderings for independent |
the chain and applied in a strictly FIFO manner even |
computing base that can bring us into the next century |
it is possible to circumvent the network failure and in |
access different caches may observe different orderings for independent update |
different caches may observe different orderings for independent update transactions |
is possible to circumvent the network failure and in such |
chain and applied in a strictly FIFO manner even when |
One of the markets where we will see the main |
possible to circumvent the network failure and in such a |
and applied in a strictly FIFO manner even when nodes |
of the markets where we will see the main competitive |
every partial execution that includes all update transactions in and |
to circumvent the network failure and in such a way |
applied in a strictly FIFO manner even when nodes fail |
the markets where we will see the main competitive battle |
partial execution that includes all update transactions in and all |
circumvent the network failure and in such a way collect |
in a strictly FIFO manner even when nodes fail and |
markets where we will see the main competitive battle between |
execution that includes all update transactions in and all read |
the network failure and in such a way collect information |
a strictly FIFO manner even when nodes fail and the |
where we will see the main competitive battle between Microsoft |
network failure and in such a way collect information about |
Our solution seeks to approximate cache serializability with bounded caches |
strictly FIFO manner even when nodes fail and the chain |
we will see the main competitive battle between Microsoft and |
failure and in such a way collect information about the |
solution seeks to approximate cache serializability with bounded caches and |
FIFO manner even when nodes fail and the chain is |
will see the main competitive battle between Microsoft and others |
and in such a way collect information about the remote |
seeks to approximate cache serializability with bounded caches and asynchronous |
manner even when nodes fail and the chain is restructured |
see the main competitive battle between Microsoft and others will |
in such a way collect information about the remote process |
to approximate cache serializability with bounded caches and asynchronous communication |
the main competitive battle between Microsoft and others will be |
approximate cache serializability with bounded caches and asynchronous communication with |
and that queries are sent to the tail of the |
main competitive battle between Microsoft and others will be that |
cache serializability with bounded caches and asynchronous communication with the |
The report contains information about the results of these probes |
that queries are sent to the tail of the chain |
competitive battle between Microsoft and others will be that of |
serializability with bounded caches and asynchronous communication with the DB |
Early triggers Many systems find it desirable to detect failure |
battle between Microsoft and others will be that of the |
triggers Many systems find it desirable to detect failure of |
between Microsoft and others will be that of the E |
Strong consistency follows easily because query requests and update requests |
Many systems find it desirable to detect failure of remote |
consistency follows easily because query requests and update requests are |
systems find it desirable to detect failure of remote processes |
and for web albums the ACL objects and the pictures |
follows easily because query requests and update requests are processed |
find it desirable to detect failure of remote processes even |
for web albums the ACL objects and the pictures assigned |
web albums the ACL objects and the pictures assigned to |
it desirable to detect failure of remote processes even if |
easily because query requests and update requests are processed serially |
albums the ACL objects and the pictures assigned to them |
because query requests and update requests are processed serially at |
desirable to detect failure of remote processes even if there |
in some cases applications explicitly cluster their data accesses to |
query requests and update requests are processed serially at the |
to detect failure of remote processes even if there is |
some cases applications explicitly cluster their data accesses to benefit |
requests and update requests are processed serially at the tail |
detect failure of remote processes even if there is no |
cases applications explicitly cluster their data accesses to benefit from |
are really pushing the envelope of all operating systems that |
and update requests are processed serially at the tail element |
failure of remote processes even if there is no data |
applications explicitly cluster their data accesses to benefit from improved |
really pushing the envelope of all operating systems that are |
of remote processes even if there is no data exchange |
explicitly cluster their data accesses to benefit from improved parallelism |
pushing the envelope of all operating systems that are currently |
remote processes even if there is no data exchange actually |
The gossip based chain replication weakens the model in two |
although there will also be some frequency of transactions that |
gossip based chain replication weakens the model in two key |
the envelope of all operating systems that are currently on |
processes even if there is no data exchange actually under |
there will also be some frequency of transactions that access |
based chain replication weakens the model in two key respects |
envelope of all operating systems that are currently on the |
even if there is no data exchange actually under way |
will also be some frequency of transactions that access unrelated |
of all operating systems that are currently on the market |
also be some frequency of transactions that access unrelated objects |
be some frequency of transactions that access unrelated objects in |
Systems are free to implement whatever scheme they find appropriate |
some frequency of transactions that access unrelated objects in different |
are free to implement whatever scheme they find appropriate and |
Windows NT is still considered to be the new kid |
frequency of transactions that access unrelated objects in different clusters |
free to implement whatever scheme they find appropriate and use |
our solution might sometimes use the wrong head of the |
NT is still considered to be the new kid on |
p The revenue of Pool i in step t taken |
Our solution requires minor changes to the database object representation |
to implement whatever scheme they find appropriate and use the |
solution might sometimes use the wrong head of the chain |
is still considered to be the new kid on the |
The revenue of Pool i in step t taken through |
solution requires minor changes to the database object representation format |
implement whatever scheme they find appropriate and use the failure |
still considered to be the new kid on the block |
revenue of Pool i in step t taken through infiltration |
for example if an update source is operating with inaccurate |
whatever scheme they find appropriate and use the failure investigator |
considered to be the new kid on the block in |
of Pool i in step t taken through infiltration from |
This overhead involves tracking and caching what we refer to |
example if an update source is operating with inaccurate membership |
scheme they find appropriate and use the failure investigator from |
to be the new kid on the block in the |
Pool i in step t taken through infiltration from pool |
overhead involves tracking and caching what we refer to as |
i in step t taken through infiltration from pool j |
they find appropriate and use the failure investigator from the |
be the new kid on the block in the Internet |
if an update source is operating with inaccurate membership information |
involves tracking and caching what we refer to as dependency |
in step t taken through infiltration from pool j s |
find appropriate and use the failure investigator from the previous |
the new kid on the block in the Internet services |
tracking and caching what we refer to as dependency lists |
for example if the chain is disrupted by a failure |
appropriate and use the failure investigator from the previous section |
new kid on the block in the Internet services world |
step t taken through infiltration from pool j s revenue |
example if the chain is disrupted by a failure and |
and use the failure investigator from the previous section to |
length lists of object identifiers and the associated version numbers |
t taken through infiltration from pool j s revenue in |
if the chain is disrupted by a failure and some |
but it is clear that the risks that are taken |
use the failure investigator from the previous section to handle |
taken through infiltration from pool j s revenue in step |
each representing some recently updated objects upon which the cached |
the chain is disrupted by a failure and some updates |
it is clear that the risks that are taken now |
the failure investigator from the previous section to handle the |
through infiltration from pool j s revenue in step t |
representing some recently updated objects upon which the cached object |
chain is disrupted by a failure and some updates arrive |
is clear that the risks that are taken now are |
failure investigator from the previous section to handle the suspicions |
some recently updated objects upon which the cached object depends |
is disrupted by a failure and some updates arrive via |
clear that the risks that are taken now are the |
disrupted by a failure and some updates arrive via the |
or they can make use of two standardized schemes implemented |
sized list can omit dependency information required to detect inconsistencies |
that the risks that are taken now are the right |
by a failure and some updates arrive via the gossip |
they can make use of two standardized schemes implemented by |
hence it is important to use a bound large enough |
the risks that are taken now are the right moves |
a failure and some updates arrive via the gossip protocol |
can make use of two standardized schemes implemented by the |
it is important to use a bound large enough to |
risks that are taken now are the right moves to |
make use of two standardized schemes implemented by the failure |
is important to use a bound large enough to capture |
These changes substantially simplify the algorithm but they also weaken |
that are taken now are the right moves to prepare |
use of two standardized schemes implemented by the failure manager |
important to use a bound large enough to capture most |
changes substantially simplify the algorithm but they also weaken the |
are taken now are the right moves to prepare the |
of two standardized schemes implemented by the failure manager library |
to use a bound large enough to capture most of |
i ij And the revenue vector at step t is |
substantially simplify the algorithm but they also weaken the properties |
taken now are the right moves to prepare the operating |
use a bound large enough to capture most of the |
ij And the revenue vector at step t is r |
simplify the algorithm but they also weaken the properties of |
now are the right moves to prepare the operating system |
a bound large enough to capture most of the relevant |
the algorithm but they also weaken the properties of the |
are the right moves to prepare the operating system for |
bound large enough to capture most of the relevant dependencies |
alive messages to a group of processes using multiple point |
algorithm but they also weaken the properties of the solution |
the right moves to prepare the operating system for operation |
At present we lack an automated way to do this |
right moves to prepare the operating system for operation in |
moves to prepare the operating system for operation in these |
we require the developer to tune the length so that |
to prepare the operating system for operation in these emerging |
require the developer to tune the length so that the |
In the pool game pools try to optimize their infiltration |
prepare the operating system for operation in these emerging massive |
the developer to tune the length so that the frequency |
but in ways that seem to match the class of |
Each process keeps track of the reception times of messages |
the pool game pools try to optimize their infiltration rates |
the operating system for operation in these emerging massive computing |
developer to tune the length so that the frequency of |
in ways that seem to match the class of applications |
process keeps track of the reception times of messages and |
pool game pools try to optimize their infiltration rates of |
operating system for operation in these emerging massive computing environments |
to tune the length so that the frequency of errors |
ways that seem to match the class of applications of |
keeps track of the reception times of messages and if |
game pools try to optimize their infiltration rates of other |
tune the length so that the frequency of errors is |
that seem to match the class of applications of interest |
track of the reception times of messages and if a |
pools try to optimize their infiltration rates of other pools |
the length so that the frequency of errors is reduced |
One of the costs of introducing a significant amount of |
of the reception times of messages and if a number |
try to optimize their infiltration rates of other pools to |
length so that the frequency of errors is reduced to |
of the costs of introducing a significant amount of new |
the reception times of messages and if a number of |
to optimize their infiltration rates of other pools to maximize |
so that the frequency of errors is reduced to an |
the costs of introducing a significant amount of new code |
reception times of messages and if a number of consecutive |
optimize their infiltration rates of other pools to maximize their |
SSA uses gossip to detect and repair the inconsistencies that |
that the frequency of errors is reduced to an acceptable |
costs of introducing a significant amount of new code is |
times of messages and if a number of consecutive heartbeats |
their infiltration rates of other pools to maximize their revenue |
uses gossip to detect and repair the inconsistencies that can |
the frequency of errors is reduced to an acceptable level |
of introducing a significant amount of new code is the |
of messages and if a number of consecutive heartbeats from |
gossip to detect and repair the inconsistencies that can arise |
The overall number of miners and the number of miners |
introducing a significant amount of new code is the number |
messages and if a number of consecutive heartbeats from a |
to detect and repair the inconsistencies that can arise after |
overall number of miners and the number of miners loyal |
a significant amount of new code is the number of |
and if a number of consecutive heartbeats from a destination |
dependency lists should be roughly the same size as the |
detect and repair the inconsistencies that can arise after a |
number of miners and the number of miners loyal to |
significant amount of new code is the number of software |
if a number of consecutive heartbeats from a destination is |
lists should be roughly the same size as the size |
and repair the inconsistencies that can arise after a failure |
of miners and the number of miners loyal to each |
amount of new code is the number of software defects |
a number of consecutive heartbeats from a destination is missed |
should be roughly the same size as the size of |
repair the inconsistencies that can arise after a failure or |
miners and the number of miners loyal to each pool |
of new code is the number of software defects per |
number of consecutive heartbeats from a destination is missed a |
be roughly the same size as the size of the |
the inconsistencies that can arise after a failure or when |
and the number of miners loyal to each pool remain |
new code is the number of software defects per lines |
of consecutive heartbeats from a destination is missed a suspicion |
roughly the same size as the size of the workload |
inconsistencies that can arise after a failure or when a |
the number of miners loyal to each pool remain constant |
code is the number of software defects per lines of |
consecutive heartbeats from a destination is missed a suspicion is |
the same size as the size of the workload s |
that can arise after a failure or when a node |
number of miners loyal to each pool remain constant throughout |
is the number of software defects per lines of codes |
heartbeats from a destination is missed a suspicion is raised |
same size as the size of the workload s clusters |
can arise after a failure or when a node joins |
of miners loyal to each pool remain constant throughout the |
the number of software defects per lines of codes increases |
Our extensions offer a transactional interface to the cache in |
miners loyal to each pool remain constant throughout the game |
extensions offer a transactional interface to the cache in addition |
each process in the system runs a periodic local timer |
offer a transactional interface to the cache in addition to |
While measurements actually let us believe that Microsoft products are |
a transactional interface to the cache in addition to the |
measurements actually let us believe that Microsoft products are quite |
Let s be a constant integer large enough that revenue |
transactional interface to the cache in addition to the standard |
actually let us believe that Microsoft products are quite reliable |
s be a constant integer large enough that revenue can |
interface to the cache in addition to the standard read |
let us believe that Microsoft products are quite reliable at |
be a constant integer large enough that revenue can be |
us believe that Microsoft products are quite reliable at Operating |
a constant integer large enough that revenue can be approximated |
believe that Microsoft products are quite reliable at Operating Systems |
The application can provide application specific data to be piggybacked |
constant integer large enough that revenue can be approximated as |
application can provide application specific data to be piggybacked on |
integer large enough that revenue can be approximated as its |
can provide application specific data to be piggybacked on the |
or invalidating a cached object which can then force a |
large enough that revenue can be approximated as its convergence |
provide application specific data to be piggybacked on the heartbeats |
Quick delivery is more important than reliability for gossip messages |
invalidating a cached object which can then force a read |
enough that revenue can be approximated as its convergence limit |
a cached object which can then force a read from |
The second scheme uses a polling method to collect acknowledgments |
hence we favor UDP datagrams over TCP for this kind |
cached object which can then force a read from the |
The outlook becomes even more worrisome when we realize that |
second scheme uses a polling method to collect acknowledgments from |
In each round the system takes s steps and then |
we favor UDP datagrams over TCP for this kind of |
object which can then force a read from the database |
outlook becomes even more worrisome when we realize that Microsoft |
scheme uses a polling method to collect acknowledgments from the |
each round the system takes s steps and then a |
favor UDP datagrams over TCP for this kind of communication |
becomes even more worrisome when we realize that Microsoft is |
uses a polling method to collect acknowledgments from the peer |
round the system takes s steps and then a single |
When the dependency lists fail to document a necessary dependency |
even more worrisome when we realize that Microsoft is not |
The recipient compares the gossiped information with its own state |
a polling method to collect acknowledgments from the peer processes |
the system takes s steps and then a single pool |
identifying information known to the sender but unknown to itself |
side applications that are unlikely to validate against the back |
more worrisome when we realize that Microsoft is not only |
if no acknowledgments are received after a number of retries |
or known to it but apparently unknown to the sender |
worrisome when we realize that Microsoft is not only introducing |
no acknowledgments are received after a number of retries a |
for many of our intended uses some level of undetected |
when we realize that Microsoft is not only introducing new |
acknowledgments are received after a number of retries a suspicion |
many of our intended uses some level of undetected inconsistency |
we realize that Microsoft is not only introducing new code |
are received after a number of retries a suspicion is |
of our intended uses some level of undetected inconsistency can |
containing information the sender might find useful and requesting information |
received after a number of retries a suspicion is raised |
our intended uses some level of undetected inconsistency can slip |
The pool taking a step knows the rate of infiltrators |
information the sender might find useful and requesting information it |
An automated process is converting all of the Windows NT |
intended uses some level of undetected inconsistency can slip past |
pool taking a step knows the rate of infiltrators attacking |
the sender might find useful and requesting information it lacks |
automated process is converting all of the Windows NT code |
taking a step knows the rate of infiltrators attacking it |
and retransmission limits are configurable by the application or can |
because the developer would often be able to tune the |
process is converting all of the Windows NT code to |
retransmission limits are configurable by the application or can be |
and the revenue rates of each of the other pools |
the originator of the exchange will send a final message |
is converting all of the Windows NT code to be |
This knowledge is required to optimize a pool s revenue |
limits are configurable by the application or can be adapted |
originator of the exchange will send a final message containing |
the developer would often be able to tune the mechanism |
are configurable by the application or can be adapted by |
We explain in Section VIII how a pool can technically |
of the exchange will send a final message containing any |
configurable by the application or can be adapted by the |
explain in Section VIII how a pool can technically obtain |
the exchange will send a final message containing any data |
by the application or can be adapted by the failure |
thousand lines of code per day and is believed to |
in Section VIII how a pool can technically obtain this |
With clustered workloads we will demonstrate that it is sufficient |
exchange will send a final message containing any data that |
the application or can be adapted by the failure manager |
lines of code per day and is believed to catch |
Section VIII how a pool can technically obtain this knowledge |
clustered workloads we will demonstrate that it is sufficient to |
will send a final message containing any data that was |
application or can be adapted by the failure manager to |
of code per day and is believed to catch all |
workloads we will demonstrate that it is sufficient to store |
send a final message containing any data that was solicited |
or can be adapted by the failure manager to the |
code per day and is believed to catch all pointer |
General Analysis Recall that mi is the number of miners |
we will demonstrate that it is sufficient to store a |
a final message containing any data that was solicited by |
can be adapted by the failure manager to the network |
per day and is believed to catch all pointer arithmetic |
Analysis Recall that mi is the number of miners loyal |
will demonstrate that it is sufficient to store a small |
final message containing any data that was solicited by the |
day and is believed to catch all pointer arithmetic cases |
Recall that mi is the number of miners loyal to |
it is necessary to instrument the operating environment with support |
message containing any data that was solicited by the receiver |
demonstrate that it is sufficient to store a small set |
that mi is the number of miners loyal to pool |
is necessary to instrument the operating environment with support for |
An important question is whether the introduced functionality is worth |
Thus during a round each process will send a message |
mi is the number of miners loyal to pool i |
necessary to instrument the operating environment with support for process |
that it is sufficient to store a small set of |
important question is whether the introduced functionality is worth the |
to instrument the operating environment with support for process investigation |
it is sufficient to store a small set of dependencies |
question is whether the introduced functionality is worth the unavoidable |
is sufficient to store a small set of dependencies to |
is whether the introduced functionality is worth the unavoidable initial |
It has always been argued that in a distributed system |
sufficient to store a small set of dependencies to detect |
is the number of miners used by pool i to |
whether the introduced functionality is worth the unavoidable initial instability |
has always been argued that in a distributed system it |
to store a small set of dependencies to detect most |
The load imposed on the network will thus be linear |
the number of miners used by pool i to infiltrate |
the introduced functionality is worth the unavoidable initial instability that |
always been argued that in a distributed system it is |
store a small set of dependencies to detect most inconsistencies |
load imposed on the network will thus be linear in |
number of miners used by pool i to infiltrate pool |
introduced functionality is worth the unavoidable initial instability that is |
been argued that in a distributed system it is impossible |
imposed on the network will thus be linear in the |
We also investigate workloads where the clustered access pattern is |
of miners used by pool i to infiltrate pool j |
functionality is worth the unavoidable initial instability that is bound |
argued that in a distributed system it is impossible to |
on the network will thus be linear in the number |
also investigate workloads where the clustered access pattern is less |
miners used by pool i to infiltrate pool j at |
is worth the unavoidable initial instability that is bound to |
that in a distributed system it is impossible to distinguish |
the network will thus be linear in the number of |
investigate workloads where the clustered access pattern is less strongly |
used by pool i to infiltrate pool j at step |
worth the unavoidable initial instability that is bound to occur |
in a distributed system it is impossible to distinguish a |
network will thus be linear in the number of processes |
workloads where the clustered access pattern is less strongly evident |
by pool i to infiltrate pool j at step t |
a distributed system it is impossible to distinguish a crashed |
there is always the down side that there is some |
distributed system it is impossible to distinguish a crashed process |
our approach is less effective even with longer dependency list |
The mining rate of pool i is therefore the number |
is always the down side that there is some change |
system it is impossible to distinguish a crashed process from |
approach is less effective even with longer dependency list lengths |
mining rate of pool i is therefore the number of |
always the down side that there is some change of |
it is impossible to distinguish a crashed process from one |
rate of pool i is therefore the number of its |
the down side that there is some change of failure |
is impossible to distinguish a crashed process from one that |
of pool i is therefore the number of its loyal |
down side that there is some change of failure and |
impossible to distinguish a crashed process from one that is |
if a fault breaks a chain or disables the head |
pool i is therefore the number of its loyal miners |
Database We assume that the database tags each object with |
to distinguish a crashed process from one that is slow |
a fault breaks a chain or disables the head of |
side that there is some change of failure and it |
i is therefore the number of its loyal miners minus |
We assume that the database tags each object with a |
fault breaks a chain or disables the head of a |
that there is some change of failure and it is |
is therefore the number of its loyal miners minus the |
but with the proper system support this is no longer |
breaks a chain or disables the head of a chain |
there is some change of failure and it is likely |
assume that the database tags each object with a version |
therefore the number of its loyal miners minus the miners |
with the proper system support this is no longer true |
is some change of failure and it is likely that |
that the database tags each object with a version number |
gossip is used to detect the problem and repair involves |
the number of its loyal miners minus the miners it |
some change of failure and it is likely that we |
the database tags each object with a version number specific |
is used to detect the problem and repair involves designating |
the operating system can determine whether or not the process |
number of its loyal miners minus the miners it uses |
change of failure and it is likely that we will |
database tags each object with a version number specific to |
used to detect the problem and repair involves designating a |
operating system can determine whether or not the process has |
of its loyal miners minus the miners it uses for |
of failure and it is likely that we will see |
tags each object with a version number specific to the |
to detect the problem and repair involves designating a new |
system can determine whether or not the process has crashed |
its loyal miners minus the miners it uses for infiltration |
failure and it is likely that we will see a |
each object with a version number specific to the transaction |
detect the problem and repair involves designating a new head |
and it is likely that we will see a number |
The failure management integrated into the OS offers processes a |
object with a version number specific to the transaction that |
This effective mining rate is divided by the total mining |
the problem and repair involves designating a new head for |
it is likely that we will see a number of |
failure management integrated into the OS offers processes a mechanism |
with a version number specific to the transaction that most |
effective mining rate is divided by the total mining rate |
problem and repair involves designating a new head for the |
is likely that we will see a number of components |
management integrated into the OS offers processes a mechanism to |
a version number specific to the transaction that most recently |
mining rate is divided by the total mining rate in |
and repair involves designating a new head for the chain |
likely that we will see a number of components of |
integrated into the OS offers processes a mechanism to register |
version number specific to the transaction that most recently updated |
rate is divided by the total mining rate in the |
repair involves designating a new head for the chain or |
that we will see a number of components of NT |
into the OS offers processes a mechanism to register and |
number specific to the transaction that most recently updated it |
is divided by the total mining rate in the system |
involves designating a new head for the chain or establishing |
we will see a number of components of NT coming |
the OS offers processes a mechanism to register and request |
and that there is a total ordering on version numbers |
designating a new head for the chain or establishing a |
will see a number of components of NT coming under |
namely the number of all miners that do not engage |
OS offers processes a mechanism to register and request a |
a new head for the chain or establishing a new |
see a number of components of NT coming under intense |
The version of a transaction is chosen to be larger |
the number of all miners that do not engage in |
offers processes a mechanism to register and request a certain |
new head for the chain or establishing a new TCP |
a number of components of NT coming under intense scrutiny |
version of a transaction is chosen to be larger than |
number of all miners that do not engage in block |
processes a mechanism to register and request a certain level |
head for the chain or establishing a new TCP connection |
number of components of NT coming under intense scrutiny from |
of a transaction is chosen to be larger than the |
of all miners that do not engage in block withholding |
a mechanism to register and request a certain level of |
for the chain or establishing a new TCP connection bridging |
of components of NT coming under intense scrutiny from industry |
a transaction is chosen to be larger than the versions |
mechanism to register and request a certain level of service |
the chain or establishing a new TCP connection bridging the |
components of NT coming under intense scrutiny from industry and |
Denote the direct mining rate of pool i at step |
transaction is chosen to be larger than the versions of |
chain or establishing a new TCP connection bridging the gap |
of NT coming under intense scrutiny from industry and academia |
the direct mining rate of pool i at step t |
is chosen to be larger than the versions of all |
is a simple binary test performed by the OS upon |
direct mining rate of pool i at step t by |
chosen to be larger than the versions of all objects |
a simple binary test performed by the OS upon receipt |
mining rate of pool i at step t by Pp |
to be larger than the versions of all objects accessed |
We assume that all forms of information are uniquely named |
may become a performance bottleneck in the overall distributed operation |
simple binary test performed by the OS upon receipt of |
rate of pool i at step t by Pp mi |
be larger than the versions of all objects accessed by |
assume that all forms of information are uniquely named and |
binary test performed by the OS upon receipt of an |
of pool i at step t by Pp mi j |
or the wide spread security integration could introduce a critical |
larger than the versions of all objects accessed by the |
that all forms of information are uniquely named and that |
test performed by the OS upon receipt of an inquiry |
the wide spread security integration could introduce a critical dependency |
than the versions of all objects accessed by the transaction |
all forms of information are uniquely named and that updates |
wide spread security integration could introduce a critical dependency on |
The database stores for each object o a list of |
forms of information are uniquely named and that updates are |
indicating whether the process is still present in the process |
spread security integration could introduce a critical dependency on the |
database stores for each object o a list of k |
of information are uniquely named and that updates are ordered |
whether the process is still present in the process table |
security integration could introduce a critical dependency on the high |
stores for each object o a list of k dependencies |
information are uniquely named and that updates are ordered separately |
the process is still present in the process table and |
are uniquely named and that updates are ordered separately by |
process is still present in the process table and thus |
uniquely named and that updates are ordered separately by each |
is still present in the process table and thus not |
named and that updates are ordered separately by each update |
still present in the process table and thus not has |
and that updates are ordered separately by each update source |
k The revenue density of pool i at the end |
which has distribution at its core greatly outweighs the consequences |
present in the process table and thus not has crashed |
The revenue density of pool i at the end of |
has distribution at its core greatly outweighs the consequences of |
of update X and a member m that lacks X |
in the process table and thus not has crashed or |
revenue density of pool i at the end of step |
distribution at its core greatly outweighs the consequences of working |
the process table and thus not has crashed or voluntary |
gossip can be used to detect this and m can |
density of pool i at the end of step t |
at its core greatly outweighs the consequences of working with |
process table and thus not has crashed or voluntary exited |
can be used to detect this and m can then |
of pool i at the end of step t is |
its core greatly outweighs the consequences of working with a |
be used to detect this and m can then send |
pool i at the end of step t is its |
core greatly outweighs the consequences of working with a cutting |
used to detect this and m can then send X |
provide a remote process with information about the progress the |
i at the end of step t is its revenue |
greatly outweighs the consequences of working with a cutting edge |
to detect this and m can then send X to |
a remote process with information about the progress the local |
at the end of step t is its revenue from |
This is a list of identifiers and versions of other |
outweighs the consequences of working with a cutting edge operating |
detect this and m can then send X to m |
remote process with information about the progress the local process |
the end of step t is its revenue from direct |
is a list of identifiers and versions of other objects |
the consequences of working with a cutting edge operating system |
this and m can then send X to m directly |
process with information about the progress the local process is |
end of step t is its revenue from direct mining |
a list of identifiers and versions of other objects that |
with information about the progress the local process is making |
However I must admit that at more then one occasion |
of step t is its revenue from direct mining together |
list of identifiers and versions of other objects that the |
information about the progress the local process is making which |
I must admit that at more then one occasion my |
step t is its revenue from direct mining together with |
of identifiers and versions of other objects that the current |
about the progress the local process is making which is |
must admit that at more then one occasion my students |
t is its revenue from direct mining together with its |
identifiers and versions of other objects that the current version |
the progress the local process is making which is useful |
admit that at more then one occasion my students had |
is its revenue from direct mining together with its revenue |
the delay before information spreads to all members of a |
progress the local process is making which is useful in |
that at more then one occasion my students had to |
and versions of other objects that the current version of |
its revenue from direct mining together with its revenue from |
delay before information spreads to all members of a system |
the local process is making which is useful in the |
at more then one occasion my students had to control |
versions of other objects that the current version of o |
revenue from direct mining together with its revenue from infiltrated |
before information spreads to all members of a system may |
local process is making which is useful in the investigation |
more then one occasion my students had to control their |
of other objects that the current version of o depends |
from direct mining together with its revenue from infiltrated pools |
information spreads to all members of a system may still |
process is making which is useful in the investigation of |
then one occasion my students had to control their murderous |
other objects that the current version of o depends on |
spreads to all members of a system may still be |
is making which is useful in the investigation of processes |
divided by the number of its loyal miners together with |
one occasion my students had to control their murderous intentions |
to all members of a system may still be small |
making which is useful in the investigation of processes that |
by the number of its loyal miners together with block |
only transaction that sees the current version of o must |
occasion my students had to control their murderous intentions towards |
which is useful in the investigation of processes that are |
transaction that sees the current version of o must not |
my students had to control their murderous intentions towards the |
is useful in the investigation of processes that are alive |
that sees the current version of o must not see |
there are exponentially many paths by which information can pass |
students had to control their murderous intentions towards the IIS |
sees the current version of o must not see object |
are exponentially many paths by which information can pass from |
had to control their murderous intentions towards the IIS or |
the current version of o must not see object di |
exponentially many paths by which information can pass from point |
At certain intervals the process logs checkpoint timestamps with the |
to control their murderous intentions towards the IIS or MTS |
current version of o must not see object di with |
many paths by which information can pass from point A |
paths by which information can pass from point A to |
control their murderous intentions towards the IIS or MTS developers |
version of o must not see object di with version |
certain intervals the process logs checkpoint timestamps with the failure |
by which information can pass from point A to point |
their murderous intentions towards the IIS or MTS developers or |
of o must not see object di with version smaller |
intervals the process logs checkpoint timestamps with the failure service |
which information can pass from point A to point B |
murderous intentions towards the IIS or MTS developers or were |
o must not see object di with version smaller than |
intentions towards the IIS or MTS developers or were they |
must not see object di with version smaller than vi |
hence almost any imaginable disruption short of a lasting partitioning |
The response to an inquiry request holds the last checkpoint |
towards the IIS or MTS developers or were they kept |
almost any imaginable disruption short of a lasting partitioning failure |
response to an inquiry request holds the last checkpoint timestamp |
When a transaction t with version vt touches objects o |
the IIS or MTS developers or were they kept their |
any imaginable disruption short of a lasting partitioning failure can |
IIS or MTS developers or were they kept their good |
imaginable disruption short of a lasting partitioning failure can be |
whether the process has been allocated CPU time since the |
or MTS developers or were they kept their good spirits |
Hereinafter we move to a static state analysis and omit |
disruption short of a lasting partitioning failure can be overcome |
the process has been allocated CPU time since the last |
MTS developers or were they kept their good spirits by |
we move to a static state analysis and omit the |
process has been allocated CPU time since the last checkpoint |
The gossip protocols implemented in the SSA have been designed |
developers or were they kept their good spirits by contemplating |
move to a static state analysis and omit the t |
gossip protocols implemented in the SSA have been designed specifically |
and whether the process has consumed any messages since the |
or were they kept their good spirits by contemplating the |
to a static state analysis and omit the t argument |
protocols implemented in the SSA have been designed specifically for |
whether the process has consumed any messages since the last |
were they kept their good spirits by contemplating the horrible |
a static state analysis and omit the t argument in |
implemented in the SSA have been designed specifically for use |
the process has consumed any messages since the last checkpoint |
they kept their good spirits by contemplating the horrible tortures |
static state analysis and omit the t argument in the |
in the SSA have been designed specifically for use in |
kept their good spirits by contemplating the horrible tortures one |
state analysis and omit the t argument in the expressions |
the SSA have been designed specifically for use in our |
Upon receipt of an inquiry the operating system uses an |
their good spirits by contemplating the horrible tortures one could |
SSA have been designed specifically for use in our modified |
receipt of an inquiry the operating system uses an upcall |
good spirits by contemplating the horrible tortures one could perform |
have been designed specifically for use in our modified version |
Since the row sums of the infiltration matrix are smaller |
spirits by contemplating the horrible tortures one could perform on |
been designed specifically for use in our modified version of |
the row sums of the infiltration matrix are smaller than |
to interrupt the process and requests that the process prepares |
by contemplating the horrible tortures one could perform on the |
designed specifically for use in our modified version of chain |
row sums of the infiltration matrix are smaller than one |
interrupt the process and requests that the process prepares a |
contemplating the horrible tortures one could perform on the person |
specifically for use in our modified version of chain replication |
the process and requests that the process prepares a special |
the horrible tortures one could perform on the person that |
process and requests that the process prepares a special response |
and with the goal of running in large clusters or |
horrible tortures one could perform on the person that had |
with the goal of running in large clusters or datacenters |
tortures one could perform on the person that had designed |
one could perform on the person that had designed the |
could perform on the person that had designed the COM |
The previous sections all deal with provisions targeted towards the |
perform on the person that had designed the COM security |
and let p be a process in that group p |
previous sections all deal with provisions targeted towards the failure |
on the person that had designed the COM security architecture |
sections all deal with provisions targeted towards the failure management |
all deal with provisions targeted towards the failure management of |
deal with provisions targeted towards the failure management of processes |
Windows Research There are some properties of Windows NT that |
Research There are some properties of Windows NT that make |
exploiting the close coupled nature of a process and the |
There are some properties of Windows NT that make it |
the close coupled nature of a process and the operating |
are some properties of Windows NT that make it particularly |
close coupled nature of a process and the operating system |
some properties of Windows NT that make it particularly suitable |
coupled nature of a process and the operating system it |
properties of Windows NT that make it particularly suitable for |
nature of a process and the operating system it runs |
of Windows NT that make it particularly suitable for research |
of a process and the operating system it runs under |
Windows NT that make it particularly suitable for research purposes |
Our work assumes that the network within a cluster does |
work assumes that the network within a cluster does not |
To aid accurate detection in the case of node failure |
The operating system kernel for example is designed with extensibility |
assumes that the network within a cluster does not partition |
aid accurate detection in the case of node failure the |
operating system kernel for example is designed with extensibility in |
accurate detection in the case of node failure the fault |
system kernel for example is designed with extensibility in mind |
detection in the case of node failure the fault management |
probability failure patterns that could temporarily partition some subservice in |
in the case of node failure the fault management system |
failure patterns that could temporarily partition some subservice in a |
the case of node failure the fault management system implements |
The Pool Game If no pool engages in block withholding |
new protocols and file systems to add their functionality to |
patterns that could temporarily partition some subservice in a logical |
case of node failure the fault management system implements a |
protocols and file systems to add their functionality to the |
that could temporarily partition some subservice in a logical sense |
of node failure the fault management system implements a node |
and file systems to add their functionality to the system |
node failure the fault management system implements a node management |
file systems to add their functionality to the system without |
failure the fault management system implements a node management service |
systems to add their functionality to the system without much |
to add their functionality to the system without much effort |
process p chooses a random subset of a particular size |
which is based on the experience that local failure investigation |
All kernel code is developed following a strict object oriented |
p chooses a random subset of a particular size view |
is based on the experience that local failure investigation on |
kernel code is developed following a strict object oriented paradigm |
based on the experience that local failure investigation on a |
code is developed following a strict object oriented paradigm and |
on the experience that local failure investigation on a subnet |
is developed following a strict object oriented paradigm and its |
this update is done for all objects in the transaction |
and commences a dialog with each process in the set |
the experience that local failure investigation on a subnet is |
developed following a strict object oriented paradigm and its functionality |
The initial message is a compact state digest summarizing the |
experience that local failure investigation on a subnet is more |
and there are transient effects that are not covered by |
following a strict object oriented paradigm and its functionality can |
initial message is a compact state digest summarizing the state |
update is done for all objects in the transaction at |
that local failure investigation on a subnet is more accurate |
there are transient effects that are not covered by this |
a strict object oriented paradigm and its functionality can only |
message is a compact state digest summarizing the state of |
is done for all objects in the transaction at once |
local failure investigation on a subnet is more accurate than |
are transient effects that are not covered by this stable |
strict object oriented paradigm and its functionality can only be |
is a compact state digest summarizing the state of the |
failure investigation on a subnet is more accurate than investigation |
object oriented paradigm and its functionality can only be accessed |
a compact state digest summarizing the state of the sender |
investigation on a subnet is more accurate than investigation over |
oriented paradigm and its functionality can only be accessed through |
on a subnet is more accurate than investigation over the |
the database aggregates them to a single full dependency list |
paradigm and its functionality can only be accessed through interfaces |
The follow up dialog consists of an explicit request of |
a subnet is more accurate than investigation over the Internet |
database aggregates them to a single full dependency list as |
follow up dialog consists of an explicit request of missing |
aggregates them to a single full dependency list as follows |
One of the designs abstractions of the Windows NT kernel |
up dialog consists of an explicit request of missing update |
On a participating subnet one or more Node Failure Monitors |
of the designs abstractions of the Windows NT kernel I |
dialog consists of an explicit request of missing update operations |
and will choose the value that maximizes the revenue density |
the designs abstractions of the Windows NT kernel I find |
designs abstractions of the Windows NT kernel I find it |
Several details of the epidemic protocols employed in the framework |
These are simple services capable of performing local failure investigations |
abstractions of the Windows NT kernel I find it particularly |
details of the epidemic protocols employed in the framework turned |
are simple services capable of performing local failure investigations upon |
of the Windows NT kernel I find it particularly fascinating |
of the epidemic protocols employed in the framework turned out |
simple services capable of performing local failure investigations upon requests |
the Windows NT kernel I find it particularly fascinating to |
the epidemic protocols employed in the framework turned out to |
is maximized at a single point in the feasible range |
services capable of performing local failure investigations upon requests from |
Windows NT kernel I find it particularly fascinating to work |
epidemic protocols employed in the framework turned out to be |
capable of performing local failure investigations upon requests from remote |
readSet writeSet This list is pruned to match the target |
NT kernel I find it particularly fascinating to work with |
protocols employed in the framework turned out to be important |
of performing local failure investigations upon requests from remote nodes |
writeSet This list is pruned to match the target size |
kernel I find it particularly fascinating to work with is |
employed in the framework turned out to be important determinants |
This list is pruned to match the target size using |
I find it particularly fascinating to work with is the |
in the framework turned out to be important determinants of |
list is pruned to match the target size using LRU |
find it particularly fascinating to work with is the device |
multicast to announce their availability within the organization where their |
the framework turned out to be important determinants of system |
it particularly fascinating to work with is the device object |
to announce their availability within the organization where their presence |
A list entry can be discarded if the same entry |
framework turned out to be important determinants of system performance |
announce their availability within the organization where their presence is |
list entry can be discarded if the same entry s |
A device object in an instance created by driver objects |
turned out to be important determinants of system performance and |
their availability within the organization where their presence is being |
entry can be discarded if the same entry s object |
out to be important determinants of system performance and behavior |
availability within the organization where their presence is being tracked |
and the values of the corresponding revenues of the pools |
can be discarded if the same entry s object appears |
within the organization where their presence is being tracked by |
Suppose that a process disseminates information via epidemics about a |
These objects have the interesting property that they can be |
the values of the corresponding revenues of the pools with |
be discarded if the same entry s object appears in |
the organization where their presence is being tracked by the |
that a process disseminates information via epidemics about a subject |
objects have the interesting property that they can be attached |
values of the corresponding revenues of the pools with r |
discarded if the same entry s object appears in another |
organization where their presence is being tracked by the other |
a process disseminates information via epidemics about a subject s |
have the interesting property that they can be attached to |
if the same entry s object appears in another entry |
where their presence is being tracked by the other NFM |
the interesting property that they can be attached to other |
Process p gossips about subject s a finite number of |
the same entry s object appears in another entry with |
interesting property that they can be attached to other device |
p gossips about subject s a finite number of times |
same entry s object appears in another entry with a |
An NFM accepts queries from remote nodes about the availability |
property that they can be attached to other device objects |
entry s object appears in another entry with a larger |
NFM accepts queries from remote nodes about the availability of |
and as such can intercept and manipulate all requests flowing |
s object appears in another entry with a larger version |
Explicit requests for copies of missed messages are limited in |
accepts queries from remote nodes about the availability of a |
as such can intercept and manipulate all requests flowing to |
requests for copies of missed messages are limited in size |
queries from remote nodes about the availability of a node |
such can intercept and manipulate all requests flowing to and |
from remote nodes about the availability of a node within |
to prevent a process that lagged behind or just joined |
can intercept and manipulate all requests flowing to and from |
remote nodes about the availability of a node within its |
dependency lists could quickly grow to include all objects in |
prevent a process that lagged behind or just joined from |
intercept and manipulate all requests flowing to and from the |
nodes about the availability of a node within its organization |
lists could quickly grow to include all objects in the |
O NE ATTACKER We begin our analysis with a simplified |
a process that lagged behind or just joined from trying |
and manipulate all requests flowing to and from the original |
could quickly grow to include all objects in the database |
NE ATTACKER We begin our analysis with a simplified game |
It will forward this request to an NFM on the |
process that lagged behind or just joined from trying to |
manipulate all requests flowing to and from the original device |
ATTACKER We begin our analysis with a simplified game of |
will forward this request to an NFM on the particular |
that lagged behind or just joined from trying to catch |
all requests flowing to and from the original device object |
We begin our analysis with a simplified game of two |
forward this request to an NFM on the particular subnet |
lagged behind or just joined from trying to catch up |
the cache interacts with the database in essentially the same |
begin our analysis with a simplified game of two pools |
this request to an NFM on the particular subnet which |
This way it is relatively simple to add for example |
behind or just joined from trying to catch up all |
cache interacts with the database in essentially the same manner |
request to an NFM on the particular subnet which will |
way it is relatively simple to add for example a |
or just joined from trying to catch up all at |
interacts with the database in essentially the same manner as |
to an NFM on the particular subnet which will investigate |
it is relatively simple to add for example a file |
just joined from trying to catch up all at once |
with the database in essentially the same manner as for |
an NFM on the particular subnet which will investigate the |
is relatively simple to add for example a file system |
the database in essentially the same manner as for a |
which would result in enormous messages and serious fluctuations in |
NFM on the particular subnet which will investigate the availability |
relatively simple to add for example a file system object |
or with closed pools that do not attack and cannot |
database in essentially the same manner as for a consistency |
would result in enormous messages and serious fluctuations in system |
on the particular subnet which will investigate the availability of |
simple to add for example a file system object that |
with closed pools that do not attack and cannot be |
result in enormous messages and serious fluctuations in system load |
the particular subnet which will investigate the availability of the |
to add for example a file system object that compresses |
closed pools that do not attack and cannot be attacked |
particular subnet which will investigate the availability of the node |
add for example a file system object that compresses or |
such a process may need to catch up over many |
subnet which will investigate the availability of the node by |
for example a file system object that compresses or encrypts |
a process may need to catch up over many seconds |
which will investigate the availability of the node by launching |
example a file system object that compresses or encrypts data |
will investigate the availability of the node by launching a |
Explicit message requests are honored if the requested messages are |
a file system object that compresses or encrypts data before |
investigate the availability of the node by launching a number |
message requests are honored if the requested messages are still |
file system object that compresses or encrypts data before the |
the availability of the node by launching a number of |
requests are honored if the requested messages are still in |
system object that compresses or encrypts data before the data |
the caches read from the database not only the object |
availability of the node by launching a number of fault |
are honored if the requested messages are still in the |
object that compresses or encrypts data before the data reaches |
caches read from the database not only the object s |
of the node by launching a number of fault test |
honored if the requested messages are still in the bounded |
that compresses or encrypts data before the data reaches the |
read from the database not only the object s value |
the node by launching a number of fault test requests |
if the requested messages are still in the bounded buffers |
compresses or encrypts data before the data reaches the under |
or encrypts data before the data reaches the under laying |
if this is support by the host under investigation or |
The Bitcoin system normalizes these rates by the total number |
Once a message has been delivered to the upper levels |
encrypts data before the data reaches the under laying file |
this is support by the host under investigation or by |
Bitcoin system normalizes these rates by the total number of |
data before the data reaches the under laying file system |
and it has been expunged from the buffers located at |
is support by the host under investigation or by ICMP |
system normalizes these rates by the total number of miners |
it has been expunged from the buffers located at the |
Client read requests are extended with a transaction identifier and |
support by the host under investigation or by ICMP echo |
normalizes these rates by the total number of miners that |
has been expunged from the buffers located at the gossiper |
read requests are extended with a transaction identifier and a |
The strict object oriented approach is very well done from |
by the host under investigation or by ICMP echo requests |
these rates by the total number of miners that publish |
been expunged from the buffers located at the gossiper level |
requests are extended with a transaction identifier and a last |
strict object oriented approach is very well done from a |
the host under investigation or by ICMP echo requests if |
rates by the total number of miners that publish full |
object oriented approach is very well done from a design |
host under investigation or by ICMP echo requests if not |
by the total number of miners that publish full proofs |
the requesting process would have to try to find the |
oriented approach is very well done from a design point |
requesting process would have to try to find the missing |
The result of the query is then returned to the |
approach is very well done from a design point of |
process would have to try to find the missing data |
result of the query is then returned to the requesting |
is very well done from a design point of view |
would have to try to find the missing data elsewhere |
of the query is then returned to the requesting node |
The transaction identifier txnID allows the cache to recognize reads |
transaction identifier txnID allows the cache to recognize reads belonging |
style hacker s heart starts bleeding when he or she |
The NFM also functions as proxy for process availability queries |
we signal this to the application by delivering an exception |
hacker s heart starts bleeding when he or she realizes |
identifier txnID allows the cache to recognize reads belonging to |
NFM also functions as proxy for process availability queries in |
signal this to the application by delivering an exception upcall |
s heart starts bleeding when he or she realizes that |
txnID allows the cache to recognize reads belonging to the |
also functions as proxy for process availability queries in the |
heart starts bleeding when he or she realizes that he |
allows the cache to recognize reads belonging to the same |
functions as proxy for process availability queries in the case |
and leave it to the application to decide how to |
starts bleeding when he or she realizes that he can |
the cache to recognize reads belonging to the same transaction |
divides its revenue among its loyal miners and the miners |
as proxy for process availability queries in the case where |
leave it to the application to decide how to handle |
bleeding when he or she realizes that he can no |
its revenue among its loyal miners and the miners that |
proxy for process availability queries in the case where a |
The cache responds with either the value of the requested |
it to the application to decide how to handle the |
when he or she realizes that he can no longer |
revenue among its loyal miners and the miners that infiltrated |
for process availability queries in the case where a firewall |
cache responds with either the value of the requested object |
to the application to decide how to handle the problem |
he or she realizes that he can no longer do |
among its loyal miners and the miners that infiltrated it |
process availability queries in the case where a firewall obstructs |
or she realizes that he can no longer do a |
or with an abort if it detects an inconsistency between |
availability queries in the case where a firewall obstructs the |
she realizes that he can no longer do a quick |
but this rule implies that certain kinds of failures may |
with an abort if it detects an inconsistency between this |
queries in the case where a firewall obstructs the free |
realizes that he can no longer do a quick fix |
this rule implies that certain kinds of failures may be |
an abort if it detects an inconsistency between this read |
in the case where a firewall obstructs the free querying |
rule implies that certain kinds of failures may be unrecoverable |
inspect a few data structures and secretly swivel some pointers |
abort if it detects an inconsistency between this read and |
the case where a firewall obstructs the free querying of |
implies that certain kinds of failures may be unrecoverable within |
a few data structures and secretly swivel some pointers to |
if it detects an inconsistency between this read and any |
The revenue includes both its direct mining revenue and the |
case where a firewall obstructs the free querying of the |
that certain kinds of failures may be unrecoverable within the |
few data structures and secretly swivel some pointers to make |
it detects an inconsistency between this read and any of |
revenue includes both its direct mining revenue and the revenue |
where a firewall obstructs the free querying of the nodes |
certain kinds of failures may be unrecoverable within the SSA |
data structures and secretly swivel some pointers to make things |
detects an inconsistency between this read and any of the |
Digests are bounded in the number of messages they advertise |
a firewall obstructs the free querying of the nodes by |
structures and secretly swivel some pointers to make things work |
includes both its direct mining revenue and the revenue its |
an inconsistency between this read and any of the previous |
are bounded in the number of messages they advertise about |
firewall obstructs the free querying of the nodes by their |
and secretly swivel some pointers to make things work better |
both its direct mining revenue and the revenue its infiltrators |
inconsistency between this read and any of the previous reads |
bounded in the number of messages they advertise about in |
obstructs the free querying of the nodes by their peers |
secretly swivel some pointers to make things work better or |
its direct mining revenue and the revenue its infiltrators obtained |
between this read and any of the previous reads with |
in the number of messages they advertise about in one |
swivel some pointers to make things work better or make |
direct mining revenue and the revenue its infiltrators obtained from |
this read and any of the previous reads with the |
the number of messages they advertise about in one single |
s are configured with domain and ACL mechanisms to control |
some pointers to make things work better or make more |
mining revenue and the revenue its infiltrators obtained from pool |
read and any of the previous reads with the same |
number of messages they advertise about in one single datagram |
are configured with domain and ACL mechanisms to control access |
pointers to make things work better or make more informed |
and any of the previous reads with the same transaction |
of messages they advertise about in one single datagram packet |
configured with domain and ACL mechanisms to control access to |
to make things work better or make more informed decisions |
any of the previous reads with the same transaction ID |
with domain and ACL mechanisms to control access to the |
domain and ACL mechanisms to control access to the information |
but it appears there are always some things one cannot |
it appears there are always some things one cannot do |
An extension which is under investigation is to have nodes |
appears there are always some things one cannot do as |
extension which is under investigation is to have nodes multicast |
Messages that are potentially in transit are not retransmitted to |
collect its transaction record after responding to the last read |
there are always some things one cannot do as efficient |
which is under investigation is to have nodes multicast heartbeats |
that are potentially in transit are not retransmitted to requesting |
its transaction record after responding to the last read operation |
are always some things one cannot do as efficient as |
is under investigation is to have nodes multicast heartbeats with |
are potentially in transit are not retransmitted to requesting processes |
transaction record after responding to the last read operation of |
always some things one cannot do as efficient as possible |
under investigation is to have nodes multicast heartbeats with local |
record after responding to the last read operation of the |
For example if a process p makes an explicit request |
investigation is to have nodes multicast heartbeats with local node |
after responding to the last read operation of the transaction |
example if a process p makes an explicit request for |
in four years of NT kernel hacking only on one |
is to have nodes multicast heartbeats with local node information |
if a process p makes an explicit request for a |
The cache will treat subsequent accesses with the same transaction |
four years of NT kernel hacking only on one occasion |
to have nodes multicast heartbeats with local node information periodically |
a process p makes an explicit request for a message |
cache will treat subsequent accesses with the same transaction ID |
years of NT kernel hacking only on one occasion we |
process p makes an explicit request for a message m |
will treat subsequent accesses with the same transaction ID as |
of NT kernel hacking only on one occasion we needed |
s and shared in compressed form among the other NFM |
p makes an explicit request for a message m and |
treat subsequent accesses with the same transaction ID as new |
makes an explicit request for a message m and the |
NT kernel hacking only on one occasion we needed to |
subsequent accesses with the same transaction ID as new transactions |
an explicit request for a message m and the request |
kernel hacking only on one occasion we needed to break |
Local system management tools can connect to an NFM to |
explicit request for a message m and the request lands |
the cache maintains a record of each transaction with its |
request for a message m and the request lands at |
system management tools can connect to an NFM to retrieve |
hacking only on one occasion we needed to break through |
cache maintains a record of each transaction with its read |
for a message m and the request lands at process |
management tools can connect to an NFM to retrieve the |
only on one occasion we needed to break through the |
maintains a record of each transaction with its read values |
a message m and the request lands at process q |
tools can connect to an NFM to retrieve the information |
on one occasion we needed to break through the standard |
message m and the request lands at process q that |
can connect to an NFM to retrieve the information and |
We vary the sizes of the pools through the entire |
m and the request lands at process q that has |
one occasion we needed to break through the standard kernel |
connect to an NFM to retrieve the information and set |
vary the sizes of the pools through the entire feasible |
and the request lands at process q that has already |
occasion we needed to break through the standard kernel interface |
The cache checks the currently read object against each of |
to an NFM to retrieve the information and set trap |
the sizes of the pools through the entire feasible range |
the request lands at process q that has already sent |
cache checks the currently read object against each of the |
an NFM to retrieve the information and set trap conditions |
we wanted to add a fast trap into the kernel |
sizes of the pools through the entire feasible range and |
request lands at process q that has already sent p |
checks the currently read object against each of the previously |
wanted to add a fast trap into the kernel for |
In distributed systems build on top of a web of |
lands at process q that has already sent p a |
the currently read object against each of the previously read |
of the pools through the entire feasible range and depict |
to add a fast trap into the kernel for fast |
distributed systems build on top of a web of interconnected |
at process q that has already sent p a copy |
currently read object against each of the previously read objects |
the pools through the entire feasible range and depict the |
add a fast trap into the kernel for fast user |
systems build on top of a web of interconnected networks |
process q that has already sent p a copy of |
pools through the entire feasible range and depict the optimal |
If a previously read version v is older than expected |
q that has already sent p a copy of m |
and the pages which hold the trap dispatch tables were |
a previously read version v is older than expected by |
through the entire feasible range and depict the optimal x |
that has already sent p a copy of m in |
the pages which hold the trap dispatch tables were protected |
previously read version v is older than expected by the |
Failures at network level are in general related to crash |
has already sent p a copy of m in the |
pages which hold the trap dispatch tables were protected after |
read version v is older than expected by the current |
at network level are in general related to crash failures |
already sent p a copy of m in the recent |
which hold the trap dispatch tables were protected after the |
version v is older than expected by the current read |
Each point in each graph represents the equilibrium point of |
network level are in general related to crash failures of |
sent p a copy of m in the recent past |
hold the trap dispatch tables were protected after the system |
v is older than expected by the current read s |
point in each graph represents the equilibrium point of a |
level are in general related to crash failures of routers |
p a copy of m in the recent past then |
the trap dispatch tables were protected after the system boot |
is older than expected by the current read s dependencies |
in each graph represents the equilibrium point of a game |
are in general related to crash failures of routers and |
a copy of m in the recent past then m |
older than expected by the current read s dependencies v |
each graph represents the equilibrium point of a game with |
in general related to crash failures of routers and gateways |
Another example of what makes Windows NT particular suitable for |
copy of m in the recent past then m will |
than expected by the current read s dependencies v k |
graph represents the equilibrium point of a game with the |
example of what makes Windows NT particular suitable for research |
of m in the recent past then m will not |
or to severe degradation of the service level due to |
represents the equilibrium point of a game with the corresponding |
of what makes Windows NT particular suitable for research is |
m in the recent past then m will not be |
to severe degradation of the service level due to network |
the equilibrium point of a game with the corresponding m |
what makes Windows NT particular suitable for research is the |
in the recent past then m will not be retransmitted |
severe degradation of the service level due to network congestion |
makes Windows NT particular suitable for research is the fundamental |
or the current read vcurr is older than expected by |
A process creates a digest based upon all the messages |
Windows NT particular suitable for research is the fundamental manner |
the current read vcurr is older than expected by the |
The top right half of the range in all graphs |
NT particular suitable for research is the fundamental manner in |
when not able to reach the node under investigation or |
current read vcurr is older than expected by the dependencies |
top right half of the range in all graphs is |
process creates a digest based upon all the messages received |
particular suitable for research is the fundamental manner in which |
not able to reach the node under investigation or a |
read vcurr is older than expected by the dependencies of |
right half of the range in all graphs is not |
creates a digest based upon all the messages received by |
suitable for research is the fundamental manner in which advanced |
able to reach the node under investigation or a relevant |
vcurr is older than expected by the dependencies of a |
half of the range in all graphs is not feasible |
a digest based upon all the messages received by means |
for research is the fundamental manner in which advanced distributed |
to reach the node under investigation or a relevant NFM |
is older than expected by the dependencies of a previous |
digest based upon all the messages received by means of |
research is the fundamental manner in which advanced distributed services |
older than expected by the dependencies of a previous read |
than expected by the dependencies of a previous read v |
based upon all the messages received by means of any |
and we use a dashed line to show the bound |
is the fundamental manner in which advanced distributed services are |
perform a path search to find the trouble spot in |
expected by the dependencies of a previous read v v |
upon all the messages received by means of any communication |
we use a dashed line to show the bound between |
the fundamental manner in which advanced distributed services are integrated |
a path search to find the trouble spot in the |
all the messages received by means of any communication channels |
use a dashed line to show the bound between this |
fundamental manner in which advanced distributed services are integrated into |
path search to find the trouble spot in the network |
a dashed line to show the bound between this value |
manner in which advanced distributed services are integrated into Windows |
Otherwise the cache returns the read value to the client |
dashed line to show the bound between this value within |
in which advanced distributed services are integrated into Windows NT |
It uses the traceroute technique of emitting small messages with |
line to show the bound between this value within the |
uses the traceroute technique of emitting small messages with limited |
It allows us to rely on ubiquitous support services and |
to show the bound between this value within the feasible |
the traceroute technique of emitting small messages with limited ttl |
this has the benefit of affecting only the running transaction |
allows us to rely on ubiquitous support services and concentrate |
show the bound between this value within the feasible range |
has the benefit of affecting only the running transaction and |
and once a message has been delivered by means of |
us to rely on ubiquitous support services and concentrate on |
the benefit of affecting only the running transaction and limiting |
once a message has been delivered by means of an |
to rely on ubiquitous support services and concentrate on advancing |
benefit of affecting only the running transaction and limiting collateral |
If an obstruction is found it is reported to the |
a message has been delivered by means of an upcall |
rely on ubiquitous support services and concentrate on advancing the |
of affecting only the running transaction and limiting collateral damage |
an obstruction is found it is reported to the caller |
message has been delivered by means of an upcall it |
on ubiquitous support services and concentrate on advancing the state |
has been delivered by means of an upcall it is |
The failure management library offers functionality to keep the obstruction |
ubiquitous support services and concentrate on advancing the state of |
b and in the entire feasible region it is strictly |
been delivered by means of an upcall it is prone |
failure management library offers functionality to keep the obstruction under |
support services and concentrate on advancing the state of the |
and in the entire feasible region it is strictly larger |
This approach guesses that future transactions are likely to abort |
delivered by means of an upcall it is prone to |
management library offers functionality to keep the obstruction under investigation |
services and concentrate on advancing the state of the art |
in the entire feasible region it is strictly larger than |
approach guesses that future transactions are likely to abort because |
by means of an upcall it is prone to be |
library offers functionality to keep the obstruction under investigation and |
and concentrate on advancing the state of the art where |
guesses that future transactions are likely to abort because of |
means of an upcall it is prone to be replaced |
offers functionality to keep the obstruction under investigation and to |
concentrate on advancing the state of the art where it |
that future transactions are likely to abort because of this |
of an upcall it is prone to be replaced by |
functionality to keep the obstruction under investigation and to notify |
on advancing the state of the art where it is |
future transactions are likely to abort because of this object |
an upcall it is prone to be replaced by the |
to keep the obstruction under investigation and to notify the |
advancing the state of the art where it is really |
upcall it is prone to be replaced by the replacement |
keep the obstruction under investigation and to notify the application |
the state of the art where it is really needed |
it is prone to be replaced by the replacement policy |
the obstruction under investigation and to notify the application once |
Windows NT Security provides a complete set of services integrated |
obstruction under investigation and to notify the application once the |
treat this access as a miss and respond to it |
Note that the total system mining power is reduced when |
NT Security provides a complete set of services integrated into |
under investigation and to notify the application once the obstruction |
this access as a miss and respond to it with |
that the total system mining power is reduced when pool |
Security provides a complete set of services integrated into all |
investigation and to notify the application once the obstruction seems |
access as a miss and respond to it with a |
provides a complete set of services integrated into all sections |
and to notify the application once the obstruction seems to |
as a miss and respond to it with a value |
Although the SSA should work well on clusters with as |
a complete set of services integrated into all sections of |
to notify the application once the obstruction seems to be |
a miss and respond to it with a value read |
the SSA should work well on clusters with as many |
complete set of services integrated into all sections of the |
notify the application once the obstruction seems to be removed |
miss and respond to it with a value read from |
SSA should work well on clusters with as many as |
set of services integrated into all sections of the operating |
This way the process does not need to keep the |
should work well on clusters with as many as thousands |
and respond to it with a value read from the |
of services integrated into all sections of the operating system |
way the process does not need to keep the partitioned |
work well on clusters with as many as thousands of |
respond to it with a value read from the database |
the process does not need to keep the partitioned processes |
If the violating object was returned to the user as |
therefore pays for the increased revenue of its attacker and |
well on clusters with as many as thousands of nodes |
process does not need to keep the partitioned processes under |
the violating object was returned to the user as the |
pays for the increased revenue of its attacker and everyone |
does not need to keep the partitioned processes under investigation |
violating object was returned to the user as the result |
companies like Google and Amazon reportedly operate centers with tens |
for the increased revenue of its attacker and everyone else |
not need to keep the partitioned processes under investigation but |
object was returned to the user as the result of |
like Google and Amazon reportedly operate centers with tens of |
the increased revenue of its attacker and everyone else in |
need to keep the partitioned processes under investigation but can |
was returned to the user as the result of a |
Google and Amazon reportedly operate centers with tens of thousands |
increased revenue of its attacker and everyone else in the |
The use of the COM object model in all the |
to keep the partitioned processes under investigation but can wait |
returned to the user as the result of a read |
and Amazon reportedly operate centers with tens of thousands of |
revenue of its attacker and everyone else in the system |
use of the COM object model in all the Windows |
keep the partitioned processes under investigation but can wait until |
to the user as the result of a read earlier |
Amazon reportedly operate centers with tens of thousands of machines |
of the COM object model in all the Windows NT |
the partitioned processes under investigation but can wait until the |
the user as the result of a read earlier in |
Implications to the general case Consider the case of p |
reportedly operate centers with tens of thousands of machines in |
the COM object model in all the Windows NT services |
partitioned processes under investigation but can wait until the connectivity |
user as the result of a read earlier in the |
to the general case Consider the case of p pools |
operate centers with tens of thousands of machines in them |
COM object model in all the Windows NT services allows |
processes under investigation but can wait until the connectivity is |
as the result of a read earlier in the transaction |
object model in all the Windows NT services allows research |
and are said to deploy some popular services on huge |
under investigation but can wait until the connectivity is restored |
model in all the Windows NT services allows research projects |
are said to deploy some popular services on huge numbers |
investigation but can wait until the connectivity is restored by |
in all the Windows NT services allows research projects to |
said to deploy some popular services on huge numbers of |
but can wait until the connectivity is restored by simply |
at least one pool will choose to perform block withholding |
all the Windows NT services allows research projects to import |
to deploy some popular services on huge numbers of nodes |
can wait until the connectivity is restored by simply monitoring |
the Windows NT services allows research projects to import these |
Cache with unbounded cache size and unbounded dependency lists implements |
wait until the connectivity is restored by simply monitoring the |
Windows NT services allows research projects to import these services |
with unbounded cache size and unbounded dependency lists implements cache |
until the connectivity is restored by simply monitoring the trouble |
our gossip protocol might need to be revisited to ensure |
NT services allows research projects to import these services in |
the connectivity is restored by simply monitoring the trouble spot |
gossip protocol might need to be revisited to ensure that |
services allows research projects to import these services in a |
is by constructing a serialization of the transactions in the |
protocol might need to be revisited to ensure that messages |
allows research projects to import these services in a very |
by constructing a serialization of the transactions in the database |
might need to be revisited to ensure that messages do |
research projects to import these services in a very simple |
constructing a serialization of the transactions in the database and |
need to be revisited to ensure that messages do not |
projects to import these services in a very simple manner |
a serialization of the transactions in the database and in |
to be revisited to ensure that messages do not become |
serialization of the transactions in the database and in one |
be revisited to ensure that messages do not become excessively |
of the transactions in the database and in one cache |
The existence of COM makes it trivial for research projects |
revisited to ensure that messages do not become excessively large |
This is the setting analyzed above and we have seen |
based on the fact that the transactions in the database |
existence of COM makes it trivial for research projects to |
is the setting analyzed above and we have seen there |
on the fact that the transactions in the database are |
The request contains sufficient information for the NFM to construct |
One way to accomplish this might be to modify the |
of COM makes it trivial for research projects to export |
the setting analyzed above and we have seen there that |
the fact that the transactions in the database are serializable |
request contains sufficient information for the NFM to construct a |
way to accomplish this might be to modify the epidemic |
COM makes it trivial for research projects to export their |
setting analyzed above and we have seen there that pool |
fact that the transactions in the database are serializable by |
contains sufficient information for the NFM to construct a symmetric |
to accomplish this might be to modify the epidemic protocol |
makes it trivial for research projects to export their interfaces |
that the transactions in the database are serializable by definition |
can increase its revenue by performing a block withholding attack |
sufficient information for the NFM to construct a symmetric return |
accomplish this might be to modify the epidemic protocol using |
it trivial for research projects to export their interfaces in |
increase its revenue by performing a block withholding attack on |
information for the NFM to construct a symmetric return path |
this might be to modify the epidemic protocol using spatial |
trivial for research projects to export their interfaces in a |
its revenue by performing a block withholding attack on pool |
might be to modify the epidemic protocol using spatial distributions |
Protocols that can exploit this type of information are under |
for research projects to export their interfaces in a language |
Cache converges to perfect detection when stable clusters are as |
be to modify the epidemic protocol using spatial distributions to |
that can exploit this type of information are under development |
research projects to export their interfaces in a language independent |
converges to perfect detection when stable clusters are as large |
to modify the epidemic protocol using spatial distributions to improve |
projects to export their interfaces in a language independent manner |
to perfect detection when stable clusters are as large as |
modify the epidemic protocol using spatial distributions to improve the |
perfect detection when stable clusters are as large as its |
the epidemic protocol using spatial distributions to improve the performance |
The Ensemble project for example has developed a protocol environment |
detection when stable clusters are as large as its dependency |
Ensemble project for example has developed a protocol environment for |
when stable clusters are as large as its dependency lists |
project for example has developed a protocol environment for distributed |
for example has developed a protocol environment for distributed operations |
example has developed a protocol environment for distributed operations in |
the dependency lists are large enough to describe all relevant |
has developed a protocol environment for distributed operations in the |
Such an approach would let us restrict information to the |
dependency lists are large enough to describe all relevant dependencies |
developed a protocol environment for distributed operations in the ML |
an approach would let us restrict information to the vicinity |
Reasons for false suspicions were overload in the receiver OS |
a protocol environment for distributed operations in the ML programming |
approach would let us restrict information to the vicinity of |
E XPERIMENTAL S ETUP To evaluate the effectiveness of our |
protocol environment for distributed operations in the ML programming language |
would let us restrict information to the vicinity of the |
XPERIMENTAL S ETUP To evaluate the effectiveness of our scheme |
let us restrict information to the vicinity of the nodes |
and by using a COM interface are the services offered |
us restrict information to the vicinity of the nodes where |
by using a COM interface are the services offered by |
restrict information to the vicinity of the nodes where it |
using a COM interface are the services offered by Ensemble |
information to the vicinity of the nodes where it might |
a COM interface are the services offered by Ensemble available |
to the vicinity of the nodes where it might be |
namely a single cache backed by a single database server |
COM interface are the services offered by Ensemble available to |
the vicinity of the nodes where it might be needed |
interface are the services offered by Ensemble available to C |
one was never guaranteed that the process had truly crashed |
in effect adding an additional layer of hierarchy to the |
effect adding an additional layer of hierarchy to the architecture |
The time needed by the failure detector to come to |
time needed by the failure detector to come to a |
needed by the failure detector to come to a result |
by the failure detector to come to a result has |
the failure detector to come to a result has been |
failure detector to come to a result has been greatly |
detector to come to a result has been greatly reduced |
to come to a result has been greatly reduced in |
come to a result has been greatly reduced in the |
Epidemic Analytical Model One benefit of using gossip in the |
to a result has been greatly reduced in the optimistic |
Analytical Model One benefit of using gossip in the SSA |
Microsoft is very generous to academia and makes all their |
Model One benefit of using gossip in the SSA is |
common case that the node on which the process was |
is very generous to academia and makes all their tools |
A set of cache clients perform readonly transactions through a |
One benefit of using gossip in the SSA is that |
case that the node on which the process was running |
very generous to academia and makes all their tools from |
set of cache clients perform readonly transactions through a single |
benefit of using gossip in the SSA is that we |
that the node on which the process was running is |
generous to academia and makes all their tools from operating |
of cache clients perform readonly transactions through a single cache |
of using gossip in the SSA is that we can |
the node on which the process was running is reachable |
to academia and makes all their tools from operating systems |
cache clients perform readonly transactions through a single cache server |
using gossip in the SSA is that we can use |
academia and makes all their tools from operating systems to |
gossip in the SSA is that we can use analytical |
the node is able to indicate whether or not the |
The cache serves the requests from its local storage if |
and makes all their tools from operating systems to compilers |
in the SSA is that we can use analytical methods |
node is able to indicate whether or not the process |
cache serves the requests from its local storage if possible |
the SSA is that we can use analytical methods to |
is able to indicate whether or not the process has |
including tons of documentation as well as subscriptions to the |
SSA is that we can use analytical methods to predict |
able to indicate whether or not the process has crashed |
tons of documentation as well as subscriptions to the developer |
is that we can use analytical methods to predict the |
the cache registers an upcall that can be used by |
of documentation as well as subscriptions to the developer network |
that we can use analytical methods to predict the behavior |
cache registers an upcall that can be used by the |
trip time is sufficient at the local network to get |
we can use analytical methods to predict the behavior of |
registers an upcall that can be used by the database |
time is sufficient at the local network to get a |
can use analytical methods to predict the behavior of a |
an upcall that can be used by the database to |
is sufficient at the local network to get a result |
and was only once used to make actual changes to |
use analytical methods to predict the behavior of a cluster |
upcall that can be used by the database to report |
was only once used to make actual changes to the |
that can be used by the database to report invalidations |
area case this time is a function of the level |
only once used to make actual changes to the operating |
case this time is a function of the level of |
after each update transaction the database asynchronously sends invalidations to |
A basic result of epidemic theory states that simple epidemics |
once used to make actual changes to the operating systems |
this time is a function of the level of congestion |
each update transaction the database asynchronously sends invalidations to the |
basic result of epidemic theory states that simple epidemics eventually |
time is a function of the level of congestion in |
update transaction the database asynchronously sends invalidations to the cache |
result of epidemic theory states that simple epidemics eventually infect |
is a function of the level of congestion in the |
transaction the database asynchronously sends invalidations to the cache for |
of epidemic theory states that simple epidemics eventually infect the |
a function of the level of congestion in the network |
the database asynchronously sends invalidations to the cache for all |
epidemic theory states that simple epidemics eventually infect the entire |
function of the level of congestion in the network path |
database asynchronously sends invalidations to the cache for all objects |
to examine unexpected behaviour or to provide templates for similar |
theory states that simple epidemics eventually infect the entire population |
asynchronously sends invalidations to the cache for all objects that |
examine unexpected behaviour or to provide templates for similar projects |
The OS extensions also improve the confidence in the failure |
states that simple epidemics eventually infect the entire population with |
sends invalidations to the cache for all objects that were |
OS extensions also improve the confidence in the failure investigation |
As one can perform complete source code level debugging of |
that simple epidemics eventually infect the entire population with probability |
invalidations to the cache for all objects that were modified |
extensions also improve the confidence in the failure investigation process |
one can perform complete source code level debugging of all |
also improve the confidence in the failure investigation process in |
can perform complete source code level debugging of all parts |
improve the confidence in the failure investigation process in the |
Moreover starting with a single infected site this is achieved |
perform complete source code level debugging of all parts of |
the confidence in the failure investigation process in the wide |
starting with a single infected site this is achieved in |
complete source code level debugging of all parts of the |
with a single infected site this is achieved in expected |
this is extreme and would only be seen in the |
source code level debugging of all parts of the operating |
a single infected site this is achieved in expected time |
Using the old strategy of simply polling a process until |
is extreme and would only be seen in the real |
code level debugging of all parts of the operating system |
single infected site this is achieved in expected time proportional |
the old strategy of simply polling a process until a |
extreme and would only be seen in the real world |
level debugging of all parts of the operating system including |
infected site this is achieved in expected time proportional to |
old strategy of simply polling a process until a time |
and would only be seen in the real world under |
debugging of all parts of the operating system including the |
site this is achieved in expected time proportional to the |
would only be seen in the real world under conditions |
of all parts of the operating system including the kernel |
out occurs gives much less confidence in the result of |
this is achieved in expected time proportional to the log |
only be seen in the real world under conditions of |
occurs gives much less confidence in the result of the |
is achieved in expected time proportional to the log of |
source codes helps us to develop experimental services faster and |
be seen in the real world under conditions of overload |
gives much less confidence in the result of the failure |
achieved in expected time proportional to the log of the |
codes helps us to develop experimental services faster and in |
seen in the real world under conditions of overload or |
much less confidence in the result of the failure investigation |
in expected time proportional to the log of the population |
helps us to develop experimental services faster and in tune |
in the real world under conditions of overload or when |
expected time proportional to the log of the population size |
If no response was received after the maximum number of |
us to develop experimental services faster and in tune with |
the real world under conditions of overload or when the |
no response was received after the maximum number of retransmission |
to develop experimental services faster and in tune with existing |
real world under conditions of overload or when the system |
response was received after the maximum number of retransmission is |
develop experimental services faster and in tune with existing functionality |
world under conditions of overload or when the system configuration |
was received after the maximum number of retransmission is reached |
The protocol roughly falls under the category of a push |
We analyze the cases where each of the pools attacks |
under conditions of overload or when the system configuration is |
Students are free to work with the source code and |
analyze the cases where each of the pools attacks all |
it was not certain whether this was because of network |
conditions of overload or when the system configuration is changed |
are free to work with the source code and are |
the cases where each of the pools attacks all other |
and the exact formula for it can be expressed as |
was not certain whether this was because of network failure |
free to work with the source code and are not |
cases where each of the pools attacks all other open |
Both the database and the cache report all completed transactions |
the exact formula for it can be expressed as log |
to work with the source code and are not prohibited |
where each of the pools attacks all other open pools |
the database and the cache report all completed transactions to |
With the new scheme it is possible to distinguish among |
Note that attacking all pools with force proportional to their |
database and the cache report all completed transactions to a |
work with the source code and are not prohibited in |
the new scheme it is possible to distinguish among these |
that attacking all pools with force proportional to their size |
and the cache report all completed transactions to a consistency |
with the source code and are not prohibited in any |
new scheme it is possible to distinguish among these different |
attacking all pools with force proportional to their size yields |
the cache report all completed transactions to a consistency monitor |
the source code and are not prohibited in any way |
scheme it is possible to distinguish among these different failures |
all pools with force proportional to their size yields the |
source code and are not prohibited in any way from |
pools with force proportional to their size yields the same |
This server collects both committed and aborted transactions and it |
code and are not prohibited in any way from applying |
with force proportional to their size yields the same results |
Additional Information The full report contains the detailed results of |
server collects both committed and aborted transactions and it maintains |
and are not prohibited in any way from applying the |
force proportional to their size yields the same results as |
Information The full report contains the detailed results of the |
collects both committed and aborted transactions and it maintains the |
are not prohibited in any way from applying the knowledge |
proportional to their size yields the same results as attacking |
where n the number of sites participating in the epidemic |
The full report contains the detailed results of the trace |
both committed and aborted transactions and it maintains the full |
not prohibited in any way from applying the knowledge they |
to their size yields the same results as attacking a |
n the number of sites participating in the epidemic spread |
full report contains the detailed results of the trace study |
committed and aborted transactions and it maintains the full dependency |
prohibited in any way from applying the knowledge they gained |
their size yields the same results as attacking a single |
report contains the detailed results of the trace study on |
and aborted transactions and it maintains the full dependency graph |
Let pi be the probability that a site remains susceptible |
in any way from applying the knowledge they gained in |
size yields the same results as attacking a single pool |
contains the detailed results of the trace study on the |
any way from applying the knowledge they gained in their |
yields the same results as attacking a single pool of |
the detailed results of the trace study on the accuracy |
way from applying the knowledge they gained in their later |
the same results as attacking a single pool of their |
detailed results of the trace study on the accuracy and |
and calculates the rate of inconsistent transactions that committed and |
from applying the knowledge they gained in their later careers |
same results as attacking a single pool of their aggregate |
results of the trace study on the accuracy and performance |
calculates the rate of inconsistent transactions that committed and the |
results as attacking a single pool of their aggregate size |
Interactions with the Evil Empire Microsoft realizes the potential of |
of the trace study on the accuracy and performance of |
the rate of inconsistent transactions that committed and the rate |
th round if it was susceptible after the ith cycle |
Plugging in the numbers into the analysis above shows that |
with the Evil Empire Microsoft realizes the potential of widespread |
the trace study on the accuracy and performance of the |
rate of inconsistent transactions that committed and the rate of |
round if it was susceptible after the ith cycle and |
in the numbers into the analysis above shows that a |
the Evil Empire Microsoft realizes the potential of widespread adoption |
trace study on the accuracy and performance of the failure |
of inconsistent transactions that committed and the rate of consistent |
if it was susceptible after the ith cycle and it |
the numbers into the analysis above shows that a larger |
Evil Empire Microsoft realizes the potential of widespread adoption of |
study on the accuracy and performance of the failure detector |
inconsistent transactions that committed and the rate of consistent transactions |
it was susceptible after the ith cycle and it is |
numbers into the analysis above shows that a larger pool |
Empire Microsoft realizes the potential of widespread adoption of Windows |
on the accuracy and performance of the failure detector in |
transactions that committed and the rate of consistent transactions that |
was susceptible after the ith cycle and it is not |
into the analysis above shows that a larger pool needs |
Microsoft realizes the potential of widespread adoption of Windows NT |
the accuracy and performance of the failure detector in the |
that committed and the rate of consistent transactions that were |
susceptible after the ith cycle and it is not contacted |
the analysis above shows that a larger pool needs to |
realizes the potential of widespread adoption of Windows NT for |
accuracy and performance of the failure detector in the Internet |
committed and the rate of consistent transactions that were unnecessarily |
after the ith cycle and it is not contacted by |
analysis above shows that a larger pool needs to use |
the potential of widespread adoption of Windows NT for research |
and the rate of consistent transactions that were unnecessarily aborted |
the ith cycle and it is not contacted by any |
above shows that a larger pool needs to use a |
potential of widespread adoption of Windows NT for research purposes |
host failure measurements and measurements of failure detection for server |
ith cycle and it is not contacted by any infectious |
shows that a larger pool needs to use a smaller |
Our prototype does not address the issue of cache eviction |
of widespread adoption of Windows NT for research purposes and |
failure measurements and measurements of failure detection for server fail |
cycle and it is not contacted by any infectious site |
that a larger pool needs to use a smaller ratio |
prototype does not address the issue of cache eviction when |
widespread adoption of Windows NT for research purposes and there |
and it is not contacted by any infectious site in |
a larger pool needs to use a smaller ratio of |
does not address the issue of cache eviction when running |
adoption of Windows NT for research purposes and there is |
it is not contacted by any infectious site in the |
It will be available later this year through the Cornell |
larger pool needs to use a smaller ratio of its |
not address the issue of cache eviction when running out |
of Windows NT for research purposes and there is dedicated |
is not contacted by any infectious site in the i |
will be available later this year through the Cornell University |
pool needs to use a smaller ratio of its mining |
address the issue of cache eviction when running out of |
Windows NT for research purposes and there is dedicated academic |
be available later this year through the Cornell University Technical |
needs to use a smaller ratio of its mining power |
the issue of cache eviction when running out of memory |
NT for research purposes and there is dedicated academic relations |
available later this year through the Cornell University Technical Report |
to use a smaller ratio of its mining power for |
for research purposes and there is dedicated academic relations team |
later this year through the Cornell University Technical Report Server |
use a smaller ratio of its mining power for infiltration |
research purposes and there is dedicated academic relations team whose |
and eviction is only done if there is a direct |
a smaller ratio of its mining power for infiltration and |
purposes and there is dedicated academic relations team whose single |
eviction is only done if there is a direct reason |
smaller ratio of its mining power for infiltration and can |
and there is dedicated academic relations team whose single task |
ratio of its mining power for infiltration and can increase |
there is dedicated academic relations team whose single task it |
of its mining power for infiltration and can increase its |
is dedicated academic relations team whose single task it is |
its mining power for infiltration and can increase its revenue |
dedicated academic relations team whose single task it is to |
mining power for infiltration and can increase its revenue density |
academic relations team whose single task it is to facilitate |
power for infiltration and can increase its revenue density more |
We evaluate the effectiveness of our transactional cache using various |
relations team whose single task it is to facilitate the |
for infiltration and can increase its revenue density more than |
Relevant URL s The Horus Project The Cornell Cluster Computing |
evaluate the effectiveness of our transactional cache using various workloads |
team whose single task it is to facilitate the technology |
infiltration and can increase its revenue density more than a |
URL s The Horus Project The Cornell Cluster Computing Project |
the effectiveness of our transactional cache using various workloads and |
whose single task it is to facilitate the technology transfer |
and can increase its revenue density more than a small |
s The Horus Project The Cornell Cluster Computing Project Werner |
effectiveness of our transactional cache using various workloads and varying |
single task it is to facilitate the technology transfer between |
can increase its revenue density more than a small pool |
The Horus Project The Cornell Cluster Computing Project Werner Vogels |
of our transactional cache using various workloads and varying the |
task it is to facilitate the technology transfer between Microsoft |
Horus Project The Cornell Cluster Computing Project Werner Vogels personal |
our transactional cache using various workloads and varying the size |
it is to facilitate the technology transfer between Microsoft and |
Project The Cornell Cluster Computing Project Werner Vogels personal home |
transactional cache using various workloads and varying the size of |
we can predict the delay before a typical process that |
is to facilitate the technology transfer between Microsoft and academia |
The Cornell Cluster Computing Project Werner Vogels personal home page |
cache using various workloads and varying the size of the |
can predict the delay before a typical process that has |
to facilitate the technology transfer between Microsoft and academia and |
Cornell Cluster Computing Project Werner Vogels personal home page Papers |
using various workloads and varying the size of the dependency |
This represents a considerable increase of the pools net revenue |
predict the delay before a typical process that has been |
facilitate the technology transfer between Microsoft and academia and vice |
Cluster Computing Project Werner Vogels personal home page Papers on |
various workloads and varying the size of the dependency lists |
the delay before a typical process that has been disrupted |
the technology transfer between Microsoft and academia and vice versa |
Computing Project Werner Vogels personal home page Papers on Failure |
To reach the optimum it needs almost a third of |
workloads and varying the size of the dependency lists maintained |
delay before a typical process that has been disrupted by |
Project Werner Vogels personal home page Papers on Failure detection |
reach the optimum it needs almost a third of its |
and varying the size of the dependency lists maintained by |
Source licensing is very liberal compared to other OS vendors |
before a typical process that has been disrupted by a |
Werner Vogels personal home page Papers on Failure detection http |
the optimum it needs almost a third of its power |
varying the size of the dependency lists maintained by the |
licensing is very liberal compared to other OS vendors and |
a typical process that has been disrupted by a failure |
optimum it needs almost a third of its power for |
the size of the dependency lists maintained by the cache |
is very liberal compared to other OS vendors and several |
typical process that has been disrupted by a failure will |
it needs almost a third of its power for attacking |
size of the dependency lists maintained by the cache and |
very liberal compared to other OS vendors and several institutions |
process that has been disrupted by a failure will learn |
needs almost a third of its power for attacking but |
of the dependency lists maintained by the cache and the |
liberal compared to other OS vendors and several institutions are |
that has been disrupted by a failure will learn about |
almost a third of its power for attacking but increases |
the dependency lists maintained by the cache and the database |
compared to other OS vendors and several institutions are involved |
has been disrupted by a failure will learn about inconsistency |
a third of its power for attacking but increases its |
to other OS vendors and several institutions are involved in |
been disrupted by a failure will learn about inconsistency introduced |
third of its power for attacking but increases its revenue |
other OS vendors and several institutions are involved in active |
disrupted by a failure will learn about inconsistency introduced by |
of its power for attacking but increases its revenue density |
OS vendors and several institutions are involved in active exchanges |
by a failure will learn about inconsistency introduced by the |
its power for attacking but increases its revenue density by |
An open question for further study is whether there are |
a failure will learn about inconsistency introduced by the failure |
vendors and several institutions are involved in active exchanges with |
power for attacking but increases its revenue density by merely |
open question for further study is whether there are workloads |
failure will learn about inconsistency introduced by the failure and |
and several institutions are involved in active exchanges with product |
question for further study is whether there are workloads that |
will learn about inconsistency introduced by the failure and can |
several institutions are involved in active exchanges with product and |
for further study is whether there are workloads that might |
learn about inconsistency introduced by the failure and can initiate |
institutions are involved in active exchanges with product and research |
further study is whether there are workloads that might require |
about inconsistency introduced by the failure and can initiate repair |
are involved in active exchanges with product and research groups |
study is whether there are workloads that might require limited |
involved in active exchanges with product and research groups within |
is whether there are workloads that might require limited but |
in active exchanges with product and research groups within Microsoft |
if the model predicts that for a given gossip rate |
whether there are workloads that might require limited but larger |
there are workloads that might require limited but larger values |
joint papers are starting to appear and academics frequently present |
Note that dependencies arise from the topology of the object |
papers are starting to appear and academics frequently present cutting |
that dependencies arise from the topology of the object graph |
are starting to appear and academics frequently present cutting edge |
one can anticipate that the disruption associated with a failure |
starting to appear and academics frequently present cutting edge result |
and not from the size of the transactions read and |
can anticipate that the disruption associated with a failure should |
to appear and academics frequently present cutting edge result to |
not from the size of the transactions read and write |
anticipate that the disruption associated with a failure should be |
appear and academics frequently present cutting edge result to Microsoft |
from the size of the transactions read and write sets |
that the disruption associated with a failure should be limited |
and academics frequently present cutting edge result to Microsoft developers |
the disruption associated with a failure should be limited to |
academics frequently present cutting edge result to Microsoft developers and |
disruption associated with a failure should be limited to the |
frequently present cutting edge result to Microsoft developers and researchers |
associated with a failure should be limited to the maximum |
it reduces the probability of inconsistency by limiting the life |
with a failure should be limited to the maximum number |
reduces the probability of inconsistency by limiting the life span |
a failure should be limited to the maximum number of |
the probability of inconsistency by limiting the life span of |
failure should be limited to the maximum number of updates |
probability of inconsistency by limiting the life span of cache |
should be limited to the maximum number of updates that |
of inconsistency by limiting the life span of cache entries |
be limited to the maximum number of updates that would |
limited to the maximum number of updates that would be |
We compare this method against our transactional cache by measuring |
to the maximum number of updates that would be sent |
compare this method against our transactional cache by measuring its |
the maximum number of updates that would be sent to |
this method against our transactional cache by measuring its effectiveness |
maximum number of updates that would be sent to a |
Operating Systems There is a direct impact of academia on |
method against our transactional cache by measuring its effectiveness with |
number of updates that would be sent to a given |
Systems There is a direct impact of academia on Microsoft |
against our transactional cache by measuring its effectiveness with a |
of updates that would be sent to a given subservice |
There is a direct impact of academia on Microsoft products |
our transactional cache by measuring its effectiveness with a varying |
updates that would be sent to a given subservice during |
transactional cache by measuring its effectiveness with a varying time |
that would be sent to a given subservice during a |
through involvement in the strategy phases of products as well |
involvement in the strategy phases of products as well as |
in the strategy phases of products as well as through |
the strategy phases of products as well as through academic |
strategy phases of products as well as through academic knowledge |
phases of products as well as through academic knowledge transfer |
of products as well as through academic knowledge transfer into |
products as well as through academic knowledge transfer into products |
as well as through academic knowledge transfer into products and |
well as through academic knowledge transfer into products and design |
as through academic knowledge transfer into products and design groups |
and we know the size limit on data sent in |
T WO P OOLS We proceed to analyze the case |
we know the size limit on data sent in response |
while passing all update transactions directly to the backend database |
WO P OOLS We proceed to analyze the case where |
Microsoft also provides research funding for some relevant groups and |
know the size limit on data sent in response to |
P OOLS We proceed to analyze the case where two |
also provides research funding for some relevant groups and fellowship |
Each cache server is unaware of the other servers it |
the size limit on data sent in response to explicit |
OOLS We proceed to analyze the case where two pools |
provides research funding for some relevant groups and fellowship and |
cache server is unaware of the other servers it has |
size limit on data sent in response to explicit requests |
We proceed to analyze the case where two pools may |
research funding for some relevant groups and fellowship and research |
server is unaware of the other servers it has its |
proceed to analyze the case where two pools may attack |
funding for some relevant groups and fellowship and research internships |
we can predict the amount of time that will be |
is unaware of the other servers it has its own |
to analyze the case where two pools may attack each |
for some relevant groups and fellowship and research internships for |
can predict the amount of time that will be needed |
unaware of the other servers it has its own clients |
analyze the case where two pools may attack each other |
some relevant groups and fellowship and research internships for students |
predict the amount of time that will be needed to |
of the other servers it has its own clients and |
the case where two pools may attack each other and |
the amount of time that will be needed to repair |
the other servers it has its own clients and communicates |
Summary Four years of research on Windows NT have taught |
case where two pools may attack each other and the |
amount of time that will be needed to repair the |
other servers it has its own clients and communicates directly |
Four years of research on Windows NT have taught us |
where two pools may attack each other and the other |
of time that will be needed to repair the resulting |
servers it has its own clients and communicates directly with |
years of research on Windows NT have taught us that |
two pools may attack each other and the other miners |
time that will be needed to repair the resulting data |
it has its own clients and communicates directly with the |
of research on Windows NT have taught us that we |
pools may attack each other and the other miners mine |
that will be needed to repair the resulting data inconsistency |
has its own clients and communicates directly with the backend |
research on Windows NT have taught us that we made |
may attack each other and the other miners mine solo |
its own clients and communicates directly with the backend database |
on Windows NT have taught us that we made the |
These capabilities should help the developer parameterize the cluster to |
Windows NT have taught us that we made the right |
capabilities should help the developer parameterize the cluster to balance |
NT have taught us that we made the right choice |
only transactions can be arbitrarily high or low in this |
should help the developer parameterize the cluster to balance overhead |
have taught us that we made the right choice in |
transactions can be arbitrarily high or low in this situation |
help the developer parameterize the cluster to balance overhead for |
taught us that we made the right choice in leaving |
the developer parameterize the cluster to balance overhead for gossip |
us that we made the right choice in leaving the |
developer parameterize the cluster to balance overhead for gossip against |
that we made the right choice in leaving the Unix |
parameterize the cluster to balance overhead for gossip against repair |
Our simulation focuses on just a single cache it would |
The total mining power in the system is m x |
we made the right choice in leaving the Unix behind |
the cluster to balance overhead for gossip against repair times |
simulation focuses on just a single cache it would behave |
cluster to balance overhead for gossip against repair times desired |
focuses on just a single cache it would behave the |
to balance overhead for gossip against repair times desired by |
on just a single cache it would behave the same |
balance overhead for gossip against repair times desired by the |
just a single cache it would behave the same had |
of the pools from mining are their effective mining rates |
overhead for gossip against repair times desired by the application |
a single cache it would behave the same had there |
single cache it would behave the same had there been |
cache it would behave the same had there been many |
it would behave the same had there been many cache |
It took quite some time to reach the same level |
would behave the same had there been many cache servers |
Membership Some readers may be curious about what will seem |
took quite some time to reach the same level of |
Some readers may be curious about what will seem to |
quite some time to reach the same level of knowledge |
readers may be curious about what will seem to be |
some time to reach the same level of knowledge and |
may be curious about what will seem to be a |
Cache can be used with any transactional backend and any |
time to reach the same level of knowledge and insight |
be curious about what will seem to be a chicken |
can be used with any transactional backend and any transactional |
to reach the same level of knowledge and insight we |
be used with any transactional backend and any transactional workload |
reach the same level of knowledge and insight we used |
the same level of knowledge and insight we used to |
same level of knowledge and insight we used to have |
level of knowledge and insight we used to have of |
of knowledge and insight we used to have of Unix |
we use gossip epidemics to propagate information about membership changes |
knowledge and insight we used to have of Unix systems |
Yet the gossip protocol uses membership information to select gossip |
but now that we have arrived at that same knowledge |
the gossip protocol uses membership information to select gossip peers |
now that we have arrived at that same knowledge point |
we will use synthetic workloads so we can evaluate how |
will use synthetic workloads so we can evaluate how much |
is it clear that our research is making progress faster |
use synthetic workloads so we can evaluate how much inconsistency |
it clear that our research is making progress faster than |
extracted from a group management service component that list the |
synthetic workloads so we can evaluate how much inconsistency can |
clear that our research is making progress faster than ever |
from a group management service component that list the nodes |
workloads so we can evaluate how much inconsistency can be |
that our research is making progress faster than ever before |
a group management service component that list the nodes in |
so we can evaluate how much inconsistency can be observed |
group management service component that list the nodes in the |
we can evaluate how much inconsistency can be observed as |
management service component that list the nodes in the cluster |
can evaluate how much inconsistency can be observed as a |
service component that list the nodes in the cluster and |
but because of the zealous attacks by colleagues and other |
evaluate how much inconsistency can be observed as a function |
component that list the nodes in the cluster and the |
because of the zealous attacks by colleagues and other researchers |
how much inconsistency can be observed as a function of |
that list the nodes in the cluster and the rough |
much inconsistency can be observed as a function of the |
list the nodes in the cluster and the rough mapping |
Publishing papers about research performed on Windows NT is still |
inconsistency can be observed as a function of the amount |
the nodes in the cluster and the rough mapping of |
papers about research performed on Windows NT is still quite |
can be observed as a function of the amount of |
nodes in the cluster and the rough mapping of services |
about research performed on Windows NT is still quite difficult |
be observed as a function of the amount of clustering |
in the cluster and the rough mapping of services to |
research performed on Windows NT is still quite difficult as |
observed as a function of the amount of clustering in |
the cluster and the rough mapping of services to those |
performed on Windows NT is still quite difficult as many |
as a function of the amount of clustering in the |
cluster and the rough mapping of services to those nodes |
on Windows NT is still quite difficult as many of |
a function of the amount of clustering in the workload |
The total revenue of each pool is its direct mining |
Windows NT is still quite difficult as many of our |
total revenue of each pool is its direct mining revenue |
NT is still quite difficult as many of our peer |
This also allows us to look at the dynamic behavior |
A different concern relates to behavior when membership information is |
is still quite difficult as many of our peer still |
also allows us to look at the dynamic behavior of |
which is the attacked pool s total revenue multiplied by |
different concern relates to behavior when membership information is perceived |
still quite difficult as many of our peer still believe |
allows us to look at the dynamic behavior of the |
is the attacked pool s total revenue multiplied by its |
concern relates to behavior when membership information is perceived differently |
quite difficult as many of our peer still believe that |
us to look at the dynamic behavior of the system |
the attacked pool s total revenue multiplied by its infiltration |
relates to behavior when membership information is perceived differently at |
difficult as many of our peer still believe that no |
attacked pool s total revenue multiplied by its infiltration rate |
to behavior when membership information is perceived differently at different |
when the amount of clustering and the clustering formation change |
as many of our peer still believe that no good |
The pool s total revenue is divided among its loyal |
behavior when membership information is perceived differently at different nodes |
the amount of clustering and the clustering formation change over |
many of our peer still believe that no good research |
pool s total revenue is divided among its loyal miners |
amount of clustering and the clustering formation change over time |
of our peer still believe that no good research can |
s total revenue is divided among its loyal miners and |
these quickly resolve as additional rounds of gossip replace stale |
our peer still believe that no good research can be |
total revenue is divided among its loyal miners and miners |
quickly resolve as additional rounds of gossip replace stale data |
we will look at workloads based on Amazon s product |
peer still believe that no good research can be performed |
revenue is divided among its loyal miners and miners that |
resolve as additional rounds of gossip replace stale data with |
will look at workloads based on Amazon s product co |
still believe that no good research can be performed on |
is divided among its loyal miners and miners that infiltrated |
as additional rounds of gossip replace stale data with more |
believe that no good research can be performed on Windows |
divided among its loyal miners and miners that infiltrated it |
purchasing and Orkut s social network to see how much |
additional rounds of gossip replace stale data with more accurate |
that no good research can be performed on Windows NT |
and Orkut s social network to see how much inconsistency |
Orkut s social network to see how much inconsistency T |
We hope that eventually the advanced technical nature of the |
hope that eventually the advanced technical nature of the operating |
Cache can detect as a function of dependency list length |
that eventually the advanced technical nature of the operating system |
we have never observed a membership inconsistency that persisted for |
eventually the advanced technical nature of the operating system will |
have never observed a membership inconsistency that persisted for longer |
the advanced technical nature of the operating system will prevail |
never observed a membership inconsistency that persisted for longer than |
advanced technical nature of the operating system will prevail in |
particularly the additional load on the backend database that could |
observed a membership inconsistency that persisted for longer than a |
technical nature of the operating system will prevail in the |
the additional load on the backend database that could form |
a membership inconsistency that persisted for longer than a few |
nature of the operating system will prevail in the discussion |
additional load on the backend database that could form if |
membership inconsistency that persisted for longer than a few hundred |
load on the backend database that could form if the |
inconsistency that persisted for longer than a few hundred milliseconds |
and that we can have a community where research results |
on the backend database that could form if the the |
that we can have a community where research results can |
the backend database that could form if the the rate |
we can have a community where research results can be |
backend database that could form if the the rate of |
can have a community where research results can be shared |
database that could form if the the rate of cache |
have a community where research results can be shared without |
that could form if the the rate of cache misses |
a community where research results can be shared without sarcasm |
could form if the the rate of cache misses increases |
community where research results can be shared without sarcasm or |
where research results can be shared without sarcasm or the |
research results can be shared without sarcasm or the risk |
results can be shared without sarcasm or the risk of |
can be shared without sarcasm or the risk of igniting |
be shared without sarcasm or the risk of igniting yet |
shared without sarcasm or the risk of igniting yet another |
without sarcasm or the risk of igniting yet another holy |
Failure and recovery Process failure detection is accomplished by means |
sarcasm or the risk of igniting yet another holy war |
and recovery Process failure detection is accomplished by means of |
Synthetic Workloads Synthetic workloads allow us to understand the efficacy |
recovery Process failure detection is accomplished by means of two |
Workloads Synthetic workloads allow us to understand the efficacy of |
Process failure detection is accomplished by means of two mechanisms |
Synthetic workloads allow us to understand the efficacy of T |
Process Communication Primitives for Programming Distributed Systems Robbert van Renesse |
in our case they are TCP channels with low value |
Representation The following position paper describes a new InterProcess Communication |
our case they are TCP channels with low value for |
case they are TCP channels with low value for the |
primitive that is designed to make it easier to program |
they are TCP channels with low value for the SO |
that is designed to make it easier to program distributed |
are TCP channels with low value for the SO TIMEOUT |
is designed to make it easier to program distributed algorithms |
TCP channels with low value for the SO TIMEOUT property |
It is largely based on my experience in implementing algorithms |
is largely based on my experience in implementing algorithms such |
largely based on my experience in implementing algorithms such as |
based on my experience in implementing algorithms such as distributed |
on my experience in implementing algorithms such as distributed consensus |
I would be happy to present this idea at the |
would be happy to present this idea at the workshop |
measures how many inconsistencies we can detect as a function |
IPC allows processes to share information and to synchronize actions |
how many inconsistencies we can detect as a function of |
the information is propagated within the group in two ways |
many inconsistencies we can detect as a function of clustering |
inconsistencies we can detect as a function of clustering and |
we can detect as a function of clustering and Section |
each pool will optimize its infiltration rate of the other |
can detect as a function of clustering and Section V |
while SM allows processes to share data directly while synchronizing |
the process that has detected the membership change feeds the |
SM allows processes to share data directly while synchronizing using |
process that has detected the membership change feeds the event |
allows processes to share data directly while synchronizing using such |
that has detected the membership change feeds the event description |
processes to share data directly while synchronizing using such primitives |
has detected the membership change feeds the event description into |
to share data directly while synchronizing using such primitives as |
detected the membership change feeds the event description into the |
share data directly while synchronizing using such primitives as mutexes |
the membership change feeds the event description into the chain |
data directly while synchronizing using such primitives as mutexes and |
compares the efficacy of various approaches to dealing with detected |
membership change feeds the event description into the chain itself |
directly while synchronizing using such primitives as mutexes and condition |
the efficacy of various approaches to dealing with detected inconsistencies |
while synchronizing using such primitives as mutexes and condition variables |
MC is dominant as e orts to support the SM |
is dominant as e orts to support the SM paradigm |
dominant as e orts to support the SM paradigm have |
as e orts to support the SM paradigm have not |
e orts to support the SM paradigm have not been |
orts to support the SM paradigm have not been successful |
the same detector process starts up a backup gossip notification |
The MC and SM paradigms are duals in that one |
same detector process starts up a backup gossip notification stream |
MC and SM paradigms are duals in that one can |
and SM paradigms are duals in that one can be |
SM paradigms are duals in that one can be implememted |
paradigms are duals in that one can be implememted using |
are duals in that one can be implememted using the |
duals in that one can be implememted using the other |
but they also each have their advantages and disadvantages when |
they also each have their advantages and disadvantages when compared |
also each have their advantages and disadvantages when compared with |
each have their advantages and disadvantages when compared with one |
have their advantages and disadvantages when compared with one another |
The FIFO channels are rebuilt appropriately by the processes that |
It is useful to consider how distributed algorithms such as |
FIFO channels are rebuilt appropriately by the processes that identify |
is useful to consider how distributed algorithms such as replication |
channels are rebuilt appropriately by the processes that identify themselves |
are rebuilt appropriately by the processes that identify themselves to |
rebuilt appropriately by the processes that identify themselves to be |
in order for some process to be able to make |
appropriately by the processes that identify themselves to be affected |
order for some process to be able to make a |
by the processes that identify themselves to be affected by |
for some process to be able to make a transition |
the processes that identify themselves to be affected by the |
it needs to know that one or more other processes |
processes that identify themselves to be affected by the membership |
needs to know that one or more other processes have |
that identify themselves to be affected by the membership change |
to know that one or more other processes have reached |
know that one or more other processes have reached a |
that one or more other processes have reached a particular |
one or more other processes have reached a particular milestone |
a new leader in Paxos needs to know that a |
new leader in Paxos needs to know that a quorum |
leader in Paxos needs to know that a quorum of |
in Paxos needs to know that a quorum of acceptors |
Paxos needs to know that a quorum of acceptors have |
update sources can use this update to reconnect to a |
needs to know that a quorum of acceptors have progressed |
sources can use this update to reconnect to a new |
to know that a quorum of acceptors have progressed to |
can use this update to reconnect to a new head |
know that a quorum of acceptors have progressed to its |
use this update to reconnect to a new head of |
that a quorum of acceptors have progressed to its proposed |
this update to reconnect to a new head of any |
a quorum of acceptors have progressed to its proposed ballot |
update to reconnect to a new head of any chain |
quorum of acceptors have progressed to its proposed ballot and |
to reconnect to a new head of any chain that |
of acceptors have progressed to its proposed ballot and it |
reconnect to a new head of any chain that may |
acceptors have progressed to its proposed ballot and it needs |
to a new head of any chain that may have |
have progressed to its proposed ballot and it needs to |
a new head of any chain that may have lost |
progressed to its proposed ballot and it needs to know |
new head of any chain that may have lost its |
to its proposed ballot and it needs to know what |
head of any chain that may have lost its previous |
clustering is perfect and each transaction chooses a single cluster |
its proposed ballot and it needs to know what the |
of any chain that may have lost its previous head |
is perfect and each transaction chooses a single cluster and |
proposed ballot and it needs to know what the highest |
any chain that may have lost its previous head as |
perfect and each transaction chooses a single cluster and chooses |
ballot and it needs to know what the highest accepted |
chain that may have lost its previous head as a |
and it needs to know what the highest accepted proposals |
that may have lost its previous head as a consequence |
it needs to know what the highest accepted proposals from |
needs to know what the highest accepted proposals from those |
may have lost its previous head as a consequence of |
times with repetitions within this cluster to establish its access |
to know what the highest accepted proposals from those acceptors |
have lost its previous head as a consequence of the |
with repetitions within this cluster to establish its access set |
know what the highest accepted proposals from those acceptors are |
lost its previous head as a consequence of the crash |
Many if not all distributed algorithms can be cleanly expressed |
The revenue function for ri is concave in xi for |
if not all distributed algorithms can be cleanly expressed this |
In the second type of workloads access is not fully |
revenue function for ri is concave in xi for all |
not all distributed algorithms can be cleanly expressed this way |
the second type of workloads access is not fully contained |
as a collection of transition specifications that specify under which |
function for ri is concave in xi for all feasible |
second type of workloads access is not fully contained within |
a collection of transition specifications that specify under which conditions |
for ri is concave in xi for all feasible values |
type of workloads access is not fully contained within each |
it starts by sending a request to a random member |
collection of transition specifications that specify under which conditions they |
ri is concave in xi for all feasible values of |
of workloads access is not fully contained within each cluster |
starts by sending a request to a random member of |
of transition specifications that specify under which conditions they are |
is concave in xi for all feasible values of the |
by sending a request to a random member of the |
transition specifications that specify under which conditions they are enabled |
concave in xi for all feasible values of the variables |
sending a request to a random member of the group |
specifications that specify under which conditions they are enabled and |
that specify under which conditions they are enabled and what |
specify under which conditions they are enabled and what state |
under which conditions they are enabled and what state they |
which conditions they are enabled and what state they need |
conditions they are enabled and what state they need from |
they are enabled and what state they need from other |
are enabled and what state they need from other processes |
Each object is chosen using a bounded Pareto distribution starting |
the group member will commence a membership change protocol as |
object is chosen using a bounded Pareto distribution starting at |
While the SM paradigm seems the best fit for this |
group member will commence a membership change protocol as described |
is chosen using a bounded Pareto distribution starting at Detected |
are unique and are either at the borders of the |
the SM paradigm seems the best fit for this model |
member will commence a membership change protocol as described above |
chosen using a bounded Pareto distribution starting at Detected Inconsistencies |
unique and are either at the borders of the feasible |
SM paradigm seems the best fit for this model of |
and are either at the borders of the feasible region |
paradigm seems the best fit for this model of distributed |
Again once all the nodes receive the membership event and |
are either at the borders of the feasible region or |
seems the best fit for this model of distributed algorithms |
once all the nodes receive the membership event and update |
either at the borders of the feasible region or where |
all the nodes receive the membership event and update their |
at the borders of the feasible region or where ri |
the nodes receive the membership event and update their view |
it is notoriously errorprone as programmers are having difficulty utilizing |
is notoriously errorprone as programmers are having difficulty utilizing the |
notoriously errorprone as programmers are having difficulty utilizing the synchronization |
errorprone as programmers are having difficulty utilizing the synchronization primitives |
as programmers are having difficulty utilizing the synchronization primitives correctly |
since each pool can increase its revenue by choosing a |
The MC paradigm can be used instead but is awkward |
each pool can increase its revenue by choosing a strictly |
MC paradigm can be used instead but is awkward and |
pool can increase its revenue by choosing a strictly positive |
paradigm can be used instead but is awkward and error |
Implementation Details The framework was implemented using the Java language |
can increase its revenue by choosing a strictly positive infiltration |
prone as well it requires the programmer to figure out |
Details The framework was implemented using the Java language and |
increase its revenue by choosing a strictly positive infiltration rate |
as well it requires the programmer to figure out which |
The framework was implemented using the Java language and its |
well it requires the programmer to figure out which processes |
framework was implemented using the Java language and its non |
it requires the programmer to figure out which processes should |
requires the programmer to figure out which processes should send |
the programmer to figure out which processes should send which |
programmer to figure out which processes should send which data |
to figure out which processes should send which data to |
figure out which processes should send which data to which |
out which processes should send which data to which destinations |
which processes should send which data to which destinations at |
processes should send which data to which destinations at which |
should send which data to which destinations at which times |
The system design was strongly influenced by prior work on |
send which data to which destinations at which times in |
system design was strongly influenced by prior work on highperformance |
which data to which destinations at which times in order |
design was strongly influenced by prior work on highperformance services |
data to which destinations at which times in order to |
was strongly influenced by prior work on highperformance services platforms |
to which destinations at which times in order to ensure |
which destinations at which times in order to ensure that |
destinations at which times in order to ensure that recipients |
at which times in order to ensure that recipients of |
which times in order to ensure that recipients of this |
times in order to ensure that recipients of this data |
in order to ensure that recipients of this data can |
order to ensure that recipients of this data can make |
to ensure that recipients of this data can make progress |
Sometimes messages are lost if the receiver starts execution after |
messages are lost if the receiver starts execution after the |
are lost if the receiver starts execution after the sender |
lost if the receiver starts execution after the sender has |
if the receiver starts execution after the sender has started |
the receiver starts execution after the sender has started sending |
receiver starts execution after the sender has started sending messages |
starts execution after the sender has started sending messages to |
execution after the sender has started sending messages to it |
Often needless information is sent as more recent information makes |
There are only four distinct control threads in the component |
needless information is sent as more recent information makes old |
are only four distinct control threads in the component stack |
information is sent as more recent information makes old messages |
only four distinct control threads in the component stack of |
is sent as more recent information makes old messages obsolete |
four distinct control threads in the component stack of a |
distinct control threads in the component stack of a process |
we see that there is a single pair of values |
see that there is a single pair of values for |
that there is a single pair of values for which |
there is a single pair of values for which Equation |
But most MC implementations will carefully deliver each and every |
most MC implementations will carefully deliver each and every one |
delaying delivery of the important information until all obsoleted information |
delivery of the important information until all obsoleted information has |
of the important information until all obsoleted information has been |
the important information until all obsoleted information has been delivered |
important information until all obsoleted information has been delivered as |
information until all obsoleted information has been delivered as well |
We simulate the pool game for a range of pool |
simulate the pool game for a range of pool sizes |
potential deadlock situations due to flow control leading to deadly |
we start the simulation when both pools do not infiltrate |
deadlock situations due to flow control leading to deadly embrace |
start the simulation when both pools do not infiltrate each |
the simulation when both pools do not infiltrate each other |
that that tries to combine the best features of SM |
that tries to combine the best features of SM and |
tries to combine the best features of SM and MC |
nization on state rather than providing a stream of state |
on state rather than providing a stream of state updates |
while from MC it inherits an efficient implementation over the |
from MC it inherits an efficient implementation over the existing |
MC it inherits an efficient implementation over the existing physical |
it inherits an efficient implementation over the existing physical infrastructure |
At each round one pool chooses its optimal infiltration rate |
each round one pool chooses its optimal infiltration rate based |
round one pool chooses its optimal infiltration rate based on |
one pool chooses its optimal infiltration rate based on the |
pool chooses its optimal infiltration rate based on the pool |
chooses its optimal infiltration rate based on the pool sizes |
its optimal infiltration rate based on the pool sizes and |
optimal infiltration rate based on the pool sizes and the |
infiltration rate based on the pool sizes and the rate |
rate based on the pool sizes and the rate with |
based on the pool sizes and the rate with which |
on the pool sizes and the rate with which it |
the pool sizes and the rate with which it is |
pool sizes and the rate with which it is infiltrated |
Recall the players in the pool game are chosen with |
If the pareto variable plus the offset results in a |
the players in the pool game are chosen with the |
it will be the publishers that actively try to push |
the pareto variable plus the offset results in a number |
players in the pool game are chosen with the Round |
will be the publishers that actively try to push new |
pareto variable plus the offset results in a number outside |
in the pool game are chosen with the Round Robin |
be the publishers that actively try to push new facts |
variable plus the offset results in a number outside the |
the pool game are chosen with the Round Robin policy |
the publishers that actively try to push new facts to |
plus the offset results in a number outside the range |
publishers that actively try to push new facts to the |
that actively try to push new facts to the subscribers |
Paxos leaders publish new ballots and push these to acceptors |
leaders publish new ballots and push these to acceptors as |
publish new ballots and push these to acceptors as acceptors |
values results in a single point in each graph in |
new ballots and push these to acceptors as acceptors do |
results in a single point in each graph in Figure |
ballots and push these to acceptors as acceptors do not |
and push these to acceptors as acceptors do not necessarily |
push these to acceptors as acceptors do not necessarily know |
these to acceptors as acceptors do not necessarily know what |
to acceptors as acceptors do not necessarily know what the |
acceptors as acceptors do not necessarily know what the set |
as acceptors do not necessarily know what the set of |
acceptors do not necessarily know what the set of leaders |
do not necessarily know what the set of leaders is |
it will be the subscribers that actively poll the publishers |
leaders and learners both subscribe to acceptors accepting pvalues and |
and learners both subscribe to acceptors accepting pvalues and poll |
learners both subscribe to acceptors accepting pvalues and poll for |
both subscribe to acceptors accepting pvalues and poll for these |
subscribe to acceptors accepting pvalues and poll for these facts |
as subscribers that su ered communication loss due to a |
subscribers that su ered communication loss due to a network |
that su ered communication loss due to a network partition |
su ered communication loss due to a network partition or |
ered communication loss due to a network partition or having |
communication loss due to a network partition or having been |
loss due to a network partition or having been temporarily |
due to a network partition or having been temporarily subscribe |
We start by exploring the importance of the cluster structure |
are the points in each of the graphs with the |
start by exploring the importance of the cluster structure by |
the points in each of the graphs with the respective |
by exploring the importance of the cluster structure by varying |
points in each of the graphs with the respective coordinates |
exploring the importance of the cluster structure by varying the |
the importance of the cluster structure by varying the parameter |
j graphs we draw a border around the region where |
importance of the cluster structure by varying the parameter of |
graphs we draw a border around the region where there |
will The interface requires that the fact type for a |
of the cluster structure by varying the parameter of the |
we draw a border around the region where there is |
The interface requires that the fact type for a par |
the cluster structure by varying the parameter of the Pareto |
continue to poll publishers to receive facts they have ticular |
draw a border around the region where there is no |
cluster structure by varying the parameter of the Pareto distribution |
to poll publishers to receive facts they have ticular topic |
For the ri graphs we draw a line around the |
poll publishers to receive facts they have ticular topic is |
the ri graphs we draw a line around the region |
publishers to receive facts they have ticular topic is totally |
ri graphs we draw a line around the region where |
to receive facts they have ticular topic is totally ordered |
graphs we draw a line around the region where the |
we draw a line around the region where the revenue |
draw a line around the region where the revenue is |
a line around the region where the revenue is the |
line around the region where the revenue is the same |
All this is invisible to the core application be delivered |
around the region where the revenue is the same as |
this is invisible to the core application be delivered in |
the region where the revenue is the same as in |
is invisible to the core application be delivered in order |
region where the revenue is the same as in the |
where the revenue is the same as in the no |
but can be managed through the contally ordered by tagging |
We first observe that only in extreme cases a pool |
can be managed through the contally ordered by tagging it |
first observe that only in extreme cases a pool does |
be managed through the contally ordered by tagging it with |
observe that only in extreme cases a pool does not |
managed through the contally ordered by tagging it with a |
that only in extreme cases a pool does not attack |
through the contally ordered by tagging it with a sequence |
only in extreme cases a pool does not attack its |
the contally ordered by tagging it with a sequence number |
in extreme cases a pool does not attack its counterpart |
at equilibrium a pool will refrain from attacking only if |
equilibrium a pool will refrain from attacking only if the |
based IPC will simplify disbut often times facts such as |
a pool will refrain from attacking only if the other |
IPC will simplify disbut often times facts such as ballots |
pool will refrain from attacking only if the other pool |
will simplify disbut often times facts such as ballots are |
will refrain from attacking only if the other pool is |
simplify disbut often times facts such as ballots are totally |
refrain from attacking only if the other pool is larger |
disbut often times facts such as ballots are totally ortributed |
from attacking only if the other pool is larger than |
often times facts such as ballots are totally ortributed programming |
attacking only if the other pool is larger than about |
times facts such as ballots are totally ortributed programming and |
facts such as ballots are totally ortributed programming and make |
such as ballots are totally ortributed programming and make it |
as ballots are totally ortributed programming and make it easier |
ballots are totally ortributed programming and make it easier to |
are totally ortributed programming and make it easier to reason |
totally ortributed programming and make it easier to reason dered |
ortributed programming and make it easier to reason dered already |
we observe that a pool improves its revenue compared to |
observe that a pool improves its revenue compared to the |
that a pool improves its revenue compared to the no |
and the inconsistency detection ratio is low the dependency lists |
the inconsistency detection ratio is low the dependency lists are |
attacks scenario only when it controls a strict majority of |
inconsistency detection ratio is low the dependency lists are too |
scenario only when it controls a strict majority of the |
most recent fact need be delivered that the paradigm allows |
detection ratio is low the dependency lists are too small |
only when it controls a strict majority of the total |
recent fact need be delivered that the paradigm allows the |
ratio is low the dependency lists are too small to |
when it controls a strict majority of the total mining |
fact need be delivered that the paradigm allows the programmer |
is low the dependency lists are too small to hold |
it controls a strict majority of the total mining power |
need be delivered that the paradigm allows the programmer to |
low the dependency lists are too small to hold all |
be delivered that the paradigm allows the programmer to clearly |
the dependency lists are too small to hold all relevant |
delivered that the paradigm allows the programmer to clearly eventually |
dependency lists are too small to hold all relevant information |
the revenue of the pool is inferior compared to the |
revenue of the pool is inferior compared to the no |
Also specify transitions and under which conditions they di erent |
specify transitions and under which conditions they di erent from |
transitions and under which conditions they di erent from pub |
where neither pool controls a strict majority of the mining |
neither pool controls a strict majority of the mining power |
if no more facts are enabled without having to worry |
no more facts are enabled without having to worry much |
both pools will earn less at equilibrium than if both |
more facts are enabled without having to worry much about |
pools will earn less at equilibrium than if both pools |
facts are enabled without having to worry much about how |
will earn less at equilibrium than if both pools ran |
are enabled without having to worry much about how are |
earn less at equilibrium than if both pools ran without |
enabled without having to worry much about how are published |
the distribution is so spiked that almost all accesses of |
less at equilibrium than if both pools ran without attacking |
without having to worry much about how are published but |
distribution is so spiked that almost all accesses of a |
having to worry much about how are published but some |
We can analyze in this case a game where each |
is so spiked that almost all accesses of a transaction |
to worry much about how are published but some process |
can analyze in this case a game where each pool |
so spiked that almost all accesses of a transaction are |
worry much about how are published but some process later |
analyze in this case a game where each pool chooses |
spiked that almost all accesses of a transaction are within |
much about how are published but some process later subscribes |
in this case a game where each pool chooses either |
that almost all accesses of a transaction are within a |
this case a game where each pool chooses either to |
almost all accesses of a transaction are within a cluster |
case a game where each pool chooses either to attack |
a game where each pool chooses either to attack and |
game where each pool chooses either to attack and optimize |
where each pool chooses either to attack and optimize its |
each pool chooses either to attack and optimize its revenue |
There is also a control interface that controls routing of |
We note that the rate of detected inconsistencies is so |
is also a control interface that controls routing of facts |
note that the rate of detected inconsistencies is so high |
also a control interface that controls routing of facts for |
that the rate of detected inconsistencies is so high at |
a control interface that controls routing of facts for a |
the rate of detected inconsistencies is so high at this |
control interface that controls routing of facts for a particular |
rate of detected inconsistencies is so high at this point |
interface that controls routing of facts for a particular topic |
of detected inconsistencies is so high at this point that |
detected inconsistencies is so high at this point that much |
Paxos acceptors subscribe to ballots and to new proposals from |
inconsistencies is so high at this point that much of |
acceptors subscribe to ballots and to new proposals from leaders |
is so high at this point that much of the |
so high at this point that much of the load |
high at this point that much of the load goes |
and the underlying communication layer will continue retransmission until either |
at this point that much of the load goes to |
the underlying communication layer will continue retransmission until either acknowledged |
this point that much of the load goes to the |
underlying communication layer will continue retransmission until either acknowledged or |
point that much of the load goes to the backend |
communication layer will continue retransmission until either acknowledged or another |
that much of the load goes to the backend database |
layer will continue retransmission until either acknowledged or another fact |
much of the load goes to the backend database and |
will continue retransmission until either acknowledged or another fact renders |
of the load goes to the backend database and saturates |
continue retransmission until either acknowledged or another fact renders it |
the load goes to the backend database and saturates it |
retransmission until either acknowledged or another fact renders it obsolete |
the revenue of each pool is smaller than its revenue |
revenue of each pool is smaller than its revenue if |
of each pool is smaller than its revenue if neither |
each pool is smaller than its revenue if neither pool |
pool is smaller than its revenue if neither pool attacked |
where each pool can change its strategy between attack and |
each pool can change its strategy between attack and no |
over the entire run of each experiment accesses are confined |
the entire run of each experiment accesses are confined to |
entire run of each experiment accesses are confined to the |
run of each experiment accesses are confined to the same |
Cache converges to maintain the correct dependency lists as clusters |
converges to maintain the correct dependency lists as clusters change |
UNIX Application Portability to Windows NT via an Alternative Environment |
Application Portability to Windows NT via an Alternative Environment Subsystem |
Since the dependency lists of the objects are updated using |
the dependency lists of the objects are updated using LRU |
the dependency list of an object o tends to include |
dependency list of an object o tends to include those |
list of an object o tends to include those objects |
of an object o tends to include those objects that |
an object o tends to include those objects that are |
object o tends to include those objects that are frequently |
o tends to include those objects that are frequently accessed |
tends to include those objects that are frequently accessed together |
to include those objects that are frequently accessed together with |
include those objects that are frequently accessed together with o |
Department ABSTRACT The growth in Internet traffic associated with video |
ABSTRACT The growth in Internet traffic associated with video streaming |
Dependencies in a new cluster automatically push out dependencies that |
The growth in Internet traffic associated with video streaming and |
in a new cluster automatically push out dependencies that are |
growth in Internet traffic associated with video streaming and sharing |
a new cluster automatically push out dependencies that are now |
in Internet traffic associated with video streaming and sharing of |
new cluster automatically push out dependencies that are now outside |
Internet traffic associated with video streaming and sharing of videos |
The revenue density of each pool is determined by the |
cluster automatically push out dependencies that are now outside the |
traffic associated with video streaming and sharing of videos is |
revenue density of each pool is determined by the decision |
automatically push out dependencies that are now outside the cluster |
associated with video streaming and sharing of videos is so |
density of each pool is determined by the decision of |
with video streaming and sharing of videos is so rapid |
of each pool is determined by the decision of both |
video streaming and sharing of videos is so rapid that |
each pool is determined by the decision of both pools |
streaming and sharing of videos is so rapid that it |
pool is determined by the decision of both pools whether |
and sharing of videos is so rapid that it may |
is determined by the decision of both pools whether to |
sharing of videos is so rapid that it may soon |
determined by the decision of both pools whether to attack |
of videos is so rapid that it may soon dwarf |
Initially accesses are uniformly at random from the entire set |
by the decision of both pools whether to attack or |
videos is so rapid that it may soon dwarf all |
the decision of both pools whether to attack or not |
is so rapid that it may soon dwarf all other |
so rapid that it may soon dwarf all other forms |
rapid that it may soon dwarf all other forms of |
that it may soon dwarf all other forms of Internet |
however the payoff of both would be larger if they |
it may soon dwarf all other forms of Internet content |
the payoff of both would be larger if they both |
payoff of both would be larger if they both refrain |
of both would be larger if they both refrain from |
One reason for this is that only some forms of |
both would be larger if they both refrain from attacking |
reason for this is that only some forms of content |
for this is that only some forms of content can |
this is that only some forms of content can be |
a pool can detect whether it is being attacked and |
is that only some forms of content can be cached |
then at a single moment they become perfectly clustered into |
pool can detect whether it is being attacked and deduce |
at a single moment they become perfectly clustered into clusters |
can detect whether it is being attacked and deduce that |
a single moment they become perfectly clustered into clusters of |
Data generated in real time such as by live video |
detect whether it is being attacked and deduce that the |
single moment they become perfectly clustered into clusters of size |
generated in real time such as by live video broadcasts |
whether it is being attacked and deduce that the other |
it is being attacked and deduce that the other pool |
is being attacked and deduce that the other pool is |
being attacked and deduce that the other pool is violating |
attacked and deduce that the other pool is violating the |
and deduce that the other pool is violating the agreement |
cooperation where neither pool attacks is a possible stable state |
immersive virtual reality applications and games typically can t be |
virtual reality applications and games typically can t be cached |
reality applications and games typically can t be cached at |
applications and games typically can t be cached at all |
each client may pull such information on its own point |
despite the fact that the single Nash equilibrium in every |
the fact that the single Nash equilibrium in every round |
fact that the single Nash equilibrium in every round is |
that the single Nash equilibrium in every round is to |
even if large numbers of clients share interest in at |
the single Nash equilibrium in every round is to attack |
if large numbers of clients share interest in at least |
large numbers of clients share interest in at least some |
numbers of clients share interest in at least some aspects |
of clients share interest in at least some aspects of |
clients share interest in at least some aspects of the |
shows the percentage of transactions that commit and are consistent |
share interest in at least some aspects of the data |
case As an example we take again the pool sizes |
As an example we take again the pool sizes shown |
an example we take again the pool sizes shown in |
We propose a new system called G RADIENT aimed at |
example we take again the pool sizes shown in Figure |
propose a new system called G RADIENT aimed at reducing |
a new system called G RADIENT aimed at reducing the |
new system called G RADIENT aimed at reducing the load |
system called G RADIENT aimed at reducing the load on |
called G RADIENT aimed at reducing the load on providers |
G RADIENT aimed at reducing the load on providers of |
RADIENT aimed at reducing the load on providers of such |
aimed at reducing the load on providers of such and |
at reducing the load on providers of such and enabling |
reducing the load on providers of such and enabling scalable |
The core of the system is an overlay networking architecture |
core of the system is an overlay networking architecture intended |
of the system is an overlay networking architecture intended to |
the system is an overlay networking architecture intended to run |
system is an overlay networking architecture intended to run directly |
is an overlay networking architecture intended to run directly on |
an overlay networking architecture intended to run directly on a |
overlay networking architecture intended to run directly on a content |
networking architecture intended to run directly on a content hosting |
architecture intended to run directly on a content hosting platform |
flight data to match the ideal stream quality expressed as |
data to match the ideal stream quality expressed as an |
to match the ideal stream quality expressed as an economic |
match the ideal stream quality expressed as an economic utility |
the ideal stream quality expressed as an economic utility of |
ideal stream quality expressed as an economic utility of the |
stream quality expressed as an economic utility of the consuming |
quality expressed as an economic utility of the consuming client |
q I DENTICAL P OOLS Let there be q pools |
I DENTICAL P OOLS Let there be q pools of |
DENTICAL P OOLS Let there be q pools of identical |
INTRODUCTION Recent years have seen skyrocketing demand for Internet bandwidth |
P OOLS Let there be q pools of identical size |
OOLS Let there be q pools of identical size that |
Let there be q pools of identical size that engage |
there be q pools of identical size that engage in |
be q pools of identical size that engage in block |
q pools of identical size that engage in block withholding |
pools of identical size that engage in block withholding against |
of identical size that engage in block withholding against one |
identical size that engage in block withholding against one another |
If trends continue then Internet video alone will generate almost |
It controls its attack rates each of the other pools |
Each of the other pools can attack its peers as |
of the other pools can attack its peers as well |
ISPs and content providers are exploring technologies to help satisfy |
and content providers are exploring technologies to help satisfy the |
content providers are exploring technologies to help satisfy the growing |
providers are exploring technologies to help satisfy the growing demand |
are exploring technologies to help satisfy the growing demand alongside |
exploring technologies to help satisfy the growing demand alongside the |
technologies to help satisfy the growing demand alongside the purchase |
to help satisfy the growing demand alongside the purchase of |
help satisfy the growing demand alongside the purchase of expensive |
satisfy the growing demand alongside the purchase of expensive infrastructure |
Reducing the bandwidth consumption of simultaneous replicated content is a |
the bandwidth consumption of simultaneous replicated content is a challenge |
bandwidth consumption of simultaneous replicated content is a challenge which |
consumption of simultaneous replicated content is a challenge which usually |
of simultaneous replicated content is a challenge which usually leverages |
simultaneous replicated content is a challenge which usually leverages two |
replicated content is a challenge which usually leverages two main |
content is a challenge which usually leverages two main tools |
such as downloads of unencrypted movies or films where many |
as downloads of unencrypted movies or films where many users |
downloads of unencrypted movies or films where many users will |
of unencrypted movies or films where many users will share |
unencrypted movies or films where many users will share the |
movies or films where many users will share the same |
or films where many users will share the same encryption |
films where many users will share the same encryption key |
fledged system will use a software partitioning mechanism based on |
system will use a software partitioning mechanism based on the |
will use a software partitioning mechanism based on the web |
use a software partitioning mechanism based on the web services |
a software partitioning mechanism based on the web services request |
software partitioning mechanism based on the web services request invocation |
partitioning mechanism based on the web services request invocation model |
multicast techniques can reduce the overall network traffic by taking |
techniques can reduce the overall network traffic by taking advantage |
can reduce the overall network traffic by taking advantage of |
reduce the overall network traffic by taking advantage of the |
Although extracting the partitioning key from incoming requests will impose |
and Ma rk Jelasity There is a growing class of |
the overall network traffic by taking advantage of the packet |
extracting the partitioning key from incoming requests will impose some |
Ma rk Jelasity There is a growing class of distributed |
overall network traffic by taking advantage of the packet replication |
the partitioning key from incoming requests will impose some overhead |
rk Jelasity There is a growing class of distributed systems |
network traffic by taking advantage of the packet replication and |
Jelasity There is a growing class of distributed systems applications |
traffic by taking advantage of the packet replication and forwarding |
There is a growing class of distributed systems applications in |
by taking advantage of the packet replication and forwarding within |
fledged system to deviate significantly from what is reported below |
is a growing class of distributed systems applications in which |
taking advantage of the packet replication and forwarding within the |
a growing class of distributed systems applications in which data |
advantage of the packet replication and forwarding within the network |
growing class of distributed systems applications in which data stored |
of the packet replication and forwarding within the network infrastructure |
class of distributed systems applications in which data stored on |
of distributed systems applications in which data stored on client |
distributed systems applications in which data stored on client platforms |
Cache as a function of the strategy taken for handling |
systems applications in which data stored on client platforms must |
as a function of the strategy taken for handling detected |
applications in which data stored on client platforms must be |
a function of the strategy taken for handling detected inconsistencies |
in which data stored on client platforms must be aggregated |
which data stored on client platforms must be aggregated or |
data stored on client platforms must be aggregated or analyzed |
stored on client platforms must be aggregated or analyzed without |
on client platforms must be aggregated or analyzed without revealing |
client platforms must be aggregated or analyzed without revealing private |
platforms must be aggregated or analyzed without revealing private information |
must be aggregated or analyzed without revealing private information to |
be aggregated or analyzed without revealing private information to the |
and EVICT and RETRY reduce the rate of uncommitable transactions |
aggregated or analyzed without revealing private information to the operator |
The devices used by content subscribers have become increasingly heterogeneous |
EVICT and RETRY reduce the rate of uncommitable transactions to |
devices used by content subscribers have become increasingly heterogeneous mobile |
and RETRY reduce the rate of uncommitable transactions to about |
used by content subscribers have become increasingly heterogeneous mobile devices |
and traffic analysis in large cities all depend on the |
traffic analysis in large cities all depend on the analysis |
analysis in large cities all depend on the analysis of |
in large cities all depend on the analysis of data |
large cities all depend on the analysis of data supplied |
cities all depend on the analysis of data supplied by |
all depend on the analysis of data supplied by measurement |
depend on the analysis of data supplied by measurement devices |
yet the clients being tracked are unwilling to reveal such |
implying that a range of subscription rates and policies must |
the clients being tracked are unwilling to reveal such measurement |
that a range of subscription rates and policies must be |
clients being tracked are unwilling to reveal such measurement data |
a range of subscription rates and policies must be applied |
being tracked are unwilling to reveal such measurement data directly |
range of subscription rates and policies must be applied over |
Since the function is concave the equation yields a single |
tracked are unwilling to reveal such measurement data directly to |
of subscription rates and policies must be applied over the |
the function is concave the equation yields a single feasible |
are unwilling to reveal such measurement data directly to the |
subscription rates and policies must be applied over the user |
function is concave the equation yields a single feasible solution |
unwilling to reveal such measurement data directly to the system |
rates and policies must be applied over the user base |
to reveal such measurement data directly to the system owner |
which is a function of the attack rates of the |
is a function of the attack rates of the other |
a function of the attack rates of the other pools |
These systems thus may elicit public opposition despite their useful |
systems thus may elicit public opposition despite their useful features |
a smartphone user will need a differently transcoded version than |
thus may elicit public opposition despite their useful features because |
smartphone user will need a differently transcoded version than the |
may elicit public opposition despite their useful features because of |
user will need a differently transcoded version than the people |
elicit public opposition despite their useful features because of a |
will need a differently transcoded version than the people watching |
public opposition despite their useful features because of a perceived |
need a differently transcoded version than the people watching via |
opposition despite their useful features because of a perceived privacy |
a differently transcoded version than the people watching via Internet |
despite their useful features because of a perceived privacy risk |
differently transcoded version than the people watching via Internet television |
There are ways to upload sensitive data to an aggregator |
are ways to upload sensitive data to an aggregator without |
ways to upload sensitive data to an aggregator without compromising |
to upload sensitive data to an aggregator without compromising privacy |
different consumer groups may desire different local ads or sub |
The equilibrium infiltration rate and the matching revenues are shown |
equilibrium infiltration rate and the matching revenues are shown in |
infiltration rate and the matching revenues are shown in Equation |
One possibility is to keep the data encrypted with keys |
avatars in a virtual world can be viewed as subscribers |
possibility is to keep the data encrypted with keys known |
in a virtual world can be viewed as subscribers to |
is to keep the data encrypted with keys known only |
a virtual world can be viewed as subscribers to updates |
to keep the data encrypted with keys known only to |
virtual world can be viewed as subscribers to updates about |
keep the data encrypted with keys known only to the |
world can be viewed as subscribers to updates about objects |
the data encrypted with keys known only to the clients |
can be viewed as subscribers to updates about objects in |
the revenue at the symmetric equilibrium is inferior to the |
but this requires expensive homomorphic encryption if the aggregator is |
be viewed as subscribers to updates about objects in their |
revenue at the symmetric equilibrium is inferior to the no |
this requires expensive homomorphic encryption if the aggregator is to |
viewed as subscribers to updates about objects in their vicinity |
requires expensive homomorphic encryption if the aggregator is to compute |
expensive homomorphic encryption if the aggregator is to compute directly |
and may want more detailed updates for objects that are |
homomorphic encryption if the aggregator is to compute directly on |
may want more detailed updates for objects that are closer |
encryption if the aggregator is to compute directly on it |
want more detailed updates for objects that are closer to |
more detailed updates for objects that are closer to them |
detailed updates for objects that are closer to them in |
up Our analysis addresses the eventual revenue of the pools |
updates for objects that are closer to them in this |
for objects that are closer to them in this world |
assuming the mining difficulty is set based on the effective |
the mining difficulty is set based on the effective mining |
mining difficulty is set based on the effective mining power |
research on CDNs has generally assumed a homogeneous population of |
on CDNs has generally assumed a homogeneous population of end |
but this imposes restrictions on the kind of aggregation that |
this imposes restrictions on the kind of aggregation that can |
imposes restrictions on the kind of aggregation that can be |
restrictions on the kind of aggregation that can be done |
it would be beneficial to execute needed computation directly on |
would be beneficial to execute needed computation directly on the |
be beneficial to execute needed computation directly on the client |
which has been true for the majority of Bitcoin s |
beneficial to execute needed computation directly on the client platforms |
has been true for the majority of Bitcoin s history |
so that the system operator or analyst only sees aggregate |
Thus most current systems assume multiple video streams to be |
that the system operator or analyst only sees aggregate results |
most current systems assume multiple video streams to be sent |
current systems assume multiple video streams to be sent from |
This approach would provide a better alternative to central aggregation |
systems assume multiple video streams to be sent from the |
approach would provide a better alternative to central aggregation provided |
assume multiple video streams to be sent from the source |
would provide a better alternative to central aggregation provided it |
multiple video streams to be sent from the source at |
if an attacker purchases new mining hardware and employs it |
provide a better alternative to central aggregation provided it is |
video streams to be sent from the source at different |
an attacker purchases new mining hardware and employs it directly |
a better alternative to central aggregation provided it is privacy |
streams to be sent from the source at different resolutions |
attacker purchases new mining hardware and employs it directly for |
to be sent from the source at different resolutions or |
purchases new mining hardware and employs it directly for block |
be sent from the source at different resolutions or that |
new mining hardware and employs it directly for block withholding |
sent from the source at different resolutions or that a |
from the source at different resolutions or that a single |
this mining power is never included in the difficulty calculation |
the source at different resolutions or that a single highquality |
mining power is never included in the difficulty calculation the |
source at different resolutions or that a single highquality stream |
power is never included in the difficulty calculation the system |
at different resolutions or that a single highquality stream is |
is never included in the difficulty calculation the system is |
different resolutions or that a single highquality stream is transcoded |
never included in the difficulty calculation the system is never |
resolutions or that a single highquality stream is transcoded by |
Our experiments were conducted using the SSA framework deployed on |
included in the difficulty calculation the system is never aware |
or that a single highquality stream is transcoded by the |
experiments were conducted using the SSA framework deployed on a |
in the difficulty calculation the system is never aware of |
that a single highquality stream is transcoded by the receiver |
were conducted using the SSA framework deployed on a tightly |
the difficulty calculation the system is never aware of it |
a single highquality stream is transcoded by the receiver who |
conducted using the SSA framework deployed on a tightly coupled |
single highquality stream is transcoded by the receiver who then |
The difficulty is therefore already correctly calculated and the attack |
using the SSA framework deployed on a tightly coupled homogeneous |
highquality stream is transcoded by the receiver who then incurs |
The inconsistency rate drops as the abort rate rises this |
difficulty is therefore already correctly calculated and the attack is |
the SSA framework deployed on a tightly coupled homogeneous cluster |
stream is transcoded by the receiver who then incurs cost |
inconsistency rate drops as the abort rate rises this is |
is therefore already correctly calculated and the attack is profitable |
SSA framework deployed on a tightly coupled homogeneous cluster of |
is transcoded by the receiver who then incurs cost for |
rate drops as the abort rate rises this is desired |
therefore already correctly calculated and the attack is profitable immediately |
they can no longer take advantage of the powerful management |
transcoded by the receiver who then incurs cost for last |
drops as the abort rate rises this is desired as |
can no longer take advantage of the powerful management tools |
The nodes are connected by two separate high speed Ethernet |
as the abort rate rises this is desired as well |
no longer take advantage of the powerful management tools developed |
the attack becomes profitable only after the Bitcoin system has |
nodes are connected by two separate high speed Ethernet backbone |
longer take advantage of the powerful management tools developed for |
attack becomes profitable only after the Bitcoin system has normalized |
The overall rate of consistent committed transactions drops because the |
are connected by two separate high speed Ethernet backbone planes |
take advantage of the powerful management tools developed for today |
becomes profitable only after the Bitcoin system has normalized the |
over the Internet to large number of heterogeneous users simultaneously |
overall rate of consistent committed transactions drops because the probability |
advantage of the powerful management tools developed for today s |
profitable only after the Bitcoin system has normalized the revenues |
the Internet to large number of heterogeneous users simultaneously while |
some placed the control traffic on a different switched Ethernet |
rate of consistent committed transactions drops because the probability of |
of the powerful management tools developed for today s cloud |
only after the Bitcoin system has normalized the revenues by |
Internet to large number of heterogeneous users simultaneously while balancing |
placed the control traffic on a different switched Ethernet segment |
of consistent committed transactions drops because the probability of conflicts |
the powerful management tools developed for today s cloud computing |
after the Bitcoin system has normalized the revenues by adjusting |
to large number of heterogeneous users simultaneously while balancing bandwidth |
the control traffic on a different switched Ethernet segment while |
consistent committed transactions drops because the probability of conflicts in |
powerful management tools developed for today s cloud computing model |
the Bitcoin system has normalized the revenues by adjusting difficulty |
large number of heterogeneous users simultaneously while balancing bandwidth costs |
control traffic on a different switched Ethernet segment while others |
committed transactions drops because the probability of conflicts in the |
traffic on a different switched Ethernet segment while others aggregated |
transactions drops because the probability of conflicts in the clustered |
the revenue of an attacking pool is reduced due to |
on a different switched Ethernet segment while others aggregated both |
drops because the probability of conflicts in the clustered scenario |
clients are isolated network hosts rather than devices within a |
revenue of an attacking pool is reduced due to the |
live content refers to content streams that must be transmitted |
a different switched Ethernet segment while others aggregated both the |
because the probability of conflicts in the clustered scenario is |
are isolated network hosts rather than devices within a single |
of an attacking pool is reduced due to the reduction |
content refers to content streams that must be transmitted to |
different switched Ethernet segment while others aggregated both the control |
the probability of conflicts in the clustered scenario is higher |
isolated network hosts rather than devices within a single administrative |
an attacking pool is reduced due to the reduction in |
refers to content streams that must be transmitted to multiple |
switched Ethernet segment while others aggregated both the control traffic |
network hosts rather than devices within a single administrative domain |
attacking pool is reduced due to the reduction in block |
to content streams that must be transmitted to multiple receivers |
Ethernet segment while others aggregated both the control traffic and |
pool is reduced due to the reduction in block generation |
and often have difficulty maintaining connections to each other through |
content streams that must be transmitted to multiple receivers simultaneously |
segment while others aggregated both the control traffic and the |
is reduced due to the reduction in block generation of |
often have difficulty maintaining connections to each other through firewalls |
while others aggregated both the control traffic and the data |
reduced due to the reduction in block generation of both |
have difficulty maintaining connections to each other through firewalls and |
others aggregated both the control traffic and the data traffic |
ticker updates for financial stocks or object updates in a |
due to the reduction in block generation of both the |
difficulty maintaining connections to each other through firewalls and address |
aggregated both the control traffic and the data traffic on |
updates for financial stocks or object updates in a virtual |
to the reduction in block generation of both the attacking |
maintaining connections to each other through firewalls and address translation |
both the control traffic and the data traffic on the |
for financial stocks or object updates in a virtual world |
the reduction in block generation of both the attacking and |
connections to each other through firewalls and address translation barriers |
the control traffic and the data traffic on the same |
reduction in block generation of both the attacking and attacked |
control traffic and the data traffic on the same segment |
we are not focused on streams with a pause or |
in block generation of both the attacking and attacked pools |
are not focused on streams with a pause or rewind |
not focused on streams with a pause or rewind functions |
since there is no one entity that knows the identities |
focused on streams with a pause or rewind functions or |
but this may be because our control traffic consisted mainly |
there is no one entity that knows the identities of |
on streams with a pause or rewind functions or the |
this may be because our control traffic consisted mainly of |
is no one entity that knows the identities of all |
streams with a pause or rewind functions or the video |
may be because our control traffic consisted mainly of fast |
no one entity that knows the identities of all the |
one entity that knows the identities of all the clients |
and changes in membership may not be detected and propagated |
changes in membership may not be detected and propagated in |
in membership may not be detected and propagated in a |
In the future we hope to explore scenarios that generate |
THE GRADIENT CDN To make progress towards the research question |
membership may not be detected and propagated in a timely |
the future we hope to explore scenarios that generate exceptionally |
we propose a novel networked content delivery system called G |
may not be detected and propagated in a timely fashion |
future we hope to explore scenarios that generate exceptionally heavy |
propose a novel networked content delivery system called G RADIENT |
we hope to explore scenarios that generate exceptionally heavy control |
a novel networked content delivery system called G RADIENT to |
hope to explore scenarios that generate exceptionally heavy control traffic |
novel networked content delivery system called G RADIENT to address |
Without a centralized service to assign and manage node identities |
networked content delivery system called G RADIENT to address the |
which would allow us to explore the benefits of isolation |
content delivery system called G RADIENT to address the complex |
would allow us to explore the benefits of isolation of |
This leads to a sudden increased inconsistency rate that converges |
delivery system called G RADIENT to address the complex caching |
allow us to explore the benefits of isolation of that |
leads to a sudden increased inconsistency rate that converges back |
system called G RADIENT to address the complex caching and |
us to explore the benefits of isolation of that traffic |
to a sudden increased inconsistency rate that converges back to |
called G RADIENT to address the complex caching and multicasting |
peer system is extremely vulnerable to a few malicious peers |
to explore the benefits of isolation of that traffic with |
a sudden increased inconsistency rate that converges back to zero |
G RADIENT to address the complex caching and multicasting issues |
system is extremely vulnerable to a few malicious peers becoming |
explore the benefits of isolation of that traffic with respect |
RADIENT to address the complex caching and multicasting issues associated |
is extremely vulnerable to a few malicious peers becoming a |
the benefits of isolation of that traffic with respect to |
to address the complex caching and multicasting issues associated with |
extremely vulnerable to a few malicious peers becoming a majority |
benefits of isolation of that traffic with respect to data |
address the complex caching and multicasting issues associated with live |
vulnerable to a few malicious peers becoming a majority of |
of isolation of that traffic with respect to data traffic |
the complex caching and multicasting issues associated with live streaming |
to a few malicious peers becoming a majority of the |
B presented three possible strategies for the cache to deal |
complex caching and multicasting issues associated with live streaming of |
a few malicious peers becoming a majority of the apparent |
In the interest of brevity we did not perform any |
presented three possible strategies for the cache to deal with |
caching and multicasting issues associated with live streaming of dynamic |
few malicious peers becoming a majority of the apparent nodes |
the interest of brevity we did not perform any experiments |
three possible strategies for the cache to deal with inconsistency |
and multicasting issues associated with live streaming of dynamic content |
malicious peers becoming a majority of the apparent nodes in |
interest of brevity we did not perform any experiments to |
possible strategies for the cache to deal with inconsistency detection |
multicasting issues associated with live streaming of dynamic content to |
peers becoming a majority of the apparent nodes in the |
of brevity we did not perform any experiments to evaluate |
and solving we obtain a single expression for any ri |
issues associated with live streaming of dynamic content to a |
becoming a majority of the apparent nodes in the system |
brevity we did not perform any experiments to evaluate the |
associated with live streaming of dynamic content to a heterogeneous |
since in the In order to choose its optimal infiltration |
we did not perform any experiments to evaluate the load |
with live streaming of dynamic content to a heterogeneous user |
in the In order to choose its optimal infiltration rate |
because peers usually do not store the entire membership list |
did not perform any experiments to evaluate the load balancing |
live streaming of dynamic content to a heterogeneous user population |
peers usually do not store the entire membership list locally |
a pool has to know the rate at which it |
not perform any experiments to evaluate the load balancing component |
and it is fairly easy for malicious peers to poison |
The systems architecture consists of one or more content providers |
pool has to know the rate at which it is |
perform any experiments to evaluate the load balancing component but |
it is fairly easy for malicious peers to poison local |
systems architecture consists of one or more content providers which |
has to know the rate at which it is attacked |
any experiments to evaluate the load balancing component but we |
is fairly easy for malicious peers to poison local mem |
architecture consists of one or more content providers which together |
experiments to evaluate the load balancing component but we plan |
fairly easy for malicious peers to poison local mem Cornell |
consists of one or more content providers which together form |
A pool can estimate the rate with which it is |
to evaluate the load balancing component but we plan to |
easy for malicious peers to poison local mem Cornell bership |
of one or more content providers which together form a |
pool can estimate the rate with which it is attacked |
evaluate the load balancing component but we plan to do |
for malicious peers to poison local mem Cornell bership views |
one or more content providers which together form a cooperative |
can estimate the rate with which it is attacked by |
the load balancing component but we plan to do so |
malicious peers to poison local mem Cornell bership views so |
or more content providers which together form a cooperative network |
estimate the rate with which it is attacked by comparing |
load balancing component but we plan to do so in |
peers to poison local mem Cornell bership views so that |
more content providers which together form a cooperative network of |
the rate with which it is attacked by comparing the |
balancing component but we plan to do so in the |
to poison local mem Cornell bership views so that they |
content providers which together form a cooperative network of G |
rate with which it is attacked by comparing the rates |
component but we plan to do so in the future |
poison local mem Cornell bership views so that they will |
providers which together form a cooperative network of G RADIENT |
with which it is attacked by comparing the rates of |
local mem Cornell bership views so that they will be |
which together form a cooperative network of G RADIENT CDN |
All the experiments involved a single partitioned and replicated service |
which it is attacked by comparing the rates of partial |
mem Cornell bership views so that they will be preferred |
together form a cooperative network of G RADIENT CDN nodes |
it is attacked by comparing the rates of partial and |
Cornell bership views so that they will be preferred as |
is attacked by comparing the rates of partial and full |
bership views so that they will be preferred as neighbors |
The CDN nodes form a dynamic overlay over which the |
the lower portion of the graph is the ratio of |
attacked by comparing the rates of partial and full proofs |
views so that they will be preferred as neighbors by |
CDN nodes form a dynamic overlay over which the content |
lower portion of the graph is the ratio of committed |
with updates coming from client applications that read a high |
by comparing the rates of partial and full proofs of |
so that they will be preferred as neighbors by honest |
nodes form a dynamic overlay over which the content is |
portion of the graph is the ratio of committed transactions |
comparing the rates of partial and full proofs of work |
that they will be preferred as neighbors by honest nodes |
form a dynamic overlay over which the content is delivered |
of the graph is the ratio of committed transactions that |
the rates of partial and full proofs of work it |
All of our partitioning scenarios included at least four subservices |
the graph is the ratio of committed transactions that The |
and for our initial prototypes we will look at spanning |
rates of partial and full proofs of work it receives |
graph is the ratio of committed transactions that The abort |
for our initial prototypes we will look at spanning trees |
of partial and full proofs of work it receives from |
is the ratio of committed transactions that The abort strategy |
partial and full proofs of work it receives from its |
the ratio of committed transactions that The abort strategy provides |
and full proofs of work it receives from its miners |
An architecture in which CDN servers are hosted by ISPs |
We expect these to be typical cases for real deployments |
ratio of committed transactions that The abort strategy provides a |
architecture in which CDN servers are hosted by ISPs to |
expect these to be typical cases for real deployments of |
we explore a new approach that combines the features of |
of committed transactions that The abort strategy provides a significant |
in which CDN servers are hosted by ISPs to reduce |
In order to estimate the revenue densities of the other |
these to be typical cases for real deployments of the |
explore a new approach that combines the features of these |
committed transactions that The abort strategy provides a significant improvement |
which CDN servers are hosted by ISPs to reduce redundant |
order to estimate the revenue densities of the other pools |
to be typical cases for real deployments of the SSA |
a new approach that combines the features of these two |
transactions that The abort strategy provides a significant improvement over |
CDN servers are hosted by ISPs to reduce redundant incoming |
new approach that combines the features of these two extremes |
that The abort strategy provides a significant improvement over a |
servers are hosted by ISPs to reduce redundant incoming bandwidth |
The abort strategy provides a significant improvement over a normal |
Although the idea of a communication system that combines some |
are hosted by ISPs to reduce redundant incoming bandwidth is |
can result in degenerate behavior and are not appropriate configurations |
the idea of a communication system that combines some centralized |
hosted by ISPs to reduce redundant incoming bandwidth is a |
result in degenerate behavior and are not appropriate configurations for |
idea of a communication system that combines some centralized control |
by ISPs to reduce redundant incoming bandwidth is a logical |
in degenerate behavior and are not appropriate configurations for the |
of a communication system that combines some centralized control with |
ISPs to reduce redundant incoming bandwidth is a logical scenario |
degenerate behavior and are not appropriate configurations for the SSA |
a communication system that combines some centralized control with a |
behavior and are not appropriate configurations for the SSA architecture |
and another example is that G RADIENT nodes may as |
communication system that combines some centralized control with a peer |
another example is that G RADIENT nodes may as well |
example is that G RADIENT nodes may as well be |
is that G RADIENT nodes may as well be integrated |
that G RADIENT nodes may as well be integrated into |
G RADIENT nodes may as well be integrated into set |
we are the first to use such a system to |
are the first to use such a system to preserve |
the first to use such a system to preserve privacy |
first to use such a system to preserve privacy while |
to use such a system to preserve privacy while computing |
use such a system to preserve privacy while computing on |
such a system to preserve privacy while computing on sensitive |
a system to preserve privacy while computing on sensitive data |
they are too old for objects that are likely to |
are too old for objects that are likely to be |
This combination is a sensible tradeoff for the kinds of |
too old for objects that are likely to be accessed |
and in fact the expected deployment model would employ a |
combination is a sensible tradeoff for the kinds of systems |
old for objects that are likely to be accessed together |
in fact the expected deployment model would employ a geographically |
is a sensible tradeoff for the kinds of systems we |
for objects that are likely to be accessed together with |
fact the expected deployment model would employ a geographically distributed |
a sensible tradeoff for the kinds of systems we target |
objects that are likely to be accessed together with them |
the expected deployment model would employ a geographically distributed set |
in which there is an owner or operator who can |
that are likely to be accessed together with them in |
expected deployment model would employ a geographically distributed set of |
which there is an owner or operator who can be |
by convention the head of the chain for each group |
are likely to be accessed together with them in future |
deployment model would employ a geographically distributed set of ISPs |
there is an owner or operator who can be trusted |
convention the head of the chain for each group was |
likely to be accessed together with them in future transactions |
model would employ a geographically distributed set of ISPs or |
is an owner or operator who can be trusted to |
the head of the chain for each group was called |
would employ a geographically distributed set of ISPs or small |
an owner or operator who can be trusted to provide |
head of the chain for each group was called node |
employ a geographically distributed set of ISPs or small data |
owner or operator who can be trusted to provide basic |
a geographically distributed set of ISPs or small data centers |
or operator who can be trusted to provide basic services |
geographically distributed set of ISPs or small data centers of |
and all update requests for a partition were routed towards |
operator who can be trusted to provide basic services such |
distributed set of ISPs or small data centers of the |
all update requests for a partition were routed towards this |
who can be trusted to provide basic services such as |
set of ISPs or small data centers of the kind |
Cache with workloads based on two sampled topologies from the |
update requests for a partition were routed towards this node |
can be trusted to provide basic services such as node |
of ISPs or small data centers of the kind operated |
with workloads based on two sampled topologies from the online |
be trusted to provide basic services such as node identification |
ISPs or small data centers of the kind operated by |
Since delivery delays in the chain were measured relative to |
workloads based on two sampled topologies from the online retailer |
trusted to provide basic services such as node identification and |
or small data centers of the kind operated by today |
delivery delays in the chain were measured relative to node |
based on two sampled topologies from the online retailer Amazon |
to provide basic services such as node identification and membership |
small data centers of the kind operated by today s |
on two sampled topologies from the online retailer Amazon and |
provide basic services such as node identification and membership tracking |
data centers of the kind operated by today s CDN |
two sampled topologies from the online retailer Amazon and the |
basic services such as node identification and membership tracking but |
centers of the kind operated by today s CDN providers |
sampled topologies from the online retailer Amazon and the social |
services such as node identification and membership tracking but not |
Expression for ri in a system with pools of equal |
topologies from the online retailer Amazon and the social network |
such as node identification and membership tracking but not to |
for ri in a system with pools of equal size |
from the online retailer Amazon and the social network Orkut |
as node identification and membership tracking but not to see |
node identification and membership tracking but not to see non |
our focus is on content that cannot be usefully cached |
The G RADIENT project aims to exploit and develop two |
who will keep the system running correctly but cannot be |
G RADIENT project aims to exploit and develop two techniques |
Cache on these workloads as a function of maximum dependency |
will keep the system running correctly but cannot be allowed |
RADIENT project aims to exploit and develop two techniques that |
and the previously failed node would become the new tail |
on these workloads as a function of maximum dependency list |
keep the system running correctly but cannot be allowed to |
project aims to exploit and develop two techniques that improve |
the previously failed node would become the new tail of |
these workloads as a function of maximum dependency list size |
the system running correctly but cannot be allowed to see |
aims to exploit and develop two techniques that improve on |
previously failed node would become the new tail of the |
system running correctly but cannot be allowed to see more |
to exploit and develop two techniques that improve on existing |
failed node would become the new tail of the chain |
running correctly but cannot be allowed to see more information |
exploit and develop two techniques that improve on existing CDNs |
The scenario is intended to model a common case in |
correctly but cannot be allowed to see more information than |
compares the efficacy of the three strategies of dealing with |
scenario is intended to model a common case in which |
but cannot be allowed to see more information than he |
the efficacy of the three strategies of dealing with detected |
is intended to model a common case in which the |
cannot be allowed to see more information than he or |
efficacy of the three strategies of dealing with detected inconsistencies |
intended to model a common case in which the failure |
be allowed to see more information than he or she |
to model a common case in which the failure detection |
allowed to see more information than he or she needs |
model a common case in which the failure detection mechanism |
to see more information than he or she needs to |
a common case in which the failure detection mechanism senses |
see more information than he or she needs to know |
common case in which the failure detection mechanism senses a |
case in which the failure detection mechanism senses a transient |
in which the failure detection mechanism senses a transient problem |
We started from a snapshot of Amazon s product co |
Widespread use of streaming video occurs when Internet users watch |
a node that has become overloaded or is unresponsive for |
use of streaming video occurs when Internet users watch major |
node that has become overloaded or is unresponsive for some |
of streaming video occurs when Internet users watch major events |
we introduce a method for constructing a communication overlay among |
that has become overloaded or is unresponsive for some other |
streaming video occurs when Internet users watch major events online |
introduce a method for constructing a communication overlay among the |
has become overloaded or is unresponsive for some other reason |
a method for constructing a communication overlay among the client |
method for constructing a communication overlay among the client nodes |
for constructing a communication overlay among the client nodes that |
Each product sold by the online retailer is a node |
constructing a communication overlay among the client nodes that can |
and does not respond to the heartbeat within the accepted |
product sold by the online retailer is a node and |
a communication overlay among the client nodes that can safely |
does not respond to the heartbeat within the accepted window |
sold by the online retailer is a node and each |
communication overlay among the client nodes that can safely be |
but since they may access the streams from a variety |
by the online retailer is a node and each pair |
overlay among the client nodes that can safely be used |
since they may access the streams from a variety of |
the online retailer is a node and each pair of |
among the client nodes that can safely be used to |
they may access the streams from a variety of devices |
q Symmetric equilibrium values for a system of q pools |
the client nodes that can safely be used to perform |
online retailer is a node and each pair of products |
Symmetric equilibrium values for a system of q pools of |
A node crash that results in a reboot would result |
client nodes that can safely be used to perform aggregation |
retailer is a node and each pair of products purchased |
equilibrium values for a system of q pools of equal |
node crash that results in a reboot would result in |
nodes that can safely be used to perform aggregation and |
is a node and each pair of products purchased in |
The current solution is to provide each user with a |
values for a system of q pools of equal sizes |
crash that results in a reboot would result in similar |
that can safely be used to perform aggregation and computation |
a node and each pair of products purchased in a |
current solution is to provide each user with a direct |
that results in a reboot would result in similar behavior |
often publish this data to demonstrate their honesty to their |
can safely be used to perform aggregation and computation on |
node and each pair of products purchased in a single |
solution is to provide each user with a direct connection |
publish this data to demonstrate their honesty to their miners |
safely be used to perform aggregation and computation on private |
and each pair of products purchased in a single user |
is to provide each user with a direct connection to |
be used to perform aggregation and computation on private data |
causing the TCP link to the upstream node to become |
to provide each user with a direct connection to a |
each pair of products purchased in a single user session |
the TCP link to the upstream node to become congested |
provide each user with a direct connection to a content |
Although this overlay is set up and operated by the |
pair of products purchased in a single user session is |
TCP link to the upstream node to become congested and |
this overlay is set up and operated by the system |
of products purchased in a single user session is an |
link to the upstream node to become congested and starving |
overlay is set up and operated by the system owner |
products purchased in a single user session is an edge |
to the upstream node to become congested and starving downstream |
it provides minimal opportunity for the owner to learn any |
the upstream node to become congested and starving downstream nodes |
provides minimal opportunity for the owner to learn any information |
minimal opportunity for the owner to learn any information about |
opportunity for the owner to learn any information about the |
for the owner to learn any information about the data |
This scenario models a behavior common in experiments on our |
a pool can infiltrate each of the other pools with |
the owner to learn any information about the data being |
scenario models a behavior common in experiments on our cluster |
pool can infiltrate each of the other pools with some |
owner to learn any information about the data being aggregated |
can infiltrate each of the other pools with some nominal |
when a node becomes very busy or the communication subsystem |
to learn any information about the data being aggregated other |
infiltrate each of the other pools with some nominal probing |
a node becomes very busy or the communication subsystem becomes |
learn any information about the data being aggregated other than |
each of the other pools with some nominal probing mining |
node becomes very busy or the communication subsystem becomes heavily |
or reduce the update rate for distant objects in the |
any information about the data being aggregated other than the |
we used a snapshot of the friendship relations graph in |
of the other pools with some nominal probing mining power |
becomes very busy or the communication subsystem becomes heavily loaded |
reduce the update rate for distant objects in the virtual |
information about the data being aggregated other than the final |
TCP at the node upstream from it will sense congestion |
the other pools with some nominal probing mining power and |
the update rate for distant objects in the virtual world |
used a snapshot of the friendship relations graph in the |
about the data being aggregated other than the final result |
at the node upstream from it will sense congestion and |
other pools with some nominal probing mining power and measure |
update rate for distant objects in the virtual world to |
a snapshot of the friendship relations graph in the Orkut |
the data being aggregated other than the final result of |
the node upstream from it will sense congestion and reduce |
pools with some nominal probing mining power and measure the |
rate for distant objects in the virtual world to which |
snapshot of the friendship relations graph in the Orkut social |
data being aggregated other than the final result of the |
node upstream from it will sense congestion and reduce its |
with some nominal probing mining power and measure the revenue |
for distant objects in the virtual world to which the |
of the friendship relations graph in the Orkut social network |
being aggregated other than the final result of the computation |
upstream from it will sense congestion and reduce its window |
some nominal probing mining power and measure the revenue density |
distant objects in the virtual world to which the user |
from it will sense congestion and reduce its window size |
nominal probing mining power and measure the revenue density directly |
objects in the virtual world to which the user has |
If the impacted node is in the middle of the |
it can be used to ensure that no query made |
probing mining power and measure the revenue density directly by |
in the virtual world to which the user has subscribed |
the impacted node is in the middle of the chain |
can be used to ensure that no query made to |
mining power and measure the revenue density directly by monitoring |
The same mechanism will also allow the system to tailor |
be used to ensure that no query made to the |
power and measure the revenue density directly by monitoring the |
same mechanism will also allow the system to tailor to |
used to ensure that no query made to the system |
each user is a node and each pair of users |
and measure the revenue density directly by monitoring the probe |
mechanism will also allow the system to tailor to heterogeneous |
to ensure that no query made to the system reveals |
user is a node and each pair of users with |
measure the revenue density directly by monitoring the probe s |
will also allow the system to tailor to heterogeneous devices |
ensure that no query made to the system reveals the |
is a node and each pair of users with a |
the revenue density directly by monitoring the probe s rewards |
that no query made to the system reveals the contribution |
a node and each pair of users with a friend |
revenue density directly by monitoring the probe s rewards from |
no query made to the system reveals the contribution of |
node and each pair of users with a friend relationship |
density directly by monitoring the probe s rewards from the |
transcoding a high definition broadcast to adapt its resolution to |
query made to the system reveals the contribution of any |
and each pair of users with a friend relationship is |
directly by monitoring the probe s rewards from the pool |
a high definition broadcast to adapt its resolution to serve |
made to the system reveals the contribution of any particular |
each pair of users with a friend relationship is an |
an upstream node can deliberately drop updates on congested TCP |
high definition broadcast to adapt its resolution to serve a |
to the system reveals the contribution of any particular node |
pair of users with a friend relationship is an edge |
Block Withholding Recycling We assume that the infiltrating miners are |
upstream node can deliberately drop updates on congested TCP connections |
definition broadcast to adapt its resolution to serve a population |
Our overlay network looks a bit like a gossip infrastructure |
Withholding Recycling We assume that the infiltrating miners are loyal |
broadcast to adapt its resolution to serve a population of |
Recycling We assume that the infiltrating miners are loyal to |
clock service to evaluate the behavior of the overall system |
to adapt its resolution to serve a population of heterogeneous |
We assume that the infiltrating miners are loyal to the |
service to evaluate the behavior of the overall system in |
adapt its resolution to serve a population of heterogeneous devices |
assume that the infiltrating miners are loyal to the attacker |
with the key difference that the random peer selection of |
its resolution to serve a population of heterogeneous devices from |
to evaluate the behavior of the overall system in various |
the key difference that the random peer selection of gossip |
resolution to serve a population of heterogeneous devices from cell |
Because the sampled topologies are large and we only need |
some of the pool s members may be disloyal infiltrators |
evaluate the behavior of the overall system in various scenarios |
key difference that the random peer selection of gossip is |
to serve a population of heterogeneous devices from cell phones |
the sampled topologies are large and we only need to |
the behavior of the overall system in various scenarios and |
When sending disloyal miners to perform block withholding at other |
difference that the random peer selection of gossip is replaced |
serve a population of heterogeneous devices from cell phones to |
sampled topologies are large and we only need to simulate |
behavior of the overall system in various scenarios and with |
sending disloyal miners to perform block withholding at other pools |
that the random peer selection of gossip is replaced with |
a population of heterogeneous devices from cell phones to tablets |
topologies are large and we only need to simulate a |
of the overall system in various scenarios and with different |
the random peer selection of gossip is replaced with a |
population of heterogeneous devices from cell phones to tablets to |
are large and we only need to simulate a single |
the overall system in various scenarios and with different parameters |
random peer selection of gossip is replaced with a completely |
of heterogeneous devices from cell phones to tablets to IPTV |
large and we only need to simulate a single column |
peer selection of gossip is replaced with a completely deterministic |
A stream of updates of various rates is injected into |
heterogeneous devices from cell phones to tablets to IPTV lowering |
and we only need to simulate a single column of |
selection of gossip is replaced with a completely deterministic function |
stream of updates of various rates is injected into the |
devices from cell phones to tablets to IPTV lowering overall |
we only need to simulate a single column of the |
of updates of various rates is injected into the head |
from cell phones to tablets to IPTV lowering overall bandwidth |
Nodes are assigned virtual IDs that are either integers or |
only need to simulate a single column of the system |
updates of various rates is injected into the head of |
cell phones to tablets to IPTV lowering overall bandwidth costs |
are assigned virtual IDs that are either integers or finite |
need to simulate a single column of the system for |
of various rates is injected into the head of the |
phones to tablets to IPTV lowering overall bandwidth costs without |
assigned virtual IDs that are either integers or finite field |
to simulate a single column of the system for our |
various rates is injected into the head of the chain |
to tablets to IPTV lowering overall bandwidth costs without affecting |
virtual IDs that are either integers or finite field elements |
simulate a single column of the system for our purposes |
tablets to IPTV lowering overall bandwidth costs without affecting viewing |
and each node uses a function based on either modular |
a single column of the system for our purposes one |
a pool needs a sufficient number of verified miners miners |
to IPTV lowering overall bandwidth costs without affecting viewing experience |
each node uses a function based on either modular arithmetic |
single column of the system for our purposes one database |
pool needs a sufficient number of verified miners miners that |
node uses a function based on either modular arithmetic or |
a victim node receives a command that forces it to |
column of the system for our purposes one database server |
network transformations will be applied with pluggable serverlets designed to |
uses a function based on either modular arithmetic or finite |
victim node receives a command that forces it to halt |
needs a sufficient number of verified miners miners that it |
of the system for our purposes one database server and |
the node continues to listen for commands that would restart |
a function based on either modular arithmetic or finite fields |
a sufficient number of verified miners miners that it knows |
transformations will be applied with pluggable serverlets designed to execute |
the system for our purposes one database server and one |
node continues to listen for commands that would restart it |
function based on either modular arithmetic or finite fields to |
sufficient number of verified miners miners that it knows to |
will be applied with pluggable serverlets designed to execute within |
system for our purposes one database server and one cache |
based on either modular arithmetic or finite fields to compute |
number of verified miners miners that it knows to be |
be applied with pluggable serverlets designed to execute within the |
for our purposes one database server and one cache server |
on either modular arithmetic or finite fields to compute the |
send a crash command to the victim node once a |
of verified miners miners that it knows to be loyal |
applied with pluggable serverlets designed to execute within the CDN |
our purposes one database server and one cache server we |
either modular arithmetic or finite fields to compute the order |
a crash command to the victim node once a certain |
with pluggable serverlets designed to execute within the CDN nodes |
purposes one database server and one cache server we down |
modular arithmetic or finite fields to compute the order in |
crash command to the victim node once a certain number |
pluggable serverlets designed to execute within the CDN nodes of |
arithmetic or finite fields to compute the order in which |
command to the victim node once a certain number of |
but this is only in extreme cases when pools are |
serverlets designed to execute within the CDN nodes of G |
or finite fields to compute the order in which it |
to the victim node once a certain number of updates |
this is only in extreme cases when pools are large |
designed to execute within the CDN nodes of G RADIENT |
finite fields to compute the order in which it should |
the victim node once a certain number of updates were |
We use a technique based on random walks that maintains |
fields to compute the order in which it should communicate |
victim node once a certain number of updates were injected |
use a technique based on random walks that maintains important |
speci n Ac details such as the stream data format |
to compute the order in which it should communicate with |
node once a certain number of updates were injected into |
a technique based on random walks that maintains important properties |
compute the order in which it should communicate with the |
once a certain number of updates were injected into the |
technique based on random walks that maintains important properties of |
the order in which it should communicate with the other |
pools typically have loyal mining power either run directly by |
a certain number of updates were injected into the chain |
based on random walks that maintains important properties of the |
order in which it should communicate with the other nodes |
Open issues include understanding what kinds of content may be |
typically have loyal mining power either run directly by the |
on random walks that maintains important properties of the original |
issues include understanding what kinds of content may be subject |
We construct this function to ensure that the network is |
have loyal mining power either run directly by the pool |
random walks that maintains important properties of the original graph |
the victim node will stop participating in the normal protocol |
include understanding what kinds of content may be subject to |
construct this function to ensure that the network is optimally |
loyal mining power either run directly by the pool owners |
victim node will stop participating in the normal protocol and |
understanding what kinds of content may be subject to such |
this function to ensure that the network is optimally robust |
mining power either run directly by the pool owners or |
node will stop participating in the normal protocol and will |
what kinds of content may be subject to such transformation |
function to ensure that the network is optimally robust and |
power either run directly by the pool owners or sold |
We start by choosing a node uniformly and random and |
will stop participating in the normal protocol and will handle |
kinds of content may be subject to such transformation and |
to ensure that the network is optimally robust and efficient |
either run directly by the pool owners or sold as |
start by choosing a node uniformly and random and start |
converging in logarithmic time and tolerating message failures with minimal |
of content may be subject to such transformation and which |
run directly by the pool owners or sold as a |
stop participating in the normal protocol and will handle only |
by choosing a node uniformly and random and start a |
in logarithmic time and tolerating message failures with minimal delay |
content may be subject to such transformation and which dynamic |
directly by the pool owners or sold as a service |
participating in the normal protocol and will handle only wakeup |
choosing a node uniformly and random and start a random |
may be subject to such transformation and which dynamic content |
ensuring that the the system operator cannot infer anything about |
in the normal protocol and will handle only wakeup commands |
a node uniformly and random and start a random walk |
by the pool owners or sold as a service but |
be subject to such transformation and which dynamic content is |
that the the system operator cannot infer anything about the |
the normal protocol and will handle only wakeup commands from |
node uniformly and random and start a random walk from |
the pool owners or sold as a service but run |
subject to such transformation and which dynamic content is not |
the the system operator cannot infer anything about the data |
normal protocol and will handle only wakeup commands from this |
uniformly and random and start a random walk from that |
pool owners or sold as a service but run on |
the system operator cannot infer anything about the data being |
protocol and will handle only wakeup commands from this moment |
to assess the effect of the transformation on quality and |
and random and start a random walk from that location |
owners or sold as a service but run on the |
system operator cannot infer anything about the data being aggregated |
and will handle only wakeup commands from this moment onwards |
assess the effect of the transformation on quality and traffic |
or sold as a service but run on the pool |
operator cannot infer anything about the data being aggregated by |
the effect of the transformation on quality and traffic rates |
sold as a service but run on the pool owners |
cannot infer anything about the data being aggregated by observing |
After a number of updates have been injected since the |
as a service but run on the pool owners hardware |
infer anything about the data being aggregated by observing network |
how transformation should be meaningfully expressed and used by content |
the walk reverts back to the first node and start |
a number of updates have been injected since the crash |
anything about the data being aggregated by observing network traffic |
transformation should be meaningfully expressed and used by content providers |
walk reverts back to the first node and start again |
number of updates have been injected since the crash command |
Even the communication pattern is completely predictable and hence reveals |
of updates have been injected since the crash command was |
and to learn how computationally intensive such transformation methods can |
This is repeated until the target number of nodes have |
the communication pattern is completely predictable and hence reveals nothing |
updates have been injected since the crash command was issued |
to learn how computationally intensive such transformation methods can be |
is repeated until the target number of nodes have been |
However the size of this mining power is considered a |
learn how computationally intensive such transformation methods can be without |
repeated until the target number of nodes have been visited |
malicious nodes cannot significantly deviate from correct behavior without being |
the size of this mining power is considered a trade |
how computationally intensive such transformation methods can be without overloading |
nodes cannot significantly deviate from correct behavior without being detected |
size of this mining power is considered a trade secret |
computationally intensive such transformation methods can be without overloading the |
it has to catch up by obtaining copies of updates |
of this mining power is considered a trade secret and |
intensive such transformation methods can be without overloading the nodes |
has to catch up by obtaining copies of updates that |
and it even tolerates Byzantine failure by a small minority |
this mining power is considered a trade secret and is |
to catch up by obtaining copies of updates that it |
it even tolerates Byzantine failure by a small minority of |
mining power is considered a trade secret and is not |
catch up by obtaining copies of updates that it has |
even tolerates Byzantine failure by a small minority of clients |
The G RADI ENT content delivery system is currently designed |
power is considered a trade secret and is not published |
up by obtaining copies of updates that it has missed |
G RADI ENT content delivery system is currently designed to |
This ensures that important queries will not be corrupted or |
RADI ENT content delivery system is currently designed to use |
Countermeasures As in the case of classical block withholding explained |
ensures that important queries will not be corrupted or blocked |
ENT content delivery system is currently designed to use a |
repetitions of each experiment were enough to yield accurate measurements |
As in the case of classical block withholding explained in |
that important queries will not be corrupted or blocked by |
content delivery system is currently designed to use a spanningtree |
of each experiment were enough to yield accurate measurements with |
transactions are likely to access objects that are topologically close |
in the case of classical block withholding explained in Section |
important queries will not be corrupted or blocked by compromised |
delivery system is currently designed to use a spanningtree overlay |
each experiment were enough to yield accurate measurements with low |
are likely to access objects that are topologically close to |
the case of classical block withholding explained in Section II |
queries will not be corrupted or blocked by compromised devices |
experiment were enough to yield accurate measurements with low variance |
likely to access objects that are topologically close to one |
to access objects that are topologically close to one another |
and that an adversary cannot compromise the privacy of client |
but cannot detect which of its miners is the attacker |
that an adversary cannot compromise the privacy of client data |
shows the update delivery delay for a set of four |
an adversary cannot compromise the privacy of client data by |
it is likely that objects bought together are also viewed |
we need to optimize the overlay to deliver the exact |
adversary cannot compromise the privacy of client data by gaining |
various techniques can be used to encourage miners to submit |
the update delivery delay for a set of four consecutive |
is likely that objects bought together are also viewed and |
need to optimize the overlay to deliver the exact stream |
cannot compromise the privacy of client data by gaining control |
techniques can be used to encourage miners to submit full |
update delivery delay for a set of four consecutive nodes |
likely that objects bought together are also viewed and updated |
to optimize the overlay to deliver the exact stream quality |
compromise the privacy of client data by gaining control of |
can be used to encourage miners to submit full blocks |
delivery delay for a set of four consecutive nodes in |
that objects bought together are also viewed and updated together |
optimize the overlay to deliver the exact stream quality demanded |
the privacy of client data by gaining control of a |
delay for a set of four consecutive nodes in a |
A pool can pay a bonus for submitting a full |
the overlay to deliver the exact stream quality demanded by |
privacy of client data by gaining control of a few |
for a set of four consecutive nodes in a chain |
pool can pay a bonus for submitting a full proof |
overlay to deliver the exact stream quality demanded by users |
of client data by gaining control of a few devices |
can pay a bonus for submitting a full proof of |
to deliver the exact stream quality demanded by users while |
client data by gaining control of a few devices in |
pay a bonus for submitting a full proof of work |
it is likely that data of befriended users are viewed |
deliver the exact stream quality demanded by users while minimizing |
data by gaining control of a few devices in the |
is likely that data of befriended users are viewed and |
This would increase the revenue of the miner that found |
the exact stream quality demanded by users while minimizing bandwidth |
by gaining control of a few devices in the system |
likely that data of befriended users are viewed and updated |
would increase the revenue of the miner that found a |
exact stream quality demanded by users while minimizing bandwidth costs |
that data of befriended users are viewed and updated together |
increase the revenue of the miner that found a block |
the revenue of the miner that found a block while |
revenue of the miner that found a block while reducing |
of the miner that found a block while reducing the |
the miner that found a block while reducing the revenue |
miner that found a block while reducing the revenue of |
that found a block while reducing the revenue of the |
considering two primary inputs in determining the optimal network overlay |
There are three anomalies that can be seen on the |
found a block while reducing the revenue of the other |
are three anomalies that can be seen on the graphs |
a block while reducing the revenue of the other miners |
block while reducing the revenue of the other miners from |
we consider the cost for network edges to carry traffic |
while reducing the revenue of the other miners from this |
The first one is experienced by the victim node for |
Gossip learning with linear models on fully distributed This work |
reducing the revenue of the other miners from this block |
We run a set of experiments similar to the T |
first one is experienced by the victim node for updates |
learning with linear models on fully distributed This work was |
While the average revenue of each miner would stay the |
one is experienced by the victim node for updates injected |
with linear models on fully distributed This work was supported |
the average revenue of each miner would stay the same |
varying cache entry TTL to evaluate the efficacy of this |
is experienced by the victim node for updates injected between |
the exact solution for this optimization problem is intractable it |
cache entry TTL to evaluate the efficacy of this method |
Another approach is to introduce a joining fee by paying |
exact solution for this optimization problem is intractable it is |
entry TTL to evaluate the efficacy of this method in |
approach is to introduce a joining fee by paying new |
solution for this optimization problem is intractable it is NP |
TTL to evaluate the efficacy of this method in reducing |
is to introduce a joining fee by paying new miners |
to evaluate the efficacy of this method in reducing inconsistencies |
The second is experienced by all the other nodes for |
to introduce a joining fee by paying new miners less |
evaluate the efficacy of this method in reducing inconsistencies and |
we have developed algorithms that give an approximate optimal solution |
introduce a joining fee by paying new miners less for |
second is experienced by all the other nodes for update |
the efficacy of this method in reducing inconsistencies and the |
a joining fee by paying new miners less for their |
is experienced by all the other nodes for update messages |
efficacy of this method in reducing inconsistencies and the corresponding |
joining fee by paying new miners less for their work |
in the case of video streams whose quality and traffic |
experienced by all the other nodes for update messages injected |
of this method in reducing inconsistencies and the corresponding overhead |
fee by paying new miners less for their work until |
the case of video streams whose quality and traffic rates |
by all the other nodes for update messages injected at |
by paying new miners less for their work until they |
case of video streams whose quality and traffic rates can |
all the other nodes for update messages injected at around |
paying new miners less for their work until they have |
of video streams whose quality and traffic rates can be |
new miners less for their work until they have established |
video streams whose quality and traffic rates can be downgraded |
miners less for their work until they have established a |
By increasing database access rate to more than twice its |
streams whose quality and traffic rates can be downgraded by |
less for their work until they have established a reputation |
while the third one is a smaller mixed burst for |
increasing database access rate to more than twice its original |
whose quality and traffic rates can be downgraded by G |
for their work until they have established a reputation with |
the third one is a smaller mixed burst for updates |
database access rate to more than twice its original load |
quality and traffic rates can be downgraded by G RADIENT |
their work until they have established a reputation with the |
third one is a smaller mixed burst for updates injected |
access rate to more than twice its original load we |
and traffic rates can be downgraded by G RADIENT CDN |
work until they have established a reputation with the pool |
one is a smaller mixed burst for updates injected at |
rate to more than twice its original load we only |
Miners that seek flexibility may not accept this policy and |
traffic rates can be downgraded by G RADIENT CDN nodes |
to more than twice its original load we only observe |
that seek flexibility may not accept this policy and choose |
more than twice its original load we only observe a |
seek flexibility may not accept this policy and choose another |
we have derived a primaldual approximation algorithm which produces a |
than twice its original load we only observe a reduction |
flexibility may not accept this policy and choose another pool |
axes have different scales to observe how the system handles |
have derived a primaldual approximation algorithm which produces a solution |
twice its original load we only observe a reduction of |
have different scales to observe how the system handles the |
derived a primaldual approximation algorithm which produces a solution whose |
the pool can use a honeypot trap by sending the |
its original load we only observe a reduction of inconsistencies |
different scales to observe how the system handles the transient |
a primaldual approximation algorithm which produces a solution whose total |
pool can use a honeypot trap by sending the miners |
original load we only observe a reduction of inconsistencies of |
scales to observe how the system handles the transient failure |
primaldual approximation algorithm which produces a solution whose total cost |
can use a honeypot trap by sending the miners tasks |
load we only observe a reduction of inconsistencies of about |
to observe how the system handles the transient failure better |
use a honeypot trap by sending the miners tasks which |
the difference between total network traffic costs and aggregate end |
therefore the third anomaly appears to grow with the chain |
a honeypot trap by sending the miners tasks which it |
the third anomaly appears to grow with the chain distance |
honeypot trap by sending the miners tasks which it knows |
third anomaly appears to grow with the chain distance from |
This is more than twice the rate of inconsistencies achieved |
trap by sending the miners tasks which it knows will |
anomaly appears to grow with the chain distance from the |
is more than twice the rate of inconsistencies achieved by |
by sending the miners tasks which it knows will result |
appears to grow with the chain distance from the victim |
more than twice the rate of inconsistencies achieved by T |
sending the miners tasks which it knows will result in |
to grow with the chain distance from the victim node |
the miners tasks which it knows will result in a |
Cache for the retailer workload and only slightly better than |
miners tasks which it knows will result in a full |
for the retailer workload and only slightly better than the |
tasks which it knows will result in a full proof |
since the cause of this anomaly is an artifact of |
the retailer workload and only slightly better than the rate |
which it knows will result in a full proof of |
the cause of this anomaly is an artifact of Java |
retailer workload and only slightly better than the rate of |
it knows will result in a full proof of work |
we see that the algorithm has lower total cost compared |
workload and only slightly better than the rate of inconsistencies |
cause of this anomaly is an artifact of Java s |
see that the algorithm has lower total cost compared to |
and only slightly better than the rate of inconsistencies achieved |
of this anomaly is an artifact of Java s garbage |
that the algorithm has lower total cost compared to a |
only slightly better than the rate of inconsistencies achieved by |
this anomaly is an artifact of Java s garbage collection |
If a miner fails to submit the full proof of |
the algorithm has lower total cost compared to a single |
slightly better than the rate of inconsistencies achieved by T |
anomaly is an artifact of Java s garbage collection mechanism |
a miner fails to submit the full proof of work |
algorithm has lower total cost compared to a single stream |
is an artifact of Java s garbage collection mechanism kicking |
miner fails to submit the full proof of work it |
has lower total cost compared to a single stream source |
an artifact of Java s garbage collection mechanism kicking in |
we generate a transactional workload that accesses products that are |
lower total cost compared to a single stream source and |
fails to submit the full proof of work it is |
A private framework for distributed comsium on Principles of Distributed |
generate a transactional workload that accesses products that are topologically |
total cost compared to a single stream source and a |
to submit the full proof of work it is tagged |
private framework for distributed comsium on Principles of Distributed Computing |
a transactional workload that accesses products that are topologically close |
performed recovery for the updates it has missed during the |
cost compared to a single stream source and a minimum |
submit the full proof of work it is tagged as |
recovery for the updates it has missed during the period |
compared to a single stream source and a minimum spanning |
the full proof of work it is tagged as an |
for the updates it has missed during the period it |
Each transaction starts by picking a node uniformly at random |
to a single stream source and a minimum spanning tree |
full proof of work it is tagged as an attacker |
the updates it has missed during the period it was |
transaction starts by picking a node uniformly at random and |
a single stream source and a minimum spanning tree streaming |
updates it has missed during the period it was down |
starts by picking a node uniformly at random and takes |
single stream source and a minimum spanning tree streaming protocol |
Pools can also incorporate out of band mechanisms to deter |
because the chain delivers new updates at the moment of |
stream source and a minimum spanning tree streaming protocol in |
can also incorporate out of band mechanisms to deter attacks |
the chain delivers new updates at the moment of rejoin |
The nodes visited by the random walk are the objects |
source and a minimum spanning tree streaming protocol in a |
such as verifying the identity of miners or using trusted |
nodes visited by the random walk are the objects the |
all past updates were solely recovered by means of epidemics |
and a minimum spanning tree streaming protocol in a simulation |
as verifying the identity of miners or using trusted computing |
visited by the random walk are the objects the transaction |
a minimum spanning tree streaming protocol in a simulation based |
verifying the identity of miners or using trusted computing technologies |
The second anomaly that shows up in the update delivery |
by the random walk are the objects the transaction accesses |
minimum spanning tree streaming protocol in a simulation based on |
second anomaly that shows up in the update delivery delay |
spanning tree streaming protocol in a simulation based on a |
anomaly that shows up in the update delivery delay for |
tree streaming protocol in a simulation based on a collection |
This would require miners to use specialized hardware and software |
that shows up in the update delivery delay for the |
streaming protocol in a simulation based on a collection of |
shows up in the update delivery delay for the nodes |
protocol in a simulation based on a collection of AS |
up in the update delivery delay for the nodes downstream |
in the update delivery delay for the nodes downstream from |
all these techniques reduce the pool s attractiveness and deter |
the update delivery delay for the nodes downstream from the |
these techniques reduce the pool s attractiveness and deter miners |
update delivery delay for the nodes downstream from the victim |
We found that the abort rate is negligible in all |
delivery delay for the nodes downstream from the victim node |
found that the abort rate is negligible in all runs |
Block Withholding in Practice Long term block withholding attacks are |
delay for the nodes downstream from the victim node reflects |
Withholding in Practice Long term block withholding attacks are difficult |
Efficacy is therefore defined to be the ratio of inconsistent |
for the nodes downstream from the victim node reflects the |
in Practice Long term block withholding attacks are difficult to |
is therefore defined to be the ratio of inconsistent transactions |
the nodes downstream from the victim node reflects the period |
Uniform node sampling service robust against collusions of malicious nodes |
Practice Long term block withholding attacks are difficult to hide |
therefore defined to be the ratio of inconsistent transactions out |
nodes downstream from the victim node reflects the period when |
defined to be the ratio of inconsistent transactions out of |
since miners using an attacked pool would notice the reduced |
downstream from the victim node reflects the period when the |
to be the ratio of inconsistent transactions out of all |
miners using an attacked pool would notice the reduced revenue |
from the victim node reflects the period when the chain |
be the ratio of inconsistent transactions out of all commits |
using an attacked pool would notice the reduced revenue density |
the victim node reflects the period when the chain is |
victim node reflects the period when the chain is broken |
and we can therefore conclude that they are indeed rare |
dependency list maintenance implies storage and bandwidth overhead at both |
During the time it took for the failure detection mechanism |
list maintenance implies storage and bandwidth overhead at both the |
A recent exception is an attack on the Eligius pool |
the time it took for the failure detection mechanism to |
maintenance implies storage and bandwidth overhead at both the database |
recent exception is an attack on the Eligius pool performed |
time it took for the failure detection mechanism to declare |
implies storage and bandwidth overhead at both the database and |
exception is an attack on the Eligius pool performed in |
it took for the failure detection mechanism to declare the |
storage and bandwidth overhead at both the database and the |
is an attack on the Eligius pool performed in May |
took for the failure detection mechanism to declare the node |
and bandwidth overhead at both the database and the cache |
an attack on the Eligius pool performed in May and |
for the failure detection mechanism to declare the node deceased |
as well as compute overhead for dependency list merging at |
attack on the Eligius pool performed in May and June |
well as compute overhead for dependency list merging at the |
as compute overhead for dependency list merging at the server |
compute overhead for dependency list merging at the server and |
overhead for dependency list merging at the server and consistency |
for dependency list merging at the server and consistency checks |
dependency list merging at the server and consistency checks at |
list merging at the server and consistency checks at the |
merging at the server and consistency checks at the cache |
and hence the updates circumvent the gap by means of |
hence the updates circumvent the gap by means of gossip |
the storage required is only for object IDs and versions |
Updates can bypass nodes in the chain using the gossip |
can bypass nodes in the chain using the gossip as |
bypass nodes in the chain using the gossip as it |
nodes in the chain using the gossip as it can |
in the chain using the gossip as it can be |
the chain using the gossip as it can be seen |
chain using the gossip as it can be seen in |
using the gossip as it can be seen in the |
the gossip as it can be seen in the figure |
in the number of objects in the system and O |
more Bitcoin before realizing they were not receiving their payout |
but this phenomenon is less likely as the node receiving |
this phenomenon is less likely as the node receiving the |
The reasons the attack was so easily subverted is the |
phenomenon is less likely as the node receiving the update |
reasons the attack was so easily subverted is the limited |
is less likely as the node receiving the update is |
the attack was so easily subverted is the limited efforts |
less likely as the node receiving the update is farther |
attack was so easily subverted is the limited efforts of |
likely as the node receiving the update is farther away |
The second and potentially more significant overhead is the effect |
was so easily subverted is the limited efforts of the |
as the node receiving the update is farther away downstream |
second and potentially more significant overhead is the effect on |
so easily subverted is the limited efforts of the attackers |
the node receiving the update is farther away downstream from |
The G RADIENT optimization is effective compared to a centralized |
and potentially more significant overhead is the effect on cache |
easily subverted is the limited efforts of the attackers to |
node receiving the update is farther away downstream from the |
G RADIENT optimization is effective compared to a centralized source |
potentially more significant overhead is the effect on cache hit |
subverted is the limited efforts of the attackers to hide |
receiving the update is farther away downstream from the victim |
RADIENT optimization is effective compared to a centralized source and |
more significant overhead is the effect on cache hit ratio |
is the limited efforts of the attackers to hide themselves |
the update is farther away downstream from the victim node |
optimization is effective compared to a centralized source and a |
significant overhead is the effect on cache hit ratio due |
They have only used two payout addresses to collect their |
is effective compared to a centralized source and a minimum |
overhead is the effect on cache hit ratio due to |
have only used two payout addresses to collect their payouts |
effective compared to a centralized source and a minimum spanning |
is the effect on cache hit ratio due to evictions |
compared to a centralized source and a minimum spanning tree |
and so it was possible for the alert pool manager |
the effect on cache hit ratio due to evictions and |
so it was possible for the alert pool manager to |
effect on cache hit ratio due to evictions and hence |
it was possible for the alert pool manager to cluster |
on cache hit ratio due to evictions and hence the |
was possible for the alert pool manager to cluster the |
cache hit ratio due to evictions and hence the database |
possible for the alert pool manager to cluster the attacking |
hit ratio due to evictions and hence the database load |
milliseconds showing that the behavior of the scheme is not |
for the alert pool manager to cluster the attacking miners |
showing that the behavior of the scheme is not a |
the alert pool manager to cluster the attacking miners and |
that the behavior of the scheme is not a fluke |
alert pool manager to cluster the attacking miners and obtain |
The details are deferred to a full report on G |
pool manager to cluster the attacking miners and obtain a |
details are deferred to a full report on G RADIENT |
Note that the delay of the updates delivered at the |
manager to cluster the attacking miners and obtain a statistically |
that the delay of the updates delivered at the victim |
to cluster the attacking miners and obtain a statistically significant |
the delay of the updates delivered at the victim node |
cluster the attacking miners and obtain a statistically significant proof |
CONCLUSION A number of interesting open questions remain the focus |
delay of the updates delivered at the victim node is |
even a minor deterioration in hit ratio can yield a |
the attacking miners and obtain a statistically significant proof of |
A number of interesting open questions remain the focus of |
of the updates delivered at the victim node is significantly |
a minor deterioration in hit ratio can yield a prohibitive |
attacking miners and obtain a statistically significant proof of their |
number of interesting open questions remain the focus of our |
the updates delivered at the victim node is significantly larger |
minor deterioration in hit ratio can yield a prohibitive load |
miners and obtain a statistically significant proof of their wrongdoing |
of interesting open questions remain the focus of our continued |
updates delivered at the victim node is significantly larger than |
deterioration in hit ratio can yield a prohibitive load on |
interesting open questions remain the focus of our continued investigation |
It is unknown whether this was a classical block withholding |
delivered at the victim node is significantly larger than that |
in hit ratio can yield a prohibitive load on the |
is unknown whether this was a classical block withholding attack |
at the victim node is significantly larger than that of |
How diverse are the classes of content that are amenable |
hit ratio can yield a prohibitive load on the backend |
the victim node is significantly larger than that of the |
diverse are the classes of content that are amenable to |
ratio can yield a prohibitive load on the backend database |
victim node is significantly larger than that of the nodes |
are the classes of content that are amenable to our |
node is significantly larger than that of the nodes downstream |
the classes of content that are amenable to our in |
is significantly larger than that of the nodes downstream of |
Each data point is the result of a single run |
significantly larger than that of the nodes downstream of it |
implemented an experimental Bitcoin test network and demonstrated the practicality |
We vary the dependency list size and for each value |
How do we best assess the effect of such transformations |
larger than that of the nodes downstream of it in |
an experimental Bitcoin test network and demonstrated the practicality of |
vary the dependency list size and for each value run |
do we best assess the effect of such transformations on |
than that of the nodes downstream of it in the |
experimental Bitcoin test network and demonstrated the practicality of the |
the dependency list size and for each value run the |
we best assess the effect of such transformations on stream |
that of the nodes downstream of it in the chain |
Bitcoin test network and demonstrated the practicality of the attack |
dependency list size and for each value run the experiment |
In Proceedings of the Sixth Annual ACM Symposium on Principles |
best assess the effect of such transformations on stream quality |
list size and for each value run the experiment for |
Proceedings of the Sixth Annual ACM Symposium on Principles of |
the only node to experience any significant inconsistency window is |
size and for each value run the experiment for the |
of the Sixth Annual ACM Symposium on Principles of Distributed |
Bitcoin s Health Large pools hinder Bitcoin s distributed nature |
only node to experience any significant inconsistency window is the |
and for each value run the experiment for the two |
the Sixth Annual ACM Symposium on Principles of Distributed Computing |
and utilized by the originating content providers to best balance |
s Health Large pools hinder Bitcoin s distributed nature as |
node to experience any significant inconsistency window is the node |
for each value run the experiment for the two workloads |
utilized by the originating content providers to best balance content |
Health Large pools hinder Bitcoin s distributed nature as they |
to experience any significant inconsistency window is the node that |
each value run the experiment for the two workloads and |
Large pools hinder Bitcoin s distributed nature as they put |
experience any significant inconsistency window is the node that failed |
value run the experiment for the two workloads and measure |
How can our overlay respond to churn among G RADIENT |
pools hinder Bitcoin s distributed nature as they put a |
run the experiment for the two workloads and measure the |
queries are performed against its data before it has time |
hinder Bitcoin s distributed nature as they put a lot |
can our overlay respond to churn among G RADIENT nodes |
the experiment for the two workloads and measure the average |
are performed against its data before it has time to |
Bitcoin s distributed nature as they put a lot of |
our overlay respond to churn among G RADIENT nodes realistically |
experiment for the two workloads and measure the average values |
performed against its data before it has time to fully |
s distributed nature as they put a lot of mining |
overlay respond to churn among G RADIENT nodes realistically low |
for the two workloads and measure the average values of |
against its data before it has time to fully recover |
distributed nature as they put a lot of mining power |
respond to churn among G RADIENT nodes realistically low in |
the two workloads and measure the average values of these |
nature as they put a lot of mining power in |
to churn among G RADIENT nodes realistically low in many |
two workloads and measure the average values of these metrics |
as they put a lot of mining power in the |
churn among G RADIENT nodes realistically low in many common |
There were rare cases when gossip circumvented the chain replication |
they put a lot of mining power in the hands |
among G RADIENT nodes realistically low in many common cases |
were rare cases when gossip circumvented the chain replication even |
put a lot of mining power in the hands of |
G RADIENT nodes realistically low in many common cases such |
rare cases when gossip circumvented the chain replication even though |
a lot of mining power in the hands of a |
RADIENT nodes realistically low in many common cases such as |
cases when gossip circumvented the chain replication even though the |
lot of mining power in the hands of a few |
nodes realistically low in many common cases such as video |
when gossip circumvented the chain replication even though the chain |
of mining power in the hands of a few pool |
realistically low in many common cases such as video streaming |
gossip circumvented the chain replication even though the chain was |
mining power in the hands of a few pool managers |
circumvented the chain replication even though the chain was not |
the chain replication even though the chain was not broken |
This has been mostly addressed by community pressure on miners |
has been mostly addressed by community pressure on miners to |
been mostly addressed by community pressure on miners to avoid |
but this happened only for gossip rates close to the |
mostly addressed by community pressure on miners to avoid forming |
how do we ensure that the computational intensity of our |
this happened only for gossip rates close to the update |
addressed by community pressure on miners to avoid forming large |
do we ensure that the computational intensity of our transformations |
happened only for gossip rates close to the update injection |
by community pressure on miners to avoid forming large pools |
we ensure that the computational intensity of our transformations do |
only for gossip rates close to the update injection rate |
In both workloads there is no visible effect on cache |
ensure that the computational intensity of our transformations do not |
Later in this section we will show that even with |
both workloads there is no visible effect on cache hit |
that the computational intensity of our transformations do not place |
in this section we will show that even with these |
workloads there is no visible effect on cache hit ratio |
and mining is still dominated by a small number of |
this section we will show that even with these rapid |
the computational intensity of our transformations do not place too |
mining is still dominated by a small number of large |
The reduction in inconsistency ratio is significantly better for the |
computational intensity of our transformations do not place too much |
section we will show that even with these rapid repairs |
is still dominated by a small number of large pools |
reduction in inconsistency ratio is significantly better for the Next |
intensity of our transformations do not place too much load |
in inconsistency ratio is significantly better for the Next we |
of our transformations do not place too much load on |
inconsistency ratio is significantly better for the Next we compared |
our transformations do not place too much load on our |
ratio is significantly better for the Next we compared our |
transformations do not place too much load on our G |
is significantly better for the Next we compared our technique |
do not place too much load on our G RADIENT |
significantly better for the Next we compared our technique with |
not place too much load on our G RADIENT overlay |
of the messages were delivered by gossip ahead of the |
better for the Next we compared our technique with a |
place too much load on our G RADIENT overlay nodes |
the messages were delivered by gossip ahead of the chain |
for the Next we compared our technique with a simple |
messages were delivered by gossip ahead of the chain for |
the Next we compared our technique with a simple approach |
were delivered by gossip ahead of the chain for gossip |
G RADIENT contributes a novel platform for continued study and |
Next we compared our technique with a simple approach in |
delivered by gossip ahead of the chain for gossip rate |
RADIENT contributes a novel platform for continued study and progress |
we compared our technique with a simple approach in which |
by gossip ahead of the chain for gossip rate identical |
contributes a novel platform for continued study and progress to |
compared our technique with a simple approach in which we |
The fact that block withholding attacks are rarely observed may |
gossip ahead of the chain for gossip rate identical to |
a novel platform for continued study and progress to ever |
our technique with a simple approach in which we limited |
fact that block withholding attacks are rarely observed may indicate |
ahead of the chain for gossip rate identical to the |
novel platform for continued study and progress to ever more |
technique with a simple approach in which we limited the |
that block withholding attacks are rarely observed may indicate that |
of the chain for gossip rate identical to the update |
platform for continued study and progress to ever more effective |
with a simple approach in which we limited the life |
block withholding attacks are rarely observed may indicate that the |
the chain for gossip rate identical to the update injection |
for continued study and progress to ever more effective delivery |
a simple approach in which we limited the life span |
withholding attacks are rarely observed may indicate that the active |
chain for gossip rate identical to the update injection rate |
continued study and progress to ever more effective delivery mechanisms |
attacks are rarely observed may indicate that the active pools |
are rarely observed may indicate that the active pools have |
rarely observed may indicate that the active pools have reached |
observed may indicate that the active pools have reached an |
may indicate that the active pools have reached an implicit |
contains a plot of update injection time against update delivery |
indicate that the active pools have reached an implicit or |
but their probability of being witnessed is reduced by having |
a plot of update injection time against update delivery time |
that the active pools have reached an implicit or explicit |
their probability of being witnessed is reduced by having the |
plot of update injection time against update delivery time for |
the active pools have reached an implicit or explicit agreement |
probability of being witnessed is reduced by having the cache |
of update injection time against update delivery time for the |
active pools have reached an implicit or explicit agreement not |
of being witnessed is reduced by having the cache evict |
update injection time against update delivery time for the victim |
pools have reached an implicit or explicit agreement not to |
being witnessed is reduced by having the cache evict entries |
injection time against update delivery time for the victim node |
have reached an implicit or explicit agreement not to attack |
witnessed is reduced by having the cache evict entries after |
reached an implicit or explicit agreement not to attack one |
Ideally this is a straight line because of chain replication |
is reduced by having the cache evict entries after a |
an implicit or explicit agreement not to attack one another |
reduced by having the cache evict entries after a certain |
by having the cache evict entries after a certain period |
it gracefully catches up and does so quickly for both |
having the cache evict entries after a certain period even |
gracefully catches up and does so quickly for both gossip |
an attacked pool cannot detect which of its miners are |
the cache evict entries after a certain period even if |
catches up and does so quickly for both gossip rates |
attacked pool cannot detect which of its miners are attacking |
cache evict entries after a certain period even if the |
up and does so quickly for both gossip rates identical |
pool cannot detect which of its miners are attacking it |
evict entries after a certain period even if the database |
and does so quickly for both gossip rates identical and |
entries after a certain period even if the database did |
does so quickly for both gossip rates identical and half |
after a certain period even if the database did not |
At some point a pool might miscalculate and decide to |
so quickly for both gossip rates identical and half the |
a certain period even if the database did not indicate |
some point a pool might miscalculate and decide to try |
quickly for both gossip rates identical and half the update |
certain period even if the database did not indicate they |
point a pool might miscalculate and decide to try and |
for both gossip rates identical and half the update injection |
period even if the database did not indicate they are |
a pool might miscalculate and decide to try and increase |
both gossip rates identical and half the update injection rate |
even if the database did not indicate they are invalid |
pool might miscalculate and decide to try and increase its |
might miscalculate and decide to try and increase its revenue |
possibly leading to a constant rate of attacks among pools |
leading to a constant rate of attacks among pools and |
to a constant rate of attacks among pools and a |
a constant rate of attacks among pools and a reduced |
constant rate of attacks among pools and a reduced revenue |
If open pools reach a state where their revenue density |
EVICT and RETRY policies with the Amazon and Orkut workloads |
open pools reach a state where their revenue density is |
pools reach a state where their revenue density is reduced |
reach a state where their revenue density is reduced due |
a state where their revenue density is reduced due to |
state where their revenue density is reduced due to attacks |
miners will leave them in favor of other available options |
evicting conflicting transactions is an effective way of invalidating stale |
conflicting transactions is an effective way of invalidating stale objects |
transactions is an effective way of invalidating stale objects that |
is an effective way of invalidating stale objects that might |
an effective way of invalidating stale objects that might cause |
effective way of invalidating stale objects that might cause problems |
way of invalidating stale objects that might cause problems for |
Such a change may be in favor of Bitcoin as |
of invalidating stale objects that might cause problems for future |
a change may be in favor of Bitcoin as a |
invalidating stale objects that might cause problems for future transactions |
change may be in favor of Bitcoin as a whole |
and form a fine grained distribution of mining power with |
form a fine grained distribution of mining power with many |
Ordering Transactions with Prediction in Distributed Object Stores Ittay Eyal |
a fine grained distribution of mining power with many small |
fine grained distribution of mining power with many small pools |
grained distribution of mining power with many small pools and |
distribution of mining power with many small pools and solo |
of mining power with many small pools and solo miners |
A pool may engage in an attack against another pool |
pool may engage in an attack against another pool not |
may engage in an attack against another pool not to |
engage in an attack against another pool not to increase |
in an attack against another pool not to increase its |
an attack against another pool not to increase its absolute |
attack against another pool not to increase its absolute revenue |
but rather to attract miners by temporarily increasing its revenue |
rather to attract miners by temporarily increasing its revenue relative |
to attract miners by temporarily increasing its revenue relative to |
attract miners by temporarily increasing its revenue relative to a |
miners by temporarily increasing its revenue relative to a competing |
by temporarily increasing its revenue relative to a competing pool |
Recent work has investigated the motivation of pools to utilize |
work has investigated the motivation of pools to utilize part |
has investigated the motivation of pools to utilize part of |
investigated the motivation of pools to utilize part of their |
the motivation of pools to utilize part of their resources |
motivation of pools to utilize part of their resources towards |
of pools to utilize part of their resources towards sabotage |
pools to utilize part of their resources towards sabotage attacks |
to utilize part of their resources towards sabotage attacks against |
Recent years have seen a surge of progress in the |
utilize part of their resources towards sabotage attacks against each |
years have seen a surge of progress in the development |
part of their resources towards sabotage attacks against each other |
have seen a surge of progress in the development of |
because this model facilitates reasoning about system properties and makes |
seen a surge of progress in the development of scalable |
this model facilitates reasoning about system properties and makes possible |
a surge of progress in the development of scalable object |
model facilitates reasoning about system properties and makes possible a |
surge of progress in the development of scalable object stores |
facilitates reasoning about system properties and makes possible a variety |
of progress in the development of scalable object stores that |
reasoning about system properties and makes possible a variety of |
progress in the development of scalable object stores that support |
about system properties and makes possible a variety of highassurance |
in the development of scalable object stores that support transactions |
system properties and makes possible a variety of highassurance guarantees |
The model of those works is different from the pool |
model of those works is different from the pool game |
the ACID model is often avoided in today s large |
of those works is different from the pool game model |
those works is different from the pool game model in |
works is different from the pool game model in two |
is different from the pool game model in two major |
different from the pool game model in two major ways |
from the pool game model in two major ways a |
the pool game model in two major ways a sabotage |
pool game model in two major ways a sabotage attack |
game model in two major ways a sabotage attack does |
Existing approaches typically run transactions speculatively and perform certification after |
model in two major ways a sabotage attack does not |
approaches typically run transactions speculatively and perform certification after they |
in two major ways a sabotage attack does not transfer |
typically run transactions speculatively and perform certification after they complete |
two major ways a sabotage attack does not transfer revenue |
run transactions speculatively and perform certification after they complete to |
major ways a sabotage attack does not transfer revenue from |
transactions speculatively and perform certification after they complete to preserve |
ways a sabotage attack does not transfer revenue from victim |
speculatively and perform certification after they complete to preserve consistency |
a sabotage attack does not transfer revenue from victim to |
sabotage attack does not transfer revenue from victim to attacker |
RAIN an architecture for ACID transactions in a Resilient Archive |
The model is parametrized by the cost of the attack |
Several recent systems implement full fledged atomicity while preserving the |
an architecture for ACID transactions in a Resilient Archive with |
model is parametrized by the cost of the attack and |
recent systems implement full fledged atomicity while preserving the system |
architecture for ACID transactions in a Resilient Archive with Independent |
is parametrized by the cost of the attack and by |
systems implement full fledged atomicity while preserving the system s |
for ACID transactions in a Resilient Archive with Independent Nodes |
parametrized by the cost of the attack and by the |
implement full fledged atomicity while preserving the system s scalability |
The system orders transactions before they begin by employing predictors |
by the cost of the attack and by the mobility |
full fledged atomicity while preserving the system s scalability with |
system orders transactions before they begin by employing predictors that |
the cost of the attack and by the mobility of |
fledged atomicity while preserving the system s scalability with a |
orders transactions before they begin by employing predictors that estimate |
cost of the attack and by the mobility of the |
atomicity while preserving the system s scalability with a wide |
transactions before they begin by employing predictors that estimate the |
of the attack and by the mobility of the miners |
while preserving the system s scalability with a wide variety |
before they begin by employing predictors that estimate the set |
preserving the system s scalability with a wide variety of |
and the analysis demonstrates that when considering only sabotage attacks |
they begin by employing predictors that estimate the set of |
the system s scalability with a wide variety of workloads |
the analysis demonstrates that when considering only sabotage attacks there |
begin by employing predictors that estimate the set of objects |
analysis demonstrates that when considering only sabotage attacks there are |
by employing predictors that estimate the set of objects each |
demonstrates that when considering only sabotage attacks there are regions |
employing predictors that estimate the set of objects each transaction |
that when considering only sabotage attacks there are regions where |
predictors that estimate the set of objects each transaction will |
when considering only sabotage attacks there are regions where no |
that estimate the set of objects each transaction will access |
The miner s dilemma is therefore not manifested in that |
miner s dilemma is therefore not manifested in that model |
Pool competition for miners is an incentive in and of |
competition for miners is an incentive in and of its |
for miners is an incentive in and of its own |
miners is an incentive in and of its own for |
is an incentive in and of its own for mutual |
a transaction reserves a version of each object it will |
an incentive in and of its own for mutual attacks |
transaction reserves a version of each object it will use |
and a pool may therefore choose to perform block withholding |
a pool may therefore choose to perform block withholding even |
pool may therefore choose to perform block withholding even if |
may therefore choose to perform block withholding even if its |
therefore choose to perform block withholding even if its revenue |
choose to perform block withholding even if its revenue would |
to perform block withholding even if its revenue would increase |
perform block withholding even if its revenue would increase only |
block withholding even if its revenue would increase only after |
withholding even if its revenue would increase only after the |
even if its revenue would increase only after the next |
if its revenue would increase only after the next difficult |
its revenue would increase only after the next difficult adjustment |
the analysis of their combination is left for future work |
use lock chains and assume transactions are known in advance |
These methods all scale well and in many cases allow |
methods all scale well and in many cases allow databases |
progress should never depend on the responsiveness of any single |
all scale well and in many cases allow databases to |
should never depend on the responsiveness of any single machine |
We assumed in our analysis that pools do not charge |
scale well and in many cases allow databases to accept |
assumed in our analysis that pools do not charge fees |
well and in many cases allow databases to accept loads |
in our analysis that pools do not charge fees from |
and in many cases allow databases to accept loads similar |
our analysis that pools do not charge fees from their |
in many cases allow databases to accept loads similar to |
analysis that pools do not charge fees from their members |
many cases allow databases to accept loads similar to those |
that pools do not charge fees from their members since |
cases allow databases to accept loads similar to those handled |
pools do not charge fees from their members since such |
allow databases to accept loads similar to those handled by |
do not charge fees from their members since such fees |
databases to accept loads similar to those handled by non |
not charge fees from their members since such fees are |
charge fees from their members since such fees are typically |
fees from their members since such fees are typically nominal |
it prevents concurrency by aborting all but one of the |
prevents concurrency by aborting all but one of the contending |
concurrency by aborting all but one of the contending transactions |
ordering transactions in advance based on the objects they are |
only incoherent caches that respond to queries without access to |
transactions in advance based on the objects they are likely |
incoherent caches that respond to queries without access to the |
in advance based on the objects they are likely to |
caches that respond to queries without access to the backend |
advance based on the objects they are likely to access |
that respond to queries without access to the backend database |
providing ACID transactions in a Resilient Archive with Independent Nodes |
Fees would add a friction element to the flow of |
would add a friction element to the flow of revenue |
add a friction element to the flow of revenue among |
a friction element to the flow of revenue among infiltrated |
friction element to the flow of revenue among infiltrated and |
element to the flow of revenue among infiltrated and infiltrating |
to the flow of revenue among infiltrated and infiltrating pools |
To allow fast recovery from failures our scheme does not |
allow fast recovery from failures our scheme does not introduce |
fast recovery from failures our scheme does not introduce any |
recovery from failures our scheme does not introduce any locks |
would change to take into account a pool fee of |
change to take into account a pool fee of f |
The system consistency and durability rely on a single scalable |
to take into account a pool fee of f Pp |
system consistency and durability rely on a single scalable tier |
take into account a pool fee of f Pp Ri |
consistency and durability rely on a single scalable tier of |
and durability rely on a single scalable tier of highly |
a Platform for Distributed Service Deployment in End User Homes |
supports transactions using locks or communication with the database on |
transactions using locks or communication with the database on each |
using locks or communication with the database on each transaction |
only at a single tier of the system a set |
at a single tier of the system a set of |
a single tier of the system a set of independent |
single tier of the system a set of independent highly |
tier of the system a set of independent highly available |
of the system a set of independent highly available logs |
All other entities may fail and can be replaced instantly |
other entities may fail and can be replaced instantly on |
entities may fail and can be replaced instantly on failure |
the architecture maintains consistency even in the event of false |
architecture maintains consistency even in the event of false suspicion |
reservations serve as suggestions a reservation that is not used |
Update delay as seen by individual processes during persistent link |
serve as suggestions a reservation that is not used because |
delay as seen by individual processes during persistent link congestion |
as suggestions a reservation that is not used because of |
as seen by individual processes during persistent link congestion node |
A pool with a fee of f is a less |
suggestions a reservation that is not used because of a |
pool with a fee of f is a less attractive |
a reservation that is not used because of a sluggish |
with a fee of f is a less attractive target |
reservation that is not used because of a sluggish or |
a fee of f is a less attractive target for |
that is not used because of a sluggish or dead |
fee of f is a less attractive target for block |
is not used because of a sluggish or dead owner |
of f is a less attractive target for block withholding |
not used because of a sluggish or dead owner is |
used because of a sluggish or dead owner is ignored |
However it is also less attractive for miners in general |
Trading off the two for best protection is left for |
off the two for best protection is left for future |
the two for best protection is left for future work |
The Block Withholding Attack The danger of a block withholding |
Block Withholding Attack The danger of a block withholding attack |
Withholding Attack The danger of a block withholding attack is |
We contrast the effectiveness of employing prediction and the scalability |
Attack The danger of a block withholding attack is as |
contrast the effectiveness of employing prediction and the scalability of |
The danger of a block withholding attack is as old |
the effectiveness of employing prediction and the scalability of ACID |
danger of a block withholding attack is as old as |
of a block withholding attack is as old as Bitcoin |
a block withholding attack is as old as Bitcoin pools |
as pools were becoming a dominant player in the Bitcoin |
pools were becoming a dominant player in the Bitcoin world |
used by a miner to sabotage a pool at the |
by a miner to sabotage a pool at the cost |
a miner to sabotage a pool at the cost of |
miner to sabotage a pool at the cost of reducing |
to sabotage a pool at the cost of reducing its |
sabotage a pool at the cost of reducing its own |
a pool at the cost of reducing its own revenue |
A more general view of fairness in proof of work |
more general view of fairness in proof of work schemes |
general view of fairness in proof of work schemes was |
view of fairness in proof of work schemes was discussed |
of fairness in proof of work schemes was discussed in |
System Structure The structure of the system is illustrated in |
Structure The structure of the system is illustrated in Figure |
available logs that together describe the state of the entire |
Early work did not address the possibility of pools infiltrating |
logs that together describe the state of the entire system |
work did not address the possibility of pools infiltrating other |
did not address the possibility of pools infiltrating other pools |
not address the possibility of pools infiltrating other pools for |
address the possibility of pools infiltrating other pools for block |
the possibility of pools infiltrating other pools for block withholding |
time between node failure and rejoin as number of consecutive |
that caches the data and provides the data structure abstraction |
between node failure and rejoin as number of consecutive updates |
caches the data and provides the data structure abstraction exporting |
node failure and rejoin as number of consecutive updates missed |
the data and provides the data structure abstraction exporting read |
failure and rejoin as number of consecutive updates missed by |
data and provides the data structure abstraction exporting read and |
and rejoin as number of consecutive updates missed by the |
and provides the data structure abstraction exporting read and write |
rejoin as number of consecutive updates missed by the victim |
provides the data structure abstraction exporting read and write operations |
as number of consecutive updates missed by the victim node |
experimentally demonstrate that block withholding can increase the attacker s |
demonstrate that block withholding can increase the attacker s revenue |
They receive instructions from clients to start and end a |
receive instructions from clients to start and end a transaction |
and operations to perform on individual objects within the transaction |
have recently noted that a pool can increase its overall |
recently noted that a pool can increase its overall revenue |
the TMs predict which objects it is likely to access |
noted that a pool can increase its overall revenue with |
that a pool can increase its overall revenue with block |
a pool can increase its overall revenue with block withholding |
pool can increase its overall revenue with block withholding if |
can increase its overall revenue with block withholding if all |
they speculatively perform each operation with the help of the |
increase its overall revenue with block withholding if all other |
speculatively perform each operation with the help of the appropriate |
its overall revenue with block withholding if all other mining |
perform each operation with the help of the appropriate OMs |
overall revenue with block withholding if all other mining is |
each operation with the help of the appropriate OMs and |
revenue with block withholding if all other mining is performed |
operation with the help of the appropriate OMs and according |
with block withholding if all other mining is performed by |
with the help of the appropriate OMs and according to |
block withholding if all other mining is performed by honest |
the help of the appropriate OMs and according to the |
withholding if all other mining is performed by honest pools |
help of the appropriate OMs and according to the order |
of the appropriate OMs and according to the order set |
the appropriate OMs and according to the order set by |
We consider the general case where not all mining is |
appropriate OMs and according to the order set by the |
consider the general case where not all mining is performed |
OMs and according to the order set by the reservations |
the general case where not all mining is performed through |
general case where not all mining is performed through public |
case where not all mining is performed through public pools |
they certify the transaction by checking for conflicts in each |
certify the transaction by checking for conflicts in each log |
Membership monitors are in charge of deciding and publishing which |
monitors are in charge of deciding and publishing which machines |
are in charge of deciding and publishing which machines perform |
in charge of deciding and publishing which machines perform which |
charge of deciding and publishing which machines perform which roles |
and our results for the special case analyzed there can |
our results for the special case analyzed there can be |
namely which machines run the log and Model and Goal |
results for the special case analyzed there can be explained |
which machines run the log and Model and Goal We |
for the special case analyzed there can be explained by |
machines run the log and Model and Goal We assume |
the special case analyzed there can be explained by the |
run the log and Model and Goal We assume unreliable |
special case analyzed there can be explained by the strong |
the log and Model and Goal We assume unreliable servers |
case analyzed there can be explained by the strong approximations |
log and Model and Goal We assume unreliable servers that |
analyzed there can be explained by the strong approximations in |
and Model and Goal We assume unreliable servers that may |
there can be explained by the strong approximations in that |
Model and Goal We assume unreliable servers that may crash |
can be explained by the strong approximations in that work |
and Goal We assume unreliable servers that may crash or |
Goal We assume unreliable servers that may crash or hang |
we calculate exactly how infiltrating miners reduce the revenue density |
calculate exactly how infiltrating miners reduce the revenue density of |
exactly how infiltrating miners reduce the revenue density of the |
how infiltrating miners reduce the revenue density of the infiltrated |
infiltrating miners reduce the revenue density of the infiltrated pool |
Temporary Block Withholding In the Block withholding attack discussed in |
Block Withholding In the Block withholding attack discussed in this |
Withholding In the Block withholding attack discussed in this work |
In the Block withholding attack discussed in this work the |
the Block withholding attack discussed in this work the withheld |
The system exposes a transactional data store supporting serializable transactions |
Block withholding attack discussed in this work the withheld blocks |
withholding attack discussed in this work the withheld blocks are |
attack discussed in this work the withheld blocks are never |
discussed in this work the withheld blocks are never published |
A miner or a pool can perform a selfish mining |
miner or a pool can perform a selfish mining attack |
With selfish mining the attacker increases its revenue by temporarily |
and the system responds with either a commit or an |
selfish mining the attacker increases its revenue by temporarily withholding |
the system responds with either a commit or an abort |
mining the attacker increases its revenue by temporarily withholding its |
the attacker increases its revenue by temporarily withholding its blocks |
attacker increases its revenue by temporarily withholding its blocks and |
TMs are equipped with predictors that foresee which objects a |
increases its revenue by temporarily withholding its blocks and publishing |
are equipped with predictors that foresee which objects a transaction |
its revenue by temporarily withholding its blocks and publishing them |
equipped with predictors that foresee which objects a transaction is |
revenue by temporarily withholding its blocks and publishing them in |
with predictors that foresee which objects a transaction is likely |
by temporarily withholding its blocks and publishing them in response |
predictors that foresee which objects a transaction is likely to |
temporarily withholding its blocks and publishing them in response to |
that foresee which objects a transaction is likely to access |
withholding its blocks and publishing them in response to block |
foresee which objects a transaction is likely to access on |
its blocks and publishing them in response to block publication |
which objects a transaction is likely to access on its |
blocks and publishing them in response to block publication by |
objects a transaction is likely to access on its initiation |
and publishing them in response to block publication by other |
publishing them in response to block publication by other pools |
them in response to block publication by other pools and |
in response to block publication by other pools and miners |
This attack is independent of the block withholding attack we |
In an implementation of the system one may use multiple |
attack is independent of the block withholding attack we discuss |
an implementation of the system one may use multiple OMs |
is independent of the block withholding attack we discuss here |
implementation of the system one may use multiple OMs per |
independent of the block withholding attack we discuss here and |
of the system one may use multiple OMs per log |
of the block withholding attack we discuss here and the |
the block withholding attack we discuss here and the two |
block withholding attack we discuss here and the two can |
withholding attack we discuss here and the two can be |
attack we discuss here and the two can be performed |
we discuss here and the two can be performed in |
discuss here and the two can be performed in concert |
The choice depends on the throughput of the specific implementations |
choice depends on the throughput of the specific implementations chosen |
An attacker can also perform a double spending attack as |
depends on the throughput of the specific implementations chosen for |
attacker can also perform a double spending attack as follows |
on the throughput of the specific implementations chosen for each |
the throughput of the specific implementations chosen for each service |
We start with an overview of the system s structure |
the attacker publishes the withheld block to revoke the former |
start with an overview of the system s structure in |
attacker publishes the withheld block to revoke the former transaction |
with an overview of the system s structure in Section |
This attack is performed by miners or pools against service |
attack is performed by miners or pools against service providers |
is performed by miners or pools against service providers that |
performed by miners or pools against service providers that accept |
by miners or pools against service providers that accept Bitcoin |
where finding proof of work is the result of solution |
Any client can access any TM for any given transaction |
time between node failure and rejoin as number of consecutive |
finding proof of work is the result of solution guessing |
between node failure and rejoin as number of consecutive updates |
proof of work is the result of solution guessing and |
node failure and rejoin as number of consecutive updates missed |
of work is the result of solution guessing and checking |
failure and rejoin as number of consecutive updates missed by |
and rejoin as number of consecutive updates missed by the |
All of the algorithms we are aware of are susceptible |
Experiments with workloads based on a web retailer product affinity |
rejoin as number of consecutive updates missed by the victim |
of the algorithms we are aware of are susceptible to |
with workloads based on a web retailer product affinity topology |
as number of consecutive updates missed by the victim node |
the algorithms we are aware of are susceptible to the |
workloads based on a web retailer product affinity topology and |
algorithms we are aware of are susceptible to the block |
but this may change due to an unjustified crash suspicion |
based on a web retailer product affinity topology and a |
we are aware of are susceptible to the block withholding |
this may change due to an unjustified crash suspicion whereupon |
on a web retailer product affinity topology and a social |
are aware of are susceptible to the block withholding attack |
may change due to an unjustified crash suspicion whereupon an |
a web retailer product affinity topology and a social network |
as in all of them the miner can check whether |
change due to an unjustified crash suspicion whereupon an object |
web retailer product affinity topology and a social network topology |
in all of them the miner can check whether she |
retailer product affinity topology and a social network topology illustrated |
all of them the miner can check whether she found |
product affinity topology and a social network topology illustrated in |
of them the miner can check whether she found a |
The Miner s Dilemma Ittay Eyal Cornell University Abstract An |
them the miner can check whether she found a full |
Miner s Dilemma Ittay Eyal Cornell University Abstract An open |
the miner can check whether she found a full or |
s Dilemma Ittay Eyal Cornell University Abstract An open distributed |
miner can check whether she found a full or a |
Dilemma Ittay Eyal Cornell University Abstract An open distributed system |
can check whether she found a full or a partial |
Inconsistency window against the ratio between injection rate and gossip |
Ittay Eyal Cornell University Abstract An open distributed system can |
check whether she found a full or a partial proof |
window against the ratio between injection rate and gossip rate |
Eyal Cornell University Abstract An open distributed system can be |
whether she found a full or a partial proof of |
Cornell University Abstract An open distributed system can be secured |
she found a full or a partial proof of work |
University Abstract An open distributed system can be secured by |
Abstract An open distributed system can be secured by requiring |
An open distributed system can be secured by requiring participants |
open distributed system can be secured by requiring participants to |
distributed system can be secured by requiring participants to present |
system can be secured by requiring participants to present proof |
can be secured by requiring participants to present proof of |
be secured by requiring participants to present proof of work |
secured by requiring participants to present proof of work and |
by requiring participants to present proof of work and rewarding |
s overload by dropping updates on its inbound and outbound |
requiring participants to present proof of work and rewarding them |
overload by dropping updates on its inbound and outbound FIFO |
this could work well if a system has multiple classes |
participants to present proof of work and rewarding them for |
An OM may instruct the log to truncate its prefix |
could work well if a system has multiple classes of |
by dropping updates on its inbound and outbound FIFO channels |
to present proof of work and rewarding them for participation |
work well if a system has multiple classes of objects |
dropping updates on its inbound and outbound FIFO channels according |
It is possible to use an alternative proof of work |
updates on its inbound and outbound FIFO channels according to |
is possible to use an alternative proof of work mechanism |
which is adopted by almost all contemporary digital currencies and |
on its inbound and outbound FIFO channels according to a |
possible to use an alternative proof of work mechanism in |
is adopted by almost all contemporary digital currencies and related |
its inbound and outbound FIFO channels according to a random |
to use an alternative proof of work mechanism in which |
adopted by almost all contemporary digital currencies and related services |
inbound and outbound FIFO channels according to a random distribution |
use an alternative proof of work mechanism in which miners |
A natural process leads participants of such systems to form |
and outbound FIFO channels according to a random distribution throughout |
an alternative proof of work mechanism in which miners would |
natural process leads participants of such systems to form pools |
outbound FIFO channels according to a random distribution throughout the |
alternative proof of work mechanism in which miners would not |
FIFO channels according to a random distribution throughout the first |
proof of work mechanism in which miners would not be |
channels according to a random distribution throughout the first three |
Experience with Bitcoin shows that the largest pools are often |
Consistent Inconsistent Aborted AB EV RE AB EV RE I |
and they respond with the latest unreserved timestamp of each |
according to a random distribution throughout the first three quarters |
with Bitcoin shows that the largest pools are often open |
of work mechanism in which miners would not be able |
Inconsistent Aborted AB EV RE AB EV RE I I |
they respond with the latest unreserved timestamp of each object |
to a random distribution throughout the first three quarters of |
work mechanism in which miners would not be able to |
Aborted AB EV RE AB EV RE I I TR |
It has long been known that a member can sabotage |
a random distribution throughout the first three quarters of the |
The TM chooses a timestamp larger than maximum among the |
mechanism in which miners would not be able to distinguish |
AB EV RE AB EV RE I I TR TR |
has long been known that a member can sabotage an |
random distribution throughout the first three quarters of the experiment |
TM chooses a timestamp larger than maximum among the responses |
in which miners would not be able to distinguish partial |
EV RE AB EV RE I I TR TR O |
and asks the OMs to reserve the objects with this |
which miners would not be able to distinguish partial from |
long been known that a member can sabotage an open |
RE AB EV RE I I TR TR O O |
asks the OMs to reserve the objects with this timestamp |
miners would not be able to distinguish partial from full |
been known that a member can sabotage an open pool |
AB EV RE I I TR TR O O RT |
the OMs to reserve the objects with this timestamp to |
would not be able to distinguish partial from full proofs |
known that a member can sabotage an open pool by |
EV RE I I TR TR O O RT CT |
OMs to reserve the objects with this timestamp to txnID |
not be able to distinguish partial from full proofs of |
that a member can sabotage an open pool by seemingly |
The OMs confirm the reservation if no concurrent TM has |
be able to distinguish partial from full proofs of work |
RE I I TR TR O O RT CT RT |
a member can sabotage an open pool by seemingly joining |
OMs confirm the reservation if no concurrent TM has reserved |
I I TR TR O O RT CT RT CT |
member can sabotage an open pool by seemingly joining it |
confirm the reservation if no concurrent TM has reserved a |
I TR TR O O RT CT RT CT Y |
can sabotage an open pool by seemingly joining it but |
the reservation if no concurrent TM has reserved a larger |
TR TR O O RT CT RT CT Y Y |
sabotage an open pool by seemingly joining it but never |
reservation if no concurrent TM has reserved a larger timestamp |
TR O O RT CT RT CT Y Y Amazon |
an open pool by seemingly joining it but never sharing |
if no concurrent TM has reserved a larger timestamp in |
O O RT CT RT CT Y Y Amazon Orkut |
open pool by seemingly joining it but never sharing its |
no concurrent TM has reserved a larger timestamp in the |
O RT CT RT CT Y Y Amazon Orkut Fig |
pool by seemingly joining it but never sharing its proofs |
concurrent TM has reserved a larger timestamp in the meantime |
by seemingly joining it but never sharing its proofs of |
Updates that were initially dropped and eventually made their way |
seemingly joining it but never sharing its proofs of work |
Such a solution could reduce or remove the danger of |
that were initially dropped and eventually made their way through |
The TM then proceeds to serve transaction operations by routing |
a solution could reduce or remove the danger of block |
Cache as a function of the inconsistency handling strategy for |
were initially dropped and eventually made their way through gossip |
TM then proceeds to serve transaction operations by routing them |
solution could reduce or remove the danger of block withholding |
as a function of the inconsistency handling strategy for realistic |
We define and analyze a game where pools use some |
initially dropped and eventually made their way through gossip could |
then proceeds to serve transaction operations by routing them to |
a function of the inconsistency handling strategy for realistic workloads |
define and analyze a game where pools use some of |
making such a change may not be in the interest |
dropped and eventually made their way through gossip could later |
proceeds to serve transaction operations by routing them to the |
and analyze a game where pools use some of their |
such a change may not be in the interest of |
and eventually made their way through gossip could later be |
to serve transaction operations by routing them to the appropriate |
Much work has been done on creating consistent caches for |
analyze a game where pools use some of their participants |
a change may not be in the interest of the |
eventually made their way through gossip could later be sent |
serve transaction operations by routing them to the appropriate OMs |
work has been done on creating consistent caches for web |
a game where pools use some of their participants to |
Each operation is sent to the OM in charge of |
made their way through gossip could later be sent via |
has been done on creating consistent caches for web servers |
change may not be in the interest of the community |
game where pools use some of their participants to infiltrate |
operation is sent to the OM in charge of the |
their way through gossip could later be sent via FIFO |
where pools use some of their participants to infiltrate other |
is sent to the OM in charge of the object |
way through gossip could later be sent via FIFO channels |
pools use some of their participants to infiltrate other pools |
through gossip could later be sent via FIFO channels as |
use some of their participants to infiltrate other pools and |
gossip could later be sent via FIFO channels as shown |
some of their participants to infiltrate other pools and perform |
could later be sent via FIFO channels as shown by |
of their participants to infiltrate other pools and perform such |
later be sent via FIFO channels as shown by the |
their participants to infiltrate other pools and perform such an |
be sent via FIFO channels as shown by the increasingly |
participants to infiltrate other pools and perform such an attack |
sent via FIFO channels as shown by the increasingly large |
via FIFO channels as shown by the increasingly large density |
FIFO channels as shown by the increasingly large density of |
the timestamp of the latest transaction that wrote this object |
channels as shown by the increasingly large density of dark |
timestamp of the latest transaction that wrote this object is |
But the question of whether a pool is run by |
of the latest transaction that wrote this object is returned |
the question of whether a pool is run by a |
the latest transaction that wrote this object is returned to |
We study the special cases where either two pools or |
question of whether a pool is run by a centralized |
latest transaction that wrote this object is returned to the |
As before note that the yaxes have different scales to |
study the special cases where either two pools or any |
of whether a pool is run by a centralized manager |
transaction that wrote this object is returned to the TM |
before note that the yaxes have different scales to observe |
the special cases where either two pools or any number |
The transaction s timestamp is chosen to be larger than |
note that the yaxes have different scales to observe the |
whether a pool is run by a centralized manager or |
special cases where either two pools or any number of |
transaction s timestamp is chosen to be larger than the |
that the yaxes have different scales to observe the delays |
a pool is run by a centralized manager or with |
cases where either two pools or any number of identical |
s timestamp is chosen to be larger than the largest |
the yaxes have different scales to observe the delays better |
pool is run by a centralized manager or with a |
where either two pools or any number of identical pools |
timestamp is chosen to be larger than the largest timestamp |
is run by a centralized manager or with a decentralized |
The figures show that even for a gossip rate half |
either two pools or any number of identical pools play |
is chosen to be larger than the largest timestamp returned |
run by a centralized manager or with a decentralized architecture |
figures show that even for a gossip rate half the |
two pools or any number of identical pools play the |
chosen to be larger than the largest timestamp returned by |
by a centralized manager or with a decentralized architecture is |
show that even for a gossip rate half the injection |
pools or any number of identical pools play the game |
to be larger than the largest timestamp returned by its |
a centralized manager or with a decentralized architecture is almost |
that even for a gossip rate half the injection rate |
or any number of identical pools play the game and |
be larger than the largest timestamp returned by its operations |
centralized manager or with a decentralized architecture is almost immaterial |
any number of identical pools play the game and the |
manager or with a decentralized architecture is almost immaterial for |
number of identical pools play the game and the rest |
or with a decentralized architecture is almost immaterial for the |
of identical pools play the game and the rest of |
the epidemics could deliver messages with a delay of about |
with a decentralized architecture is almost immaterial for the attack |
identical pools play the game and the rest of the |
a decentralized architecture is almost immaterial for the attack we |
pools play the game and the rest of the participants |
decentralized architecture is almost immaterial for the attack we describe |
play the game and the rest of the participants are |
the game and the rest of the participants are uninvolved |
In both of these cases there exists an equilibrium that |
both of these cases there exists an equilibrium that constitutes |
of these cases there exists an equilibrium that constitutes a |
Pool code can be changed to support attacks against other |
an OM appends to its log an entry consisting of |
s for the rest of the chain during a congestion |
these cases there exists an equilibrium that constitutes a tragedy |
code can be changed to support attacks against other pools |
OM appends to its log an entry consisting of the |
for the rest of the chain during a congestion that |
cases there exists an equilibrium that constitutes a tragedy of |
appends to its log an entry consisting of the txnID |
the rest of the chain during a congestion that took |
Pool can be used by groups of miners to easily |
there exists an equilibrium that constitutes a tragedy of the |
can be used by groups of miners to easily form |
These systems generally try to avoid staleness through techniques such |
exists an equilibrium that constitutes a tragedy of the commons |
be used by groups of miners to easily form closed |
systems generally try to avoid staleness through techniques such as |
an equilibrium that constitutes a tragedy of the commons where |
used by groups of miners to easily form closed pools |
generally try to avoid staleness through techniques such as Time |
the gossip rate must be carefully tuned to compensate for |
equilibrium that constitutes a tragedy of the commons where the |
gossip rate must be carefully tuned to compensate for the |
that constitutes a tragedy of the commons where the participating |
rate must be carefully tuned to compensate for the losses |
constitutes a tragedy of the commons where the participating pools |
must be carefully tuned to compensate for the losses induced |
C ONCLUSION We explored a block withholding attack among Bitcoin |
a tragedy of the commons where the participating pools attack |
be carefully tuned to compensate for the losses induced by |
ONCLUSION We explored a block withholding attack among Bitcoin mining |
tragedy of the commons where the participating pools attack one |
carefully tuned to compensate for the losses induced by the |
We explored a block withholding attack among Bitcoin mining pools |
of the commons where the participating pools attack one another |
tuned to compensate for the losses induced by the congested |
explored a block withholding attack among Bitcoin mining pools an |
the commons where the participating pools attack one another and |
to compensate for the losses induced by the congested TCP |
Early work on scalable database caching mostly ignored transactional consistency |
a block withholding attack among Bitcoin mining pools an attack |
commons where the participating pools attack one another and earn |
compensate for the losses induced by the congested TCP channels |
block withholding attack among Bitcoin mining pools an attack that |
where the participating pools attack one another and earn less |
withholding attack among Bitcoin mining pools an attack that is |
action should be either committed or aborted in all its |
the participating pools attack one another and earn less than |
The second round of experiments quantified the average and maximum |
work has been done on creating consistent caches for databases |
should be either committed or aborted in all its logs |
participating pools attack one another and earn less than they |
attack among Bitcoin mining pools an attack that is possible |
second round of experiments quantified the average and maximum inconsistency |
pools attack one another and earn less than they would |
and therefore cannot be removed from any of them before |
among Bitcoin mining pools an attack that is possible in |
round of experiments quantified the average and maximum inconsistency window |
attack one another and earn less than they would have |
therefore cannot be removed from any of them before the |
extends a centralized database with support for caches that provide |
Bitcoin mining pools an attack that is possible in any |
of experiments quantified the average and maximum inconsistency window for |
one another and earn less than they would have if |
cannot be removed from any of them before the result |
a centralized database with support for caches that provide snapshot |
mining pools an attack that is possible in any similar |
experiments quantified the average and maximum inconsistency window for a |
another and earn less than they would have if none |
be removed from any of them before the result is |
centralized database with support for caches that provide snapshot isolation |
pools an attack that is possible in any similar system |
quantified the average and maximum inconsistency window for a service |
and earn less than they would have if none had |
removed from any of them before the result is published |
database with support for caches that provide snapshot isolation semantics |
an attack that is possible in any similar system that |
earn less than they would have if none had attacked |
attack that is possible in any similar system that rewards |
the committing TM appends a GC entry to all the |
that is possible in any similar system that rewards for |
committing TM appends a GC entry to all the transaction |
is possible in any similar system that rewards for proof |
We define the inconsistency window as the time interval during |
the decision whether or not to attack is the miner |
TM appends a GC entry to all the transaction s |
possible in any similar system that rewards for proof of |
define the inconsistency window as the time interval during which |
where the cache holds several versions of an object and |
decision whether or not to attack is the miner s |
appends a GC entry to all the transaction s logs |
in any similar system that rewards for proof of work |
the inconsistency window as the time interval during which queries |
the cache holds several versions of an object and enables |
whether or not to attack is the miner s dilemma |
a GC entry to all the transaction s logs after |
inconsistency window as the time interval during which queries against |
cache holds several versions of an object and enables the |
GC entry to all the transaction s logs after receiving |
window as the time interval during which queries against the |
holds several versions of an object and enables the cache |
The game is played daily by the active Bitcoin pools |
entry to all the transaction s logs after receiving an |
as the time interval during which queries against the service |
several versions of an object and enables the cache to |
to all the transaction s logs after receiving an acknowledgement |
a pool can increase its revenue by attacking the others |
the time interval during which queries against the service return |
versions of an object and enables the cache to choose |
all the transaction s logs after receiving an acknowledgement that |
time interval during which queries against the service return a |
of an object and enables the cache to choose a |
the transaction s logs after receiving an acknowledgement that they |
interval during which queries against the service return a stale |
an object and enables the cache to choose a version |
transaction s logs after receiving an acknowledgement that they all |
during which queries against the service return a stale value |
and it can retaliate by attacking and increase its revenue |
object and enables the cache to choose a version that |
s logs after receiving an acknowledgement that they all registered |
and enables the cache to choose a version that allows |
logs after receiving an acknowledgement that they all registered the |
shows that the inconsistency window grows slowly as the gap |
enables the cache to choose a version that allows a |
after receiving an acknowledgement that they all registered the transaction |
at Nash equilibrium both earn less than they would have |
that the inconsistency window grows slowly as the gap between |
the cache to choose a version that allows a transaction |
receiving an acknowledgement that they all registered the transaction s |
Nash equilibrium both earn less than they would have if |
the inconsistency window grows slowly as the gap between the |
cache to choose a version that allows a transaction to |
an acknowledgement that they all registered the transaction s result |
equilibrium both earn less than they would have if neither |
inconsistency window grows slowly as the gap between the update |
to choose a version that allows a transaction to commit |
both earn less than they would have if neither attacked |
window grows slowly as the gap between the update injection |
An OM can invoke log prefix truncation if the prefix |
With multiple pools of equal size a similar situation arises |
grows slowly as the gap between the update injection rate |
OM can invoke log prefix truncation if the prefix was |
multiple pools of equal size a similar situation arises with |
slowly as the gap between the update injection rate and |
can invoke log prefix truncation if the prefix was summarized |
pools of equal size a similar situation arises with a |
as the gap between the update injection rate and the |
of equal size a similar situation arises with a symmetric |
the gap between the update injection rate and the gossip |
equal size a similar situation arises with a symmetric equilibrium |
then waits for the entry to appear in the log |
gap between the update injection rate and the gossip rate |
between the update injection rate and the gossip rate widens |
The fact that block withholding is not common may be |
A transaction is committed if and only if it is |
the graph s x axis represents the ratio between the |
fact that block withholding is not common may be explained |
and it is used by most contemporary digital currencies and |
transaction is committed if and only if it is written |
graph s x axis represents the ratio between the update |
that block withholding is not common may be explained by |
it is used by most contemporary digital currencies and related |
is committed if and only if it is written to |
s x axis represents the ratio between the update injection |
block withholding is not common may be explained by modeling |
is used by most contemporary digital currencies and related services |
committed if and only if it is written to all |
x axis represents the ratio between the update injection rate |
withholding is not common may be explained by modeling the |
if and only if it is written to all logs |
axis represents the ratio between the update injection rate and |
is not common may be explained by modeling the attack |
represents the ratio between the update injection rate and gossip |
and it does not conflict with previous transactions on any |
not common may be explained by modeling the attack decisions |
the ratio between the update injection rate and gossip rate |
it does not conflict with previous transactions on any of |
common may be explained by modeling the attack decisions as |
does not conflict with previous transactions on any of them |
may be explained by modeling the attack decisions as an |
This confirms that epidemics are a robust tunable mechanism providing |
be explained by modeling the attack decisions as an iterative |
confirms that epidemics are a robust tunable mechanism providing graceful |
explained by modeling the attack decisions as an iterative prisoner |
that epidemics are a robust tunable mechanism providing graceful degradation |
allows update transactions to read stale data out of caches |
by modeling the attack decisions as an iterative prisoner s |
update transactions to read stale data out of caches and |
Each OM checks for local conflicts by checking timestamps in |
modeling the attack decisions as an iterative prisoner s dilemma |
transactions to read stale data out of caches and provide |
the inconsistency window shifts in accordance with the update injection |
OM checks for local conflicts by checking timestamps in the |
to read stale data out of caches and provide bounds |
inconsistency window shifts in accordance with the update injection rate |
checks for local conflicts by checking timestamps in the prefix |
we argue that the situation is unstable since the attack |
read stale data out of caches and provide bounds on |
for local conflicts by checking timestamps in the prefix of |
argue that the situation is unstable since the attack can |
stale data out of caches and provide bounds on how |
notice that the difference between the maximum inconsistency window and |
local conflicts by checking timestamps in the prefix of the |
that the situation is unstable since the attack can be |
data out of caches and provide bounds on how much |
that the difference between the maximum inconsistency window and the |
conflicts by checking timestamps in the prefix of the log |
but we use Bitcoin terminology and examples since it serves |
the situation is unstable since the attack can be done |
out of caches and provide bounds on how much staleness |
the difference between the maximum inconsistency window and the average |
by checking timestamps in the prefix of the log up |
we use Bitcoin terminology and examples since it serves as |
situation is unstable since the attack can be done anonymously |
of caches and provide bounds on how much staleness is |
difference between the maximum inconsistency window and the average inconsistency |
checking timestamps in the prefix of the log up to |
use Bitcoin terminology and examples since it serves as an |
caches and provide bounds on how much staleness is allowed |
between the maximum inconsistency window and the average inconsistency window |
timestamps in the prefix of the log up to the |
one pool may decide to increase its revenue and drag |
Bitcoin terminology and examples since it serves as an active |
the maximum inconsistency window and the average inconsistency window is |
in the prefix of the log up to the transaction |
These techniques require fast communication between the cache and the |
pool may decide to increase its revenue and drag the |
terminology and examples since it serves as an active and |
maximum inconsistency window and the average inconsistency window is two |
the prefix of the log up to the transaction entry |
techniques require fast communication between the cache and the database |
may decide to increase its revenue and drag the others |
and examples since it serves as an active and archetypal |
inconsistency window and the average inconsistency window is two orders |
require fast communication between the cache and the database for |
decide to increase its revenue and drag the others to |
examples since it serves as an active and archetypal example |
window and the average inconsistency window is two orders of |
fast communication between the cache and the database for good |
to increase its revenue and drag the others to attack |
and the average inconsistency window is two orders of magnitude |
communication between the cache and the database for good performance |
Bitcoin implements its incentive systems with a data structure called |
increase its revenue and drag the others to attack as |
implements its incentive systems with a data structure called the |
The TM notifies the client of the transaction result and |
This reflects the degree to which the victim node lags |
its revenue and drag the others to attack as well |
its incentive systems with a data structure called the blockchain |
TM notifies the client of the transaction result and instructs |
reflects the degree to which the victim node lags the |
notifies the client of the transaction result and instructs the |
the degree to which the victim node lags the other |
The inferior revenue would push miners to join private pools |
the client of the transaction result and instructs the OMs |
It is a single global ledger maintained by an open |
degree to which the victim node lags the other nodes |
F UTURE D IRECTIONS The dependency list sizes for all |
client of the transaction result and instructs the OMs to |
which can verify that their registered miners do not withhold |
is a single global ledger maintained by an open distributed |
to which the victim node lags the other nodes during |
UTURE D IRECTIONS The dependency list sizes for all objects |
of the transaction result and instructs the OMs to place |
can verify that their registered miners do not withhold blocks |
a single global ledger maintained by an open distributed system |
which the victim node lags the other nodes during the |
D IRECTIONS The dependency list sizes for all objects in |
Since anyone can join the open system and participate in |
the victim node lags the other nodes during the period |
the transaction result and instructs the OMs to place this |
IRECTIONS The dependency list sizes for all objects in T |
and so ultimately to a better environment for Bitcoin as |
anyone can join the open system and participate in maintaining |
victim node lags the other nodes during the period before |
transaction result and instructs the OMs to place this result |
so ultimately to a better environment for Bitcoin as a |
can join the open system and participate in maintaining the |
node lags the other nodes during the period before it |
result and instructs the OMs to place this result in |
ultimately to a better environment for Bitcoin as a whole |
join the open system and participate in maintaining the blockchain |
if the workload accesses objects in clusters of different sizes |
lags the other nodes during the period before it has |
and instructs the OMs to place this result in the |
Bitcoin uses a proof of work mechanism to deter attacks |
the other nodes during the period before it has fully |
instructs the OMs to place this result in the logs |
other nodes during the period before it has fully caught |
it may be possible to improve performance by dynamically changing |
A participant who proves she has exerted enough resources with |
nodes during the period before it has fully caught up |
may be possible to improve performance by dynamically changing per |
The OMs notify the TM once the results are logged |
participant who proves she has exerted enough resources with a |
Next we evaluated the inconsistency window of a service running |
who proves she has exerted enough resources with a proof |
we evaluated the inconsistency window of a service running at |
balancing between objects to maintain the same overall space overhead |
proves she has exerted enough resources with a proof of |
evaluated the inconsistency window of a service running at a |
Another option is to explore an approach in which each |
another TM may read the transaction entry in one of |
the inconsistency window of a service running at a particular |
she has exerted enough resources with a proof of work |
option is to explore an approach in which each type |
TM may read the transaction entry in one of the |
inconsistency window of a service running at a particular update |
has exerted enough resources with a proof of work is |
is to explore an approach in which each type of |
may read the transaction entry in one of the logs |
window of a service running at a particular update rate |
exerted enough resources with a proof of work is allowed |
to explore an approach in which each type of object |
enough resources with a proof of work is allowed to |
and for three different intervals in which the victim node |
explore an approach in which each type of object would |
resources with a proof of work is allowed to take |
for three different intervals in which the victim node is |
an approach in which each type of object would have |
If a TM places a transaction entry in a strict |
with a proof of work is allowed to take a |
three different intervals in which the victim node is halted |
approach in which each type of object would have its |
a TM places a transaction entry in a strict subset |
a proof of work is allowed to take a step |
in which each type of object would have its own |
TM places a transaction entry in a strict subset of |
proof of work is allowed to take a step in |
which each type of object would have its own dependency |
places a transaction entry in a strict subset of the |
show average and maximum inconsistency windows for both the victim |
of work is allowed to take a step in the |
each type of object would have its own dependency list |
a transaction entry in a strict subset of the transaction |
average and maximum inconsistency windows for both the victim and |
work is allowed to take a step in the protocol |
type of object would have its own dependency list bound |
transaction entry in a strict subset of the transaction s |
and maximum inconsistency windows for both the victim and for |
is allowed to take a step in the protocol by |
entry in a strict subset of the transaction s log |
maximum inconsistency windows for both the victim and for the |
allowed to take a step in the protocol by generating |
in a strict subset of the transaction s log set |
agnostic and treats all objects and object relations as equal |
to take a step in the protocol by generating a |
inconsistency windows for both the victim and for the other |
take a step in the protocol by generating a block |
using an LRU policy to trim the list of dependencies |
windows for both the victim and for the other processes |
it cannot tell whether the original TM is crashed or |
for both the victim and for the other processes of |
Participants are compensated for their efforts with newly minted Bitcoins |
cannot tell whether the original TM is crashed or slow |
both the victim and for the other processes of one |
there may be cases in which the application could explicitly |
the victim and for the other processes of one subservice |
may be cases in which the application could explicitly inform |
be cases in which the application could explicitly inform the |
The fixing TM places a poison entry in the logs |
cases in which the application could explicitly inform the cache |
fixing TM places a poison entry in the logs that |
in which the application could explicitly inform the cache of |
TM places a poison entry in the logs that miss |
which the application could explicitly inform the cache of relevant |
places a poison entry in the logs that miss the |
the application could explicitly inform the cache of relevant object |
a poison entry in the logs that miss the original |
application could explicitly inform the cache of relevant object dependencies |
poison entry in the logs that miss the original entry |
and those could then be treated as more important and |
a miner may have to wait for an extended period |
A poison is interpreted as a transaction entry with a |
those could then be treated as more important and retained |
miner may have to wait for an extended period to |
poison is interpreted as a transaction entry with a conflict |
while other less important ones are managed by some other |
may have to wait for an extended period to create |
other less important ones are managed by some other policy |
have to wait for an extended period to create a |
less important ones are managed by some other policy such |
to wait for an extended period to create a block |
important ones are managed by some other policy such as |
wait for an extended period to create a block and |
ones are managed by some other policy such as LRU |
for an extended period to create a block and earn |
Any TM can therefore observe the log and consistently determine |
an extended period to create a block and earn the |
TM can therefore observe the log and consistently determine the |
extended period to create a block and earn the actual |
in a web album the set of pictures and their |
can therefore observe the log and consistently determine the state |
period to create a block and earn the actual Bitcoins |
a web album the set of pictures and their ACL |
therefore observe the log and consistently determine the state of |
web album the set of pictures and their ACL is |
observe the log and consistently determine the state of the |
album the set of pictures and their ACL is an |
the log and consistently determine the state of the transaction |
the set of pictures and their ACL is an important |
where all members mine concurrently and they share their revenue |
set of pictures and their ACL is an important dependency |
all members mine concurrently and they share their revenue whenever |
of pictures and their ACL is an important dependency whereas |
members mine concurrently and they share their revenue whenever one |
pictures and their ACL is an important dependency whereas occasional |
mine concurrently and they share their revenue whenever one of |
and their ACL is an important dependency whereas occasional tagging |
concurrently and they share their revenue whenever one of them |
their ACL is an important dependency whereas occasional tagging operations |
If the transaction accesses an object that was not predicted |
and they share their revenue whenever one of them creates |
ACL is an important dependency whereas occasional tagging operations that |
they share their revenue whenever one of them creates a |
is an important dependency whereas occasional tagging operations that relate |
share their revenue whenever one of them creates a block |
Accessing it can therefore result in a conflict of the |
an important dependency whereas occasional tagging operations that relate pictures |
it can therefore result in a conflict of the transaction |
Pools are typically implemented as a pool manager and a |
important dependency whereas occasional tagging operations that relate pictures to |
can therefore result in a conflict of the transaction or |
are typically implemented as a pool manager and a cohort |
dependency whereas occasional tagging operations that relate pictures to users |
therefore result in a conflict of the transaction or of |
typically implemented as a pool manager and a cohort of |
whereas occasional tagging operations that relate pictures to users may |
result in a conflict of the transaction or of the |
implemented as a pool manager and a cohort of miners |
occasional tagging operations that relate pictures to users may be |
in a conflict of the transaction or of the following |
tagging operations that relate pictures to users may be less |
The pool manager joins the Bitcoin system as a single |
a conflict of the transaction or of the following ones |
operations that relate pictures to users may be less important |
pool manager joins the Bitcoin system as a single miner |
It may be straightforward to extend the cache API to |
may be straightforward to extend the cache API to allow |
be straightforward to extend the cache API to allow the |
but if one does it will be detected at certification |
straightforward to extend the cache API to allow the application |
if one does it will be detected at certification time |
to extend the cache API to allow the application to |
the pool manager accepts partial proof of work and estimates |
extend the cache API to allow the application to specify |
pool manager accepts partial proof of work and estimates each |
the cache API to allow the application to specify such |
manager accepts partial proof of work and estimates each miner |
cache API to allow the application to specify such dependencies |
accepts partial proof of work and estimates each miner s |
If a transaction does not access an object that was |
API to allow the application to specify such dependencies and |
partial proof of work and estimates each miner s power |
a transaction does not access an object that was predicted |
to allow the application to specify such dependencies and to |
in Proceedings of the IEEE Symposium on Security and Privacy |
proof of work and estimates each miner s power according |
allow the application to specify such dependencies and to modify |
the TM must still release the reservation when the transaction |
of work and estimates each miner s power according to |
the application to specify such dependencies and to modify T |
TM must still release the reservation when the transaction ends |
work and estimates each miner s power according to the |
and estimates each miner s power according to the rate |
This reservation might slow the processing of other transactions that |
estimates each miner s power according to the rate with |
reservation might slow the processing of other transactions that wait |
each miner s power according to the rate with which |
might slow the processing of other transactions that wait for |
scale computing frameworks make heavy use of edge caches to |
miner s power according to the rate with which it |
slow the processing of other transactions that wait for its |
computing frameworks make heavy use of edge caches to reduce |
s power according to the rate with which it submits |
the processing of other transactions that wait for its release |
frameworks make heavy use of edge caches to reduce client |
power according to the rate with which it submits such |
make heavy use of edge caches to reduce client latency |
according to the rate with which it submits such partial |
to the rate with which it submits such partial proof |
but this form of caching has not been available for |
the rate with which it submits such partial proof of |
this form of caching has not been available for transactional |
rate with which it submits such partial proof of work |
form of caching has not been available for transactional applications |
We believe this is one reason that transactions are generally |
believe this is one reason that transactions are generally not |
it sends it to the pool manager which publishes this |
RAIN by comparing its performance to the classical approach that |
this is one reason that transactions are generally not considered |
sends it to the pool manager which publishes this proof |
by comparing its performance to the classical approach that does |
is one reason that transactions are generally not considered to |
it to the pool manager which publishes this proof of |
comparing its performance to the classical approach that does not |
one reason that transactions are generally not considered to be |
to the pool manager which publishes this proof of work |
its performance to the classical approach that does not use |
reason that transactions are generally not considered to be a |
the pool manager which publishes this proof of work to |
performance to the classical approach that does not use prediction |
that transactions are generally not considered to be a viable |
pool manager which publishes this proof of work to the |
to the classical approach that does not use prediction and |
transactions are generally not considered to be a viable option |
manager which publishes this proof of work to the Bitcoin |
the classical approach that does not use prediction and compare |
are generally not considered to be a viable option in |
which publishes this proof of work to the Bitcoin system |
classical approach that does not use prediction and compare its |
generally not considered to be a viable option in extremely |
approach that does not use prediction and compare its certification |
The pool manager thus receives the full revenue of the |
not considered to be a viable option in extremely large |
that does not use prediction and compare its certification protocol |
pool manager thus receives the full revenue of the block |
considered to be a viable option in extremely large systems |
does not use prediction and compare its certification protocol with |
manager thus receives the full revenue of the block and |
not use prediction and compare its certification protocol with other |
thus receives the full revenue of the block and distributes |
use prediction and compare its certification protocol with other certification |
right bars denote a transient failure corroborated with a link |
receives the full revenue of the block and distributes it |
a variant of serializability that is suitable for incoherent caches |
prediction and compare its certification protocol with other certification schemes |
bars denote a transient failure corroborated with a link congestion |
the full revenue of the block and distributes it fairly |
which cannot communicate with the backend database on every read |
denote a transient failure corroborated with a link congestion phenomenon |
full revenue of the block and distributes it fairly according |
cannot communicate with the backend database on every read access |
a transient failure corroborated with a link congestion phenomenon modeled |
revenue of the block and distributes it fairly according to |
transient failure corroborated with a link congestion phenomenon modeled by |
of the block and distributes it fairly according to its |
the block and distributes it fairly according to its members |
Our workloads are an adaptation of the transactional YCSB specification |
block and distributes it fairly according to its members power |
The system extends the edge cache by allowing it to |
system extends the edge cache by allowing it to offer |
Many of the pools are open they allow any miner |
extends the edge cache by allowing it to offer a |
of the pools are open they allow any miner to |
the edge cache by allowing it to offer a transactional |
the pools are open they allow any miner to join |
edge cache by allowing it to offer a transactional interface |
pools are open they allow any miner to join them |
are open they allow any miner to join them using |
open they allow any miner to join them using a |
they allow any miner to join them using a public |
allow any miner to join them using a public Internet |
any miner to join them using a public Internet interface |
Such open pools are susceptible to the classical block withholding |
open pools are susceptible to the classical block withholding attack |
trip to the database in case there is a cache |
to the database in case there is a cache miss |
the database in case there is a cache miss there |
database in case there is a cache miss there is |
in case there is a cache miss there is no |
case there is a cache miss there is no additional |
there is a cache miss there is no additional traffic |
is a cache miss there is no additional traffic and |
where a miner sends only partial proof of work to |
a cache miss there is no additional traffic and delays |
a miner sends only partial proof of work to the |
cache miss there is no additional traffic and delays to |
miner sends only partial proof of work to the pool |
miss there is no additional traffic and delays to ensure |
sends only partial proof of work to the pool manager |
there is no additional traffic and delays to ensure cache |
only partial proof of work to the pool manager and |
is no additional traffic and delays to ensure cache coherence |
GC Logs are truncated to conserve resources and to reduce |
partial proof of work to the pool manager and discards |
Logs are truncated to conserve resources and to reduce log |
proof of work to the pool manager and discards full |
are truncated to conserve resources and to reduce log replay |
of work to the pool manager and discards full proof |
while leaving the interaction between the backend systems and the |
truncated to conserve resources and to reduce log replay time |
work to the pool manager and discards full proof of |
leaving the interaction between the backend systems and the cache |
to conserve resources and to reduce log replay time on |
to the pool manager and discards full proof of work |
the interaction between the backend systems and the cache otherwise |
conserve resources and to reduce log replay time on OM |
Due to the partial proof of work it sends to |
interaction between the backend systems and the cache otherwise unchanged |
resources and to reduce log replay time on OM recovery |
to the partial proof of work it sends to the |
the partial proof of work it sends to the pool |
the miner is considered a regular pool member and the |
miner is considered a regular pool member and the pool |
is considered a regular pool member and the pool can |
we show that inconsistency can be greatly reduced or even |
considered a regular pool member and the pool can estimate |
the presence of a summary of the log up to |
show that inconsistency can be greatly reduced or even completely |
a regular pool member and the pool can estimate its |
presence of a summary of the log up to a |
that inconsistency can be greatly reduced or even completely eliminated |
regular pool member and the pool can estimate its power |
of a summary of the log up to a certain |
inconsistency can be greatly reduced or even completely eliminated in |
a summary of the log up to a certain entry |
can be greatly reduced or even completely eliminated in some |
summary of the log up to a certain entry is |
the attacker shares the revenue obtained by the other pool |
be greatly reduced or even completely eliminated in some cases |
of the log up to a certain entry is not |
attacker shares the revenue obtained by the other pool members |
the log up to a certain entry is not sufficient |
log up to a certain entry is not sufficient to |
up to a certain entry is not sufficient to allow |
to a certain entry is not sufficient to allow truncation |
a certain entry is not sufficient to allow truncation at |
certain entry is not sufficient to allow truncation at that |
entry is not sufficient to allow truncation at that entry |
This reason is that truncation must not break transaction certification |
pools and the classical block withholding attack in Section II |
Cache to be effective in realistic workloads based on datasets |
to be effective in realistic workloads based on datasets from |
Prediction Our first test scenario imposes a load substantially below |
be effective in realistic workloads based on datasets from Amazon |
For a broader view of the protocol and ecosystem the |
Our first test scenario imposes a load substantially below the |
effective in realistic workloads based on datasets from Amazon and |
a broader view of the protocol and ecosystem the reader |
first test scenario imposes a load substantially below the system |
in realistic workloads based on datasets from Amazon and Orkut |
broader view of the protocol and ecosystem the reader may |
test scenario imposes a load substantially below the system s |
view of the protocol and ecosystem the reader may refer |
scenario imposes a load substantially below the system s capacity |
of the protocol and ecosystem the reader may refer to |
imposes a load substantially below the system s capacity with |
the protocol and ecosystem the reader may refer to the |
protocol and ecosystem the reader may refer to the survey |
and ecosystem the reader may refer to the survey by |
ecosystem the reader may refer to the survey by Bonneau |
the reader may refer to the survey by Bonneau et |
reader may refer to the survey by Bonneau et al |
and was also able to increase consistent transaction rate by |
with the exception of a small shortcut OMs grant reservations |
the exception of a small shortcut OMs grant reservations by |
exception of a small shortcut OMs grant reservations by arrival |
of the messages were delivered by gossip for the nodes |
of a small shortcut OMs grant reservations by arrival time |
In this work we analyze block withholding attacks among pools |
the messages were delivered by gossip for the nodes to |
a small shortcut OMs grant reservations by arrival time rather |
A pool that employs the pool block withholding attack registers |
Cache s efficacy depends on the clustering level of the |
messages were delivered by gossip for the nodes to the |
small shortcut OMs grant reservations by arrival time rather than |
pool that employs the pool block withholding attack registers with |
s efficacy depends on the clustering level of the workload |
were delivered by gossip for the nodes to the left |
shortcut OMs grant reservations by arrival time rather than by |
that employs the pool block withholding attack registers with the |
delivered by gossip for the nodes to the left of |
OMs grant reservations by arrival time rather than by timestamp |
Cache adapts to dynamically changing workloads where clusters change over |
employs the pool block withholding attack registers with the victim |
by gossip for the nodes to the left of the |
adapts to dynamically changing workloads where clusters change over time |
the pool block withholding attack registers with the victim pool |
gossip for the nodes to the left of the victim |
pool block withholding attack registers with the victim pool as |
block withholding attack registers with the victim pool as a |
This confirms that gossip rarely is used to circumvent chain |
withholding attack registers with the victim pool as a regular |
which is naturally imperfect and does not include all dependencies |
confirms that gossip rarely is used to circumvent chain replication |
attack registers with the victim pool as a regular miner |
the average ratio of objects the predictor guesses out of |
that gossip rarely is used to circumvent chain replication in |
It receives tasks from the victim pool and transfers them |
average ratio of objects the predictor guesses out of the |
gossip rarely is used to circumvent chain replication in the |
receives tasks from the victim pool and transfers them to |
ratio of objects the predictor guesses out of the set |
rarely is used to circumvent chain replication in the normal |
tasks from the victim pool and transfers them to some |
of objects the predictor guesses out of the set the |
is used to circumvent chain replication in the normal case |
from the victim pool and transfers them to some of |
objects the predictor guesses out of the set the transaction |
the victim pool and transfers them to some of its |
the predictor guesses out of the set the transaction eventually |
victim pool and transfers them to some of its own |
predictor guesses out of the set the transaction eventually accesses |
pool and transfers them to some of its own miners |
and the mining power spent by a pool the infiltration |
It is more significant on the left hand side figure |
the mining power spent by a pool the infiltration rate |
Because we observed this phenomenon only with update rates of |
we suspect that the network stack is more efficient in |
suspect that the network stack is more efficient in dealing |
that the network stack is more efficient in dealing with |
the network stack is more efficient in dealing with UDP |
network stack is more efficient in dealing with UDP packets |
stack is more efficient in dealing with UDP packets then |
is more efficient in dealing with UDP packets then with |
more efficient in dealing with UDP packets then with TCP |
efficient in dealing with UDP packets then with TCP ones |
in dealing with UDP packets then with TCP ones under |
dealing with UDP packets then with TCP ones under heavy |
with UDP packets then with TCP ones under heavy load |
the attacking pool s infiltrating miners deliver partial proofs of |
attacking pool s infiltrating miners deliver partial proofs of work |
When the infiltrating miners deliver a full proof of work |
This attack affects the revenues of the pools in several |
attack affects the revenues of the pools in several ways |
since some of its miners are used for block withholding |
but it earns additional revenue through its infiltration of the |
it earns additional revenue through its infiltration of the other |
earns additional revenue through its infiltration of the other pool |
the total effective mining power in the system is reduced |
we observe that a pool might be able to increase |
observe that a pool might be able to increase its |
that a pool might be able to increase its revenue |
a pool might be able to increase its revenue by |
pool might be able to increase its revenue by attacking |
might be able to increase its revenue by attacking other |
We define slack to be the average ratio between the |
be able to increase its revenue by attacking other pools |
define slack to be the average ratio between the number |
slack to be the average ratio between the number of |
Each pool therefore makes a choice of whether to attack |
to be the average ratio between the number of accesses |
pool therefore makes a choice of whether to attack each |
be the average ratio between the number of accesses predicted |
therefore makes a choice of whether to attack each of |
the average ratio between the number of accesses predicted and |
makes a choice of whether to attack each of the |
average ratio between the number of accesses predicted and the |
consistency windows is slightly more than an order of magnitude |
a choice of whether to attack each of the other |
ratio between the number of accesses predicted and the number |
choice of whether to attack each of the other pools |
and this is attributable to the victim node observe that |
between the number of accesses predicted and the number of |
of whether to attack each of the other pools in |
this is attributable to the victim node observe that the |
the number of accesses predicted and the number of objects |
whether to attack each of the other pools in the |
is attributable to the victim node observe that the two |
number of accesses predicted and the number of objects accessed |
to attack each of the other pools in the system |
attributable to the victim node observe that the two graphs |
of accesses predicted and the number of objects accessed by |
to the victim node observe that the two graphs denoting |
accesses predicted and the number of objects accessed by the |
the victim node observe that the two graphs denoting the |
predicted and the number of objects accessed by the transaction |
We specify this game and provide initial analysis in Section |
victim node observe that the two graphs denoting the maximum |
specify this game and provide initial analysis in Section IV |
node observe that the two graphs denoting the maximum inconsistency |
observe that the two graphs denoting the maximum inconsistency windows |
In Section V we analyze the scenario where exactly two |
that the two graphs denoting the maximum inconsistency windows for |
Section V we analyze the scenario where exactly two of |
the two graphs denoting the maximum inconsistency windows for the |
V we analyze the scenario where exactly two of the |
two graphs denoting the maximum inconsistency windows for the victim |
we analyze the scenario where exactly two of the pools |
graphs denoting the maximum inconsistency windows for the victim node |
analyze the scenario where exactly two of the pools take |
denoting the maximum inconsistency windows for the victim node and |
the scenario where exactly two of the pools take part |
the maximum inconsistency windows for the victim node and for |
scenario where exactly two of the pools take part in |
maximum inconsistency windows for the victim node and for the |
where exactly two of the pools take part in the |
inconsistency windows for the victim node and for the entire |
now with uniform random load and a variable number of |
exactly two of the pools take part in the game |
windows for the victim node and for the entire chain |
with uniform random load and a variable number of objects |
two of the pools take part in the game and |
for the victim node and for the entire chain are |
of the pools take part in the game and only |
the victim node and for the entire chain are identical |
the pools take part in the game and only one |
pools take part in the game and only one can |
take part in the game and only one can attack |
which means that clients perceiving significant inconsistency are the ones |
part in the game and only one can attack the |
means that clients perceiving significant inconsistency are the ones that |
in the game and only one can attack the other |
that clients perceiving significant inconsistency are the ones that are |
clients perceiving significant inconsistency are the ones that are querying |
perceiving significant inconsistency are the ones that are querying the |
significant inconsistency are the ones that are querying the victim |
inconsistency are the ones that are querying the victim node |
are the ones that are querying the victim node while |
the ones that are querying the victim node while it |
ones that are querying the victim node while it is |
that are querying the victim node while it is still |
are querying the victim node while it is still recovering |
querying the victim node while it is still recovering state |
Finally we performed a set of experiments to determine the |
we performed a set of experiments to determine the distribution |
performed a set of experiments to determine the distribution of |
a set of experiments to determine the distribution of messages |
Ordering transactions in advance reduces conflicts and increases commit ratio |
set of experiments to determine the distribution of messages delivered |
of experiments to determine the distribution of messages delivered by |
High conflict rates occur without with uniform access to a |
experiments to determine the distribution of messages delivered by the |
conflict rates occur without with uniform access to a small |
to determine the distribution of messages delivered by the chain |
rates occur without with uniform access to a small number |
the revenue of each pool affects the revenue of the |
determine the distribution of messages delivered by the chain vs |
occur without with uniform access to a small number of |
revenue of each pool affects the revenue of the other |
the distribution of messages delivered by the chain vs delivered |
without with uniform access to a small number of objects |
of each pool affects the revenue of the other through |
distribution of messages delivered by the chain vs delivered by |
each pool affects the revenue of the other through the |
of messages delivered by the chain vs delivered by gossip |
pool affects the revenue of the other through the infiltrating |
affects the revenue of the other through the infiltrating miners |
We prove that for a static choice of infiltration rates |
prove that for a static choice of infiltration rates the |
that for a static choice of infiltration rates the pool |
The runs are eight times longer than the runs before |
for a static choice of infiltration rates the pool revenues |
a static choice of infiltration rates the pool revenues converge |
both in total experiment time and time the victim node |
in total experiment time and time the victim node is |
total experiment time and time the victim node is halted |
once one pool changes its infiltration rate of the other |
Commit ratio is affected if the predictor reserves unnecessary objects |
the latter may prefer to change its infiltration rate of |
ratio is affected if the predictor reserves unnecessary objects by |
latter may prefer to change its infiltration rate of the |
is affected if the predictor reserves unnecessary objects by a |
may prefer to change its infiltration rate of the former |
affected if the predictor reserves unnecessary objects by a factor |
show the number of messages delivered by the chain replication |
if the predictor reserves unnecessary objects by a factor of |
the number of messages delivered by the chain replication mechanism |
the predictor reserves unnecessary objects by a factor of slack |
We show analytically that the game has a single Nash |
number of messages delivered by the chain replication mechanism and |
show analytically that the game has a single Nash Equilibrium |
of messages delivered by the chain replication mechanism and the |
analytically that the game has a single Nash Equilibrium and |
messages delivered by the chain replication mechanism and the ones |
that the game has a single Nash Equilibrium and numerically |
delivered by the chain replication mechanism and the ones delivered |
Note that when all accesses are to the hot zone |
the game has a single Nash Equilibrium and numerically study |
by the chain replication mechanism and the ones delivered by |
game has a single Nash Equilibrium and numerically study the |
the chain replication mechanism and the ones delivered by the |
has a single Nash Equilibrium and numerically study the equilibrium |
chain replication mechanism and the ones delivered by the epidemics |
a single Nash Equilibrium and numerically study the equilibrium points |
single Nash Equilibrium and numerically study the equilibrium points for |
Nash Equilibrium and numerically study the equilibrium points for different |
Equilibrium and numerically study the equilibrium points for different pool |
again we omitted the head of the chain node because |
and numerically study the equilibrium points for different pool sizes |
commit rates are lower with imperfect prediction than in the |
we omitted the head of the chain node because its |
rates are lower with imperfect prediction than in the uniform |
omitted the head of the chain node because its behavior |
are lower with imperfect prediction than in the uniform random |
the head of the chain node because its behavior is |
lower with imperfect prediction than in the uniform random case |
head of the chain node because its behavior is not |
with imperfect prediction than in the uniform random case with |
of the chain node because its behavior is not representative |
at the equilibrium point both pools earn less than they |
the equilibrium point both pools earn less than they would |
equilibrium point both pools earn less than they would have |
point both pools earn less than they would have in |
both pools earn less than they would have in the |
pools earn less than they would have in the nonequilibrium |
earn less than they would have in the nonequilibrium no |
Since pools can decide to start or stop attacking at |
pools can decide to start or stop attacking at any |
can decide to start or stop attacking at any point |
As the nodes get further away from the victim node |
this can be modeled as the miner s dilemma an |
more of the messages were delivered by means of the |
can be modeled as the miner s dilemma an instance |
zone go through a single OM that becomes a bottleneck |
of the messages were delivered by means of the chain |
be modeled as the miner s dilemma an instance of |
modeled as the miner s dilemma an instance of the |
because the repair mechanism relinked the chain and chain replication |
as the miner s dilemma an instance of the iterative |
since object access conflicts occur only at a single shard |
the repair mechanism relinked the chain and chain replication began |
the miner s dilemma an instance of the iterative prisoner |
the reservations prevent deadlocks and result in perfect commit ratio |
repair mechanism relinked the chain and chain replication began to |
miner s dilemma an instance of the iterative prisoner s |
reservations prevent deadlocks and result in perfect commit ratio with |
mechanism relinked the chain and chain replication began to function |
s dilemma an instance of the iterative prisoner s dilemma |
prevent deadlocks and result in perfect commit ratio with perfect |
relinked the chain and chain replication began to function normally |
deadlocks and result in perfect commit ratio with perfect prediction |
The speed with which the chain is restored depends on |
speed with which the chain is restored depends on the |
with which the chain is restored depends on the rate |
which the chain is restored depends on the rate of |
the chain is restored depends on the rate of the |
where some of the objects belong to a so called |
chain is restored depends on the rate of the fast |
we address in Section VII the case where the participants |
some of the objects belong to a so called hot |
address in Section VII the case where the participants are |
in Section VII the case where the participants are an |
Section VII the case where the participants are an arbitrary |
VII the case where the participants are an arbitrary number |
Future development The current SSA implementation uses gossip in situations |
the case where the participants are an arbitrary number of |
development The current SSA implementation uses gossip in situations where |
case where the participants are an arbitrary number of identical |
The current SSA implementation uses gossip in situations where faster |
where the participants are an arbitrary number of identical pools |
current SSA implementation uses gossip in situations where faster notifications |
SSA implementation uses gossip in situations where faster notifications might |
implementation uses gossip in situations where faster notifications might be |
There exists a symmetric equilibrium in which each participating pool |
uses gossip in situations where faster notifications might be helpful |
exists a symmetric equilibrium in which each participating pool attacks |
a symmetric equilibrium in which each participating pool attacks each |
symmetric equilibrium in which each participating pool attacks each of |
equilibrium in which each participating pool attacks each of the |
and transactions arrivals are governed by a Poisson process with |
in which each participating pool attacks each of the other |
it would be useful to spread the news as quickly |
transactions arrivals are governed by a Poisson process with the |
which each participating pool attacks each of the other participating |
would be useful to spread the news as quickly as |
arrivals are governed by a Poisson process with the required |
each participating pool attacks each of the other participating pools |
be useful to spread the news as quickly as possible |
are governed by a Poisson process with the required TPUT |
We realize that for some particular tasks gossip could be |
realize that for some particular tasks gossip could be done |
that for some particular tasks gossip could be done more |
here too at equilibrium all pools earn less than with |
for some particular tasks gossip could be done more efficiently |
too at equilibrium all pools earn less than with the |
We are unaware of work that uses prediction to order |
We are therefore exploring the use of IP multicast for |
at equilibrium all pools earn less than with the no |
are unaware of work that uses prediction to order distributed |
are therefore exploring the use of IP multicast for dissemination |
unaware of work that uses prediction to order distributed transactions |
therefore exploring the use of IP multicast for dissemination of |
of work that uses prediction to order distributed transactions before |
exploring the use of IP multicast for dissemination of urgent |
Our results imply that block withholding by pools leads to |
work that uses prediction to order distributed transactions before certification |
the use of IP multicast for dissemination of urgent information |
results imply that block withholding by pools leads to an |
use of IP multicast for dissemination of urgent information as |
imply that block withholding by pools leads to an unfavorable |
of IP multicast for dissemination of urgent information as long |
that block withholding by pools leads to an unfavorable equilibrium |
IP multicast for dissemination of urgent information as long as |
uses static analysis to allow separate workers to process independent |
multicast for dissemination of urgent information as long as the |
static analysis to allow separate workers to process independent transactions |
for dissemination of urgent information as long as the physical |
analysis to allow separate workers to process independent transactions without |
dissemination of urgent information as long as the physical nodes |
to allow separate workers to process independent transactions without synchronization |
of urgent information as long as the physical nodes are |
urgent information as long as the physical nodes are not |
information as long as the physical nodes are not on |
as long as the physical nodes are not on a |
miners will prefer to form closed pools that cannot be |
long as the physical nodes are not on a public |
will prefer to form closed pools that cannot be attacked |
as the physical nodes are not on a public network |
prefer to form closed pools that cannot be attacked in |
the physical nodes are not on a public network segment |
to form closed pools that cannot be attacked in this |
form closed pools that cannot be attacked in this manner |
we plan to include support for the partitioning of the |
Though this may be conceived as bad news for public |
plan to include support for the partitioning of the services |
this may be conceived as bad news for public mining |
to include support for the partitioning of the services by |
may be conceived as bad news for public mining pools |
include support for the partitioning of the services by means |
support for the partitioning of the services by means of |
on the whole it may be good news to the |
for the partitioning of the services by means of registering |
the whole it may be good news to the Bitcoin |
we run multiple simulations to find the maximal TPUT the |
the partitioning of the services by means of registering partition |
whole it may be good news to the Bitcoin system |
run multiple simulations to find the maximal TPUT the system |
partitioning of the services by means of registering partition function |
multiple simulations to find the maximal TPUT the system can |
of the services by means of registering partition function handlers |
simulations to find the maximal TPUT the system can handle |
We examine the practicality of the attack in Section VIII |
the services by means of registering partition function handlers with |
examine the practicality of the attack in Section VIII and |
services by means of registering partition function handlers with a |
the practicality of the attack in Section VIII and discuss |
by means of registering partition function handlers with a global |
PC with SMR TMs is blocked by contention much earlier |
practicality of the attack in Section VIII and discuss implications |
means of registering partition function handlers with a global data |
with SMR TMs is blocked by contention much earlier than |
of the attack in Section VIII and discuss implications and |
SMR TMs is blocked by contention much earlier than ACID |
the attack in Section VIII and discuss implications and model |
attack in Section VIII and discuss implications and model extensions |
in Section VIII and discuss implications and model extensions in |
Section VIII and discuss implications and model extensions in Section |
we have implemented only the server side load balancing scheme |
We briefly review here work related to ACIDRAIN s certification |
VIII and discuss implications and model extensions in Section IX |
briefly review here work related to ACIDRAIN s certification protocol |
We are considering ways to extend our approach for use |
are considering ways to extend our approach for use in |
considering ways to extend our approach for use in settings |
One approach for certification is to use a single highly |
ways to extend our approach for use in settings where |
to extend our approach for use in settings where partitioning |
extend our approach for use in settings where partitioning is |
our approach for use in settings where partitioning is done |
Definition of the pool game where pools in a proof |
approach for use in settings where partitioning is done on |
for use in settings where partitioning is done on the |
ofwork secured system attack one another with a pool block |
use in settings where partitioning is done on the client |
secured system attack one another with a pool block withholding |
in settings where partitioning is done on the client side |
system attack one another with a pool block withholding attack |
A transaction commits if and only if it has no |
We are also developing a GUI assisted automated web service |
transaction commits if and only if it has no conflicts |
are also developing a GUI assisted automated web service deployment |
commits if and only if it has no conflicts with |
also developing a GUI assisted automated web service deployment tool |
if and only if it has no conflicts with previous |
and only if it has no conflicts with previous committed |
only if it has no conflicts with previous committed transactions |
the only Nash Equilibrium is when the pools attack one |
only Nash Equilibrium is when the pools attack one another |
and the system will generate a XML description that can |
the system will generate a XML description that can be |
system will generate a XML description that can be used |
will generate a XML description that can be used later |
generate a XML description that can be used later on |
a XML description that can be used later on to |
XML description that can be used later on to actually |
description that can be used later on to actually deploy |
that can be used later on to actually deploy the |
can be used later on to actually deploy the service |
be used later on to actually deploy the service automatically |
serialized all transactions when they enter the system to achieve |
With multiple pools of equal size there is a symmetric |
all transactions when they enter the system to achieve a |
multiple pools of equal size there is a symmetric Nash |
transactions when they enter the system to achieve a deterministic |
and deployed on the fly on top of the processing |
pools of equal size there is a symmetric Nash equilibrium |
when they enter the system to achieve a deterministic order |
deployed on the fly on top of the processing nodes |
where all pools earn less than if none had attacked |
Scaling up To turn the SSA into a full scale |
up To turn the SSA into a full scale platform |
whereas we address long running transactions and use prediction to |
inefficient equilibria for open pools may serve the system by |
one of the immediate future challenges is the necessity of |
we address long running transactions and use prediction to infer |
equilibria for open pools may serve the system by reducing |
of the immediate future challenges is the necessity of evaluating |
address long running transactions and use prediction to infer an |
for open pools may serve the system by reducing their |
the immediate future challenges is the necessity of evaluating a |
long running transactions and use prediction to infer an order |
open pools may serve the system by reducing their attraction |
immediate future challenges is the necessity of evaluating a full |
pools may serve the system by reducing their attraction and |
future challenges is the necessity of evaluating a full RAPS |
may serve the system by reducing their attraction and pushing |
challenges is the necessity of evaluating a full RAPS of |
serve the system by reducing their attraction and pushing miners |
is the necessity of evaluating a full RAPS of RACS |
the system by reducing their attraction and pushing miners towards |
the necessity of evaluating a full RAPS of RACS deployment |
system by reducing their attraction and pushing miners towards smaller |
by reducing their attraction and pushing miners towards smaller closed |
Multiple partitioned and cloned services running on our tightly coupled |
reducing their attraction and pushing miners towards smaller closed pools |
partitioned and cloned services running on our tightly coupled cluster |
and cloned services running on our tightly coupled cluster would |
The classical block withholding attack is as old as pools |
cloned services running on our tightly coupled cluster would lead |
Transactional consistency and automatic management in an application data cache |
classical block withholding attack is as old as pools themselves |
services running on our tightly coupled cluster would lead to |
running on our tightly coupled cluster would lead to a |
but its use by pools has not been suggested until |
on our tightly coupled cluster would lead to a series |
its use by pools has not been suggested until recently |
our tightly coupled cluster would lead to a series of |
tightly coupled cluster would lead to a series of other |
We overview related attacks and prior work in Section X |
The result is somewhat analogous to our separation of optimistic |
coupled cluster would lead to a series of other issues |
result is somewhat analogous to our separation of optimistic ordering |
cluster would lead to a series of other issues that |
is somewhat analogous to our separation of optimistic ordering and |
would lead to a series of other issues that should |
somewhat analogous to our separation of optimistic ordering and conservative |
P RELIMINARIES B ITCOIN AND P OOLED M INING Bitcoin |
lead to a series of other issues that should be |
analogous to our separation of optimistic ordering and conservative certification |
RELIMINARIES B ITCOIN AND P OOLED M INING Bitcoin is |
to a series of other issues that should be investigated |
B ITCOIN AND P OOLED M INING Bitcoin is a |
ITCOIN AND P OOLED M INING Bitcoin is a distributed |
we avoid prediction and measure the maximal commit rate it |
how to place the clones on physical nodes in order |
avoid prediction and measure the maximal commit rate it can |
to place the clones on physical nodes in order to |
prediction and measure the maximal commit rate it can accommodate |
place the clones on physical nodes in order to satisfy |
and measure the maximal commit rate it can accommodate with |
the clones on physical nodes in order to satisfy certain |
measure the maximal commit rate it can accommodate with an |
clones on physical nodes in order to satisfy certain constraints |
the maximal commit rate it can accommodate with an increasing |
maximal commit rate it can accommodate with an increasing number |
commit rate it can accommodate with an increasing number of |
rate it can accommodate with an increasing number of shards |
Caching placement deciding if some services would benefit if they |
writes of objects chosen uniformly at random from a small |
placement deciding if some services would benefit if they are |
of objects chosen uniformly at random from a small set |
deciding if some services would benefit if they are fitted |
objects chosen uniformly at random from a small set of |
if some services would benefit if they are fitted with |
some services would benefit if they are fitted with response |
services would benefit if they are fitted with response caches |
and ultimately placing the cache components in a smart way |
and the system s only task is to serialize transactions |
the system s only task is to serialize transactions in |
system s only task is to serialize transactions in a |
s only task is to serialize transactions in a single |
location placing multiple service clones on the same physical node |
only task is to serialize transactions in a single ledger |
placing multiple service clones on the same physical node to |
task is to serialize transactions in a single ledger and |
multiple service clones on the same physical node to exploit |
is to serialize transactions in a single ledger and reject |
service clones on the same physical node to exploit fast |
to serialize transactions in a single ledger and reject transactions |
clones on the same physical node to exploit fast IPC |
serialize transactions in a single ledger and reject transactions that |
on the same physical node to exploit fast IPC communication |
transactions in a single ledger and reject transactions that cannot |
the same physical node to exploit fast IPC communication as |
in a single ledger and reject transactions that cannot be |
Global log is an architecture where TMs submit all transactions |
same physical node to exploit fast IPC communication as opposed |
a single ledger and reject transactions that cannot be serialized |
log is an architecture where TMs submit all transactions to |
physical node to exploit fast IPC communication as opposed to |
single ledger and reject transactions that cannot be serialized due |
is an architecture where TMs submit all transactions to a |
node to exploit fast IPC communication as opposed to network |
ledger and reject transactions that cannot be serialized due to |
an architecture where TMs submit all transactions to a single |
to exploit fast IPC communication as opposed to network messages |
and reject transactions that cannot be serialized due to conflicts |
architecture where TMs submit all transactions to a single global |
exploit fast IPC communication as opposed to network messages if |
reject transactions that cannot be serialized due to conflicts with |
where TMs submit all transactions to a single global log |
fast IPC communication as opposed to network messages if the |
transactions that cannot be serialized due to conflicts with previous |
TMs submit all transactions to a single global log and |
IPC communication as opposed to network messages if the benefits |
that cannot be serialized due to conflicts with previous transactions |
submit all transactions to a single global log and check |
communication as opposed to network messages if the benefits overweigh |
all transactions to a single global log and check conflicts |
as opposed to network messages if the benefits overweigh the |
Bitcoin transactions are protected with cryptographic techniques that ensure that |
transactions to a single global log and check conflicts on |
opposed to network messages if the benefits overweigh the cost |
transactions are protected with cryptographic techniques that ensure that only |
to a single global log and check conflicts on that |
to network messages if the benefits overweigh the cost incurred |
are protected with cryptographic techniques that ensure that only the |
a single global log and check conflicts on that single |
network messages if the benefits overweigh the cost incurred by |
protected with cryptographic techniques that ensure that only the rightful |
single global log and check conflicts on that single log |
messages if the benefits overweigh the cost incurred by resource |
with cryptographic techniques that ensure that only the rightful owner |
if the benefits overweigh the cost incurred by resource contention |
cryptographic techniques that ensure that only the rightful owner of |
the benefits overweigh the cost incurred by resource contention on |
techniques that ensure that only the rightful owner of a |
benefits overweigh the cost incurred by resource contention on the |
that ensure that only the rightful owner of a Bitcoin |
overweigh the cost incurred by resource contention on the shared |
ensure that only the rightful owner of a Bitcoin can |
the cost incurred by resource contention on the shared host |
that only the rightful owner of a Bitcoin can transfer |
only the rightful owner of a Bitcoin can transfer it |
Management tools developing tools that monitor service properties such as |
tools developing tools that monitor service properties such as response |
The transaction ledger is stored by a network of miners |
developing tools that monitor service properties such as response time |
transaction ledger is stored by a network of miners in |
ledger is stored by a network of miners in a |
is stored by a network of miners in a data |
stored by a network of miners in a data structure |
by a network of miners in a data structure caller |
a network of miners in a data structure caller the |
network of miners in a data structure caller the blockchain |
Using VMM tricks virtual machines can be used to migrate |
Revenue for Proof Of Work The blockchain records the transactions |
VMM tricks virtual machines can be used to migrate transparently |
for Proof Of Work The blockchain records the transactions in |
tricks virtual machines can be used to migrate transparently a |
Proof Of Work The blockchain records the transactions in units |
virtual machines can be used to migrate transparently a collection |
Of Work The blockchain records the transactions in units of |
machines can be used to migrate transparently a collection of |
Work The blockchain records the transactions in units of blocks |
can be used to migrate transparently a collection of services |
be used to migrate transparently a collection of services on |
used to migrate transparently a collection of services on a |
to migrate transparently a collection of services on a different |
migrate transparently a collection of services on a different physical |
transparently a collection of services on a different physical processor |
A valid block contains the hash of the previous block |
RAIN is that they require a coordinator that performs transactions |
is that they require a coordinator that performs transactions to |
that they require a coordinator that performs transactions to be |
and a Bitcoin address which is to be credited with |
they require a coordinator that performs transactions to be highly |
a Bitcoin address which is to be credited with a |
require a coordinator that performs transactions to be highly available |
the SSA can be seen as a platform that leverages |
Bitcoin address which is to be credited with a reward |
SSA can be seen as a platform that leverages tradeoffs |
address which is to be credited with a reward for |
can be seen as a platform that leverages tradeoffs between |
which is to be credited with a reward for generating |
be seen as a platform that leverages tradeoffs between weaker |
is to be credited with a reward for generating the |
seen as a platform that leverages tradeoffs between weaker consistency |
to be credited with a reward for generating the block |
Related Work Our transaction ordering protocol is inspired by a |
Work Our transaction ordering protocol is inspired by a state |
Any miner may add a valid block to the chain |
miner may add a valid block to the chain by |
This is an old idea first explored in the Grapevine |
proving that it has spent a certain amount of work |
that it has spent a certain amount of work and |
it has spent a certain amount of work and publishing |
has spent a certain amount of work and publishing the |
but we have generalized the protocol to work with arbitrary |
spent a certain amount of work and publishing the block |
we have generalized the protocol to work with arbitrary overlapping |
a certain amount of work and publishing the block with |
have generalized the protocol to work with arbitrary overlapping par |
certain amount of work and publishing the block with the |
amount of work and publishing the block with the proof |
of work and publishing the block with the proof over |
work and publishing the block with the proof over an |
Several database and distributed systems take advantage of the same |
and publishing the block with the proof over an overlay |
database and distributed systems take advantage of the same tradeoff |
publishing the block with the proof over an overlay network |
the block with the proof over an overlay network to |
th ACM SIGKDD International Conference on Knowledge Discovery and Data |
block with the proof over an overlay network to all |
for example allowing multiple updates to occur simultaneously at distinct |
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining |
with the proof over an overlay network to all other |
example allowing multiple updates to occur simultaneously at distinct replicas |
the proof over an overlay network to all other miners |
allowing multiple updates to occur simultaneously at distinct replicas by |
multiple updates to occur simultaneously at distinct replicas by specifying |
updates to occur simultaneously at distinct replicas by specifying a |
to occur simultaneously at distinct replicas by specifying a maximum |
occur simultaneously at distinct replicas by specifying a maximum accepted |
simultaneously at distinct replicas by specifying a maximum accepted deviation |
at distinct replicas by specifying a maximum accepted deviation from |
transaction fee paid by the users Electronic copy available at |
distinct replicas by specifying a maximum accepted deviation from strong |
replicas by specifying a maximum accepted deviation from strong consistency |
tolerating a bounded number of consistency violations to increase concurrency |
a bounded number of consistency violations to increase concurrency of |
bounded number of consistency violations to increase concurrency of transactions |
and an amount of minted Bitcoins that are thus introduced |
an amount of minted Bitcoins that are thus introduced into |
amount of minted Bitcoins that are thus introduced into the |
of minted Bitcoins that are thus introduced into the system |
The work which a miner is required to do is |
work which a miner is required to do is to |
which a miner is required to do is to repeatedly |
a miner is required to do is to repeatedly calculate |
Our work on the SSA is the first to apply |
miner is required to do is to repeatedly calculate a |
work on the SSA is the first to apply such |
is required to do is to repeatedly calculate a a |
on the SSA is the first to apply such thinking |
required to do is to repeatedly calculate a a hash |
the SSA is the first to apply such thinking to |
to do is to repeatedly calculate a a hash function |
SSA is the first to apply such thinking to a |
do is to repeatedly calculate a a hash function specifically |
is the first to apply such thinking to a cluster |
is to repeatedly calculate a a hash function specifically the |
the first to apply such thinking to a cluster computing |
to repeatedly calculate a a hash function specifically the SHA |
first to apply such thinking to a cluster computing environment |
where the clients sequentially access objects before ending a transaction |
platform was designed to provide a cluster based environment for |
We believe our techniques could be used to reduce abort |
was designed to provide a cluster based environment for scalable |
believe our techniques could be used to reduce abort rates |
designed to provide a cluster based environment for scalable Internet |
our techniques could be used to reduce abort rates of |
to provide a cluster based environment for scalable Internet services |
techniques could be used to reduce abort rates of systems |
provide a cluster based environment for scalable Internet services of |
could be used to reduce abort rates of systems using |
a cluster based environment for scalable Internet services of the |
be used to reduce abort rates of systems using Sinfonia |
cluster based environment for scalable Internet services of the sort |
used to reduce abort rates of systems using Sinfonia or |
based environment for scalable Internet services of the sort used |
The miner places different values in this field and calculates |
to reduce abort rates of systems using Sinfonia or a |
environment for scalable Internet services of the sort used in |
miner places different values in this field and calculates the |
reduce abort rates of systems using Sinfonia or a similar |
for scalable Internet services of the sort used in web |
places different values in this field and calculates the hash |
abort rates of systems using Sinfonia or a similar certification |
scalable Internet services of the sort used in web servers |
different values in this field and calculates the hash for |
rates of systems using Sinfonia or a similar certification mechanism |
values in this field and calculates the hash for each |
in this field and calculates the hash for each value |
Service components are controlled by a front end machine that |
components are controlled by a front end machine that acts |
If the result of the hash is smaller than a |
are controlled by a front end machine that acts as |
the result of the hash is smaller than a target |
controlled by a front end machine that acts as a |
result of the hash is smaller than a target value |
by a front end machine that acts as a request |
a front end machine that acts as a request dispatcher |
front end machine that acts as a request dispatcher and |
end machine that acts as a request dispatcher and incorporates |
machine that acts as a request dispatcher and incorporates the |
The number of attempts to find a single hash is |
that acts as a request dispatcher and incorporates the load |
number of attempts to find a single hash is therefore |
acts as a request dispatcher and incorporates the load balancing |
of attempts to find a single hash is therefore random |
as a request dispatcher and incorporates the load balancing and |
attempts to find a single hash is therefore random with |
a request dispatcher and incorporates the load balancing and restart |
to find a single hash is therefore random with a |
request dispatcher and incorporates the load balancing and restart logics |
find a single hash is therefore random with a geometric |
a single hash is therefore random with a geometric distribution |
as each attempt is a Bernoulli trial with a success |
each attempt is a Bernoulli trial with a success probability |
attempt is a Bernoulli trial with a success probability determined |
TACC workers can be composed to address more complex tasks |
is a Bernoulli trial with a success probability determined by |
a Bernoulli trial with a success probability determined by the |
Bernoulli trial with a success probability determined by the target |
trial with a success probability determined by the target value |
At the existing huge hashing rates and small target values |
SSA can be seen as revisiting these architectural ideas in |
the time to find a single hash can be approximated |
can be seen as revisiting these architectural ideas in conjunction |
time to find a single hash can be approximated by |
be seen as revisiting these architectural ideas in conjunction with |
to find a single hash can be approximated by an |
seen as revisiting these architectural ideas in conjunction with chain |
find a single hash can be approximated by an exponential |
as revisiting these architectural ideas in conjunction with chain replication |
a single hash can be approximated by an exponential distribution |
The average time for a miner to find a solution |
average time for a miner to find a solution is |
time for a miner to find a solution is therefore |
for a miner to find a solution is therefore proportional |
a miner to find a solution is therefore proportional to |
miner to find a solution is therefore proportional to its |
and were the first systems to exploit the style of |
to find a solution is therefore proportional to its hashing |
were the first systems to exploit the style of partitioning |
find a solution is therefore proportional to its hashing rate |
the first systems to exploit the style of partitioning that |
a solution is therefore proportional to its hashing rate or |
first systems to exploit the style of partitioning that leads |
solution is therefore proportional to its hashing rate or mining |
systems to exploit the style of partitioning that leads to |
is therefore proportional to its hashing rate or mining power |
to exploit the style of partitioning that leads to a |
exploit the style of partitioning that leads to a RAPS |
the style of partitioning that leads to a RAPS of |
style of partitioning that leads to a RAPS of RACS |
and as part of its defense against denial of service |
of partitioning that leads to a RAPS of RACS solution |
as part of its defense against denial of service and |
part of its defense against denial of service and other |
of its defense against denial of service and other attacks |
at potentially high cost in terms of reduced availability during |
potentially high cost in terms of reduced availability during faults |
the protocol deterministically defines the target value for each block |
protocol deterministically defines the target value for each block according |
deterministically defines the target value for each block according to |
defines the target value for each block according to the |
the target value for each block according to the time |
target value for each block according to the time required |
value for each block according to the time required to |
for each block according to the time required to generate |
each block according to the time required to generate recent |
block according to the time required to generate recent blocks |
ultimately arguing for precisely the weak update model that we |
arguing for precisely the weak update model that we adopted |
for precisely the weak update model that we adopted here |
blocks such that the average time for each block to |
such that the average time for each block to be |
that the average time for each block to be found |
offer persistent state support by wrapping soft state business logic |
the average time for each block to be found is |
persistent state support by wrapping soft state business logic components |
state support by wrapping soft state business logic components on |
support by wrapping soft state business logic components on top |
by wrapping soft state business logic components on top of |
wrapping soft state business logic components on top of a |
soft state business logic components on top of a relational |
state business logic components on top of a relational or |
business logic components on top of a relational or object |
all miners switch to mine for the subsequent block b |
at t without changing their probability distribution of finding a |
t without changing their probability distribution of finding a block |
without changing their probability distribution of finding a block after |
changing their probability distribution of finding a block after t |
the probability that a miner i with mining power mi |
probability that a miner i with mining power mi finds |
that a miner i with mining power mi finds the |
a miner i with mining power mi finds the next |
Ninja is arguably more flexible than application servers in that |
miner i with mining power mi finds the next block |
is arguably more flexible than application servers in that it |
i with mining power mi finds the next block is |
arguably more flexible than application servers in that it performs |
with mining power mi finds the next block is its |
more flexible than application servers in that it performs connection |
mining power mi finds the next block is its ratio |
flexible than application servers in that it performs connection management |
power mi finds the next block is its ratio out |
than application servers in that it performs connection management and |
mi finds the next block is its ratio out of |
application servers in that it performs connection management and automatically |
finds the next block is its ratio out of the |
servers in that it performs connection management and automatically partitions |
the next block is its ratio out of the total |
in that it performs connection management and automatically partitions and |
next block is its ratio out of the total mining |
that it performs connection management and automatically partitions and replicates |
block is its ratio out of the total mining power |
it performs connection management and automatically partitions and replicates persistent |
is its ratio out of the total mining power m |
performs connection management and automatically partitions and replicates persistent state |
its ratio out of the total mining power m in |
ratio out of the total mining power m in the |
Prediction of transaction behavior has the potential to significantly decrease |
but the framework takes a different tiered approach to services |
out of the total mining power m in the system |
of transaction behavior has the potential to significantly decrease abort |
the framework takes a different tiered approach to services based |
transaction behavior has the potential to significantly decrease abort rates |
framework takes a different tiered approach to services based on |
behavior has the potential to significantly decrease abort rates in |
takes a different tiered approach to services based on bases |
has the potential to significantly decrease abort rates in large |
the potential to significantly decrease abort rates in large scale |
potential to significantly decrease abort rates in large scale transactional |
to significantly decrease abort rates in large scale transactional systems |
and represents shared state by means of distributed data structures |
significantly decrease abort rates in large scale transactional systems with |
decrease abort rates in large scale transactional systems with high |
abort rates in large scale transactional systems with high contention |
a new platform for porting a large class of service |
RAIN we employ prediction to obtain soft reservations and implement |
we employ prediction to obtain soft reservations and implement atomic |
employ prediction to obtain soft reservations and implement atomic transactions |
The SSA was designed to be as simple as possible |
prediction to obtain soft reservations and implement atomic transactions while |
to obtain soft reservations and implement atomic transactions while requiring |
obtain soft reservations and implement atomic transactions while requiring high |
soft reservations and implement atomic transactions while requiring high availability |
reservations and implement atomic transactions while requiring high availability only |
and gossip epidemics which are used to manage configuration data |
and implement atomic transactions while requiring high availability only in |
gossip epidemics which are used to manage configuration data and |
implement atomic transactions while requiring high availability only in a |
epidemics which are used to manage configuration data and initiate |
atomic transactions while requiring high availability only in a single |
which are used to manage configuration data and initiate repair |
transactions while requiring high availability only in a single tier |
are used to manage configuration data and initiate repair after |
while requiring high availability only in a single tier of |
Mining is only profitable using dedicated hardware in cutting edge |
used to manage configuration data and initiate repair after failures |
requiring high availability only in a single tier of independent |
is only profitable using dedicated hardware in cutting edge mining |
high availability only in a single tier of independent logs |
only profitable using dedicated hardware in cutting edge mining rigs |
given a gossip rate that is sufficiently fast relative to |
a gossip rate that is sufficiently fast relative to the |
gossip rate that is sufficiently fast relative to the update |
Although expected revenue from mining is proportional to the power |
rate that is sufficiently fast relative to the update rates |
expected revenue from mining is proportional to the power of |
that is sufficiently fast relative to the update rates seen |
RAIN s operations never depend on a single machine by |
revenue from mining is proportional to the power of the |
is sufficiently fast relative to the update rates seen in |
s operations never depend on a single machine by allowing |
from mining is proportional to the power of the mining |
sufficiently fast relative to the update rates seen in the |
operations never depend on a single machine by allowing fast |
mining is proportional to the power of the mining rigs |
On subversive miner strategies and block withholding attack in bitcoin |
fast relative to the update rates seen in the cluster |
never depend on a single machine by allowing fast recovery |
is proportional to the power of the mining rigs used |
subversive miner strategies and block withholding attack in bitcoin digital |
depend on a single machine by allowing fast recovery from |
miner strategies and block withholding attack in bitcoin digital currency |
we find that the SSA can rapidly and automatically reconfigure |
a single home miner using a small rig is unlikely |
on a single machine by allowing fast recovery from failures |
find that the SSA can rapidly and automatically reconfigure itself |
single home miner using a small rig is unlikely to |
a single machine by allowing fast recovery from failures and |
that the SSA can rapidly and automatically reconfigure itself after |
home miner using a small rig is unlikely to mine |
single machine by allowing fast recovery from failures and performance |
the SSA can rapidly and automatically reconfigure itself after a |
miner using a small rig is unlikely to mine a |
machine by allowing fast recovery from failures and performance hiccups |
SSA can rapidly and automatically reconfigure itself after a failure |
using a small rig is unlikely to mine a block |
can rapidly and automatically reconfigure itself after a failure and |
a small rig is unlikely to mine a block for |
rapidly and automatically reconfigure itself after a failure and can |
small rig is unlikely to mine a block for years |
and automatically reconfigure itself after a failure and can rapidly |
automatically reconfigure itself after a failure and can rapidly repair |
reconfigure itself after a failure and can rapidly repair data |
itself after a failure and can rapidly repair data inconsistencies |
after a failure and can rapidly repair data inconsistencies that |
a failure and can rapidly repair data inconsistencies that arise |
failure and can rapidly repair data inconsistencies that arise during |
and can rapidly repair data inconsistencies that arise during the |
can rapidly repair data inconsistencies that arise during the period |
rapidly repair data inconsistencies that arise during the period when |
repair data inconsistencies that arise during the period when the |
data inconsistencies that arise during the period when the cluster |
a pool is a group of miners that share their |
inconsistencies that arise during the period when the cluster configuration |
pool is a group of miners that share their revenues |
that arise during the period when the cluster configuration was |
is a group of miners that share their revenues when |
arise during the period when the cluster configuration was still |
a group of miners that share their revenues when one |
during the period when the cluster configuration was still disrupted |
group of miners that share their revenues when one of |
of miners that share their revenues when one of them |
miners that share their revenues when one of them successfully |
Our goal is to make the software available to a |
that share their revenues when one of them successfully mines |
goal is to make the software available to a general |
share their revenues when one of them successfully mines a |
is to make the software available to a general user |
their revenues when one of them successfully mines a block |
to make the software available to a general user community |
make the software available to a general user community in |
the revenue is distributed among the pool members in proportion |
revenue is distributed among the pool members in proportion to |
is distributed among the pool members in proportion to their |
distributed among the pool members in proportion to their mining |
among the pool members in proportion to their mining power |
The expected revenue of a pool member is therefore the |
Acknowledgments The authors are grateful to the research team at |
expected revenue of a pool member is therefore the same |
The authors are grateful to the research team at AFRL |
revenue of a pool member is therefore the same as |
authors are grateful to the research team at AFRL in |
of a pool member is therefore the same as its |
are grateful to the research team at AFRL in Rome |
a pool member is therefore the same as its revenue |
pool member is therefore the same as its revenue had |
member is therefore the same as its revenue had it |
is therefore the same as its revenue had it mined |
for their help in understanding the challenges of using Service |
therefore the same as its revenue had it mined solo |
their help in understanding the challenges of using Service Oriented |
help in understanding the challenges of using Service Oriented Architectures |
in understanding the challenges of using Service Oriented Architectures in |
understanding the challenges of using Service Oriented Architectures in large |
we plan to build on our simulation results by implementing |
the challenges of using Service Oriented Architectures in large scale |
plan to build on our simulation results by implementing ACID |
challenges of using Service Oriented Architectures in large scale settings |
RAIN and exploring the different aspects of its performance in |
and exploring the different aspects of its performance in realistic |
exploring the different aspects of its performance in realistic settings |
for helping us understand the architectures employed in very large |
Miners register with the pool manager and mine on its |
helping us understand the architectures employed in very large data |
register with the pool manager and mine on its behalf |
us understand the architectures employed in very large data centers |
different network topologies with a single datacenter and with multiple |
The pool manager generates tasks and the miners search for |
network topologies with a single datacenter and with multiple datacenters |
pool manager generates tasks and the miners search for solutions |
manager generates tasks and the miners search for solutions based |
generates tasks and the miners search for solutions based on |
tasks and the miners search for solutions based on these |
and the miners search for solutions based on these tasks |
the miners search for solutions based on these tasks that |
miners search for solutions based on these tasks that can |
search for solutions based on these tasks that can serve |
for solutions based on these tasks that can serve as |
solutions based on these tasks that can serve as proof |
based on these tasks that can serve as proof of |
on these tasks that can serve as proof of work |
The pool manager behaves as a single miner in the |
pool manager behaves as a single miner in the Bitcoin |
manager behaves as a single miner in the Bitcoin system |
Once it obtains a legitimate block from one of its |
it obtains a legitimate block from one of its miners |
The block transfers the revenue to the control of the |
block transfers the revenue to the control of the pool |
transfers the revenue to the control of the pool manager |
The pool manager then distributes the revenue among the miners |
pool manager then distributes the revenue among the miners according |
manager then distributes the revenue among the miners according to |
then distributes the revenue among the miners according to their |
distributes the revenue among the miners according to their mining |
the revenue among the miners according to their mining power |
In order to estimate the mining power of a miner |
the pool manager sets a partial target for each member |
lightweight elasticity in shared storage databases for the cloud using |
elasticity in shared storage databases for the cloud using live |
in shared storage databases for the cloud using live data |
shared storage databases for the cloud using live data migration |
Each miner is required to send the pool manager blocks |
miner is required to send the pool manager blocks that |
is required to send the pool manager blocks that are |
required to send the pool manager blocks that are correct |
to send the pool manager blocks that are correct according |
send the pool manager blocks that are correct according to |
the pool manager blocks that are correct according to the |
pool manager blocks that are correct according to the partial |
manager blocks that are correct according to the partial target |
such that partial solutions arrive frequently enough for the manager |
that partial solutions arrive frequently enough for the manager to |
partial solutions arrive frequently enough for the manager to accurately |
solutions arrive frequently enough for the manager to accurately estimate |
An architecture to support scalable online personalization in the web |
arrive frequently enough for the manager to accurately estimate the |
frequently enough for the manager to accurately estimate the power |
enough for the manager to accurately estimate the power of |
for the manager to accurately estimate the power of the |
the manager to accurately estimate the power of the miner |
live migration in shared nothing databases for elastic cloud platforms |
This is a simplification that is sufficient for our analysis |
therefore it is possible for two distant miners to generate |
it is possible for two distant miners to generate competing |
is possible for two distant miners to generate competing blocks |
both of which name the same block as their predecessor |
In Proceedings of the sixth annual ACM Symposium on Principles |
Proceedings of the sixth annual ACM Symposium on Principles of |
of the sixth annual ACM Symposium on Principles of distributed |
the sixth annual ACM Symposium on Principles of distributed computing |
The system has a mechanism to solve forks when they |
system has a mechanism to solve forks when they do |
has a mechanism to solve forks when they do occur |
Since the choice of the discarded block on bifurcation is |
the choice of the discarded block on bifurcation is random |
one may incorporate this event into the probability of finding |
may incorporate this event into the probability of finding a |
incorporate this event into the probability of finding a block |
and consider instead the probability of finding a block that |
consider instead the probability of finding a block that is |
instead the probability of finding a block that is not |
the probability of finding a block that is not discarded |
Pools often charge a small percentage of the revenue as |
often charge a small percentage of the revenue as fee |
We discuss in Section IX the implications of such fees |
discuss in Section IX the implications of such fees to |
in Section IX the implications of such fees to our |
Section IX the implications of such fees to our analysis |
A pool interface is typically comprised of a web interface |
pool interface is typically comprised of a web interface for |
interface is typically comprised of a web interface for registration |
is typically comprised of a web interface for registration and |
typically comprised of a web interface for registration and a |
comprised of a web interface for registration and a miner |
of a web interface for registration and a miner interface |
a web interface for registration and a miner interface for |
web interface for registration and a miner interface for the |
interface for registration and a miner interface for the mining |
for registration and a miner interface for the mining software |
supplies a Bitcoin address to receive its future shares of |
a Bitcoin address to receive its future shares of the |
Bitcoin address to receive its future shares of the revenue |
Then he feeds his credentials and the pool s address |
he feeds his credentials and the pool s address to |
feeds his credentials and the pool s address to its |
his credentials and the pool s address to its mining |
credentials and the pool s address to its mining rig |
The mining rig obtains its tasks from the pool and |
mining rig obtains its tasks from the pool and sends |
rig obtains its tasks from the pool and sends partial |
obtains its tasks from the pool and sends partial and |
its tasks from the pool and sends partial and full |
tasks from the pool and sends partial and full proof |
from the pool and sends partial and full proof of |
the pool and sends partial and full proof of work |
the pool manager credits the miner s account according to |
pool manager credits the miner s account according to its |
manager credits the miner s account according to its share |
credits the miner s account according to its share of |
the miner s account according to its share of the |
miner s account according to its share of the work |
and transfers these funds either on request or automatically to |
transfers these funds either on request or automatically to the |
these funds either on request or automatically to the aforementioned |
funds either on request or automatically to the aforementioned Bitcoin |
either on request or automatically to the aforementioned Bitcoin address |
Too Big Pools Despite their important role of enabling small |
pools can constitute a threat to the Bitcoin system if |
can constitute a threat to the Bitcoin system if their |
constitute a threat to the Bitcoin system if their size |
a threat to the Bitcoin system if their size is |
threat to the Bitcoin system if their size is too |
to the Bitcoin system if their size is too large |
Proceedings of the sixteenth ACM symposium on Operating systems principles |
warns that the system is unstable with even smaller pools |
Abstract The global network of datacenters is emerging as an |
The global network of datacenters is emerging as an important |
global network of datacenters is emerging as an important distributed |
in realistic scenarios of the Bitcoin system no pool controls |
network of datacenters is emerging as an important distributed systems |
realistic scenarios of the Bitcoin system no pool controls a |
of datacenters is emerging as an important distributed systems paradigm |
scenarios of the Bitcoin system no pool controls a majority |
Alternative architectures and protocols for providing strong consistency in dynamic |
datacenters is emerging as an important distributed systems paradigm commodity |
of the Bitcoin system no pool controls a majority of |
architectures and protocols for providing strong consistency in dynamic web |
is emerging as an important distributed systems paradigm commodity clusters |
the Bitcoin system no pool controls a majority of the |
and protocols for providing strong consistency in dynamic web applications |
emerging as an important distributed systems paradigm commodity clusters running |
Bitcoin system no pool controls a majority of the mining |
as an important distributed systems paradigm commodity clusters running high |
system no pool controls a majority of the mining power |
speed lambda networks across hundreds of milliseconds of network latency |
haul networks can cripple application performance a loss rate of |
IO reduced its relative mining power and publicly committed to |
Maelstrom is an edge appliance that masks packet loss transparently |
reduced its relative mining power and publicly committed to stay |
is an edge appliance that masks packet loss transparently and |
its relative mining power and publicly committed to stay away |
an edge appliance that masks packet loss transparently and quickly |
relative mining power and publicly committed to stay away from |
edge appliance that masks packet loss transparently and quickly from |
mining power and publicly committed to stay away from the |
appliance that masks packet loss transparently and quickly from inter |
speed encoding and using a new Forward Error Correction scheme |
encoding and using a new Forward Error Correction scheme to |
and using a new Forward Error Correction scheme to handle |
using a new Forward Error Correction scheme to handle bursty |
a new Forward Error Correction scheme to handle bursty loss |
is an attack performed by a pool member against the |
Introduction The emergence of commodity clusters and datacenters has enabled |
an attack performed by a pool member against the other |
The emergence of commodity clusters and datacenters has enabled a |
attack performed by a pool member against the other pool |
emergence of commodity clusters and datacenters has enabled a new |
performed by a pool member against the other pool members |
of commodity clusters and datacenters has enabled a new class |
commodity clusters and datacenters has enabled a new class of |
The attacking miner registers with the pool and apparently starts |
clusters and datacenters has enabled a new class of globally |
attacking miner registers with the pool and apparently starts mining |
and datacenters has enabled a new class of globally distributed |
miner registers with the pool and apparently starts mining honestly |
datacenters has enabled a new class of globally distributed highperformance |
registers with the pool and apparently starts mining honestly it |
has enabled a new class of globally distributed highperformance applications |
with the pool and apparently starts mining honestly it regularly |
enabled a new class of globally distributed highperformance applications that |
the pool and apparently starts mining honestly it regularly sends |
a new class of globally distributed highperformance applications that coordinate |
pool and apparently starts mining honestly it regularly sends the |
new class of globally distributed highperformance applications that coordinate over |
and apparently starts mining honestly it regularly sends the pool |
class of globally distributed highperformance applications that coordinate over vast |
apparently starts mining honestly it regularly sends the pool partial |
of globally distributed highperformance applications that coordinate over vast geographical |
starts mining honestly it regularly sends the pool partial proof |
globally distributed highperformance applications that coordinate over vast geographical distances |
mining honestly it regularly sends the pool partial proof of |
honestly it regularly sends the pool partial proof of work |
a financial firm s New York City datacenter may receive |
financial firm s New York City datacenter may receive real |
If it finds a full solution that constitutes a full |
it finds a full solution that constitutes a full proof |
finds a full solution that constitutes a full proof of |
a full solution that constitutes a full proof of work |
cache data in London for locality and mirror it to |
full solution that constitutes a full proof of work it |
data in London for locality and mirror it to Kansas |
solution that constitutes a full proof of work it discards |
in London for locality and mirror it to Kansas for |
that constitutes a full proof of work it discards the |
London for locality and mirror it to Kansas for disaster |
constitutes a full proof of work it discards the solution |
The attacker does not change the pool s effective mining |
attacker does not change the pool s effective mining power |
and does not affect directly the revenue of other pools |
Raw bandwidth is ubiquitous and cheaply available in the form |
bandwidth is ubiquitous and cheaply available in the form of |
is ubiquitous and cheaply available in the form of existing |
Recall that the proof of work is only valid for |
ubiquitous and cheaply available in the form of existing dark |
that the proof of work is only valid for a |
and cheaply available in the form of existing dark fiber |
the proof of work is only valid for a specific |
proof of work is only valid for a specific block |
as it is the nonce with which the block s |
it is the nonce with which the block s hash |
is the nonce with which the block s hash is |
the nonce with which the block s hash is smaller |
nonce with which the block s hash is smaller than |
with which the block s hash is smaller than its |
which the block s hash is smaller than its target |
note that the block is discarded and never introduced into |
that the block is discarded and never introduced into the |
the block is discarded and never introduced into the system |
block is discarded and never introduced into the system as |
is discarded and never introduced into the system as the |
discarded and never introduced into the system as the name |
On predictive modeling for optimizing transaction execution in parallel oltp |
and never introduced into the system as the name block |
predictive modeling for optimizing transaction execution in parallel oltp systems |
never introduced into the system as the name block withholding |
introduced into the system as the name block withholding implies |
this attack reduces the attacker s revenue compared to solo |
attack reduces the attacker s revenue compared to solo mining |
reduces the attacker s revenue compared to solo mining or |
the attacker s revenue compared to solo mining or honest |
attacker s revenue compared to solo mining or honest pool |
s revenue compared to solo mining or honest pool participation |
It suffers from the reduced revenue like the other pool |
suffers from the reduced revenue like the other pool participants |
and its revenue is less than its share of the |
its revenue is less than its share of the total |
revenue is less than its share of the total mining |
is less than its share of the total mining power |
less than its share of the total mining power in |
than its share of the total mining power in the |
its share of the total mining power in the system |
The classical block withholding attack can therefore only be used |
classical block withholding attack can therefore only be used for |
block withholding attack can therefore only be used for sabotage |
Even if a pool detects that it is under a |
if a pool detects that it is under a block |
a pool detects that it is under a block withholding |
pool detects that it is under a block withholding attack |
it might not be able to detect which of its |
might not be able to detect which of its registered |
not be able to detect which of its registered miners |
be able to detect which of its registered miners are |
able to detect which of its registered miners are the |
to detect which of its registered miners are the perpetrators |
A pool can estimate its expected mining power and its |
pool can estimate its expected mining power and its actual |
can estimate its expected mining power and its actual mining |
estimate its expected mining power and its actual mining power |
its expected mining power and its actual mining power by |
expected mining power and its actual mining power by the |
mining power and its actual mining power by the rates |
power and its actual mining power by the rates of |
and its actual mining power by the rates of partial |
its actual mining power by the rates of partial proofs |
actual mining power by the rates of partial proofs of |
mining power by the rates of partial proofs of work |
power by the rates of partial proofs of work and |
by the rates of partial proofs of work and full |
the rates of partial proofs of work and full proofs |
th international World Wide Web conference on Alternate track papers |
rates of partial proofs of work and full proofs of |
international World Wide Web conference on Alternate track papers and |
of partial proofs of work and full proofs of work |
World Wide Web conference on Alternate track papers and posters |
A difference above a set confidence interval indicates an attack |
comparing the estimated mining power of the attacker based on |
the estimated mining power of the attacker based on its |
estimated mining power of the attacker based on its partial |
mining power of the attacker based on its partial proof |
power of the attacker based on its partial proof of |
of the attacker based on its partial proof of work |
the attacker based on its partial proof of work with |
attacker based on its partial proof of work with the |
based on its partial proof of work with the fact |
on its partial proof of work with the fact it |
its partial proof of work with the fact it never |
partial proof of work with the fact it never supplies |
proof of work with the fact it never supplies a |
of work with the fact it never supplies a full |
work with the fact it never supplies a full proof |
with the fact it never supplies a full proof of |
the fact it never supplies a full proof of work |
but the pool will only expect to see a full |
the pool will only expect to see a full proof |
pool will only expect to see a full proof of |
will only expect to see a full proof of work |
only expect to see a full proof of work at |
expect to see a full proof of work at very |
to see a full proof of work at very low |
see a full proof of work at very low frequency |
it cannot obtain statistically significant results that would indicate an |
cannot obtain statistically significant results that would indicate an attack |
An attacker can use multiple small block withholding miners and |
attacker can use multiple small block withholding miners and replace |
Conservative flow control mechanisms designed to deal with the systematic |
can use multiple small block withholding miners and replace them |
flow control mechanisms designed to deal with the systematic congestion |
use multiple small block withholding miners and replace them frequently |
control mechanisms designed to deal with the systematic congestion of |
mechanisms designed to deal with the systematic congestion of the |
designed to deal with the systematic congestion of the commodity |
to deal with the systematic congestion of the commodity Internet |
deal with the systematic congestion of the commodity Internet react |
a miner whose expected full proof of work frequency is |
with the systematic congestion of the commodity Internet react too |
miner whose expected full proof of work frequency is yearly |
the systematic congestion of the commodity Internet react too sharply |
systematic congestion of the commodity Internet react too sharply to |
congestion of the commodity Internet react too sharply to ephemeral |
of the commodity Internet react too sharply to ephemeral loss |
the commodity Internet react too sharply to ephemeral loss on |
commodity Internet react too sharply to ephemeral loss on over |
provisioned links a single packet loss in ten thousand is |
In Proceedings of the eleventh international conference on World Wide |
links a single packet loss in ten thousand is enough |
Proceedings of the eleventh international conference on World Wide Web |
a single packet loss in ten thousand is enough to |
single packet loss in ten thousand is enough to reduce |
packet loss in ten thousand is enough to reduce TCP |
and one in a thousand drops it by an order |
one in a thousand drops it by an order of |
in a thousand drops it by an order of magnitude |
If the attacker replaces such a small miner every month |
time applications are impacted by the reliance of reliability mechanisms |
applications are impacted by the reliance of reliability mechanisms on |
are impacted by the reliance of reliability mechanisms on acknowledgments |
The pool must decide within this month whether the miner |
impacted by the reliance of reliability mechanisms on acknowledgments and |
pool must decide within this month whether the miner is |
by the reliance of reliability mechanisms on acknowledgments and retransmissions |
must decide within this month whether the miner is an |
decide within this month whether the miner is an attacker |
limiting the latency of packet recovery to at least the |
the latency of packet recovery to at least the Round |
latency of packet recovery to at least the Round Trip |
of packet recovery to at least the Round Trip Time |
Since an honest miner of this power is unlikely to |
an honest miner of this power is unlikely to find |
honest miner of this power is unlikely to find a |
miner of this power is unlikely to find a full |
of this power is unlikely to find a full proof |
this power is unlikely to find a full proof of |
power is unlikely to find a full proof of work |
is unlikely to find a full proof of work within |
unlikely to find a full proof of work within a |
to find a full proof of work within a month |
where standardization is the key to low and predictable maintenance |
standardization is the key to low and predictable maintenance costs |
nei This work was supported in part by grants from |
This work was supported in part by grants from AFOSR |
a pool that rejects miners based on this criterion would |
pool that rejects miners based on this criterion would reject |
that rejects miners based on this criterion would reject the |
rejects miners based on this criterion would reject the majority |
ther is eliminating loss events on a network that could |
miners based on this criterion would reject the majority of |
is eliminating loss events on a network that could NSF |
based on this criterion would reject the majority of its |
eliminating loss events on a network that could NSF and |
on this criterion would reject the majority of its honest |
loss events on a network that could NSF and Intel |
this criterion would reject the majority of its honest miners |
events on a network that could NSF and Intel Corporation |
The alternative of rejecting small miners in general or distributing |
alternative of rejecting small miners in general or distributing revenue |
of rejecting small miners in general or distributing revenue on |
rejecting small miners in general or distributing revenue on a |
small miners in general or distributing revenue on a yearly |
miners in general or distributing revenue on a yearly basis |
in general or distributing revenue on a yearly basis contradicts |
general or distributing revenue on a yearly basis contradicts the |
or distributing revenue on a yearly basis contradicts the goal |
distributing revenue on a yearly basis contradicts the goal of |
revenue on a yearly basis contradicts the goal of pooled |
on a yearly basis contradicts the goal of pooled mining |
M ODEL AND S TANDARD O PERATION We specify the |
ODEL AND S TANDARD O PERATION We specify the basic |
AND S TANDARD O PERATION We specify the basic model |
S TANDARD O PERATION We specify the basic model in |
TANDARD O PERATION We specify the basic model in which |
O PERATION We specify the basic model in which participants |
PERATION We specify the basic model in which participants operate |
We specify the basic model in which participants operate in |
specify the basic model in which participants operate in Section |
the basic model in which participants operate in Section III |
because recovery delays for lost packets translate into dramatic reductions |
recovery delays for lost packets translate into dramatic reductions in |
proceed to describe how honest miners operate in this environment |
delays for lost packets translate into dramatic reductions in application |
to describe how honest miners operate in this environment in |
describe how honest miners operate in this environment in Sections |
how honest miners operate in this environment in Sections III |
because applications and OS networking stacks in commodity datacenters cannot |
applications and OS networking stacks in commodity datacenters cannot be |
and OS networking stacks in commodity datacenters cannot be rewritten |
and how the classical block withholding attack is implemented with |
OS networking stacks in commodity datacenters cannot be rewritten from |
how the classical block withholding attack is implemented with our |
networking stacks in commodity datacenters cannot be rewritten from scratch |
the classical block withholding attack is implemented with our model |
classical block withholding attack is implemented with our model in |
block withholding attack is implemented with our model in Section |
withholding attack is implemented with our model in Section III |
Model The system is comprised of the Bitcoin network and |
The system is comprised of the Bitcoin network and nodes |
system is comprised of the Bitcoin network and nodes with |
is comprised of the Bitcoin network and nodes with unique |
comprised of the Bitcoin network and nodes with unique IDs |
packet recovery latency is independent of the RTT of the |
recovery latency is independent of the RTT of the link |
A node i generates tasks which are associated with its |
While FEC codes have been used for decades within link |
node i generates tasks which are associated with its ID |
Cache with unbounded cache size and unbounded dependency lists implements |
i generates tasks which are associated with its ID i |
with unbounded cache size and unbounded dependency lists implements cache |
A node can work on a task for the duration |
node can work on a task for the duration of |
can work on a task for the duration of a |
work on a task for the duration of a step |
the operations in an execution of update transactions update can |
The result of this work is a set of partial |
operations in an execution of update transactions update can be |
result of this work is a set of partial proofs |
in an execution of update transactions update can be serialized |
of this work is a set of partial proofs of |
an execution of update transactions update can be serialized as |
this work is a set of partial proofs of work |
execution of update transactions update can be serialized as some |
work is a set of partial proofs of work and |
of update transactions update can be serialized as some serial |
is a set of partial proofs of work and a |
update transactions update can be serialized as some serial execution |
a set of partial proofs of work and a set |
set of partial proofs of work and a set of |
of partial proofs of work and a set of full |
The next claim trivially follows from the definition of the |
partial proofs of work and a set of full proofs |
next claim trivially follows from the definition of the database |
proofs of work and a set of full proofs of |
and does not require specialized equipment in the network linking |
claim trivially follows from the definition of the database dependency |
of work and a set of full proofs of work |
does not require specialized equipment in the network linking the |
trivially follows from the definition of the database dependency list |
not require specialized equipment in the network linking the datacenters |
The number of proofs in each set has a Poisson |
follows from the definition of the database dependency list specification |
In Proceedings of the eighteenth ACM symposium on Operating systems |
number of proofs in each set has a Poisson distribution |
Proceedings of the eighteenth ACM symposium on Operating systems principles |
partial proofs with a large mean and full proofs with |
proofs with a large mean and full proofs with a |
If is a serialization of the update transactions of an |
with a large mean and full proofs with a small |
is a serialization of the update transactions of an execution |
a large mean and full proofs with a small mean |
a serialization of the update transactions of an execution update |
the version dependencies of every object match those stored in |
stable traffic rates and performs poorly if the data rate |
version dependencies of every object match those stored in its |
traffic rates and performs poorly if the data rate in |
To acquire this payoff an entity publishes a task task |
dependencies of every object match those stored in its dependency |
rates and performs poorly if the data rate in the |
acquire this payoff an entity publishes a task task and |
of every object match those stored in its dependency list |
and performs poorly if the data rate in the channel |
this payoff an entity publishes a task task and its |
performs poorly if the data rate in the channel is |
payoff an entity publishes a task task and its corresponding |
poorly if the data rate in the channel is low |
an entity publishes a task task and its corresponding proof |
if the data rate in the channel is low and |
entity publishes a task task and its corresponding proof of |
only transaction from a cache server in a serialization of |
the data rate in the channel is low and sporadic |
publishes a task task and its corresponding proof of work |
transaction from a cache server in a serialization of a |
Design and Evaluation of a ConitBased Continuous Consistency Model for |
a task task and its corresponding proof of work to |
from a cache server in a serialization of a subset |
and Evaluation of a ConitBased Continuous Consistency Model for Replicated |
task task and its corresponding proof of work to the |
a cache server in a serialization of a subset of |
Evaluation of a ConitBased Continuous Consistency Model for Replicated Services |
task and its corresponding proof of work to the network |
to form a serialization of both the update transaction and |
form a serialization of both the update transaction and the |
a serialization of both the update transaction and the read |
we present the Maelstrom Error Correction appliance a rack of |
The Bitcoin protocol normalizes revenue such that the average total |
present the Maelstrom Error Correction appliance a rack of proxies |
Bitcoin protocol normalizes revenue such that the average total revenue |
the Maelstrom Error Correction appliance a rack of proxies residing |
protocol normalizes revenue such that the average total revenue distributed |
Maelstrom Error Correction appliance a rack of proxies residing between |
normalizes revenue such that the average total revenue distributed in |
Error Correction appliance a rack of proxies residing between a |
revenue such that the average total revenue distributed in each |
Correction appliance a rack of proxies residing between a datacenter |
such that the average total revenue distributed in each step |
appliance a rack of proxies residing between a datacenter and |
that the average total revenue distributed in each step is |
a rack of proxies residing between a datacenter and its |
the average total revenue distributed in each step is a |
rack of proxies residing between a datacenter and its WAN |
average total revenue distributed in each step is a constant |
of proxies residing between a datacenter and its WAN link |
total revenue distributed in each step is a constant throughout |
revenue distributed in each step is a constant throughout the |
distributed in each step is a constant throughout the execution |
in each step is a constant throughout the execution of |
each step is a constant throughout the execution of the |
step is a constant throughout the execution of the system |
Maelstrom encodes FEC packets over traffic flowing through it and |
Any node can transact Bitcoins to another node by issuing |
encodes FEC packets over traffic flowing through it and routes |
node can transact Bitcoins to another node by issuing a |
FEC packets over traffic flowing through it and routes them |
can transact Bitcoins to another node by issuing a Bitcoin |
and each permutes a range of the transactions with respect |
packets over traffic flowing through it and routes them to |
transact Bitcoins to another node by issuing a Bitcoin transaction |
each permutes a range of the transactions with respect to |
over traffic flowing through it and routes them to a |
Nodes that generate tasks but outsource the work are called |
permutes a range of the transactions with respect to the |
traffic flowing through it and routes them to a corresponding |
that generate tasks but outsource the work are called pools |
a range of the transactions with respect to the previous |
flowing through it and routes them to a corresponding appliance |
range of the transactions with respect to the previous step |
through it and routes them to a corresponding appliance at |
it and routes them to a corresponding appliance at the |
In each step the right end of the range is |
and routes them to a corresponding appliance at the destination |
and send the partial and full proofs of work to |
each step the right end of the range is earlier |
routes them to a corresponding appliance at the destination datacenter |
send the partial and full proofs of work to the |
step the right end of the range is earlier than |
the partial and full proofs of work to the pool |
the right end of the range is earlier than in |
Maelstrom is completely transparent it does not require modification of |
right end of the range is earlier than in the |
is completely transparent it does not require modification of end |
end of the range is earlier than in the previous |
of the range is earlier than in the previous step |
host software and is agnostic to the network connecting the |
software and is agnostic to the network connecting the datacenter |
as one or more of the objects is closer to |
one or more of the objects is closer to the |
or more of the objects is closer to the value |
more of the objects is closer to the value read |
it eliminates the dependence of FEC recovery latency on the |
We assume that the number of miners is large enough |
of the objects is closer to the value read by |
eliminates the dependence of FEC recovery latency on the data |
assume that the number of miners is large enough such |
the objects is closer to the value read by T |
the dependence of FEC recovery latency on the data rate |
that the number of miners is large enough such that |
Eventually we therefore reach a permutation where at the chosen |
dependence of FEC recovery latency on the data rate in |
the number of miners is large enough such that mining |
we therefore reach a permutation where at the chosen time |
of FEC recovery latency on the data rate in any |
number of miners is large enough such that mining power |
therefore reach a permutation where at the chosen time all |
FEC recovery latency on the data rate in any single |
of miners is large enough such that mining power can |
reach a permutation where at the chosen time all read |
recovery latency on the data rate in any single node |
miners is large enough such that mining power can be |
a permutation where at the chosen time all read objects |
is large enough such that mining power can be split |
permutation where at the chosen time all read objects are |
large enough such that mining power can be split arbitrarily |
node channel by encoding over the aggregated traffic leaving the |
where at the chosen time all read objects are at |
enough such that mining power can be split arbitrarily without |
channel by encoding over the aggregated traffic leaving the datacenter |
at the chosen time all read objects are at their |
such that mining power can be split arbitrarily without resolution |
the chosen time all read objects are at their correct |
that mining power can be split arbitrarily without resolution constraints |
chosen time all read objects are at their correct versions |
We place T there to obtain the desired serialization of |
the total number of mining power in the system with |
place T there to obtain the desired serialization of the |
total number of mining power in the system with m |
T there to obtain the desired serialization of the update |
number of mining power in the system with m and |
there to obtain the desired serialization of the update transactions |
of mining power in the system with m and the |
to obtain the desired serialization of the update transactions and |
mining power in the system with m and the miners |
obtain the desired serialization of the update transactions and T |
power in the system with m and the miners participating |
and argue that the rate sensitivity of FEC codes and |
in the system with m and the miners participating in |
argue that the rate sensitivity of FEC codes and the |
the system with m and the miners participating in pool |
that the rate sensitivity of FEC codes and the opacity |
system with m and the miners participating in pool i |
the rate sensitivity of FEC codes and the opacity of |
and denote by update the projection of on the set |
rate sensitivity of FEC codes and the opacity of their |
denote by update the projection of on the set of |
sensitivity of FEC codes and the opacity of their implementations |
by update the projection of on the set of database |
of FEC codes and the opacity of their implementations present |
update the projection of on the set of database update |
We use a quasistatic analysis where miner participation in a |
FEC codes and the opacity of their implementations present major |
the projection of on the set of database update transactions |
use a quasistatic analysis where miner participation in a pool |
codes and the opacity of their implementations present major obstacles |
a quasistatic analysis where miner participation in a pool does |
and the opacity of their implementations present major obstacles to |
quasistatic analysis where miner participation in a pool does not |
the opacity of their implementations present major obstacles to their |
analysis where miner participation in a pool does not change |
opacity of their implementations present major obstacles to their usage |
where miner participation in a pool does not change over |
miner participation in a pool does not change over time |
a gateway appliance that transparently aggregates traffic and encodes over |
gateway appliance that transparently aggregates traffic and encodes over the |
Solo Mining A solo miner is a node that generates |
appliance that transparently aggregates traffic and encodes over the resulting |
Mining A solo miner is a node that generates its |
that transparently aggregates traffic and encodes over the resulting high |
A solo miner is a node that generates its own |
solo miner is a node that generates its own tasks |
works on it for the duration of the step and |
a new FEC scheme used by Maelstrom where for constant |
on it for the duration of the step and if |
new FEC scheme used by Maelstrom where for constant encoding |
it for the duration of the step and if it |
FEC scheme used by Maelstrom where for constant encoding overhead |
for the duration of the step and if it finds |
scheme used by Maelstrom where for constant encoding overhead the |
the duration of the step and if it finds a |
used by Maelstrom where for constant encoding overhead the latency |
and consider the first time when all the objects the |
duration of the step and if it finds a full |
by Maelstrom where for constant encoding overhead the latency of |
consider the first time when all the objects the transaction |
of the step and if it finds a full proof |
Maelstrom where for constant encoding overhead the latency of packet |
the first time when all the objects the transaction reads |
the step and if it finds a full proof of |
where for constant encoding overhead the latency of packet recovery |
first time when all the objects the transaction reads are |
step and if it finds a full proof of work |
for constant encoding overhead the latency of packet recovery degrades |
time when all the objects the transaction reads are at |
constant encoding overhead the latency of packet recovery degrades gracefully |
it publishes this proof of work to earn the payoff |
when all the objects the transaction reads are at a |
encoding overhead the latency of packet recovery degrades gracefully as |
all the objects the transaction reads are at a version |
overhead the latency of packet recovery degrades gracefully as losses |
the objects the transaction reads are at a version at |
Pools A pool is a node that serves as a |
the latency of packet recovery degrades gracefully as losses get |
objects the transaction reads are at a version at least |
A pool is a node that serves as a coordinator |
latency of packet recovery degrades gracefully as losses get burstier |
the transaction reads are at a version at least as |
pool is a node that serves as a coordinator and |
transaction reads are at a version at least as large |
is a node that serves as a coordinator and multiple |
reads are at a version at least as large as |
a node that serves as a coordinator and multiple miners |
are at a version at least as large as the |
node that serves as a coordinator and multiple miners can |
at a version at least as large as the versions |
that serves as a coordinator and multiple miners can register |
a version at least as large as the versions that |
serves as a coordinator and multiple miners can register to |
version at least as large as the versions that T |
as a coordinator and multiple miners can register to a |
at least as large as the versions that T reads |
a coordinator and multiple miners can register to a pool |
coordinator and multiple miners can register to a pool and |
and multiple miners can register to a pool and work |
At this time at least one object read by T |
multiple miners can register to a pool and work for |
and recovers packets with latency independent of the RTT of |
miners can register to a pool and work for it |
recovers packets with latency independent of the RTT of the |
packets with latency independent of the RTT of the link |
In every step it generates a task for each registered |
with latency independent of the RTT of the link and |
every step it generates a task for each registered miner |
Assume without loss of generality that the last version written |
latency independent of the RTT of the link and the |
step it generates a task for each registered miner and |
without loss of generality that the last version written is |
independent of the RTT of the link and the rate |
it generates a task for each registered miner and sends |
loss of generality that the last version written is vn |
of the RTT of the link and the rate in |
generates a task for each registered miner and sends it |
of generality that the last version written is vn of |
the RTT of the link and the rate in any |
a task for each registered miner and sends it over |
generality that the last version written is vn of object |
RTT of the link and the rate in any single |
task for each registered miner and sends it over the |
that the last version written is vn of object on |
of the link and the rate in any single channel |
for each registered miner and sends it over the network |
the last version written is vn of object on at |
last version written is vn of object on at step |
version written is vn of object on at step t |
Model Our focus is on pairs of geographically distant datacenters |
Each miner receives its task and works on it for |
written is vn of object on at step t of |
Our focus is on pairs of geographically distant datacenters that |
miner receives its task and works on it for the |
Denote by t the latest time at which a wrong |
focus is on pairs of geographically distant datacenters that coordinate |
receives its task and works on it for the duration |
by t the latest time at which a wrong version |
is on pairs of geographically distant datacenters that coordinate with |
its task and works on it for the duration of |
on pairs of geographically distant datacenters that coordinate with each |
task and works on it for the duration of the |
pairs of geographically distant datacenters that coordinate with each other |
and works on it for the duration of the step |
of geographically distant datacenters that coordinate with each other in |
geographically distant datacenters that coordinate with each other in real |
the miner sends the pool the full and the partial |
miner sends the pool the full and the partial proofs |
sends the pool the full and the partial proofs of |
This has long been a critical distributed computing paradigm in |
the pool the full and the partial proofs of work |
has long been a critical distributed computing paradigm in application |
pool the full and the partial proofs of work it |
long been a critical distributed computing paradigm in application domains |
the full and the partial proofs of work it has |
been a critical distributed computing paradigm in application domains such |
full and the partial proofs of work it has found |
a critical distributed computing paradigm in application domains such as |
critical distributed computing paradigm in application domains such as finance |
The pool receives the proofs of work of all its |
distributed computing paradigm in application domains such as finance and |
pool receives the proofs of work of all its miners |
computing paradigm in application domains such as finance and aerospace |
registers the partial proofs of work and publishes the full |
the partial proofs of work and publishes the full proofs |
similar requirements are arising across the board as globalized enterprises |
requirements are arising across the board as globalized enterprises rely |
are arising across the board as globalized enterprises rely on |
arising across the board as globalized enterprises rely on networks |
across the board as globalized enterprises rely on networks for |
Each miner receives revenue proportional to its success in the |
the board as globalized enterprises rely on networks for high |
miner receives revenue proportional to its success in the current |
receives revenue proportional to its success in the current step |
The following Lemma states that there is no dependency among |
following Lemma states that there is no dependency among objects |
namely the ratio of its partial proofs of work out |
Lemma states that there is no dependency among objects in |
the ratio of its partial proofs of work out of |
cluster communication is one where any node in one cluster |
states that there is no dependency among objects in sets |
ratio of its partial proofs of work out of all |
communication is one where any node in one cluster can |
of its partial proofs of work out of all partial |
is one where any node in one cluster can communicate |
its partial proofs of work out of all partial proofs |
one where any node in one cluster can communicate with |
partial proofs of work out of all partial proofs of |
where any node in one cluster can communicate with any |
proofs of work out of all partial proofs of work |
any node in one cluster can communicate with any node |
of work out of all partial proofs of work the |
node in one cluster can communicate with any node in |
work out of all partial proofs of work the pool |
in one cluster can communicate with any node in the |
out of all partial proofs of work the pool received |
one cluster can communicate with any node in the other |
cluster can communicate with any node in the other cluster |
We assume that pools do not collect fees of the |
then version vn of object on depends on version vn |
We make no assumptions about the type of traffic flowing |
assume that pools do not collect fees of the revenue |
make no assumptions about the type of traffic flowing through |
no assumptions about the type of traffic flowing through the |
Pool fees and their implications on our analysis are discussed |
assumptions about the type of traffic flowing through the link |
fees and their implications on our analysis are discussed in |
and their implications on our analysis are discussed in Section |
their implications on our analysis are discussed in Section IX |
Block Withholding Miner A miner registered at a pool can |
Withholding Miner A miner registered at a pool can perform |
Miner A miner registered at a pool can perform the |
A miner registered at a pool can perform the classical |
miner registered at a pool can perform the classical block |
registered at a pool can perform the classical block withholding |
at a pool can perform the classical block withholding attack |
Packet loss typically occurs at two points in an end |
An attacker miner operates as if it worked for the |
attacker miner operates as if it worked for the pool |
only at the end of each round it sends only |
at the end of each round it sends only its |
the end of each round it sends only its partial |
end of each round it sends only its partial proofs |
of each round it sends only its partial proofs of |
each round it sends only its partial proofs of work |
Loss in the lambda link can occur for many reasons |
and omits full proofs of work if it had found |
omits full proofs of work if it had found any |
but cannot distinguish between miners running honestly and block withholding |
cannot distinguish between miners running honestly and block withholding miners |
low receiver power and burst switching contention are some reasons |
The implications are that a miner that engages in block |
implications are that a miner that engages in block withholding |
are that a miner that engages in block withholding does |
that a miner that engages in block withholding does not |
a miner that engages in block withholding does not contribute |
miner that engages in block withholding does not contribute to |
that engages in block withholding does not contribute to the |
engages in block withholding does not contribute to the pool |
in block withholding does not contribute to the pool s |
block withholding does not contribute to the pool s overall |
withholding does not contribute to the pool s overall mining |
does not contribute to the pool s overall mining power |
but still shares the pool s revenue according to its |
still shares the pool s revenue according to its sent |
shares the pool s revenue according to its sent partial |
the pool s revenue according to its sent partial proofs |
pool s revenue according to its sent partial proofs of |
s revenue according to its sent partial proofs of work |
To reason about a pool s efficiency we define its |
reason about a pool s efficiency we define its per |
and denote by update the projection of on the set |
Loss can also occur at receiving endhosts within the destination |
denote by update the projection of on the set of |
can also occur at receiving endhosts within the destination datacenter |
by update the projection of on the set of database |
update the projection of on the set of database update |
the projection of on the set of database update transactions |
these are usually cheap commodity machines prone to temporary overloads |
The revenue density of a pool is the ratio between |
are usually cheap commodity machines prone to temporary overloads that |
revenue density of a pool is the ratio between the |
usually cheap commodity machines prone to temporary overloads that cause |
density of a pool is the ratio between the average |
cheap commodity machines prone to temporary overloads that cause packets |
of a pool is the ratio between the average revenue |
commodity machines prone to temporary overloads that cause packets to |
a pool is the ratio between the average revenue a |
machines prone to temporary overloads that cause packets to be |
pool is the ratio between the average revenue a pool |
prone to temporary overloads that cause packets to be dropped |
is the ratio between the average revenue a pool member |
to temporary overloads that cause packets to be dropped by |
Tm a set of readonly transactions performed through a single |
the ratio between the average revenue a pool member earns |
temporary overloads that cause packets to be dropped by the |
a set of readonly transactions performed through a single T |
ratio between the average revenue a pool member earns and |
overloads that cause packets to be dropped by the kernel |
between the average revenue a pool member earns and the |
that cause packets to be dropped by the kernel in |
the average revenue a pool member earns and the average |
If the read sets of two transactions include the same |
cause packets to be dropped by the kernel in bursts |
average revenue a pool member earns and the average revenue |
the read sets of two transactions include the same object |
revenue a pool member earns and the average revenue it |
read sets of two transactions include the same object o |
a pool member earns and the average revenue it would |
pool member earns and the average revenue it would have |
we say the one that read a larger version of |
member earns and the average revenue it would have earned |
say the one that read a larger version of o |
earns and the average revenue it would have earned as |
the one that read a larger version of o depends |
and the average revenue it would have earned as a |
one that read a larger version of o depends on |
the average revenue it would have earned as a solo |
that read a larger version of o depends on the |
average revenue it would have earned as a solo miner |
read a larger version of o depends on the other |
and that of a miner working with an unattacked pool |
that of a miner working with an unattacked pool are |
of a miner working with an unattacked pool are one |
an optical network interconnecting major supercomputing sites in the US |
so it is easy to see that there are no |
it is easy to see that there are no cycles |
TeraGrid has a monitoring framework within which ten sites periodically |
is easy to see that there are no cycles such |
has a monitoring framework within which ten sites periodically send |
easy to see that there are no cycles such that |
Continuous Analysis Because our analysis will be of the average |
a monitoring framework within which ten sites periodically send each |
to see that there are no cycles such that two |
Analysis Because our analysis will be of the average revenue |
monitoring framework within which ten sites periodically send each other |
see that there are no cycles such that two transactions |
that there are no cycles such that two transactions depend |
Gbps streams of UDP packets and measure the resulting loss |
there are no cycles such that two transactions depend on |
streams of UDP packets and measure the resulting loss rate |
are no cycles such that two transactions depend on one |
no cycles such that two transactions depend on one another |
Work on a task therefore results in a deterministic fraction |
on a task therefore results in a deterministic fraction of |
The dependency graph therefore describes a partial order of the |
a task therefore results in a deterministic fraction of proof |
dependency graph therefore describes a partial order of the read |
task therefore results in a deterministic fraction of proof of |
Each site measures the loss rate to every other site |
therefore results in a deterministic fraction of proof of work |
site measures the loss rate to every other site once |
and we choose an arbitrary total ordering that respects this |
measures the loss rate to every other site once an |
we choose an arbitrary total ordering that respects this partial |
the loss rate to every other site once an hour |
choose an arbitrary total ordering that respects this partial order |
The Pool Block Withholding Attack Just as a miner can |
Pool Block Withholding Attack Just as a miner can perform |
Block Withholding Attack Just as a miner can perform block |
Withholding Attack Just as a miner can perform block withholding |
Attack Just as a miner can perform block withholding on |
Just as a miner can perform block withholding on a |
as a miner can perform block withholding on a pool |
a miner can perform block withholding on a pool j |
a pool i can use some of its mining power |
pool i can use some of its mining power to |
i can use some of its mining power to infiltrate |
can use some of its mining power to infiltrate a |
use some of its mining power to infiltrate a pool |
some of its mining power to infiltrate a pool j |
of its mining power to infiltrate a pool j and |
its mining power to infiltrate a pool j and perform |
of and permute it according to the route above to |
mining power to infiltrate a pool j and perform a |
and permute it according to the route above to place |
power to infiltrate a pool j and perform a block |
permute it according to the route above to place T |
to infiltrate a pool j and perform a block withholding |
infiltrate a pool j and perform a block withholding attack |
a pool j and perform a block withholding attack on |
pool j and perform a block withholding attack on j |
Denote the amount of such infiltrating mining power at step |
the amount of such infiltrating mining power at step t |
amount of such infiltrating mining power at step t by |
of such infiltrating mining power at step t by xi |
pool i aggregates its revenue from mining in the current |
i aggregates its revenue from mining in the current round |
aggregates its revenue from mining in the current round and |
its revenue from mining in the current round and from |
revenue from mining in the current round and from its |
from mining in the current round and from its infiltration |
mining in the current round and from its infiltration in |
in the current round and from its infiltration in the |
the current round and from its infiltration in the previous |
current round and from its infiltration in the previous round |
It distributes the revenue evenly among all its loyal miners |
distributes the revenue evenly among all its loyal miners according |
the revenue evenly among all its loyal miners according to |
revenue evenly among all its loyal miners according to their |
evenly among all its loyal miners according to their partial |
among all its loyal miners according to their partial proofs |
all its loyal miners according to their partial proofs of |
its loyal miners according to their partial proofs of work |
The pool s miners are oblivious to their role and |
pool s miners are oblivious to their role and they |
s miners are oblivious to their role and they operate |
miners are oblivious to their role and they operate as |
are oblivious to their role and they operate as regular |
oblivious to their role and they operate as regular honest |
to their role and they operate as regular honest miners |
Revenue Convergence Note that pool j sends its revenue to |
Convergence Note that pool j sends its revenue to infiltrators |
Note that pool j sends its revenue to infiltrators from |
that pool j sends its revenue to infiltrators from pool |
pool j sends its revenue to infiltrators from pool i |
j sends its revenue to infiltrators from pool i at |
sends its revenue to infiltrators from pool i at the |
These numbers reflect the loss rate experienced for UDP traffic |
its revenue to infiltrators from pool i at the end |
numbers reflect the loss rate experienced for UDP traffic on |
revenue to infiltrators from pool i at the end of |
reflect the loss rate experienced for UDP traffic on an |
to infiltrators from pool i at the end of the |
the loss rate experienced for UDP traffic on an end |
infiltrators from pool i at the end of the step |
and this revenue is calculated in pool i at the |
this revenue is calculated in pool i at the beginning |
revenue is calculated in pool i at the beginning of |
is calculated in pool i at the beginning of the |
calculated in pool i at the beginning of the subsequent |
we do not know if packets were dropped within the |
in pool i at the beginning of the subsequent step |
do not know if packets were dropped within the optical |
not know if packets were dropped within the optical network |
know if packets were dropped within the optical network or |
if packets were dropped within the optical network or at |
packets were dropped within the optical network or at intermediate |
This is a serialization of the update transactions in and |
were dropped within the optical network or at intermediate devices |
is a serialization of the update transactions in and all |
dropped within the optical network or at intermediate devices within |
a serialization of the update transactions in and all read |
within the optical network or at intermediate devices within either |
the optical network or at intermediate devices within either datacenter |
We have therefore shown that in any execution of T |
though it s unlikely that they were dropped at the |
it s unlikely that they were dropped at the end |
Cache the update transactions can be serialized with readonly transactions |
the update transactions can be serialized with readonly transactions that |
update transactions can be serialized with readonly transactions that accessed |
since the revenue from infiltration takes one step to take |
transactions can be serialized with readonly transactions that accessed a |
the revenue from infiltration takes one step to take each |
can be serialized with readonly transactions that accessed a single |
revenue from infiltration takes one step to take each hop |
be serialized with readonly transactions that accessed a single cache |
loss occurred on paths where levels of optical link utilization |
since it is only infiltrated and loses some of its |
it is only infiltrated and loses some of its revenue |
is only infiltrated and loses some of its revenue for |
only infiltrated and loses some of its revenue for pool |
comprised of its own mining and its revenue from the |
of its own mining and its revenue from the infiltration |
its own mining and its revenue from the infiltration of |
own mining and its revenue from the infiltration of pool |
with some revenue lost due to its infiltration by pool |
the pool revenues converge to a limit as time progresses |
Denote the revenue density of pool i at the end |
the revenue density of pool i at the end of |
revenue density of pool i at the end of step |
density of pool i at the end of step t |
of pool i at the end of step t by |
pool i at the end of step t by ri |
We expect privately managed lambdas to exhibit higher loss rates |
expect privately managed lambdas to exhibit higher loss rates due |
privately managed lambdas to exhibit higher loss rates due to |
managed lambdas to exhibit higher loss rates due to the |
lambdas to exhibit higher loss rates due to the inherent |
to exhibit higher loss rates due to the inherent trade |
as well as the difficulty of performing routine maintenance on |
well as the difficulty of performing routine maintenance on longdistance |
as the difficulty of performing routine maintenance on longdistance links |
IP is the default reliable communication option for contemporary networked |
is the default reliable communication option for contemporary networked applications |
most applications requiring reliable communication over any form of network |
applications requiring reliable communication over any form of network use |
requiring reliable communication over any form of network use TCP |
IP uses positive acknowledgments and retransmissions to ensure reliability the |
uses positive acknowledgments and retransmissions to ensure reliability the sender |
positive acknowledgments and retransmissions to ensure reliability the sender buffers |
acknowledgments and retransmissions to ensure reliability the sender buffers packets |
and retransmissions to ensure reliability the sender buffers packets until |
retransmissions to ensure reliability the sender buffers packets until their |
to ensure reliability the sender buffers packets until their receipt |
ensure reliability the sender buffers packets until their receipt is |
reliability the sender buffers packets until their receipt is acknowledged |
the sender buffers packets until their receipt is acknowledged by |
sender buffers packets until their receipt is acknowledged by the |
buffers packets until their receipt is acknowledged by the receiver |
and resends if an acknowledgment is not received within some |
resends if an acknowledgment is not received within some time |
if an acknowledgment is not received within some time period |
a lost packet is received in the form of a |
lost packet is received in the form of a retransmission |
packet is received in the form of a retransmission that |
is received in the form of a retransmission that arrives |
received in the form of a retransmission that arrives no |
in the form of a retransmission that arrives no earlier |
the form of a retransmission that arrives no earlier than |
The Pool Game In the pool game pools try to |
Pool Game In the pool game pools try to optimize |
Game In the pool game pools try to optimize their |
In the pool game pools try to optimize their infiltration |
the pool game pools try to optimize their infiltration rates |
pool game pools try to optimize their infiltration rates of |
The sender has to buffer each packet until it s |
game pools try to optimize their infiltration rates of other |
sender has to buffer each packet until it s acknowledged |
pools try to optimize their infiltration rates of other pools |
try to optimize their infiltration rates of other pools to |
to optimize their infiltration rates of other pools to maximize |
optimize their infiltration rates of other pools to maximize their |
their infiltration rates of other pools to maximize their revenue |
and it has to perform additional work to retransmit the |
it has to perform additional work to retransmit the packet |
The overall number of miners and the number of miners |
has to perform additional work to retransmit the packet if |
overall number of miners and the number of miners loyal |
to perform additional work to retransmit the packet if it |
number of miners and the number of miners loyal to |
perform additional work to retransmit the packet if it does |
of miners and the number of miners loyal to each |
additional work to retransmit the packet if it does not |
miners and the number of miners loyal to each pool |
work to retransmit the packet if it does not receive |
and the number of miners loyal to each pool remain |
to retransmit the packet if it does not receive the |
the number of miners loyal to each pool remain constant |
retransmit the packet if it does not receive the acknowledgment |
number of miners loyal to each pool remain constant throughout |
of miners loyal to each pool remain constant throughout the |
miners loyal to each pool remain constant throughout the game |
any packets that arrive with higher sequence numbers than that |
packets that arrive with higher sequence numbers than that of |
that arrive with higher sequence numbers than that of a |
Let s be a constant integer large enough that revenue |
arrive with higher sequence numbers than that of a lost |
s be a constant integer large enough that revenue can |
with higher sequence numbers than that of a lost packet |
be a constant integer large enough that revenue can be |
higher sequence numbers than that of a lost packet must |
a constant integer large enough that revenue can be approximated |
sequence numbers than that of a lost packet must be |
constant integer large enough that revenue can be approximated as |
numbers than that of a lost packet must be queued |
integer large enough that revenue can be approximated as its |
than that of a lost packet must be queued while |
large enough that revenue can be approximated as its convergence |
that of a lost packet must be queued while the |
enough that revenue can be approximated as its convergence limit |
of a lost packet must be queued while the receiver |
a lost packet must be queued while the receiver waits |
In each round the system takes s steps and then |
lost packet must be queued while the receiver waits for |
each round the system takes s steps and then a |
packet must be queued while the receiver waits for the |
round the system takes s steps and then a single |
must be queued while the receiver waits for the lost |
the system takes s steps and then a single pool |
be queued while the receiver waits for the lost packet |
queued while the receiver waits for the lost packet to |
while the receiver waits for the lost packet to arrive |
throughput financial banking application running in a datacenter in New |
financial banking application running in a datacenter in New York |
banking application running in a datacenter in New York City |
The pool taking a step knows the rate of infiltrators |
pool taking a step knows the rate of infiltrators attacking |
taking a step knows the rate of infiltrators attacking it |
and the revenue rates of each of the other pools |
This knowledge is required to optimize a pool s revenue |
We explain in Section VIII how a pool can technically |
explain in Section VIII how a pool can technically obtain |
in Section VIII how a pool can technically obtain this |
Section VIII how a pool can technically obtain this knowledge |
General Analysis Recall that mi is the number of miners |
Analysis Recall that mi is the number of miners loyal |
Recall that mi is the number of miners loyal to |
that mi is the number of miners loyal to pool |
mi is the number of miners loyal to pool i |
is the number of miners used by pool i to |
the number of miners used by pool i to infiltrate |
number of miners used by pool i to infiltrate pool |
of miners used by pool i to infiltrate pool j |
miners used by pool i to infiltrate pool j at |
used by pool i to infiltrate pool j at step |
by pool i to infiltrate pool j at step t |
The mining rate of pool i is therefore the number |
mining rate of pool i is therefore the number of |
rate of pool i is therefore the number of its |
of pool i is therefore the number of its loyal |
the first containing data packets numbered A C E G |
pool i is therefore the number of its loyal miners |
first containing data packets numbered A C E G X |
i is therefore the number of its loyal miners minus |
containing data packets numbered A C E G X X |
is therefore the number of its loyal miners minus the |
therefore the number of its loyal miners minus the miners |
the number of its loyal miners minus the miners it |
number of its loyal miners minus the miners it uses |
of its loyal miners minus the miners it uses for |
its loyal miners minus the miners it uses for infiltration |
This effective mining rate is divided by the total mining |
effective mining rate is divided by the total mining rate |
mining rate is divided by the total mining rate in |
rate is divided by the total mining rate in the |
is divided by the total mining rate in the system |
namely the number of all miners that do not engage |
the number of all miners that do not engage in |
number of all miners that do not engage in block |
of all miners that do not engage in block withholding |
Denote the direct mining rate of pool i at step |
the direct mining rate of pool i at step t |
direct mining rate of pool i at step t by |
the second with data packets numB D F H X |
mining rate of pool i at step t by Pp |
second with data packets numB D F H X X |
rate of pool i at step t by Pp mi |
with data packets numB D F H X X bered |
of pool i at step t by Pp mi j |
k The revenue of Pool i in step t taken |
The revenue of Pool i in step t taken through |
revenue of Pool i in step t taken through infiltration |
of Pool i in step t taken through infiltration from |
Pool i in step t taken through infiltration from pool |
i in step t taken through infiltration from pool j |
in step t taken through infiltration from pool j s |
step t taken through infiltration from pool j s revenue |
t taken through infiltration from pool j s revenue in |
taken through infiltration from pool j s revenue in step |
through infiltration from pool j s revenue in step t |
i ij The revenue density of pool i at the |
ij The revenue density of pool i at the end |
The revenue density of pool i at the end of |
revenue density of pool i at the end of step |
density of pool i at the end of step t |
Interleaving adds burst tolerance to FEC but exacerbates its sensitivFigure |
of pool i at the end of step t is |
pool i at the end of step t is its |
i at the end of step t is its revenue |
at the end of step t is its revenue from |
the end of step t is its revenue from direct |
end of step t is its revenue from direct mining |
of step t is its revenue from direct mining together |
separate encoding for ity to sending rate with an interleave |
step t is its revenue from direct mining together with |
encoding for ity to sending rate with an interleave index |
t is its revenue from direct mining together with its |
for ity to sending rate with an interleave index of |
is its revenue from direct mining together with its revenue |
ity to sending rate with an interleave index of i |
its revenue from direct mining together with its revenue from |
to sending rate with an interleave index of i and |
revenue from direct mining together with its revenue from infiltrated |
sending rate with an interleave index of i and an |
from direct mining together with its revenue from infiltrated pools |
rate with an interleave index of i and an encoding |
with an interleave index of i and an encoding rate |
an interleave index of i and an encoding rate of |
divided by the number of its loyal miners together with |
by the number of its loyal miners together with block |
the sender would have to wait for odd and even |
sender would have to wait for odd and even packets |
would have to wait for odd and even packets i |
receipt of its retransmission have to be buffered at the |
of its retransmission have to be buffered at the reThese |
its retransmission have to be buffered at the reThese two |
retransmission have to be buffered at the reThese two obstacles |
have to be buffered at the reThese two obstacles to |
to be buffered at the reThese two obstacles to using |
be buffered at the reThese two obstacles to using FEC |
buffered at the reThese two obstacles to using FEC in |
at the reThese two obstacles to using FEC in time |
tings rate sensitivity and burst susceptibility are inNotice that for |
rate sensitivity and burst susceptibility are inNotice that for this |
sensitivity and burst susceptibility are inNotice that for this commonplace |
and burst susceptibility are inNotice that for this commonplace scenario |
an interleave of i and a single packet stops all |
interleave of i and a single packet stops all traffic |
And the revenue vector at step t is Hereinafter we |
of i and a single packet stops all traffic in |
the revenue vector at step t is Hereinafter we move |
i and a single packet stops all traffic in the |
revenue vector at step t is Hereinafter we move to |
and a single packet stops all traffic in the channel |
vector at step t is Hereinafter we move to a |
a single packet stops all traffic in the channel to |
at step t is Hereinafter we move to a static |
single packet stops all traffic in the channel to the |
step t is Hereinafter we move to a static state |
packet stops all traffic in the channel to the apa |
t is Hereinafter we move to a static state analysis |
stops all traffic in the channel to the apa rate |
is Hereinafter we move to a static state analysis and |
all traffic in the channel to the apa rate of |
Hereinafter we move to a static state analysis and omit |
we move to a static state analysis and omit the |
move to a static state analysis and omit the t |
to a static state analysis and omit the t argument |
a static state analysis and omit the t argument in |
static state analysis and omit the t argument in the |
provides tolerance to a burst of up to c i |
state analysis and omit the t argument in the expressions |
tolerance to a burst of up to c i plication |
to a burst of up to c i plication for |
a burst of up to c i plication for a |
burst of up to c i plication for a seventh |
of up to c i plication for a seventh of |
up to c i plication for a seventh of a |
to c i plication for a seventh of a second |
the burst tolerance of blocks can have devastating effect on |
burst tolerance of blocks can have devastating effect on a |
tolerance of blocks can have devastating effect on a high |
throughput an FEC code can be changed by modulating either |
an FEC code can be changed by modulating either the |
FEC code can be changed by modulating either the c |
Since the row sums of the infiltration matrix are smaller |
code can be changed by modulating either the c system |
the row sums of the infiltration matrix are smaller than |
can be changed by modulating either the c system where |
row sums of the infiltration matrix are smaller than one |
be changed by modulating either the c system where every |
changed by modulating either the c system where every spare |
by modulating either the c system where every spare cycle |
modulating either the c system where every spare cycle counts |
and there are transient effects that are not covered by |
there are transient effects that are not covered by this |
are transient effects that are not covered by this stable |
a lost packet ance at the cost of network and |
lost packet ance at the cost of network and encoding |
packet ance at the cost of network and encoding overhead |
Miners Miners Miners the revenue its infiltrators obtained from pool |
potencan potentially trigger a butterfly effect of missed deadtially worsening |
potentially trigger a butterfly effect of missed deadtially worsening the |
trigger a butterfly effect of missed deadtially worsening the packet |
a butterfly effect of missed deadtially worsening the packet loss |
butterfly effect of missed deadtially worsening the packet loss experienced |
effect of missed deadtially worsening the packet loss experienced and |
of missed deadtially worsening the packet loss experienced and reducing |
missed deadtially worsening the packet loss experienced and reducing lines |
deadtially worsening the packet loss experienced and reducing lines along |
worsening the packet loss experienced and reducing lines along a |
the packet loss experienced and reducing lines along a distributed |
packet loss experienced and reducing lines along a distributed workflow |
increasing i trades off recovery periods market crashes at stock |
i trades off recovery periods market crashes at stock exchanges |
Christmas latency for better burst tolerance without adding overhead sales |
latency for better burst tolerance without adding overhead sales at |
for better burst tolerance without adding overhead sales at online |
better burst tolerance without adding overhead sales at online stores |
hosts can exhibit wait for more data packets to be |
can exhibit wait for more data packets to be transmitted |
exhibit wait for more data packets to be transmitted before |
wait for more data packets to be transmitted before it |
for more data packets to be transmitted before it can |
more data packets to be transmitted before it can continuous |
data packets to be transmitted before it can continuous packet |
packets to be transmitted before it can continuous packet loss |
and will choose the value that maximizes the revenue density |
with each lost packet driving the send error correction packets |
system further and further out of sync with respect to |
further and further out of sync with respect to its |
and further out of sync with respect to its Importantly |
is maximized at a single point in the feasible range |
with a rate and an interleave to tolerate a certain |
a rate and an interleave to tolerate a certain burst |
rate and an interleave to tolerate a certain burst Sensitive |
and an interleave to tolerate a certain burst Sensitive Flow |
an interleave to tolerate a certain burst Sensitive Flow Control |
to between ephemeral loss modes due to transient contolerate a |
between ephemeral loss modes due to transient contolerate a burst |
ephemeral loss modes due to transient contolerate a burst of |
loss modes due to transient contolerate a burst of length |
and the values of the corresponding revenues of the pools |
the values of the corresponding revenues of the pools with |
values of the corresponding revenues of the pools with r |
or dirty fiber and persistent in bursts of size less |
dirty fiber and persistent in bursts of size less than |
fiber and persistent in bursts of size less than or |
and persistent in bursts of size less than or equal |
persistent in bursts of size less than or equal to |
in bursts of size less than or equal to B |
bursts of size less than or equal to B are |
of size less than or equal to B are recovered |
size less than or equal to B are recovered with |
less than or equal to B are recovered with congestion |
The loss of one packet out of ten thousand is |
loss of one packet out of ten thousand is sufficient |
of one packet out of ten thousand is sufficient to |
one packet out of ten thousand is sufficient to reduce |
packet out of ten thousand is sufficient to reduce TCP |
IP throughput to a third of its the same latency |
throughput to a third of its the same latency and |
to a third of its the same latency and this |
a third of its the same latency and this latency |
third of its the same latency and this latency depends |
of its the same latency and this latency depends on |
its the same latency and this latency depends on the |
the same latency and this latency depends on the i |
same latency and this latency depends on the i palossless |
latency and this latency depends on the i palossless maximum |
we d like to parameterize the encoding to tolerate a |
d like to parameterize the encoding to tolerate a maximum |
like to parameterize the encoding to tolerate a maximum burst |
to parameterize the encoding to tolerate a maximum burst length |
parameterize the encoding to tolerate a maximum burst length and |
the encoding to tolerate a maximum burst length and then |
encoding to tolerate a maximum burst length and then have |
to tolerate a maximum burst length and then have recovthroughput |
tolerate a maximum burst length and then have recovthroughput collapses |
a maximum burst length and then have recovthroughput collapses to |
maximum burst length and then have recovthroughput collapses to a |
O NE ATTACKER We begin our analysis with a simplified |
burst length and then have recovthroughput collapses to a thirtieth |
NE ATTACKER We begin our analysis with a simplified game |
length and then have recovthroughput collapses to a thirtieth of |
ATTACKER We begin our analysis with a simplified game of |
and then have recovthroughput collapses to a thirtieth of the |
We begin our analysis with a simplified game of two |
then have recovthroughput collapses to a thirtieth of the maximum |
begin our analysis with a simplified game of two pools |
ery latency depend on the actual burstiness of the loss |
an FEC scheme is required where latency of FEC encoders |
FEC scheme is required where latency of FEC encoders are |
scheme is required where latency of FEC encoders are typically |
is required where latency of FEC encoders are typically parameterized |
required where latency of FEC encoders are typically parameterized with |
where latency of FEC encoders are typically parameterized with an |
or with closed pools that do not attack and cannot |
with closed pools that do not attack and cannot be |
closed pools that do not attack and cannot be attacked |
even tuple for each outgoing sequence of r data packets |
redundancy information cannot be generated and sent until all r |
information cannot be generated and sent until all r data |
cannot be generated and sent until all r data packets |
be generated and sent until all r data packets are |
generated and sent until all r data packets are available |
and sent until all r data packets are available for |
sent until all r data packets are available for sending |
the latency of packet recovery is determined by the rate |
latency of packet recovery is determined by the rate at |
of packet recovery is determined by the rate at which |
packet recovery is determined by the rate at which the |
recovery is determined by the rate at which the sender |
is determined by the rate at which the sender transmits |
determined by the rate at which the sender transmits data |
Maelstrom Design and Implemenfrom less than r data packets at |
Design and Implemenfrom less than r data packets at the |
and Implemenfrom less than r data packets at the sender |
Implemenfrom less than r data packets at the sender is |
less than r data packets at the sender is not |
than r data packets at the sender is not a |
The Bitcoin system normalizes these rates by the total number |
r data packets at the sender is not a viable |
Bitcoin system normalizes these rates by the total number of |
data packets at the sender is not a viable tation |
system normalizes these rates by the total number of miners |
packets at the sender is not a viable tation option |
normalizes these rates by the total number of miners that |
at the sender is not a viable tation option even |
these rates by the total number of miners that publish |
the sender is not a viable tation option even though |
rates by the total number of miners that publish full |
sender is not a viable tation option even though the |
by the total number of miners that publish full proofs |
is not a viable tation option even though the data |
not a viable tation option even though the data rate |
a viable tation option even though the data rate in |
viable tation option even though the data rate in this |
tation option even though the data rate in this channel |
option even though the data rate in this channel is |
even though the data rate in this channel is low |
or network could be operating at near full capacity with |
network could be operating at near full capacity with data |
could be operating at near full capacity with data from |
be operating at near full capacity with data from other |
operating at near full capacity with data from other senders |
We describe the Maelstrom appliance as a single machine FEC |
describe the Maelstrom appliance as a single machine FEC is |
the Maelstrom appliance as a single machine FEC is also |
Maelstrom appliance as a single machine FEC is also very |
appliance as a single machine FEC is also very susceptible |
as a single machine FEC is also very susceptible to |
a single machine FEC is also very susceptible to bursty |
single machine FEC is also very susceptible to bursty losses |
we will show how more machines can be added to |
will show how more machines can be added to terleaving |
divides its revenue among its loyal miners and the miners |
its revenue among its loyal miners and the miners that |
revenue among its loyal miners and the miners that infiltrated |
among its loyal miners and the miners that infiltrated it |
is a standard encoding technique used the appliance to balance |
a standard encoding technique used the appliance to balance encoding |
standard encoding technique used the appliance to balance encoding load |
encoding technique used the appliance to balance encoding load and |
technique used the appliance to balance encoding load and scale |
used the appliance to balance encoding load and scale to |
the appliance to balance encoding load and scale to multo |
appliance to balance encoding load and scale to multo combat |
to balance encoding load and scale to multo combat bursty |
balance encoding load and scale to multo combat bursty loss |
A B C D X X E F G H |
B C D X X E F G H X |
C D X X E F G H X X |
D X X E F G H X X Appliance |
The revenue includes both its direct mining revenue and B |
Numerical Analysis We analyze this game numerically by finding the |
Analysis We analyze this game numerically by finding the x |
Basic Mechanism The basic operation of Maelstrom is shown in |
Mechanism The basic operation of Maelstrom is shown in Figure |
it intercepts outgoing data packets and routes them to the |
intercepts outgoing data packets and routes them to the destination |
outgoing data packets and routes them to the destination datacenter |
We vary the sizes of the pools through the entire |
generating and injecting FEC repair packets into the stream in |
vary the sizes of the pools through the entire feasible |
and injecting FEC repair packets into the stream in their |
the sizes of the pools through the entire feasible range |
injecting FEC repair packets into the stream in their wake |
sizes of the pools through the entire feasible range and |
of the pools through the entire feasible range and depict |
the pools through the entire feasible range and depict the |
A repair packet consists of a recipe list of data |
pools through the entire feasible range and depict the optimal |
repair packet consists of a recipe list of data packet |
through the entire feasible range and depict the optimal x |
packet consists of a recipe list of data packet identifiers |
consists of a recipe list of data packet identifiers and |
of a recipe list of data packet identifiers and FEC |
a recipe list of data packet identifiers and FEC information |
recipe list of data packet identifiers and FEC information generated |
list of data packet identifiers and FEC information generated from |
of data packet identifiers and FEC information generated from these |
data packet identifiers and FEC information generated from these packets |
Each point in each graph represents the equilibrium point of |
point in each graph represents the equilibrium point of a |
in each graph represents the equilibrium point of a game |
each graph represents the equilibrium point of a game with |
graph represents the equilibrium point of a game with the |
represents the equilibrium point of a game with the corresponding |
the equilibrium point of a game with the corresponding m |
The size of the XOR is equal to the MTU |
size of the XOR is equal to the MTU of |
of the XOR is equal to the MTU of the |
the XOR is equal to the MTU of the datacenter |
XOR is equal to the MTU of the datacenter network |
and to avoid fragmentation of repair packets we require that |
to avoid fragmentation of repair packets we require that the |
avoid fragmentation of repair packets we require that the MTU |
fragmentation of repair packets we require that the MTU of |
The top right half of the range in all graphs |
of repair packets we require that the MTU of the |
top right half of the range in all graphs is |
repair packets we require that the MTU of the long |
right half of the range in all graphs is not |
half of the range in all graphs is not feasible |
since gigabit links very often use Jumbo frames of up |
gigabit links very often use Jumbo frames of up to |
and we use a dashed line to show the bound |
we use a dashed line to show the bound between |
use a dashed line to show the bound between this |
a dashed line to show the bound between this value |
dashed line to show the bound between this value within |
line to show the bound between this value within the |
to show the bound between this value within the feasible |
show the bound between this value within the feasible range |
the appliance examines incoming repair packets and uses them to |
appliance examines incoming repair packets and uses them to recover |
examines incoming repair packets and uses them to recover missing |
incoming repair packets and uses them to recover missing data |
repair packets and uses them to recover missing data packets |
b and in the entire feasible region it is strictly |
the data packet is injected transparently into the stream to |
and in the entire feasible region it is strictly larger |
data packet is injected transparently into the stream to the |
in the entire feasible region it is strictly larger than |
packet is injected transparently into the stream to the receiving |
is injected transparently into the stream to the receiving end |
but this behavior is expected by communication stacks designed for |
this behavior is expected by communication stacks designed for the |
behavior is expected by communication stacks designed for the commodity |
is expected by communication stacks designed for the commodity Internet |
Note that the total system mining power is reduced when |
that the total system mining power is reduced when pool |
terminating connections and sending back ACKs immediately before relaying data |
connections and sending back ACKs immediately before relaying data on |
and sending back ACKs immediately before relaying data on appliance |
lived flows that need to ramp up throughput quickly and |
flows that need to ramp up throughput quickly and avoid |
that need to ramp up throughput quickly and avoid the |
need to ramp up throughput quickly and avoid the slow |
therefore pays for the increased revenue of its attacker and |
pays for the increased revenue of its attacker and everyone |
for the increased revenue of its attacker and everyone else |
the increased revenue of its attacker and everyone else in |
The performance advantages of splitting longdistance connections into multiple hops |
increased revenue of its attacker and everyone else in the |
performance advantages of splitting longdistance connections into multiple hops are |
revenue of its attacker and everyone else in the system |
advantages of splitting longdistance connections into multiple hops are well |
of splitting longdistance connections into multiple hops are well known |
Implications to the general case Consider the case of p |
to the general case Consider the case of p pools |
we are primarily interested in isolating the impact of rapid |
are primarily interested in isolating the impact of rapid and |
primarily interested in isolating the impact of rapid and transparent |
interested in isolating the impact of rapid and transparent recovery |
in isolating the impact of rapid and transparent recovery of |
isolating the impact of rapid and transparent recovery of lost |
the impact of rapid and transparent recovery of lost packets |
impact of rapid and transparent recovery of lost packets by |
of rapid and transparent recovery of lost packets by Maelstrom |
at least one pool will choose to perform block withholding |
rapid and transparent recovery of lost packets by Maelstrom on |
and transparent recovery of lost packets by Maelstrom on TCP |
IP flow control allows it to steal bandwidth from other |
flow control allows it to steal bandwidth from other competing |
control allows it to steal bandwidth from other competing flows |
allows it to steal bandwidth from other competing flows running |
it to steal bandwidth from other competing flows running without |
to steal bandwidth from other competing flows running without FEC |
steal bandwidth from other competing flows running without FEC in |
bandwidth from other competing flows running without FEC in the |
from other competing flows running without FEC in the link |
IP flows is not a primary protocol design goal on |
flows is not a primary protocol design goal on over |
We see evidence for this assertion in the routine use |
see evidence for this assertion in the routine use of |
evidence for this assertion in the routine use of parallel |
for this assertion in the routine use of parallel flows |
both in commercial deployments and by researchers seeking to transfer |
in commercial deployments and by researchers seeking to transfer large |
commercial deployments and by researchers seeking to transfer large amounts |
deployments and by researchers seeking to transfer large amounts of |
and by researchers seeking to transfer large amounts of data |
by researchers seeking to transfer large amounts of data over |
researchers seeking to transfer large amounts of data over high |
FEC encoding is simply an XOR of the r data |
encoding is simply an XOR of the r data packets |
is simply an XOR of the r data packets hence |
in layered interleaving each data packet is included in c |
layered interleaving each data packet is included in c XORs |
each of which is generated at different interleaves from the |
of which is generated at different interleaves from the original |
which is generated at different interleaves from the original data |
is generated at different interleaves from the original data stream |
ensures that the c XORs containing a data packet do |
that the c XORs containing a data packet do not |
the c XORs containing a data packet do not have |
c XORs containing a data packet do not have any |
XORs containing a data packet do not have any other |
This is the setting analyzed above and we have seen |
is the setting analyzed above and we have seen there |
the setting analyzed above and we have seen there that |
setting analyzed above and we have seen there that pool |
can increase its revenue by performing a block withholding attack |
increase its revenue by performing a block withholding attack on |
its revenue by performing a block withholding attack on pool |
take this values back to the setting at hand with |
this values back to the setting at hand with p |
values back to the setting at hand with p pools |
with each XOR generated from r data packets and each |
each XOR generated from r data packets and each data |
XOR generated from r data packets and each data packet |
generated from r data packets and each data packet included |
from r data packets and each data packet included in |
r data packets and each data packet included in c |
data packets and each data packet included in c XORs |
standard FEC schemes can be made resistant to a certain |
FEC schemes can be made resistant to a certain loss |
schemes can be made resistant to a certain loss burst |
can be made resistant to a certain loss burst length |
be made resistant to a certain loss burst length at |
made resistant to a certain loss burst length at the |
resistant to a certain loss burst length at the cost |
to a certain loss burst length at the cost of |
a certain loss burst length at the cost of increased |
certain loss burst length at the cost of increased recovery |
loss burst length at the cost of increased recovery latency |
burst length at the cost of increased recovery latency for |
length at the cost of increased recovery latency for all |
at the cost of increased recovery latency for all lost |
the cost of increased recovery latency for all lost packets |
layered interleaving provides graceful degradation in the face of bursty |
interleaving provides graceful degradation in the face of bursty loss |
provides graceful degradation in the face of bursty loss for |
graceful degradation in the face of bursty loss for constant |
degradation in the face of bursty loss for constant encoding |
in the face of bursty loss for constant encoding overhead |
the face of bursty loss for constant encoding overhead singleton |
face of bursty loss for constant encoding overhead singleton random |
of bursty loss for constant encoding overhead singleton random losses |
bursty loss for constant encoding overhead singleton random losses are |
loss for constant encoding overhead singleton random losses are recovered |
for constant encoding overhead singleton random losses are recovered as |
constant encoding overhead singleton random losses are recovered as quickly |
encoding overhead singleton random losses are recovered as quickly as |
overhead singleton random losses are recovered as quickly as possible |
and each successive layer of XORs generated at a higher |
each successive layer of XORs generated at a higher interleave |
successive layer of XORs generated at a higher interleave catches |
layer of XORs generated at a higher interleave catches larger |
of XORs generated at a higher interleave catches larger bursts |
XORs generated at a higher interleave catches larger bursts missed |
generated at a higher interleave catches larger bursts missed by |
at a higher interleave catches larger bursts missed by the |
a higher interleave catches larger bursts missed by the previous |
higher interleave catches larger bursts missed by the previous layer |
The implementation of this algorithm is simple and shown in |
implementation of this algorithm is simple and shown in Figure |
a set of repair bins is maintained for each layer |
A repair bin consists of a partially constructed repair packet |
an XOR and the recipe list of identifiers of data |
XOR and the recipe list of identifiers of data packets |
and the recipe list of identifiers of data packets that |
the recipe list of identifiers of data packets that compose |
recipe list of identifiers of data packets that compose the |
list of identifiers of data packets that compose the XOR |
Each intercepted data packet is added to each layer where |
intercepted data packet is added to each layer where adding |
data packet is added to each layer where adding to |
packet is added to each layer where adding to a |
is added to each layer where adding to a layer |
added to each layer where adding to a layer simply |
to each layer where adding to a layer simply means |
each layer where adding to a layer simply means choosing |
layer where adding to a layer simply means choosing a |
where adding to a layer simply means choosing a repair |
adding to a layer simply means choosing a repair bin |
to a layer simply means choosing a repair bin from |
a layer simply means choosing a repair bin from the |
layer simply means choosing a repair bin from the layer |
simply means choosing a repair bin from the layer s |
means choosing a repair bin from the layer s set |
and adding the data packet s header to the recipe |
adding the data packet s header to the recipe list |
A counter is incremented as each data packet arrives at |
counter is incremented as each data packet arrives at the |
is incremented as each data packet arrives at the appliance |
and choosing the repair bin from the layer s set |
choosing the repair bin from the layer s set is |
the repair bin from the layer s set is done |
repair bin from the layer s set is done by |
bin from the layer s set is done by taking |
from the layer s set is done by taking the |
the layer s set is done by taking the modulo |
layer s set is done by taking the modulo of |
s set is done by taking the modulo of the |
set is done by taking the modulo of the counter |
is done by taking the modulo of the counter with |
done by taking the modulo of the counter with the |
by taking the modulo of the counter with the number |
taking the modulo of the counter with the number of |
the modulo of the counter with the number of bins |
modulo of the counter with the number of bins in |
of the counter with the number of bins in each |
the counter with the number of bins in each layer |
When a repair bin fills up its recipe list contains |
a repair bin fills up its recipe list contains r |
repair bin fills up its recipe list contains r data |
bin fills up its recipe list contains r data packets |
fills up its recipe list contains r data packets it |
up its recipe list contains r data packets it fires |
We analyze the cases where each of the pools attacks |
analyze the cases where each of the pools attacks all |
the cases where each of the pools attacks all other |
a repair packet is generated consisting of the XOR and |
cases where each of the pools attacks all other open |
repair packet is generated consisting of the XOR and the |
where each of the pools attacks all other open pools |
packet is generated consisting of the XOR and the recipe |
is generated consisting of the XOR and the recipe list |
generated consisting of the XOR and the recipe list and |
consisting of the XOR and the recipe list and is |
Note that attacking all pools with force proportional to their |
of the XOR and the recipe list and is scheduled |
that attacking all pools with force proportional to their size |
the XOR and the recipe list and is scheduled for |
attacking all pools with force proportional to their size yields |
XOR and the recipe list and is scheduled for sending |
all pools with force proportional to their size yields the |
pools with force proportional to their size yields the same |
with force proportional to their size yields the same results |
force proportional to their size yields the same results as |
proportional to their size yields the same results as attacking |
to their size yields the same results as attacking a |
their size yields the same results as attacking a single |
size yields the same results as attacking a single pool |
yields the same results as attacking a single pool of |
the same results as attacking a single pool of their |
same results as attacking a single pool of their aggregate |
if all the data packets contained in the repair s |
results as attacking a single pool of their aggregate size |
all the data packets contained in the repair s recipe |
the data packets contained in the repair s recipe list |
data packets contained in the repair s recipe list have |
Plugging in the numbers into the analysis above shows that |
packets contained in the repair s recipe list have been |
in the numbers into the analysis above shows that a |
contained in the repair s recipe list have been received |
the numbers into the analysis above shows that a larger |
in the repair s recipe list have been received successfully |
numbers into the analysis above shows that a larger pool |
into the analysis above shows that a larger pool needs |
the analysis above shows that a larger pool needs to |
If the repair s recipe list contains a single missing |
analysis above shows that a larger pool needs to use |
the repair s recipe list contains a single missing data |
above shows that a larger pool needs to use a |
repair s recipe list contains a single missing data packet |
shows that a larger pool needs to use a smaller |
that a larger pool needs to use a smaller ratio |
recovery can occur immediately by combining the XOR in the |
a larger pool needs to use a smaller ratio of |
can occur immediately by combining the XOR in the repair |
larger pool needs to use a smaller ratio of its |
occur immediately by combining the XOR in the repair with |
pool needs to use a smaller ratio of its mining |
immediately by combining the XOR in the repair with Layer |
needs to use a smaller ratio of its mining power |
to use a smaller ratio of its mining power for |
use a smaller ratio of its mining power for infiltration |
a smaller ratio of its mining power for infiltration and |
smaller ratio of its mining power for infiltration and can |
ratio of its mining power for infiltration and can increase |
of its mining power for infiltration and can increase its |
its mining power for infiltration and can increase its revenue |
mining power for infiltration and can increase its revenue density |
power for infiltration and can increase its revenue density more |
for infiltration and can increase its revenue density more than |
infiltration and can increase its revenue density more than a |
and can increase its revenue density more than a small |
can increase its revenue density more than a small pool |
This represents a considerable increase of the pools net revenue |
To reach the optimum it needs almost a third of |
reach the optimum it needs almost a third of its |
the optimum it needs almost a third of its power |
optimum it needs almost a third of its power for |
it needs almost a third of its power for attacking |
needs almost a third of its power for attacking but |
almost a third of its power for attacking but increases |
a third of its power for attacking but increases its |
third of its power for attacking but increases its revenue |
of its power for attacking but increases its revenue density |
its power for attacking but increases its revenue density by |
power for attacking but increases its revenue density by merely |
it cannot be used immediately for recovery it is instead |
cannot be used immediately for recovery it is instead stored |
be used immediately for recovery it is instead stored in |
used immediately for recovery it is instead stored in a |
immediately for recovery it is instead stored in a table |
for recovery it is instead stored in a table that |
recovery it is instead stored in a table that maps |
it is instead stored in a table that maps missing |
is instead stored in a table that maps missing data |
instead stored in a table that maps missing data packets |
stored in a table that maps missing data packets to |
in a table that maps missing data packets to repair |
a table that maps missing data packets to repair packets |
this table is checked to see if any XORs now |
table is checked to see if any XORs now have |
is checked to see if any XORs now have singleton |
checked to see if any XORs now have singleton losses |
to see if any XORs now have singleton losses due |
see if any XORs now have singleton losses due to |
if any XORs now have singleton losses due to the |
any XORs now have singleton losses due to the presence |
XORs now have singleton losses due to the presence of |
now have singleton losses due to the presence of the |
have singleton losses due to the presence of the new |
singleton losses due to the presence of the new packet |
losses due to the presence of the new packet and |
due to the presence of the new packet and can |
to the presence of the new packet and can be |
the presence of the new packet and can be used |
presence of the new packet and can be used for |
of the new packet and can be used for recovering |
the new packet and can be used for recovering other |
new packet and can be used for recovering other missing |
packet and can be used for recovering other missing packets |
T WO P OOLS We proceed to analyze the case |
XORs received from different layers interact to recover missing data |
WO P OOLS We proceed to analyze the case where |
received from different layers interact to recover missing data packets |
P OOLS We proceed to analyze the case where two |
OOLS We proceed to analyze the case where two pools |
since an XOR received at a higher interleave can recover |
We proceed to analyze the case where two pools may |
an XOR received at a higher interleave can recover a |
proceed to analyze the case where two pools may attack |
XOR received at a higher interleave can recover a packet |
to analyze the case where two pools may attack each |
received at a higher interleave can recover a packet that |
analyze the case where two pools may attack each other |
at a higher interleave can recover a packet that makes |
the case where two pools may attack each other and |
a higher interleave can recover a packet that makes an |
case where two pools may attack each other and the |
higher interleave can recover a packet that makes an earlier |
where two pools may attack each other and the other |
interleave can recover a packet that makes an earlier XOR |
two pools may attack each other and the other miners |
can recover a packet that makes an earlier XOR at |
pools may attack each other and the other miners mine |
recover a packet that makes an earlier XOR at a |
may attack each other and the other miners mine solo |
a packet that makes an earlier XOR at a lower |
packet that makes an earlier XOR at a lower interleave |
that makes an earlier XOR at a lower interleave usable |
makes an earlier XOR at a lower interleave usable hence |
its recovery power is much higher and comes close to |
recovery power is much higher and comes close to standard |
The total mining power in the system is m x |
of the pools from mining are their effective mining rates |
The total revenue of each pool is its direct mining |
total revenue of each pool is its direct mining revenue |
which is the attacked pool s total revenue multiplied by |
is the attacked pool s total revenue multiplied by its |
the attacked pool s total revenue multiplied by its infiltration |
attacked pool s total revenue multiplied by its infiltration rate |
The pool s total revenue is divided among its loyal |
pool s total revenue is divided among its loyal miners |
s total revenue is divided among its loyal miners and |
total revenue is divided among its loyal miners and miners |
revenue is divided among its loyal miners and miners that |
is divided among its loyal miners and miners that infiltrated |
divided among its loyal miners and miners that infiltrated it |
each pool will optimize its infiltration rate of the other |
Limiting In the naive implementation of the layered interleaving algorithm |
repair packets are transmitted as soon as repair bins fill |
packets are transmitted as soon as repair bins fill and |
are transmitted as soon as repair bins fill and allow |
transmitted as soon as repair bins fill and allow them |
as soon as repair bins fill and allow them to |
soon as repair bins fill and allow them to be |
as repair bins fill and allow them to be constructed |
all the repair bins in a layer fill in quick |
the repair bins in a layer fill in quick succession |
This behavior leads to a large number of repair packets |
behavior leads to a large number of repair packets being |
leads to a large number of repair packets being generated |
to a large number of repair packets being generated and |
a large number of repair packets being generated and sent |
large number of repair packets being generated and sent within |
number of repair packets being generated and sent within a |
of repair packets being generated and sent within a short |
repair packets being generated and sent within a short period |
packets being generated and sent within a short period of |
being generated and sent within a short period of time |
limit transmissions of repair packets to one for every r |
transmissions of repair packets to one for every r data |
of repair packets to one for every r data packets |
This problem is fixed by staggering the starting sizes of |
problem is fixed by staggering the starting sizes of the |
is fixed by staggering the starting sizes of the bins |
analogous to the starting positions of runners in a sprint |
the very first time bin number x in a layer |
very first time bin number x in a layer of |
first time bin number x in a layer of interleave |
time bin number x in a layer of interleave i |
bin number x in a layer of interleave i fires |
the first repair bin in the second layer with interleave |
for the first i data packets added to a layer |
the first i data packets added to a layer with |
first i data packets added to a layer with interleave |
i data packets added to a layer with interleave i |
and so on until r i data packets have been |
so on until r i data packets have been added |
on until r i data packets have been added to |
until r i data packets have been added to the |
r i data packets have been added to the layer |
i data packets have been added to the layer and |
data packets have been added to the layer and all |
packets have been added to the layer and all bins |
have been added to the layer and all bins have |
been added to the layer and all bins have fired |
added to the layer and all bins have fired exactly |
to the layer and all bins have fired exactly once |
The revenue function for ri is concave in xi for |
revenue function for ri is concave in xi for all |
function for ri is concave in xi for all feasible |
for ri is concave in xi for all feasible values |
ri is concave in xi for all feasible values of |
The outlined scheme works when i is greater than or |
is concave in xi for all feasible values of the |
outlined scheme works when i is greater than or equal |
concave in xi for all feasible values of the variables |
scheme works when i is greater than or equal to |
works when i is greater than or equal to r |
are unique and are either at the borders of the |
unique and are either at the borders of the feasible |
and are either at the borders of the feasible region |
are either at the borders of the feasible region or |
either at the borders of the feasible region or where |
at the borders of the feasible region or where ri |
If r and i are not integral multiples of each |
r and i are not integral multiples of each other |
limiting still works but is slightly less effective due to |
still works but is slightly less effective due to rounding |
since each pool can increase its revenue by choosing a |
works but is slightly less effective due to rounding errors |
each pool can increase its revenue by choosing a strictly |
pool can increase its revenue by choosing a strictly positive |
can increase its revenue by choosing a strictly positive infiltration |
increase its revenue by choosing a strictly positive infiltration rate |
repair packets are transmitted as soon as they are generated |
This results in the repair packet leaving immediately after the |
results in the repair packet leaving immediately after the last |
in the repair packet leaving immediately after the last data |
the repair packet leaving immediately after the last data packet |
repair packet leaving immediately after the last data packet that |
packet leaving immediately after the last data packet that was |
leaving immediately after the last data packet that was added |
immediately after the last data packet that was added to |
after the last data packet that was added to it |
which lowers burst tolerance if the repair packet was generated |
lowers burst tolerance if the repair packet was generated at |
burst tolerance if the repair packet was generated at interleave |
tolerance if the repair packet was generated at interleave i |
the resulting protocol can tolerate a burst of i lost |
resulting protocol can tolerate a burst of i lost data |
protocol can tolerate a burst of i lost data packets |
can tolerate a burst of i lost data packets excluding |
tolerate a burst of i lost data packets excluding the |
a burst of i lost data packets excluding the repair |
but the burst could swallow both the repair and the |
the burst could swallow both the repair and the last |
burst could swallow both the repair and the last data |
could swallow both the repair and the last data packet |
swallow both the repair and the last data packet in |
both the repair and the last data packet in it |
the repair and the last data packet in it as |
repair and the last data packet in it as they |
and the last data packet in it as they are |
the last data packet in it as they are not |
last data packet in it as they are not separated |
data packet in it as they are not separated by |
packet in it as they are not separated by the |
in it as they are not separated by the requisite |
it as they are not separated by the requisite interleave |
The solution to this is simple delay sending the repair |
solution to this is simple delay sending the repair packet |
to this is simple delay sending the repair packet generated |
this is simple delay sending the repair packet generated by |
is simple delay sending the repair packet generated by a |
simple delay sending the repair packet generated by a repair |
delay sending the repair packet generated by a repair bin |
sending the repair packet generated by a repair bin until |
the repair packet generated by a repair bin until the |
repair packet generated by a repair bin until the next |
packet generated by a repair bin until the next time |
generated by a repair bin until the next time a |
by a repair bin until the next time a data |
a repair bin until the next time a data packet |
repair bin until the next time a data packet is |
bin until the next time a data packet is added |
until the next time a data packet is added to |
the next time a data packet is added to the |
next time a data packet is added to the now |
time a data packet is added to the now empty |
a data packet is added to the now empty bin |
which happens i packets later and introduces the required interleave |
happens i packets later and introduces the required interleave between |
i packets later and introduces the required interleave between the |
packets later and introduces the required interleave between the repair |
later and introduces the required interleave between the repair packet |
and introduces the required interleave between the repair packet and |
introduces the required interleave between the repair packet and the |
the required interleave between the repair packet and the last |
required interleave between the repair packet and the last data |
interleave between the repair packet and the last data packet |
between the repair packet and the last data packet included |
the repair packet and the last data packet included in |
repair packet and the last data packet included in it |
Notice that although transmitting the XOR immediately results in faster |
that although transmitting the XOR immediately results in faster recovery |
doing so also reduces the probability of a lost packet |
so also reduces the probability of a lost packet being |
also reduces the probability of a lost packet being recovered |
we see that there is a single pair of values |
see that there is a single pair of values for |
that there is a single pair of values for which |
off results in a minor control knob permitting us to |
there is a single pair of values for which Equation |
results in a minor control knob permitting us to balance |
in a minor control knob permitting us to balance speed |
a minor control knob permitting us to balance speed against |
minor control knob permitting us to balance speed against burst |
control knob permitting us to balance speed against burst tolerance |
We simulate the pool game for a range of pool |
simulate the pool game for a range of pool sizes |
we start the simulation when both pools do not infiltrate |
start the simulation when both pools do not infiltrate each |
the simulation when both pools do not infiltrate each other |
we note that no two repair packets generated at different |
note that no two repair packets generated at different interleaves |
that no two repair packets generated at different interleaves i |
will have more than one data packet in common as |
have more than one data packet in common as long |
more than one data packet in common as long as |
than one data packet in common as long as the |
one data packet in common as long as the Least |
data packet in common as long as the Least Common |
packet in common as long as the Least Common Multiple |
At each round one pool chooses its optimal infiltration rate |
pairings of repair bins in two different layers with interleaves |
each round one pool chooses its optimal infiltration rate based |
of repair bins in two different layers with interleaves i |
round one pool chooses its optimal infiltration rate based on |
one pool chooses its optimal infiltration rate based on the |
pool chooses its optimal infiltration rate based on the pool |
chooses its optimal infiltration rate based on the pool sizes |
its optimal infiltration rate based on the pool sizes and |
optimal infiltration rate based on the pool sizes and the |
infiltration rate based on the pool sizes and the rate |
rate based on the pool sizes and the rate with |
based on the pool sizes and the rate with which |
on the pool sizes and the rate with which it |
the pool sizes and the rate with which it is |
pool sizes and the rate with which it is infiltrated |
a good rule of thumb is to select interleaves that |
good rule of thumb is to select interleaves that are |
rule of thumb is to select interleaves that are relatively |
of thumb is to select interleaves that are relatively prime |
thumb is to select interleaves that are relatively prime to |
Recall the players in the pool game are chosen with |
is to select interleaves that are relatively prime to maximize |
the players in the pool game are chosen with the |
to select interleaves that are relatively prime to maximize their |
players in the pool game are chosen with the Round |
select interleaves that are relatively prime to maximize their LCM |
in the pool game are chosen with the Round Robin |
the pool game are chosen with the Round Robin policy |
and also ensure that the larger interleave is greater than |
also ensure that the larger interleave is greater than r |
We can recover a data packet if at least one |
can recover a data packet if at least one of |
recover a data packet if at least one of the |
values results in a single point in each graph in |
a data packet if at least one of the c |
results in a single point in each graph in Figure |
data packet if at least one of the c XORs |
packet if at least one of the c XORs containing |
if at least one of the c XORs containing it |
at least one of the c XORs containing it is |
least one of the c XORs containing it is re |
all the other data packets in it have also been |
the other data packets in it have also been received |
other data packets in it have also been received correctly |
the probability of In the absence of intelligent flow control |
probability of In the absence of intelligent flow control mechanisms |
of In the absence of intelligent flow control mechanisms like |
In the absence of intelligent flow control mechanisms like which |
the absence of intelligent flow control mechanisms like which is |
absence of intelligent flow control mechanisms like which is simply |
are the points in each of the graphs with the |
the points in each of the graphs with the respective |
points in each of the graphs with the respective coordinates |
j graphs we draw a border around the region where |
hosts can be easily overwhelmed and drop packets during traffic |
graphs we draw a border around the region where there |
can be easily overwhelmed and drop packets during traffic spikes |
we draw a border around the region where there is |
be easily overwhelmed and drop packets during traffic spikes or |
draw a border around the region where there is no |
easily overwhelmed and drop packets during traffic spikes or CPU |
For the ri graphs we draw a line around the |
the probability x of a sent XOR being nance tasks |
the ri graphs we draw a line around the region |
probability x of a sent XOR being nance tasks like |
ri graphs we draw a line around the region where |
x of a sent XOR being nance tasks like garbage |
graphs we draw a line around the region where the |
of a sent XOR being nance tasks like garbage collection |
we draw a line around the region where the revenue |
draw a line around the region where the revenue is |
a line around the region where the revenue is the |
Reliable applicationdropped or unusable is the sum of the probability |
line around the region where the revenue is the same |
applicationdropped or unusable is the sum of the probability that |
around the region where the revenue is the same as |
or unusable is the sum of the probability that it |
the region where the revenue is the same as in |
unusable is the sum of the probability that it level |
region where the revenue is the same as in the |
is the sum of the probability that it level protocols |
where the revenue is the same as in the no |
the sum of the probability that it level protocols layered |
sum of the probability that it level protocols layered over |
of the probability that it level protocols layered over UDP |
the probability that it level protocols layered over UDP for |
probability that it level protocols layered over UDP for reliable |
that it level protocols layered over UDP for reliable multiwas |
We first observe that only in extreme cases a pool |
it level protocols layered over UDP for reliable multiwas dropped |
first observe that only in extreme cases a pool does |
level protocols layered over UDP for reliable multiwas dropped and |
observe that only in extreme cases a pool does not |
protocols layered over UDP for reliable multiwas dropped and the |
that only in extreme cases a pool does not attack |
layered over UDP for reliable multiwas dropped and the probability |
only in extreme cases a pool does not attack its |
over UDP for reliable multiwas dropped and the probability that |
in extreme cases a pool does not attack its counterpart |
UDP for reliable multiwas dropped and the probability that it |
for reliable multiwas dropped and the probability that it was |
reliable multiwas dropped and the probability that it was received |
multiwas dropped and the probability that it was received and |
at equilibrium a pool will refrain from attacking only if |
dropped and the probability that it was received and cast |
equilibrium a pool will refrain from attacking only if the |
a pool will refrain from attacking only if the other |
pool will refrain from attacking only if the other pool |
will refrain from attacking only if the other pool is |
refrain from attacking only if the other pool is larger |
from attacking only if the other pool is larger than |
attacking only if the other pool is larger than about |
we observe that a pool improves its revenue compared to |
observe that a pool improves its revenue compared to the |
that a pool improves its revenue compared to the no |
attacks scenario only when it controls a strict majority of |
scenario only when it controls a strict majority of the |
only when it controls a strict majority of the total |
when it controls a strict majority of the total mining |
it controls a strict majority of the total mining power |
the revenue of the pool is inferior compared to the |
revenue of the pool is inferior compared to the no |
would ordinarily go back to the sender to retrieve the |
where neither pool controls a strict majority of the mining |
ordinarily go back to the sender to retrieve the lost |
neither pool controls a strict majority of the mining power |
go back to the sender to retrieve the lost packet |
both pools will earn less at equilibrium than if both |
pools will earn less at equilibrium than if both pools |
even though it was dropped at the receiver after Since |
will earn less at equilibrium than if both pools ran |
though it was dropped at the receiver after Since it |
earn less at equilibrium than if both pools ran without |
it was dropped at the receiver after Since it is |
less at equilibrium than if both pools ran without attacking |
was dropped at the receiver after Since it is easy |
dropped at the receiver after Since it is easy to |
We can analyze in this case a game where each |
at the receiver after Since it is easy to ensure |
can analyze in this case a game where each pool |
the receiver after Since it is easy to ensure that |
analyze in this case a game where each pool chooses |
receiver after Since it is easy to ensure that no |
in this case a game where each pool chooses either |
after Since it is easy to ensure that no two |
this case a game where each pool chooses either to |
Since it is easy to ensure that no two XORs |
case a game where each pool chooses either to attack |
it is easy to ensure that no two XORs share |
a game where each pool chooses either to attack and |
is easy to ensure that no two XORs share covering |
game where each pool chooses either to attack and optimize |
easy to ensure that no two XORs share covering the |
where each pool chooses either to attack and optimize its |
to ensure that no two XORs share covering the entire |
each pool chooses either to attack and optimize its revenue |
ensure that no two XORs share covering the entire geographical |
that no two XORs share covering the entire geographical distance |
the usability probabilities of The Maelstrom proxy acts as a |
usability probabilities of The Maelstrom proxy acts as a local |
probabilities of The Maelstrom proxy acts as a local packet |
of The Maelstrom proxy acts as a local packet cache |
The probability of all ing incoming packets for a short |
probability of all ing incoming packets for a short period |
of all ing incoming packets for a short period of |
all ing incoming packets for a short period of time |
ing incoming packets for a short period of time and |
incoming packets for a short period of time and prothe |
packets for a short period of time and prothe c |
for a short period of time and prothe c XORs |
a short period of time and prothe c XORs being |
short period of time and prothe c XORs being dropped |
period of time and prothe c XORs being dropped or |
of time and prothe c XORs being dropped or unusable |
time and prothe c XORs being dropped or unusable is |
and prothe c XORs being dropped or unusable is xc |
viding hooks that allow protocols to first query the cache |
hooks that allow protocols to first query the cache the |
that allow protocols to first query the cache the probability |
allow protocols to first query the cache the probability of |
protocols to first query the cache the probability of correctly |
to first query the cache the probability of correctly receiving |
first query the cache the probability of correctly receiving at |
query the cache the probability of correctly receiving at least |
the cache the probability of correctly receiving at least one |
cache the probability of correctly receiving at least one usable |
the probability of correctly receiving at least one usable to |
probability of correctly receiving at least one usable to locate |
of correctly receiving at least one usable to locate missing |
correctly receiving at least one usable to locate missing packets |
receiving at least one usable to locate missing packets before |
at least one usable to locate missing packets before sending |
least one usable to locate missing packets before sending retransmission |
one usable to locate missing packets before sending retransmission XOR |
usable to locate missing packets before sending retransmission XOR is |
Future versions of Maelstrom ering the lost data packet is |
which expands to could potentially use knowledge of protocol internals |
expands to could potentially use knowledge of protocol internals to |
form formula only gives us a lower bound satisfying retransmission |
formula only gives us a lower bound satisfying retransmission requests |
only gives us a lower bound satisfying retransmission requests sent |
gives us a lower bound satisfying retransmission requests sent by |
us a lower bound satisfying retransmission requests sent by the |
a lower bound satisfying retransmission requests sent by the receiver |
lower bound satisfying retransmission requests sent by the receiver in |
bound satisfying retransmission requests sent by the receiver in on |
satisfying retransmission requests sent by the receiver in on the |
retransmission requests sent by the receiver in on the recovery |
requests sent by the receiver in on the recovery probability |
or by resending packets when acmula does not factor in |
by resending packets when acmula does not factor in the |
resending packets when acmula does not factor in the probability |
packets when acmula does not factor in the probability of |
when acmula does not factor in the probability of the |
acmula does not factor in the probability of the other |
does not factor in the probability of the other data |
not factor in the probability of the other data knowledgments |
factor in the probability of the other data knowledgments are |
in the probability of the other data knowledgments are not |
the probability of the other data knowledgments are not observed |
probability of the other data knowledgments are not observed within |
of the other data knowledgments are not observed within a |
the other data knowledgments are not observed within a certain |
other data knowledgments are not observed within a certain time |
data knowledgments are not observed within a certain time period |
knowledgments are not observed within a certain time period in |
are not observed within a certain time period in an |
not observed within a certain time period in an ACK |
If the lost data packet was part of a loss |
the lost data packet was part of a loss burst |
lost data packet was part of a loss burst of |
data packet was part of a loss burst of size |
The revenue density of each pool is determined by the |
packet was part of a loss burst of size b |
revenue density of each pool is determined by the decision |
density of each pool is determined by the decision of |
repair packets generated at interleaves less than b are dropped |
of each pool is determined by the decision of both |
packets generated at interleaves less than b are dropped or |
each pool is determined by the decision of both pools |
generated at interleaves less than b are dropped or useless |
pool is determined by the decision of both pools whether |
at interleaves less than b are dropped or useless with |
is determined by the decision of both pools whether to |
interleaves less than b are dropped or useless with high |
determined by the decision of both pools whether to attack |
less than b are dropped or useless with high probability |
by the decision of both pools whether to attack or |
the decision of both pools whether to attack or not |
however the payoff of both would be larger if they |
the payoff of both would be larger if they both |
payoff of both would be larger if they both refrain |
of both would be larger if they both refrain from |
both would be larger if they both refrain from attacking |
is the number of XORs generated at interleaves greater than |
the number of XORs generated at interleaves greater than b |
since packet losses with more than b intervening packets between |
packet losses with more than b intervening packets between them |
losses with more than b intervening packets between them have |
with more than b intervening packets between them have independent |
more than b intervening packets between them have independent probability |
the revenue of each pool is smaller than its revenue |
revenue of each pool is smaller than its revenue if |
of each pool is smaller than its revenue if neither |
each pool is smaller than its revenue if neither pool |
pool is smaller than its revenue if neither pool attacked |
where each pool can change its strategy between attack and |
each pool can change its strategy between attack and no |
c repair packets are generated and sent for every r |
repair packets are generated and sent for every r data |
packets are generated and sent for every r data packets |
and in each round a pool can detect whether it |
in each round a pool can detect whether it is |
and the correct delivery of any r of the r |
each round a pool can detect whether it is being |
round a pool can detect whether it is being attacked |
c packets transmitted is sufficient to reconstruct the original r |
a pool can detect whether it is being attacked and |
packets transmitted is sufficient to reconstruct the original r data |
pool can detect whether it is being attacked and deduce |
transmitted is sufficient to reconstruct the original r data packets |
can detect whether it is being attacked and deduce that |
detect whether it is being attacked and deduce that the |
whether it is being attacked and deduce that the other |
it is being attacked and deduce that the other pool |
is being attacked and deduce that the other pool is |
being attacked and deduce that the other pool is violating |
we can recover it if at least r packets are |
attacked and deduce that the other pool is violating the |
can recover it if at least r packets are received |
and deduce that the other pool is violating the agreement |
recover it if at least r packets are received correctly |
it if at least r packets are received correctly in |
if at least r packets are received correctly in the |
at least r packets are received correctly in the encoding |
cooperation where neither pool attacks is a possible stable state |
least r packets are received correctly in the encoding set |
r packets are received correctly in the encoding set of |
packets are received correctly in the encoding set of r |
c data and repair packets that the lost packet belongs |
data and repair packets that the lost packet belongs to |
the probability of recovering a lost packet is equivalent to |
probability of recovering a lost packet is equivalent to the |
of recovering a lost packet is equivalent to the probability |
despite the fact that the single Nash equilibrium in every |
recovering a lost packet is equivalent to the probability of |
the fact that the single Nash equilibrium in every round |
a lost packet is equivalent to the probability of losing |
fact that the single Nash equilibrium in every round is |
lost packet is equivalent to the probability of losing c |
that the single Nash equilibrium in every round is to |
the single Nash equilibrium in every round is to attack |
Since the number of other lost packets in the XOR |
the number of other lost packets in the XOR is |
case As an example we take again the pool sizes |
number of other lost packets in the XOR is a |
As an example we take again the pool sizes shown |
of other lost packets in the XOR is a random |
an example we take again the pool sizes shown in |
other lost packets in the XOR is a random variable |
example we take again the pool sizes shown in Figure |
lost packets in the XOR is a random variable Y |
packets in the XOR is a random variable Y and |
in the XOR is a random variable Y and has |
the XOR is a random variable Y and has a |
XOR is a random variable Y and has a binomial |
is a random variable Y and has a binomial distribution |
a random variable Y and has a binomial distribution with |
random variable Y and has a binomial distribution with parameters |
we plot the recovery probability curves for Layered Interleaving and |
plot the recovery probability curves for Layered Interleaving and Reed |
q I DENTICAL P OOLS Let there be q pools |
I DENTICAL P OOLS Let there be q pools of |
DENTICAL P OOLS Let there be q pools of identical |
P OOLS Let there be q pools of identical size |
OOLS Let there be q pools of identical size that |
Let there be q pools of identical size that engage |
there be q pools of identical size that engage in |
be q pools of identical size that engage in block |
q pools of identical size that engage in block withholding |
pools of identical size that engage in block withholding against |
of identical size that engage in block withholding against one |
identical size that engage in block withholding against one another |
note that the curves are very close to each other |
It controls its attack rates each of the other pools |
Implementation Details We initially implemented and evaluated Maelstrom as a |
Details We initially implemented and evaluated Maelstrom as a user |
Each of the other pools can attack its peers as |
of the other pools can attack its peers as well |
Performance turned out to be limited by copying and context |
and we subsequently reimplemented the system as a module that |
we subsequently reimplemented the system as a module that runs |
subsequently reimplemented the system as a module that runs within |
reimplemented the system as a module that runs within the |
the system as a module that runs within the Linux |
the experimental prototype of the kernel version reaches output speeds |
experimental prototype of the kernel version reaches output speeds close |
prototype of the kernel version reaches output speeds close to |
limited only by the capacity of the outbound network card |
Traffic would be distributed over such a rack by partitioning |
would be distributed over such a rack by partitioning the |
be distributed over such a rack by partitioning the address |
distributed over such a rack by partitioning the address space |
over such a rack by partitioning the address space of |
such a rack by partitioning the address space of the |
a rack by partitioning the address space of the remote |
rack by partitioning the address space of the remote datacenter |
by partitioning the address space of the remote datacenter and |
partitioning the address space of the remote datacenter and routing |
the address space of the remote datacenter and routing different |
address space of the remote datacenter and routing different segments |
space of the remote datacenter and routing different segments of |
of the remote datacenter and routing different segments of the |
the remote datacenter and routing different segments of the space |
remote datacenter and routing different segments of the space through |
datacenter and routing different segments of the space through distinct |
and routing different segments of the space through distinct Maelstrom |
routing different segments of the space through distinct Maelstrom appliance |
different segments of the space through distinct Maelstrom appliance pairs |
balancing schemes that might vary the IP address space partitioning |
schemes that might vary the IP address space partitioning dynamically |
that might vary the IP address space partitioning dynamically to |
might vary the IP address space partitioning dynamically to spread |
vary the IP address space partitioning dynamically to spread the |
the IP address space partitioning dynamically to spread the encoding |
IP address space partitioning dynamically to spread the encoding load |
address space partitioning dynamically to spread the encoding load over |
space partitioning dynamically to spread the encoding load over multiple |
partitioning dynamically to spread the encoding load over multiple machines |
Each proxy acts both as an ingress and egress temporarily |
in case all but one of the missing packets are |
case all but one of the missing packets are router |
all but one of the missing packets are router at |
but one of the missing packets are router at the |
one of the missing packets are router at the same |
of the missing packets are router at the same time |
the missing packets are router at the same time since |
missing packets are router at the same time since they |
packets are router at the same time since they handle |
are router at the same time since they handle duplex |
router at the same time since they handle duplex traffic |
at the same time since they handle duplex traffic in |
the same time since they handle duplex traffic in received |
same time since they handle duplex traffic in received later |
time since they handle duplex traffic in received later or |
since they handle duplex traffic in received later or recovered |
they handle duplex traffic in received later or recovered through |
handle duplex traffic in received later or recovered through other |
duplex traffic in received later or recovered through other XORs |
the recovery of the remaining missing packet from this XOR |
In practice we stored data and XOR packets in dou |
practice we stored data and XOR packets in dou The |
we stored data and XOR packets in dou The egress |
stored data and XOR packets in dou The egress router |
data and XOR packets in dou The egress router captures |
and XOR packets in dou The egress router captures IP |
XOR packets in dou The egress router captures IP packets |
packets in dou The egress router captures IP packets and |
in dou The egress router captures IP packets and creates |
dou The egress router captures IP packets and creates re |
routed through unaltered as they would have been At the |
through unaltered as they would have been At the send |
and solving we obtain a single expression for any ri |
the redundant packets are then forwarded leaving scheme store incrementally |
redundant packets are then forwarded leaving scheme store incrementally computed |
packets are then forwarded leaving scheme store incrementally computed XORs |
are then forwarded leaving scheme store incrementally computed XORs and |
then forwarded leaving scheme store incrementally computed XORs and to |
forwarded leaving scheme store incrementally computed XORs and to the |
leaving scheme store incrementally computed XORs and to the remote |
scheme store incrementally computed XORs and to the remote ingress |
store incrementally computed XORs and to the remote ingress router |
incrementally computed XORs and to the remote ingress router via |
computed XORs and to the remote ingress router via a |
XORs and to the remote ingress router via a UDP |
and to the remote ingress router via a UDP channel |
resulting in low storage overheads for each layer The ingress |
in low storage overheads for each layer The ingress router |
low storage overheads for each layer The ingress router captures |
storage overheads for each layer The ingress router captures and |
overheads for each layer The ingress router captures and stores |
for each layer The ingress router captures and stores IP |
each layer The ingress router captures and stores IP packets |
layer The ingress router captures and stores IP packets that |
The ingress router captures and stores IP packets that rise |
ingress router captures and stores IP packets that rise linearly |
router captures and stores IP packets that rise linearly with |
captures and stores IP packets that rise linearly with the |
and stores IP packets that rise linearly with the value |
stores IP packets that rise linearly with the value of |
IP packets that rise linearly with the value of the |
packets that rise linearly with the value of the interleave |
Redundant packets that can be used at a later time |
packets that can be used at a later time are |
that can be used at a later time are stored |
Since the function is concave the equation yields a single |
If the redundant packet is useless it is immediately dis |
the function is concave the equation yields a single feasible |
function is concave the equation yields a single feasible solution |
which is a function of the attack rates of the |
is a function of the attack rates of the other |
a function of the attack rates of the other pools |
Upon recovery the IP packet is sent through Maelstrom appliances |
recovery the IP packet is sent through Maelstrom appliances can |
the IP packet is sent through Maelstrom appliances can optionally |
IP packet is sent through Maelstrom appliances can optionally aggregate |
packet is sent through Maelstrom appliances can optionally aggregate small |
is sent through Maelstrom appliances can optionally aggregate small suba |
sent through Maelstrom appliances can optionally aggregate small suba raw |
through Maelstrom appliances can optionally aggregate small suba raw socket |
Maelstrom appliances can optionally aggregate small suba raw socket to |
appliances can optionally aggregate small suba raw socket to its |
can optionally aggregate small suba raw socket to its intended |
optionally aggregate small suba raw socket to its intended destination |
kilobyte packets from different flows into larger ones for Using |
packets from different flows into larger ones for Using FEC |
from different flows into larger ones for Using FEC requires |
different flows into larger ones for Using FEC requires that |
flows into larger ones for Using FEC requires that each |
into larger ones for Using FEC requires that each data |
larger ones for Using FEC requires that each data packet |
ones for Using FEC requires that each data packet have |
for Using FEC requires that each data packet have a |
Using FEC requires that each data packet have a unique |
FEC requires that each data packet have a unique better |
requires that each data packet have a unique better communication |
that each data packet have a unique better communication efficiency |
The equilibrium infiltration rate and the matching revenues are shown |
each data packet have a unique better communication efficiency over |
equilibrium infiltration rate and the matching revenues are shown in |
data packet have a unique better communication efficiency over the |
infiltration rate and the matching revenues are shown in Equation |
packet have a unique better communication efficiency over the long |
distance identifier that the receiver can use to keep track |
identifier that the receiver can use to keep track of |
that the receiver can use to keep track of re |
in split flow control mode they can ceived data packets |
split flow control mode they can ceived data packets and |
flow control mode they can ceived data packets and to |
control mode they can ceived data packets and to identify |
the revenue at the symmetric equilibrium is inferior to the |
mode they can ceived data packets and to identify missing |
revenue at the symmetric equilibrium is inferior to the no |
they can ceived data packets and to identify missing data |
can ceived data packets and to identify missing data packets |
ceived data packets and to identify missing data packets perform |
data packets and to identify missing data packets perform send |
up Our analysis addresses the eventual revenue of the pools |
assuming the mining difficulty is set based on the effective |
host s buffercould have added a header to each packet |
the mining difficulty is set based on the effective mining |
s buffercould have added a header to each packet with |
mining difficulty is set based on the effective mining power |
buffercould have added a header to each packet with a |
have added a header to each packet with a unique |
added a header to each packet with a unique ing |
a header to each packet with a unique ing capacity |
which has been true for the majority of Bitcoin s |
has been true for the majority of Bitcoin s history |
appliances send multicast packparently and need to route it without |
send multicast packparently and need to route it without modification |
multicast packparently and need to route it without modification or |
packparently and need to route it without modification or addi |
if an attacker purchases new mining hardware and employs it |
an attacker purchases new mining hardware and employs it directly |
attacker purchases new mining hardware and employs it directly for |
purchases new mining hardware and employs it directly for block |
new mining hardware and employs it directly for block withholding |
this mining power is never included in the difficulty calculation |
mining power is never included in the difficulty calculation the |
power is never included in the difficulty calculation the system |
is never included in the difficulty calculation the system is |
never included in the difficulty calculation the system is never |
included in the difficulty calculation the system is never aware |
in the difficulty calculation the system is never aware of |
IP packets by a tuple consisting of the source and |
the difficulty calculation the system is never aware of it |
packets by a tuple consisting of the source and des |
The difficulty is therefore already correctly calculated and the attack |
difficulty is therefore already correctly calculated and the attack is |
is therefore already correctly calculated and the attack is profitable |
appliances can take on other existing roles in the tination |
therefore already correctly calculated and the attack is profitable immediately |
can take on other existing roles in the tination IP |
take on other existing roles in the tination IP address |
the attack becomes profitable only after the Bitcoin system has |
attack becomes profitable only after the Bitcoin system has normalized |
acting as security and VPN gateways and as header plus |
becomes profitable only after the Bitcoin system has normalized the |
as security and VPN gateways and as header plus data |
profitable only after the Bitcoin system has normalized the revenues |
only after the Bitcoin system has normalized the revenues by |
after the Bitcoin system has normalized the revenues by adjusting |
the Bitcoin system has normalized the revenues by adjusting difficulty |
the revenue of an attacking pool is reduced due to |
revenue of an attacking pool is reduced due to the |
of an attacking pool is reduced due to the reduction |
an attacking pool is reduced due to the reduction in |
attacking pool is reduced due to the reduction in block |
pool is reduced due to the reduction in block generation |
is reduced due to the reduction in block generation of |
The checksum over the payload is necessary since the IP |
reduced due to the reduction in block generation of both |
checksum over the payload is necessary since the IP identification |
due to the reduction in block generation of both the |
over the payload is necessary since the IP identification field |
to the reduction in block generation of both the attacking |
the payload is necessary since the IP identification field is |
the reduction in block generation of both the attacking and |
payload is necessary since the IP identification field is only |
reduction in block generation of both the attacking and attacked |
in block generation of both the attacking and attacked pools |
Evaluation use the same identifier for different data packets within |
use the same identifier for different data packets within a |
the same identifier for different data packets within a fairly |
same identifier for different data packets within a fairly short |
identifier for different data packets within a fairly short interval |
for different data packets within a fairly short interval unless |
different data packets within a fairly short interval unless the |
data packets within a fairly short interval unless the checksum |
packets within a fairly short interval unless the checksum is |
within a fairly short interval unless the checksum is added |
a fairly short interval unless the checksum is added to |
fairly short interval unless the checksum is added to We |
short interval unless the checksum is added to We evaluated |
interval unless the checksum is added to We evaluated Maelstrom |
unless the checksum is added to We evaluated Maelstrom on |
the checksum is added to We evaluated Maelstrom on the |
checksum is added to We evaluated Maelstrom on the Emulab |
is added to We evaluated Maelstrom on the Emulab testbed |
added to We evaluated Maelstrom on the Emulab testbed at |
to We evaluated Maelstrom on the Emulab testbed at Utah |
We evaluated Maelstrom on the Emulab testbed at Utah differentiate |
evaluated Maelstrom on the Emulab testbed at Utah differentiate between |
Maelstrom on the Emulab testbed at Utah differentiate between them |
we used a dumbbell topoltifiers result in garbled recovery by |
used a dumbbell topoltifiers result in garbled recovery by Maelstrom |
an event ogy of two clusters of nodes connected via |
event ogy of two clusters of nodes connected via routing |
ogy of two clusters of nodes connected via routing nodes |
of two clusters of nodes connected via routing nodes which |
two clusters of nodes connected via routing nodes which will |
clusters of nodes connected via routing nodes which will be |
of nodes connected via routing nodes which will be caught |
nodes connected via routing nodes which will be caught by |
connected via routing nodes which will be caught by higher |
via routing nodes which will be caught by higher level |
routing nodes which will be caught by higher level checksums |
nodes which will be caught by higher level checksums designed |
which will be caught by higher level checksums designed with |
will be caught by higher level checksums designed with a |
be caught by higher level checksums designed with a high |
designed to emto deal with tranmission errors on commodity networks |
to emto deal with tranmission errors on commodity networks ulate |
emto deal with tranmission errors on commodity networks ulate the |
deal with tranmission errors on commodity networks ulate the setup |
with tranmission errors on commodity networks ulate the setup in |
tranmission errors on commodity networks ulate the setup in Figure |
and ran the proxy code on and hence does not |
ran the proxy code on and hence does not have |
the proxy code on and hence does not have significant |
proxy code on and hence does not have significant consequences |
code on and hence does not have significant consequences unless |
on and hence does not have significant consequences unless the |
and hence does not have significant consequences unless the routers |
shows the performance of the kernel version at Gigabit speeds |
The kernel version of Maelstrom can generate up to a |
kernel version of Maelstrom can generate up to a show |
version of Maelstrom can generate up to a show the |
of Maelstrom can generate up to a show the performance |
Maelstrom can generate up to a show the performance of |
can generate up to a show the performance of the |
generate up to a show the performance of the user |
space version at slower Gigabit per second of data and |
version at slower Gigabit per second of data and FEC |
at slower Gigabit per second of data and FEC traffic |
Expression for ri in a system with pools of equal |
for ri in a system with pools of equal size |
To emulate the MTU difference between the longput data rate |
emulate the MTU difference between the longput data rate depending |
the MTU difference between the longput data rate depending on |
MTU difference between the longput data rate depending on the |
difference between the longput data rate depending on the encoding |
between the longput data rate depending on the encoding rate |
we were able to saturate the outgoing card at set |
were able to saturate the outgoing card at set an |
able to saturate the outgoing card at set an MTU |
to saturate the outgoing card at set an MTU of |
bytes on the network connecting the rates as high as |
where each incoming data packet had to be XORed long |
q Symmetric equilibrium values for a system of q pools |
incoming data packets are buffered so that they can be |
Symmetric equilibrium values for a system of q pools of |
data packets are buffered so that they can be used |
equilibrium values for a system of q pools of equal |
packets are buffered so that they can be used in |
values for a system of q pools of equal sizes |
are buffered so that they can be used in conjunction |
buffered so that they can be used in conjunction with |
so that they can be used in conjunction with Figures |
a pool has to know the rate at which it |
pool has to know the rate at which it is |
has to know the rate at which it is attacked |
A pool can estimate the rate with which it is |
pool can estimate the rate with which it is attacked |
can estimate the rate with which it is attacked by |
and XOR that is missing more than one data packet |
estimate the rate with which it is attacked by comparing |
XOR that is missing more than one data packet is |
the rate with which it is attacked by comparing the |
that is missing more than one data packet is stored |
rate with which it is attacked by comparing the rates |
is missing more than one data packet is stored that |
with which it is attacked by comparing the rates of |
missing more than one data packet is stored that Maelstrom |
which it is attacked by comparing the rates of partial |
more than one data packet is stored that Maelstrom successfully |
it is attacked by comparing the rates of partial and |
than one data packet is stored that Maelstrom successfully masks |
is attacked by comparing the rates of partial and full |
one data packet is stored that Maelstrom successfully masks loss |
attacked by comparing the rates of partial and full proofs |
data packet is stored that Maelstrom successfully masks loss and |
by comparing the rates of partial and full proofs of |
packet is stored that Maelstrom successfully masks loss and prevents |
comparing the rates of partial and full proofs of work |
is stored that Maelstrom successfully masks loss and prevents this |
the rates of partial and full proofs of work it |
rates of partial and full proofs of work it receives |
of partial and full proofs of work it receives from |
partial and full proofs of work it receives from its |
and full proofs of work it receives from its miners |
In order to estimate the revenue densities of the other |
order to estimate the revenue densities of the other pools |
pools often publish this data to demonstrate their honesty to |
often publish this data to demonstrate their honesty to their |
publish this data to demonstrate their honesty to their miners |
a pool can infiltrate each of the other pools with |
pool can infiltrate each of the other pools with some |
can infiltrate each of the other pools with some nominal |
infiltrate each of the other pools with some nominal probing |
each of the other pools with some nominal probing mining |
of the other pools with some nominal probing mining power |
the other pools with some nominal probing mining power and |
other pools with some nominal probing mining power and measure |
pools with some nominal probing mining power and measure the |
with some nominal probing mining power and measure the revenue |
some nominal probing mining power and measure the revenue density |
nominal probing mining power and measure the revenue density directly |
probing mining power and measure the revenue density directly by |
mining power and measure the revenue density directly by monitoring |
power and measure the revenue density directly by monitoring the |
and measure the revenue density directly by monitoring the probe |
measure the revenue density directly by monitoring the probe s |
the revenue density directly by monitoring the probe s rewards |
revenue density directly by monitoring the probe s rewards from |
density directly by monitoring the probe s rewards from the |
directly by monitoring the probe s rewards from the pool |
As in the case of classical block withholding explained in |
in the case of classical block withholding explained in Section |
the case of classical block withholding explained in Section II |
but cannot detect which of its miners is the attacker |
various techniques can be used to encourage miners to submit |
techniques can be used to encourage miners to submit full |
can be used to encourage miners to submit full blocks |
A pool can pay a bonus for submitting a full |
pool can pay a bonus for submitting a full proof |
can pay a bonus for submitting a full proof of |
pay a bonus for submitting a full proof of work |
This would increase the revenue of the miner that found |
would increase the revenue of the miner that found a |
increase the revenue of the miner that found a block |
the revenue of the miner that found a block while |
revenue of the miner that found a block while reducing |
of the miner that found a block while reducing the |
the miner that found a block while reducing the revenue |
miner that found a block while reducing the revenue of |
that found a block while reducing the revenue of the |
found a block while reducing the revenue of the other |
a block while reducing the revenue of the other miners |
block while reducing the revenue of the other miners from |
while reducing the revenue of the other miners from this |
reducing the revenue of the other miners from this block |
While the average revenue of each miner would stay the |
the average revenue of each miner would stay the same |
Another approach is to introduce a joining fee by paying |
approach is to introduce a joining fee by paying new |
is to introduce a joining fee by paying new miners |
to introduce a joining fee by paying new miners less |
introduce a joining fee by paying new miners less for |
a joining fee by paying new miners less for their |
joining fee by paying new miners less for their work |
fee by paying new miners less for their work until |
by paying new miners less for their work until they |
paying new miners less for their work until they have |
new miners less for their work until they have established |
miners less for their work until they have established a |
less for their work until they have established a reputation |
for their work until they have established a reputation with |
their work until they have established a reputation with the |
work until they have established a reputation with the pool |
Miners that seek flexibility may not accept this policy and |
that seek flexibility may not accept this policy and choose |
seek flexibility may not accept this policy and choose another |
flexibility may not accept this policy and choose another pool |
the pool can use a honeypot trap by sending the |
pool can use a honeypot trap by sending the miners |
can use a honeypot trap by sending the miners tasks |
use a honeypot trap by sending the miners tasks which |
a honeypot trap by sending the miners tasks which it |
honeypot trap by sending the miners tasks which it knows |
trap by sending the miners tasks which it knows will |
by sending the miners tasks which it knows will result |
sending the miners tasks which it knows will result in |
the miners tasks which it knows will result in a |
miners tasks which it knows will result in a full |
tasks which it knows will result in a full proof |
which it knows will result in a full proof of |
it knows will result in a full proof of work |
If a miner fails to submit the full proof of |
a miner fails to submit the full proof of work |
miner fails to submit the full proof of work it |
fails to submit the full proof of work it is |
to submit the full proof of work it is tagged |
submit the full proof of work it is tagged as |
the full proof of work it is tagged as an |
full proof of work it is tagged as an attacker |
Pools can also incorporate out of band mechanisms to deter |
can also incorporate out of band mechanisms to deter attacks |
such as verifying the identity of miners or using trusted |
as verifying the identity of miners or using trusted computing |
verifying the identity of miners or using trusted computing technologies |
This would require miners to use specialized hardware and software |
all these techniques reduce the pool s attractiveness and deter |
these techniques reduce the pool s attractiveness and deter miners |
Block Withholding Recycling We assume that the infiltrating miners are |
Withholding Recycling We assume that the infiltrating miners are loyal |
Recycling We assume that the infiltrating miners are loyal to |
We assume that the infiltrating miners are loyal to the |
assume that the infiltrating miners are loyal to the attacker |
some of the pool s members may be disloyal infiltrators |
When sending disloyal miners to perform block withholding at other |
sending disloyal miners to perform block withholding at other pools |
a pool needs a sufficient number of verified miners miners |
pool needs a sufficient number of verified miners miners that |
needs a sufficient number of verified miners miners that it |
a sufficient number of verified miners miners that it knows |
sufficient number of verified miners miners that it knows to |
number of verified miners miners that it knows to be |
of verified miners miners that it knows to be loyal |
but this is only in extreme cases when pools are |
this is only in extreme cases when pools are large |
pools typically have loyal mining power either run directly by |
typically have loyal mining power either run directly by the |
have loyal mining power either run directly by the pool |
loyal mining power either run directly by the pool owners |
mining power either run directly by the pool owners or |
power either run directly by the pool owners or sold |
either run directly by the pool owners or sold as |
run directly by the pool owners or sold as a |
directly by the pool owners or sold as a service |
by the pool owners or sold as a service but |
the pool owners or sold as a service but run |
pool owners or sold as a service but run on |
owners or sold as a service but run on the |
or sold as a service but run on the pool |
sold as a service but run on the pool owners |
as a service but run on the pool owners hardware |
However the size of this mining power is considered a |
the size of this mining power is considered a trade |
size of this mining power is considered a trade secret |
of this mining power is considered a trade secret and |
this mining power is considered a trade secret and is |
mining power is considered a trade secret and is not |
power is considered a trade secret and is not published |
Block Withholding in Practice Long term block withholding attacks are |
Withholding in Practice Long term block withholding attacks are difficult |
in Practice Long term block withholding attacks are difficult to |
Practice Long term block withholding attacks are difficult to hide |
since miners using an attacked pool would notice the reduced |
miners using an attacked pool would notice the reduced revenue |
using an attacked pool would notice the reduced revenue density |
and we can therefore conclude that they are indeed rare |
A recent exception is an attack on the Eligius pool |
distance link with and without intermediary Maelstrom proxies and measuring |
recent exception is an attack on the Eligius pool performed |
link with and without intermediary Maelstrom proxies and measuring obtained |
exception is an attack on the Eligius pool performed in |
with and without intermediary Maelstrom proxies and measuring obtained throughput |
is an attack on the Eligius pool performed in May |
and without intermediary Maelstrom proxies and measuring obtained throughput while |
an attack on the Eligius pool performed in May and |
without intermediary Maelstrom proxies and measuring obtained throughput while varying |
attack on the Eligius pool performed in May and June |
intermediary Maelstrom proxies and measuring obtained throughput while varying loss |
Maelstrom proxies and measuring obtained throughput while varying loss rate |
The error bars on the graphs to the left are |
error bars on the graphs to the left are standard |
bars on the graphs to the left are standard errors |
on the graphs to the left are standard errors of |
the graphs to the left are standard errors of the |
graphs to the left are standard errors of the throughput |
to the left are standard errors of the throughput over |
the left are standard errors of the throughput over ten |
left are standard errors of the throughput over ten runs |
IP s cache of tuning parameters to allow for repeatable |
s cache of tuning parameters to allow for repeatable results |
more Bitcoin before realizing they were not receiving their payout |
The reasons the attack was so easily subverted is the |
reasons the attack was so easily subverted is the limited |
the attack was so easily subverted is the limited efforts |
attack was so easily subverted is the limited efforts of |
was so easily subverted is the limited efforts of the |
so easily subverted is the limited efforts of the attackers |
easily subverted is the limited efforts of the attackers to |
subverted is the limited efforts of the attackers to hide |
is the limited efforts of the attackers to hide themselves |
They have only used two payout addresses to collect their |
have only used two payout addresses to collect their payouts |
and so it was possible for the alert pool manager |
so it was possible for the alert pool manager to |
it was possible for the alert pool manager to cluster |
was possible for the alert pool manager to cluster the |
possible for the alert pool manager to cluster the attacking |
for the alert pool manager to cluster the attacking miners |
the alert pool manager to cluster the attacking miners and |
alert pool manager to cluster the attacking miners and obtain |
pool manager to cluster the attacking miners and obtain a |
manager to cluster the attacking miners and obtain a statistically |
to cluster the attacking miners and obtain a statistically significant |
cluster the attacking miners and obtain a statistically significant proof |
the attacking miners and obtain a statistically significant proof of |
attacking miners and obtain a statistically significant proof of their |
miners and obtain a statistically significant proof of their wrongdoing |
It is unknown whether this was a classical block withholding |
is unknown whether this was a classical block withholding attack |
second iperf flow from one node to another with and |
iperf flow from one node to another with and without |
flow from one node to another with and without Maelstrom |
from one node to another with and without Maelstrom running |
one node to another with and without Maelstrom running on |
node to another with and without Maelstrom running on the |
implemented an experimental Bitcoin test network and demonstrated the practicality |
to another with and without Maelstrom running on the routers |
an experimental Bitcoin test network and demonstrated the practicality of |
another with and without Maelstrom running on the routers and |
experimental Bitcoin test network and demonstrated the practicality of the |
with and without Maelstrom running on the routers and measuring |
Bitcoin test network and demonstrated the practicality of the attack |
and without Maelstrom running on the routers and measuring throughput |
without Maelstrom running on the routers and measuring throughput while |
Maelstrom running on the routers and measuring throughput while varying |
running on the routers and measuring throughput while varying the |
on the routers and measuring throughput while varying the random |
Bitcoin s Health Large pools hinder Bitcoin s distributed nature |
the routers and measuring throughput while varying the random loss |
s Health Large pools hinder Bitcoin s distributed nature as |
routers and measuring throughput while varying the random loss rate |
Health Large pools hinder Bitcoin s distributed nature as they |
and measuring throughput while varying the random loss rate on |
Large pools hinder Bitcoin s distributed nature as they put |
measuring throughput while varying the random loss rate on the |
pools hinder Bitcoin s distributed nature as they put a |
throughput while varying the random loss rate on the link |
hinder Bitcoin s distributed nature as they put a lot |
while varying the random loss rate on the link and |
Bitcoin s distributed nature as they put a lot of |
varying the random loss rate on the link and the |
s distributed nature as they put a lot of mining |
the random loss rate on the link and the one |
distributed nature as they put a lot of mining power |
nature as they put a lot of mining power in |
as they put a lot of mining power in the |
they put a lot of mining power in the hands |
put a lot of mining power in the hands of |
a lot of mining power in the hands of a |
we ran eight parallel iperf flows from one node to |
lot of mining power in the hands of a few |
ran eight parallel iperf flows from one node to another |
of mining power in the hands of a few pool |
eight parallel iperf flows from one node to another for |
mining power in the hands of a few pool managers |
This has been mostly addressed by community pressure on miners |
has been mostly addressed by community pressure on miners to |
been mostly addressed by community pressure on miners to avoid |
mostly addressed by community pressure on miners to avoid forming |
The curves obtained from the two versions are almost identical |
addressed by community pressure on miners to avoid forming large |
by community pressure on miners to avoid forming large pools |
we present both to show that the kernel version successfully |
present both to show that the kernel version successfully scales |
both to show that the kernel version successfully scales up |
to show that the kernel version successfully scales up the |
show that the kernel version successfully scales up the performance |
that the kernel version successfully scales up the performance of |
the kernel version successfully scales up the performance of the |
kernel version successfully scales up the performance of the user |
and mining is still dominated by a small number of |
space version to hundreds of megabits of traffic per second |
mining is still dominated by a small number of large |
is still dominated by a small number of large pools |
The fact that block withholding attacks are rarely observed may |
fact that block withholding attacks are rarely observed may indicate |
that block withholding attacks are rarely observed may indicate that |
block withholding attacks are rarely observed may indicate that the |
withholding attacks are rarely observed may indicate that the active |
attacks are rarely observed may indicate that the active pools |
are rarely observed may indicate that the active pools have |
with the kernel version achieving two orders of magnitude higher |
rarely observed may indicate that the active pools have reached |
the kernel version achieving two orders of magnitude higher throughput |
observed may indicate that the active pools have reached an |
kernel version achieving two orders of magnitude higher throughput that |
may indicate that the active pools have reached an implicit |
version achieving two orders of magnitude higher throughput that conventional |
indicate that the active pools have reached an implicit or |
achieving two orders of magnitude higher throughput that conventional TCP |
that the active pools have reached an implicit or explicit |
the active pools have reached an implicit or explicit agreement |
active pools have reached an implicit or explicit agreement not |
pools have reached an implicit or explicit agreement not to |
have reached an implicit or explicit agreement not to attack |
reached an implicit or explicit agreement not to attack one |
an implicit or explicit agreement not to attack one another |
an attacked pool cannot detect which of its miners are |
attacked pool cannot detect which of its miners are attacking |
pool cannot detect which of its miners are attacking it |
IP throughput declining on a link of increasing length when |
throughput declining on a link of increasing length when subjected |
declining on a link of increasing length when subjected to |
on a link of increasing length when subjected to uniform |
At some point a pool might miscalculate and decide to |
a link of increasing length when subjected to uniform loss |
some point a pool might miscalculate and decide to try |
link of increasing length when subjected to uniform loss rates |
point a pool might miscalculate and decide to try to |
of increasing length when subjected to uniform loss rates of |
a pool might miscalculate and decide to try to increase |
pool might miscalculate and decide to try to increase its |
might miscalculate and decide to try to increase its revenue |
possibly leading to a constant rate of attacks among pools |
leading to a constant rate of attacks among pools and |
to a constant rate of attacks among pools and a |
a constant rate of attacks among pools and a reduced |
constant rate of attacks among pools and a reduced revenue |
The top line in the graphs is the performance of |
top line in the graphs is the performance of TCP |
If open pools reach a state where their revenue density |
open pools reach a state where their revenue density is |
IP without loss and provides an upper bound for performance |
pools reach a state where their revenue density is reduced |
without loss and provides an upper bound for performance on |
reach a state where their revenue density is reduced due |
loss and provides an upper bound for performance on the |
a state where their revenue density is reduced due to |
and provides an upper bound for performance on the link |
state where their revenue density is reduced due to attacks |
miners will leave them in favor of other available options |
Maelstrom masks packet loss and tracks the lossless line closely |
lagging only when the link latency is low and TCP |
Such a change may be in favor of Bitcoin as |
a change may be in favor of Bitcoin as a |
change may be in favor of Bitcoin as a whole |
and form a fine grained distribution of mining power with |
form a fine grained distribution of mining power with many |
a fine grained distribution of mining power with many small |
fine grained distribution of mining power with many small pools |
grained distribution of mining power with many small pools and |
distribution of mining power with many small pools and solo |
of mining power with many small pools and solo miners |
A pool may engage in an attack against another pool |
pool may engage in an attack against another pool not |
may engage in an attack against another pool not to |
engage in an attack against another pool not to increase |
in an attack against another pool not to increase its |
an attack against another pool not to increase its absolute |
attack against another pool not to increase its absolute revenue |
but rather to attract miners by temporarily increasing its revenue |
rather to attract miners by temporarily increasing its revenue relative |
to attract miners by temporarily increasing its revenue relative to |
attract miners by temporarily increasing its revenue relative to a |
miners by temporarily increasing its revenue relative to a competing |
by temporarily increasing its revenue relative to a competing pool |
Recent work has investigated the motivation of pools to utilize |
work has investigated the motivation of pools to utilize part |
has investigated the motivation of pools to utilize part of |
investigated the motivation of pools to utilize part of their |
the motivation of pools to utilize part of their resources |
motivation of pools to utilize part of their resources towards |
of pools to utilize part of their resources towards sabotage |
pools to utilize part of their resources towards sabotage attacks |
to utilize part of their resources towards sabotage attacks against |
utilize part of their resources towards sabotage attacks against each |
part of their resources towards sabotage attacks against each other |
The model of those works is different from the pool |
model of those works is different from the pool game |
of those works is different from the pool game model |
those works is different from the pool game model in |
works is different from the pool game model in two |
is different from the pool game model in two major |
different from the pool game model in two major ways |
from the pool game model in two major ways a |
the pool game model in two major ways a sabotage |
pool game model in two major ways a sabotage attack |
game model in two major ways a sabotage attack does |
model in two major ways a sabotage attack does not |
in two major ways a sabotage attack does not transfer |
two major ways a sabotage attack does not transfer revenue |
major ways a sabotage attack does not transfer revenue from |
ways a sabotage attack does not transfer revenue from victim |
a sabotage attack does not transfer revenue from victim to |
sabotage attack does not transfer revenue from victim to attacker |
The model is parametrized by the cost of the attack |
model is parametrized by the cost of the attack and |
is parametrized by the cost of the attack and by |
parametrized by the cost of the attack and by the |
by the cost of the attack and by the mobility |
the cost of the attack and by the mobility of |
cost of the attack and by the mobility of the |
of the attack and by the mobility of the miners |
and the analysis demonstrates that when considering only sabotage attacks |
the analysis demonstrates that when considering only sabotage attacks there |
analysis demonstrates that when considering only sabotage attacks there are |
demonstrates that when considering only sabotage attacks there are regions |
that when considering only sabotage attacks there are regions where |
when considering only sabotage attacks there are regions where no |
The miner s dilemma is therefore not manifested in that |
miner s dilemma is therefore not manifested in that model |
Pool competition for miners is an incentive in and of |
competition for miners is an incentive in and of its |
for miners is an incentive in and of its own |
miners is an incentive in and of its own for |
is an incentive in and of its own for mutual |
an incentive in and of its own for mutual attacks |
and a pool may therefore choose to perform block withholding |
a pool may therefore choose to perform block withholding even |
pool may therefore choose to perform block withholding even if |
may therefore choose to perform block withholding even if its |
therefore choose to perform block withholding even if its revenue |
choose to perform block withholding even if its revenue would |
to perform block withholding even if its revenue would increase |
perform block withholding even if its revenue would increase only |
block withholding even if its revenue would increase only after |
withholding even if its revenue would increase only after the |
even if its revenue would increase only after the next |
if its revenue would increase only after the next difficult |
its revenue would increase only after the next difficult adjustment |
the analysis of their combination is left for future work |
We assumed in our analysis that pools do not charge |
assumed in our analysis that pools do not charge fees |
in our analysis that pools do not charge fees from |
our analysis that pools do not charge fees from their |
analysis that pools do not charge fees from their members |
that pools do not charge fees from their members since |
pools do not charge fees from their members since such |
do not charge fees from their members since such fees |
not charge fees from their members since such fees are |
charge fees from their members since such fees are typically |
fees from their members since such fees are typically nominal |
Fees would add a friction element to the flow of |
would add a friction element to the flow of revenue |
add a friction element to the flow of revenue among |
a friction element to the flow of revenue among infiltrated |
friction element to the flow of revenue among infiltrated and |
element to the flow of revenue among infiltrated and infiltrating |
to the flow of revenue among infiltrated and infiltrating pools |
would change to take into account a pool fee of |
change to take into account a pool fee of f |
to take into account a pool fee of f Pp |
take into account a pool fee of f Pp Ri |
A pool with a fee of f is a less |
pool with a fee of f is a less attractive |
with a fee of f is a less attractive target |
a fee of f is a less attractive target for |
fee of f is a less attractive target for block |
of f is a less attractive target for block withholding |
However it is also less attractive for miners in general |
Trading off the two for best protection is left for |
off the two for best protection is left for future |
the two for best protection is left for future work |
The Block Withholding Attack The danger of a block withholding |
Block Withholding Attack The danger of a block withholding attack |
Withholding Attack The danger of a block withholding attack is |
Attack The danger of a block withholding attack is as |
The danger of a block withholding attack is as old |
danger of a block withholding attack is as old as |
of a block withholding attack is as old as Bitcoin |
a block withholding attack is as old as Bitcoin pools |
as pools were becoming a dominant player in the Bitcoin |
pools were becoming a dominant player in the Bitcoin world |
used by a miner to sabotage a pool at the |
by a miner to sabotage a pool at the cost |
a miner to sabotage a pool at the cost of |
miner to sabotage a pool at the cost of reducing |
to sabotage a pool at the cost of reducing its |
sabotage a pool at the cost of reducing its own |
a pool at the cost of reducing its own revenue |
A more general view of fairness in proof of work |
more general view of fairness in proof of work schemes |
general view of fairness in proof of work schemes was |
view of fairness in proof of work schemes was discussed |
of fairness in proof of work schemes was discussed in |
Early work did not address the possibility of pools infiltrating |
work did not address the possibility of pools infiltrating other |
did not address the possibility of pools infiltrating other pools |
not address the possibility of pools infiltrating other pools for |
address the possibility of pools infiltrating other pools for block |
the possibility of pools infiltrating other pools for block withholding |
experimentally demonstrate that block withholding can increase the attacker s |
demonstrate that block withholding can increase the attacker s revenue |
we had to set the MTU of the entire path |
had to set the MTU of the entire path to |
to set the MTU of the entire path to be |
set the MTU of the entire path to be the |
the MTU of the entire path to be the maximum |
have recently noted that a pool can increase its overall |
recently noted that a pool can increase its overall revenue |
noted that a pool can increase its overall revenue with |
that a pool can increase its overall revenue with block |
a pool can increase its overall revenue with block withholding |
pool can increase its overall revenue with block withholding if |
can increase its overall revenue with block withholding if all |
increase its overall revenue with block withholding if all other |
which meant that the longhaul link had the same MTU |
its overall revenue with block withholding if all other mining |
meant that the longhaul link had the same MTU as |
overall revenue with block withholding if all other mining is |
that the longhaul link had the same MTU as the |
revenue with block withholding if all other mining is performed |
the longhaul link had the same MTU as the inter |
with block withholding if all other mining is performed by |
block withholding if all other mining is performed by honest |
withholding if all other mining is performed by honest pools |
This resulted in the fragmentation of repair packets sent over |
resulted in the fragmentation of repair packets sent over UDP |
We consider the general case where not all mining is |
in the fragmentation of repair packets sent over UDP on |
consider the general case where not all mining is performed |
the fragmentation of repair packets sent over UDP on the |
the general case where not all mining is performed through |
fragmentation of repair packets sent over UDP on the long |
general case where not all mining is performed through public |
case where not all mining is performed through public pools |
Since the loss of a single fragment resulted in the |
the loss of a single fragment resulted in the loss |
loss of a single fragment resulted in the loss of |
of a single fragment resulted in the loss of the |
a single fragment resulted in the loss of the repair |
we observed a higher loss rate for repairs than for |
observed a higher loss rate for repairs than for data |
for the special case analyzed there and our results can |
a higher loss rate for repairs than for data packets |
the special case analyzed there and our results can be |
special case analyzed there and our results can be explained |
case analyzed there and our results can be explained by |
analyzed there and our results can be explained by the |
we expect performance to be better on a network where |
there and our results can be explained by the strong |
expect performance to be better on a network where the |
and our results can be explained by the strong approximations |
performance to be better on a network where the MTU |
our results can be explained by the strong approximations in |
to be better on a network where the MTU of |
results can be explained by the strong approximations in that |
be better on a network where the MTU of the |
can be explained by the strong approximations in that work |
better on a network where the MTU of the long |
haul link is truly larger than the MTU within each |
we calculate exactly how infiltrating miners reduce the revenue density |
link is truly larger than the MTU within each cluster |
calculate exactly how infiltrating miners reduce the revenue density of |
exactly how infiltrating miners reduce the revenue density of the |
how infiltrating miners reduce the revenue density of the infiltrated |
infiltrating miners reduce the revenue density of the infiltrated pool |
Temporary Block Withholding In the block withholding attack discussed in |
Block Withholding In the block withholding attack discussed in this |
Withholding In the block withholding attack discussed in this work |
In the block withholding attack discussed in this work the |
the block withholding attack discussed in this work the withheld |
block withholding attack discussed in this work the withheld blocks |
withholding attack discussed in this work the withheld blocks are |
attack discussed in this work the withheld blocks are never |
discussed in this work the withheld blocks are never published |
A miner or a pool can perform a selfish mining |
miner or a pool can perform a selfish mining attack |
Mbps flow alongside on the same link to simulate a |
flow alongside on the same link to simulate a real |
With selfish mining the attacker increases its revenue by temporarily |
selfish mining the attacker increases its revenue by temporarily withholding |
mining the attacker increases its revenue by temporarily withholding its |
the attacker increases its revenue by temporarily withholding its blocks |
attacker increases its revenue by temporarily withholding its blocks and |
increases its revenue by temporarily withholding its blocks and publishing |
its revenue by temporarily withholding its blocks and publishing them |
revenue by temporarily withholding its blocks and publishing them in |
by temporarily withholding its blocks and publishing them in response |
temporarily withholding its blocks and publishing them in response to |
withholding its blocks and publishing them in response to block |
its blocks and publishing them in response to block publication |
blocks and publishing them in response to block publication by |
and publishing them in response to block publication by other |
publishing them in response to block publication by other pools |
them in response to block publication by other pools and |
in response to block publication by other pools and miners |
This attack is independent of the block withholding attack we |
attack is independent of the block withholding attack we discuss |
shows the same scenario with a constant uniformly random loss |
is independent of the block withholding attack we discuss here |
the same scenario with a constant uniformly random loss rate |
independent of the block withholding attack we discuss here and |
same scenario with a constant uniformly random loss rate of |
of the block withholding attack we discuss here and the |
the block withholding attack we discuss here and the two |
block withholding attack we discuss here and the two can |
withholding attack we discuss here and the two can be |
attack we discuss here and the two can be performed |
we discuss here and the two can be performed in |
discuss here and the two can be performed in concert |
Maelstrom s delivery latency is almost exactly equal to the |
An attacker can also perform a double spending attack as |
s delivery latency is almost exactly equal to the one |
attacker can also perform a double spending attack as follows |
the attacker publishes the withheld block to revoke the former |
attacker publishes the withheld block to revoke the former transaction |
the spikes in latency are triggered by losses that lead |
spikes in latency are triggered by losses that lead to |
This attack is performed by miners or pools against service |
in latency are triggered by losses that lead to packets |
attack is performed by miners or pools against service providers |
latency are triggered by losses that lead to packets piling |
is performed by miners or pools against service providers that |
are triggered by losses that lead to packets piling up |
performed by miners or pools against service providers that accept |
triggered by losses that lead to packets piling up at |
by miners or pools against service providers that accept Bitcoin |
by losses that lead to packets piling up at the |
losses that lead to packets piling up at the receiver |
A key point is that we are plotting the delivery |
key point is that we are plotting the delivery latency |
point is that we are plotting the delivery latency of |
is that we are plotting the delivery latency of all |
that we are plotting the delivery latency of all packets |
where finding proof of work is the result of solution |
finding proof of work is the result of solution guessing |
proof of work is the result of solution guessing and |
IP delays correctly received packets while waiting for missing packets |
of work is the result of solution guessing and checking |
delays correctly received packets while waiting for missing packets sequenced |
correctly received packets while waiting for missing packets sequenced earlier |
All of the algorithms we are aware of are susceptible |
received packets while waiting for missing packets sequenced earlier by |
of the algorithms we are aware of are susceptible to |
packets while waiting for missing packets sequenced earlier by the |
the algorithms we are aware of are susceptible to the |
while waiting for missing packets sequenced earlier by the sender |
algorithms we are aware of are susceptible to the block |
waiting for missing packets sequenced earlier by the sender the |
we are aware of are susceptible to the block withholding |
for missing packets sequenced earlier by the sender the effect |
are aware of are susceptible to the block withholding attack |
missing packets sequenced earlier by the sender the effect of |
packets sequenced earlier by the sender the effect of this |
as in all of them the miner can check whether |
sequenced earlier by the sender the effect of this is |
in all of them the miner can check whether she |
earlier by the sender the effect of this is shown |
all of them the miner can check whether she found |
by the sender the effect of this is shown in |
of them the miner can check whether she found a |
the sender the effect of this is shown in Figure |
them the miner can check whether she found a full |
the miner can check whether she found a full or |
miner can check whether she found a full or a |
can check whether she found a full or a partial |
check whether she found a full or a partial proof |
whether she found a full or a partial proof of |
where single packet losses cause spikes in delivery latency that |
she found a full or a partial proof of work |
single packet losses cause spikes in delivery latency that last |
packet losses cause spikes in delivery latency that last for |
losses cause spikes in delivery latency that last for hundreds |
cause spikes in delivery latency that last for hundreds of |
spikes in delivery latency that last for hundreds of packets |
which implements fast recovery and halves the congestion window on |
implements fast recovery and halves the congestion window on packet |
It is possible to use an alternative proof of work |
fast recovery and halves the congestion window on packet loss |
is possible to use an alternative proof of work mechanism |
recovery and halves the congestion window on packet loss rather |
possible to use an alternative proof of work mechanism in |
and halves the congestion window on packet loss rather than |
to use an alternative proof of work mechanism in which |
halves the congestion window on packet loss rather than resetting |
use an alternative proof of work mechanism in which miners |
the congestion window on packet loss rather than resetting it |
an alternative proof of work mechanism in which miners would |
congestion window on packet loss rather than resetting it completely |
alternative proof of work mechanism in which miners would not |
proof of work mechanism in which miners would not be |
of work mechanism in which miners would not be able |
work mechanism in which miners would not be able to |
mechanism in which miners would not be able to distinguish |
in which miners would not be able to distinguish partial |
which miners would not be able to distinguish partial from |
miners would not be able to distinguish partial from full |
would not be able to distinguish partial from full proofs |
not be able to distinguish partial from full proofs of |
be able to distinguish partial from full proofs of work |
Such a solution could reduce or remove the danger of |
a solution could reduce or remove the danger of block |
solution could reduce or remove the danger of block withholding |
making such a change may not be in the interest |
such a change may not be in the interest of |
a change may not be in the interest of the |
change may not be in the interest of the community |
But the question of whether a pool is run by |
the question of whether a pool is run by a |
question of whether a pool is run by a centralized |
of whether a pool is run by a centralized manager |
whether a pool is run by a centralized manager or |
a pool is run by a centralized manager or with |
pool is run by a centralized manager or with a |
is run by a centralized manager or with a decentralized |
run by a centralized manager or with a decentralized architecture |
by a centralized manager or with a decentralized architecture is |
a centralized manager or with a decentralized architecture is almost |
centralized manager or with a decentralized architecture is almost immaterial |
manager or with a decentralized architecture is almost immaterial for |
or with a decentralized architecture is almost immaterial for the |
with a decentralized architecture is almost immaterial for the attack |
a decentralized architecture is almost immaterial for the attack we |
decentralized architecture is almost immaterial for the attack we describe |
Pool code can be changed to support attacks against other |
code can be changed to support attacks against other pools |
Pool can be used by groups of miners to easily |
can be used by groups of miners to easily form |
be used by groups of miners to easily form closed |
used by groups of miners to easily form closed pools |
C ONCLUSION We explored a block withholding attack among Bitcoin |
ONCLUSION We explored a block withholding attack among Bitcoin mining |
We explored a block withholding attack among Bitcoin mining pools |
explored a block withholding attack among Bitcoin mining pools an |
a block withholding attack among Bitcoin mining pools an attack |
block withholding attack among Bitcoin mining pools an attack that |
withholding attack among Bitcoin mining pools an attack that is |
attack among Bitcoin mining pools an attack that is possible |
among Bitcoin mining pools an attack that is possible in |
Bitcoin mining pools an attack that is possible in any |
mining pools an attack that is possible in any similar |
pools an attack that is possible in any similar system |
an attack that is possible in any similar system that |
attack that is possible in any similar system that rewards |
that is possible in any similar system that rewards for |
is possible in any similar system that rewards for proof |
possible in any similar system that rewards for proof of |
in any similar system that rewards for proof of work |
a pool can increase its revenue by attacking the others |
Layered Interleaving and Bursty Loss Thus far we have shown |
Interleaving and Bursty Loss Thus far we have shown how |
and Bursty Loss Thus far we have shown how Maelstrom |
Bursty Loss Thus far we have shown how Maelstrom effectively |
Loss Thus far we have shown how Maelstrom effectively hides |
Thus far we have shown how Maelstrom effectively hides loss |
far we have shown how Maelstrom effectively hides loss from |
we have shown how Maelstrom effectively hides loss from TCP |
and it can retaliate by attacking and increase its revenue |
at Nash equilibrium both earn less than they would have |
Nash equilibrium both earn less than they would have if |
equilibrium both earn less than they would have if neither |
We use a loss model where packets are dropped in |
both earn less than they would have if neither attacked |
use a loss model where packets are dropped in bursts |
a loss model where packets are dropped in bursts of |
With multiple pools of equal size a similar situation arises |
loss model where packets are dropped in bursts of fixed |
multiple pools of equal size a similar situation arises with |
model where packets are dropped in bursts of fixed length |
pools of equal size a similar situation arises with a |
of equal size a similar situation arises with a symmetric |
allowing us to study the impact of burst length on |
equal size a similar situation arises with a symmetric equilibrium |
us to study the impact of burst length on performance |
The fact that block withholding is not common may be |
fact that block withholding is not common may be explained |
that block withholding is not common may be explained by |
block withholding is not common may be explained by modeling |
withholding is not common may be explained by modeling the |
is not common may be explained by modeling the attack |
not common may be explained by modeling the attack decisions |
common may be explained by modeling the attack decisions as |
may be explained by modeling the attack decisions as an |
be explained by modeling the attack decisions as an iterative |
explained by modeling the attack decisions as an iterative prisoner |
by modeling the attack decisions as an iterative prisoner s |
modeling the attack decisions as an iterative prisoner s dilemma |
we argue that the situation is unstable since the attack |
argue that the situation is unstable since the attack can |
that the situation is unstable since the attack can be |
the situation is unstable since the attack can be done |
situation is unstable since the attack can be done anonymously |
one pool may decide to increase its revenue and drag |
pool may decide to increase its revenue and drag the |
may decide to increase its revenue and drag the others |
decide to increase its revenue and drag the others to |
to increase its revenue and drag the others to attack |
increase its revenue and drag the others to attack as |
its revenue and drag the others to attack as well |
is correct for high loss rates if the interleaves are |
correct for high loss rates if the interleaves are relatively |
The inferior revenue would push miners to join private pools |
for high loss rates if the interleaves are relatively prime |
which can verify that their registered miners do not withhold |
can verify that their registered miners do not withhold blocks |
performance improves substantially when loss rates are high and losses |
improves substantially when loss rates are high and losses are |
substantially when loss rates are high and losses are bursty |
and so ultimately to a better environment for Bitcoin as |
so ultimately to a better environment for Bitcoin as a |
The graph plots the percentage of lost packets successfully recovered |
ultimately to a better environment for Bitcoin as a whole |
graph plots the percentage of lost packets successfully recovered on |
plots the percentage of lost packets successfully recovered on the |
the percentage of lost packets successfully recovered on the y |
we show the ability of layered interleaving to provide gracefully |
show the ability of layered interleaving to provide gracefully degrading |
the ability of layered interleaving to provide gracefully degrading performance |
ability of layered interleaving to provide gracefully degrading performance in |
of layered interleaving to provide gracefully degrading performance in the |
layered interleaving to provide gracefully degrading performance in the face |
interleaving to provide gracefully degrading performance in the face of |
to provide gracefully degrading performance in the face of bursty |
provide gracefully degrading performance in the face of bursty loss |
we plot the percentage of lost packets successfully recovered against |
plot the percentage of lost packets successfully recovered against the |
the percentage of lost packets successfully recovered against the length |
percentage of lost packets successfully recovered against the length of |
of lost packets successfully recovered against the length of loss |
lost packets successfully recovered against the length of loss bursts |
packets successfully recovered against the length of loss bursts for |
successfully recovered against the length of loss bursts for two |
recovered against the length of loss bursts for two different |
against the length of loss bursts for two different sets |
the length of loss bursts for two different sets of |
length of loss bursts for two different sets of interleaves |
and in the bottom graph we plot the average latency |
in the bottom graph we plot the average latency at |
the bottom graph we plot the average latency at which |
bottom graph we plot the average latency at which the |
graph we plot the average latency at which the packets |
we plot the average latency at which the packets were |
plot the average latency at which the packets were recovered |
Recovery latency is defined as the difference between the eventual |
latency is defined as the difference between the eventual delivery |
is defined as the difference between the eventual delivery time |
defined as the difference between the eventual delivery time of |
as the difference between the eventual delivery time of the |
the difference between the eventual delivery time of the recovered |
difference between the eventual delivery time of the recovered packet |
between the eventual delivery time of the recovered packet and |
the eventual delivery time of the recovered packet and the |
eventual delivery time of the recovered packet and the one |
we confirmed that the Emulab link had almost no jitter |
confirmed that the Emulab link had almost no jitter on |
that the Emulab link had almost no jitter on correctly |
the Emulab link had almost no jitter on correctly delivered |
Emulab link had almost no jitter on correctly delivered packets |
way latency an accurate estimate of expected lossless delivery time |
increasing the interleaves results in much higher recovery percentages at |
the interleaves results in much higher recovery percentages at large |
interleaves results in much higher recovery percentages at large burst |
results in much higher recovery percentages at large burst sizes |
in Proceedings of the IEEE Symposium on Security and Privacy |
Layered Interleaving Recovery Percentage and Latency comes at the cost |
Interleaving Recovery Percentage and Latency comes at the cost of |
Recovery Percentage and Latency comes at the cost of higher |
Percentage and Latency comes at the cost of higher recovery |
and Latency comes at the cost of higher recovery latency |
set of interleaves catches almost all packets in an extended |
of interleaves catches almost all packets in an extended burst |
interleaves catches almost all packets in an extended burst of |
The graphs also show recovery latency rising gracefully with the |
graphs also show recovery latency rising gracefully with the increase |
also show recovery latency rising gracefully with the increase in |
show recovery latency rising gracefully with the increase in loss |
recovery latency rising gracefully with the increase in loss burst |
latency rising gracefully with the increase in loss burst length |
we show histograms of recovery latencies for the two interleave |
show histograms of recovery latencies for the two interleave configurations |
histograms of recovery latencies for the two interleave configurations under |
of recovery latencies for the two interleave configurations under different |
recovery latencies for the two interleave configurations under different burst |
latencies for the two interleave configurations under different burst lengths |
packet recoveries take longer from left to right as we |
recoveries take longer from left to right as we increase |
take longer from left to right as we increase loss |
longer from left to right as we increase loss burst |
from left to right as we increase loss burst length |
and from top to bottom as we increase the interleave |
from top to bottom as we increase the interleave values |
illustrates the difference between a traditional FEC code and layered |
the difference between a traditional FEC code and layered interleaving |
difference between a traditional FEC code and layered interleaving by |
between a traditional FEC code and layered interleaving by plotting |
a traditional FEC code and layered interleaving by plotting a |
The channel is configured to lose singleton packets randomly at |
channel is configured to lose singleton packets randomly at a |
is configured to lose singleton packets randomly at a loss |
configured to lose singleton packets randomly at a loss rate |
to lose singleton packets randomly at a loss rate of |
and consequently both have a maximum tolerable burst length of |
the code is plugged into Maelstrom instead of layered interleaving |
showing that we can use new encodings within the same |
that we can use new encodings within the same framework |
we can use new encodings within the same framework seamlessly |
Solomon code recovers all lost packets with roughly the same |
code recovers all lost packets with roughly the same latency |
recovers all lost packets with roughly the same latency whereas |
all lost packets with roughly the same latency whereas layered |
lost packets with roughly the same latency whereas layered interleaving |
packets with roughly the same latency whereas layered interleaving recovers |
with roughly the same latency whereas layered interleaving recovers singleton |
roughly the same latency whereas layered interleaving recovers singleton losses |
the same latency whereas layered interleaving recovers singleton losses almost |
same latency whereas layered interleaving recovers singleton losses almost immediately |
latency whereas layered interleaving recovers singleton losses almost immediately and |
whereas layered interleaving recovers singleton losses almost immediately and exhibits |
layered interleaving recovers singleton losses almost immediately and exhibits latency |
interleaving recovers singleton losses almost immediately and exhibits latency spikes |
recovers singleton losses almost immediately and exhibits latency spikes whenever |
singleton losses almost immediately and exhibits latency spikes whenever the |
losses almost immediately and exhibits latency spikes whenever the longer |
almost immediately and exhibits latency spikes whenever the longer loss |
immediately and exhibits latency spikes whenever the longer loss burst |
and exhibits latency spikes whenever the longer loss burst occurs |
Related Work A significant body of work on application and |
Work A significant body of work on application and TCP |
The use of parallel sockets for higher throughput in the |
use of parallel sockets for higher throughput in the face |
of parallel sockets for higher throughput in the face of |
parallel sockets for higher throughput in the face of non |
A number of protocols have been suggested as replacements for |
number of protocols have been suggested as replacements for TCP |
numerous implementations of PEPs exist for improving TCP performance on |
implementations of PEPs exist for improving TCP performance on satellite |
but we are not aware of any PEPs that use |
we are not aware of any PEPs that use FEC |
are not aware of any PEPs that use FEC to |
not aware of any PEPs that use FEC to mask |
aware of any PEPs that use FEC to mask errors |
of any PEPs that use FEC to mask errors on |
any PEPs that use FEC to mask errors on long |
based FEC for reliable communication was first explored by Rizzo |
IP retransmissions over aggregated traffic within an overlay network in |
retransmissions over aggregated traffic within an overlay network in the |
over aggregated traffic within an overlay network in the commodity |
aggregated traffic within an overlay network in the commodity Internet |
they can broadly be categorized into optimal erasure codes and |
can broadly be categorized into optimal erasure codes and near |
which we described previously as generating c repair packets from |
we described previously as generating c repair packets from r |
described previously as generating c repair packets from r source |
previously as generating c repair packets from r source packets |
c packets can be used to reconstruct the r source |
packets can be used to reconstruct the r source packets |
off encoding speed for large data sizes against a loss |
encoding speed for large data sizes against a loss of |
speed for large data sizes against a loss of optimality |
for large data sizes against a loss of optimality the |
large data sizes against a loss of optimality the receiver |
data sizes against a loss of optimality the receiver needs |
sizes against a loss of optimality the receiver needs to |
against a loss of optimality the receiver needs to receive |
a loss of optimality the receiver needs to receive slightly |
loss of optimality the receiver needs to receive slightly more |
of optimality the receiver needs to receive slightly more than |
optimality the receiver needs to receive slightly more than r |
the receiver needs to receive slightly more than r source |
receiver needs to receive slightly more than r source or |
needs to receive slightly more than r source or repair |
to receive slightly more than r source or repair packets |
receive slightly more than r source or repair packets to |
slightly more than r source or repair packets to regenerate |
more than r source or repair packets to regenerate the |
than r source or repair packets to regenerate the original |
r source or repair packets to regenerate the original r |
source or repair packets to regenerate the original r data |
or repair packets to regenerate the original r data packets |
optimal codes are extremely fast for encoding over large sets |
codes are extremely fast for encoding over large sets of |
are extremely fast for encoding over large sets of data |
extremely fast for encoding over large sets of data but |
fast for encoding over large sets of data but not |
for encoding over large sets of data but not of |
encoding over large sets of data but not of significant |
over large sets of data but not of significant importance |
large sets of data but not of significant importance for |
sets of data but not of significant importance for real |
since optimal codes perform equally well with small data sizes |
layered interleaving uses multiple interleaves for different burst resilience levels |
interleaving uses multiple interleaves for different burst resilience levels without |
uses multiple interleaves for different burst resilience levels without modulating |
multiple interleaves for different burst resilience levels without modulating the |
interleaves for different burst resilience levels without modulating the encoding |
for different burst resilience levels without modulating the encoding rate |
provides a means to gauge the impact of packet loss |
a means to gauge the impact of packet loss on |
means to gauge the impact of packet loss on TCP |
While most published studies of packet loss are based on |
most published studies of packet loss are based on the |
published studies of packet loss are based on the commodity |
studies of packet loss are based on the commodity Internet |
of packet loss are based on the commodity Internet rather |
packet loss are based on the commodity Internet rather than |
loss are based on the commodity Internet rather than highspeed |
are based on the commodity Internet rather than highspeed lambda |
based on the commodity Internet rather than highspeed lambda links |
study the Sprint backbone and make two observations that could |
the Sprint backbone and make two observations that could be |
Sprint backbone and make two observations that could be explained |
backbone and make two observations that could be explained by |
and make two observations that could be explained by non |
Future Work Scaling Maelstrom to multiple gigabits per second of |
Work Scaling Maelstrom to multiple gigabits per second of traffic |
Scaling Maelstrom to multiple gigabits per second of traffic will |
Maelstrom to multiple gigabits per second of traffic will require |
to multiple gigabits per second of traffic will require small |
multiple gigabits per second of traffic will require small rack |
style clusters of tens of machines to distribute encoding load |
clusters of tens of machines to distribute encoding load over |
and the next step in extending this protocol is to |
the next step in extending this protocol is to make |
next step in extending this protocol is to make it |
step in extending this protocol is to make it adaptive |
changing interleaves and rate as loss patterns in the link |
interleaves and rate as loss patterns in the link change |
or the commodity Internet fail to used for applications such |
the commodity Internet fail to used for applications such as |
commodity Internet fail to used for applications such as efficiently |
Internet fail to used for applications such as efficiently distributing |
fail to used for applications such as efficiently distributing bulk |
to used for applications such as efficiently distributing bulk data |
it is not obvious that these have utility in real |
protocols is not an option for commodity clusters where standardization |
is not an option for commodity clusters where standardization is |
not an option for commodity clusters where standardization is critical |
an option for commodity clusters where standardization is critical for |
option for commodity clusters where standardization is critical for cost |
for commodity clusters where standardization is critical for cost mitigation |
Maelstrom is an edge appliance that uses Forward Error Correction |
is an edge appliance that uses Forward Error Correction References |
an edge appliance that uses Forward Error Correction References to |
edge appliance that uses Forward Error Correction References to mask |
appliance that uses Forward Error Correction References to mask packet |
that uses Forward Error Correction References to mask packet loss |
uses Forward Error Correction References to mask packet loss from |
Forward Error Correction References to mask packet loss from end |
Acknowledgments We would like to thank our shepherd Robert Morris |
We would like to thank our shepherd Robert Morris and |
would like to thank our shepherd Robert Morris and the |
like to thank our shepherd Robert Morris and the other |
to thank our shepherd Robert Morris and the other reviewers |
thank our shepherd Robert Morris and the other reviewers for |
our shepherd Robert Morris and the other reviewers for extensive |
shepherd Robert Morris and the other reviewers for extensive comments |
Robert Morris and the other reviewers for extensive comments that |
Morris and the other reviewers for extensive comments that significantly |
and the other reviewers for extensive comments that significantly shaped |
the other reviewers for extensive comments that significantly shaped the |
other reviewers for extensive comments that significantly shaped the final |
reviewers for extensive comments that significantly shaped the final version |
for extensive comments that significantly shaped the final version of |
extensive comments that significantly shaped the final version of the |
comments that significantly shaped the final version of the paper |
Tom Boures provided valuable insight into the quality of existing |
Boures provided valuable insight into the quality of existing fiber |
provided valuable insight into the quality of existing fiber links |
and Paul Wefel gave us access to TeraGrid loss measurements |
On subversive miner strategies and block withholding attack in bitcoin |
subversive miner strategies and block withholding attack in bitcoin digital |
miner strategies and block withholding attack in bitcoin digital currency |
Enhanced loss differentiation algorithms for use in TCP sources over |
loss differentiation algorithms for use in TCP sources over heterogeneous |
differentiation algorithms for use in TCP sources over heterogeneous wireless |
algorithms for use in TCP sources over heterogeneous wireless networks |
end performance effects of parallel TCP sockets on a lossy |
performance effects of parallel TCP sockets on a lossy wide |
The effects of systemic packet loss on aggregate tcp flows |
level network striping for data intensive applications using high speed |
network striping for data intensive applications using high speed wide |
striping for data intensive applications using high speed wide area |
for data intensive applications using high speed wide area networks |
of the Fifth Symposium on Operating Systems Design and Implementation |
the: 6854
of: 4102
and: 3176
to: 3157
a: 3070
in: 2022
is: 1910
that: 1292
for: 1219
1: 1127
with: 948
are: 939
on: 919
The: 887
as: 822
by: 729
it: 696
be: 695
2: 643
data: 621
0: 617
we: 607
pool: 575
an: 570
at: 570
can: 570
system: 521
its: 494
s: 481
from: 473
this: 473
not: 459
or: 451
which: 418
A: 403
In: 394
file: 375
all: 355
Figure: 345
3: 339
each: 337
We: 318
4: 317
packets: 315
revenue: 315
time: 311
nodes: 307
pools: 307
other: 305
rate: 297
one: 290
have: 289
but: 288
systems: 280
5: 277
work: 270
10: 255
has: 252
such: 246
performance: 244
cache: 242
bandwidth: 237
if: 236
their: 230
will: 230
miners: 226
mining: 225
attack: 224
than: 222
only: 220
use: 219
network: 212
our: 211
packet: 209
This: 208
block: 207
loss: 207
6: 206
more: 204
8: 202
when: 200
they: 198
would: 192
number: 191
may: 187
end: 185
high: 185
where: 183
node: 180
server: 180
t: 180
two: 180
between: 178
no: 177
these: 177
used: 175
power: 174
r: 173
20: 168
different: 167
over: 166
based: 164
single: 164
services: 161
any: 159
transactions: 158
update: 158
was: 157
TCP: 156
client: 156
100: 155
7: 151
Bitcoin: 150
some: 150
IP: 149
latency: 149
D: 147
large: 145
also: 144
application: 144
them: 144
J: 143
miner: 143
updates: 141
x1: 140
i: 137
access: 136
M: 135
distributed: 135
new: 135
files: 134
set: 134
using: 133
For: 132
Maelstrom: 132
process: 129
protocol: 128
transaction: 128
MFS: 127
into: 127
rates: 127
service: 125
If: 124
example: 124
traffic: 123
applications: 122
read: 122
B: 121
C: 121
objects: 121
same: 121
repair: 118
T: 117
9: 116
chain: 116
consistency: 116
so: 116
object: 115
both: 113
were: 111
To: 110
c: 109
failure: 108
information: 108
priorities: 108
results: 108
size: 108
there: 108
40: 107
withholding: 107
database: 106
R: 104
ACM: 103
However: 103
I: 103
RPCs: 103
case: 103
11: 102
small: 102
communication: 101
could: 101
low: 101
without: 101
writes: 101
control: 100
even: 100
since: 100
RPC: 99
S: 99
gossip: 99
protocols: 99
then: 99
clients: 98
p: 98
12: 94
As: 94
writeback: 94
throughput: 93
Section: 92
Systems: 92
link: 92
does: 91
send: 91
within: 91
m1: 90
version: 90
15: 89
up: 89
content: 88
do: 88
group: 88
state: 88
user: 88
been: 87
first: 87
level: 87
log: 87
operating: 86
less: 85
memory: 85
model: 85
peer: 85
support: 85
50: 84
asynchronous: 84
out: 84
see: 84
through: 84
approach: 83
full: 83
many: 83
most: 83
proof: 83
while: 83
FEC: 82
average: 82
web: 82
200: 81
P: 81
QSM: 81
need: 81
order: 81
G: 80
Our: 80
infiltration: 80
recovery: 80
uses: 80
about: 79
against: 79
back: 79
groups: 79
message: 79
sender: 79
available: 78
disk: 78
long: 78
messages: 78
networks: 78
E: 77
possible: 77
x: 77
60: 76
disks: 76
j: 76
ms: 76
total: 76
It: 75
Windows: 75
higher: 75
increase: 75
therefore: 75
KB: 74
Web: 74
might: 74
delay: 73
experiments: 73
25: 72
accesses: 72
m2: 72
q: 72
2014: 71
80: 71
lost: 71
probability: 71
second: 71
14: 70
another: 70
architecture: 70
solution: 70
source: 70
step: 70
workloads: 70
after: 69
every: 69
value: 69
Cache: 68
When: 68
how: 68
upload: 68
write: 68
Each: 67
manager: 67
priority: 67
scale: 67
well: 67
Pool: 66
local: 66
management: 66
overhead: 66
equilibrium: 65
load: 65
run: 65
running: 65
should: 65
way: 65
30: 64
Proceedings: 64
cost: 64
due: 64
like: 64
list: 64
multiple: 64
r1: 64
show: 64
http: 63
mechanism: 63
operations: 63
partial: 63
received: 63
shown: 63
under: 63
attacking: 62
attacks: 62
because: 62
layer: 62
per: 62
point: 62
servers: 62
13: 61
An: 61
dependency: 61
must: 61
scheme: 61
shows: 61
stream: 61
attacker: 60
kernel: 60
much: 60
multicast: 60
paper: 60
proofs: 60
test: 60
At: 59
m: 59
perform: 59
research: 59
result: 59
16: 58
19: 58
provide: 58
solutions: 58
Computer: 57
analysis: 57
burst: 57
cluster: 57
code: 57
sending: 57
sent: 57
sizes: 57
very: 57
O: 56
being: 56
event: 56
maximum: 56
opportunistic: 56
round: 56
storage: 56
workload: 56
L: 55
NT: 55
reduce: 55
requests: 55
similar: 55
correct: 54
delivery: 54
game: 54
prefetching: 54
problem: 54
range: 54
replication: 54
retrieved: 54
x2: 54
Since: 53
among: 53
bitcoin: 53
cannot: 53
graph: 53
larger: 53
processes: 53
random: 53
scenario: 53
H: 52
b: 52
design: 52
interleave: 52
until: 52
18: 51
IEEE: 51
Symposium: 51
XOR: 51
according: 51
form: 51
make: 51
open: 51
present: 51
take: 51
us: 51
users: 51
able: 50
e: 50
threshold: 50
64: 49
K: 49
Loss: 49
contention: 49
had: 49
inconsistencies: 49
mechanisms: 49
real: 49
Latency: 48
One: 48
algorithm: 48
before: 48
center: 48
components: 48
either: 48
encoding: 48
fetch: 48
flow: 48
implemented: 48
mode: 48
own: 48
reads: 48
implementation: 47
important: 47
increasing: 47
n: 47
receiver: 47
side: 47
streaming: 47
three: 47
values: 47
These: 46
V: 46
caching: 46
general: 46
generated: 46
layered: 46
mi: 46
prefetch: 46
r2: 46
request: 46
store: 46
whether: 46
21: 45
University: 45
designed: 45
function: 45
operation: 45
shared: 45
significant: 45
synchronous: 45
victim: 45
window: 45
Kbps: 44
Operating: 44
attacked: 44
current: 44
host: 44
inconsistency: 44
pages: 44
smaller: 44
32: 43
Fig: 43
behavior: 43
delays: 43
download: 43
makes: 43
platform: 43
required: 43
scalable: 43
standard: 43
With: 42
X: 42
allows: 42
amount: 42
change: 42
changes: 42
component: 42
computing: 42
costs: 42
events: 42
experiment: 42
factor: 42
graphs: 42
hence: 42
seen: 42
tasks: 42
transactional: 42
300: 41
Conference: 41
Cornell: 41
F: 41
MAFS: 41
SSA: 41
blocks: 41
community: 41
density: 41
distribution: 41
hosted: 41
numbers: 41
reduced: 41
speed: 41
17: 40
2002: 40
2013: 40
W: 40
across: 40
down: 40
execution: 40
lower: 40
missing: 40
once: 40
sends: 40
strategy: 40
uniform: 40
512: 39
Services: 39
address: 39
cases: 39
cloud: 39
directly: 39
fixed: 39
future: 39
hosts: 39
increases: 39
interface: 39
mobile: 39
non: 39
now: 39
often: 39
original: 39
part: 39
simple: 39
typically: 39
128: 38
150: 38
Internet: 38
Live: 38
auditing: 38
com: 38
described: 38
entire: 38
existing: 38
receive: 38
require: 38
revenues: 38
Microsoft: 37
On: 37
System: 37
XORs: 37
al: 37
allow: 37
caches: 37
effect: 37
enough: 37
et: 37
improve: 37
losses: 37
optimal: 37
provides: 37
region: 37
section: 37
structure: 37
times: 37
1024: 36
29: 36
400: 36
Birman: 36
Data: 36
Performance: 36
better: 36
centers: 36
compared: 36
describe: 36
developers: 36
independent: 36
just: 36
loyal: 36
start: 36
types: 36
wide: 36
256: 35
28: 35
Packet: 35
clusters: 35
commodity: 35
congestion: 35
direct: 35
effective: 35
expected: 35
global: 35
https: 35
length: 35
live: 35
longer: 35
org: 35
overheads: 35
performed: 35
relative: 35
scalability: 35
still: 35
500: 34
ACID: 34
Although: 34
Peer: 34
Transactions: 34
above: 34
almost: 34
factors: 34
interleaves: 34
levels: 34
membership: 34
needs: 34
pp: 34
rather: 34
right: 34
www: 34
26: 33
End: 33
S3: 33
Y: 33
accessed: 33
adaptation: 33
constant: 33
during: 33
earlier: 33
fact: 33
interleaving: 33
links: 33
next: 33
particular: 33
previous: 33
proxy: 33
ratio: 33
sets: 33
techniques: 33
top: 33
27: 32
35: 32
P2P: 32
Recovery: 32
additional: 32
become: 32
detect: 32
environment: 32
find: 32
g: 32
including: 32
injection: 32
left: 32
limited: 32
managed: 32
overall: 32
period: 32
seconds: 32
those: 32
today: 32
token: 32
versions: 32
world: 32
2003: 31
24: 31
Distributed: 31
Miners: 31
Objects: 31
SIRP: 31
Therefore: 31
added: 31
appliance: 31
clustered: 31
codes: 31
consider: 31
detection: 31
ensure: 31
evaluate: 31
follows: 31
immediately: 31
last: 31
receivers: 31
replicated: 31
requires: 31
runs: 31
scenarios: 31
target: 31
what: 31
working: 31
23: 30
Amazon: 30
International: 30
N: 30
Network: 30
Second: 30
While: 30
achieve: 30
consistent: 30
dropped: 30
lists: 30
members: 30
performs: 30
policy: 30
receiving: 30
remote: 30
schemes: 30
share: 30
space: 30
stable: 30
1000: 29
2006: 29
Gossip: 29
New: 29
Update: 29
associated: 29
close: 29
complete: 29
delivered: 29
edu: 29
given: 29
head: 29
invalidations: 29
least: 29
receives: 29
recover: 29
related: 29
reliability: 29
settings: 29
task: 29
technology: 29
virtual: 29
assume: 28
auditors: 28
best: 28
core: 28
fast: 28
feasible: 28
further: 28
implements: 28
incoming: 28
issues: 28
k: 28
never: 28
obtain: 28
occur: 28
percentage: 28
platforms: 28
pull: 28
reliable: 28
resulting: 28
software: 28
though: 28
tools: 28
trace: 28
type: 28
2000: 27
2008: 27
2015: 27
Implementation: 27
Nov: 27
async: 27
availability: 27
build: 27
cached: 27
capacity: 27
collection: 27
commit: 27
error: 27
following: 27
here: 27
increased: 27
instance: 27
later: 27
making: 27
needed: 27
neighbors: 27
overlay: 27
prediction: 27
repository: 27
several: 27
speedup: 27
stored: 27
style: 27
varying: 27
writer: 27
01: 26
1997: 26
22: 26
41: 26
600: 26
File: 26
Finally: 26
Mbps: 26
No: 26
OS: 26
Solomon: 26
Throughput: 26
UDP: 26
becomes: 26
benefit: 26
bin: 26
certain: 26
channel: 26
configuration: 26
controls: 26
discuss: 26
fault: 26
haul: 26
however: 26
key: 26
major: 26
means: 26
reader: 26
recovered: 26
reducing: 26
resources: 26
sources: 26
tolerance: 26
First: 25
MB: 25
Nodes: 25
Principles: 25
RAIN: 25
Science: 25
Service: 25
Such: 25
TM: 25
Thus: 25
backend: 25
bursts: 25
choice: 25
conditions: 25
considered: 25
contribution: 25
copy: 25
difficulty: 25
estimate: 25
failures: 25
found: 25
invalidation: 25
minimum: 25
observed: 25
others: 25
recent: 25
reduces: 25
spent: 25
takes: 25
transport: 25
2004: 24
2007: 24
36: 24
Computing: 24
Delivery: 24
Design: 24
Ken: 24
NY: 24
Networks: 24
R1: 24
Time: 24
algorithms: 24
allowing: 24
area: 24
around: 24
bins: 24
blockchain: 24
cause: 24
closed: 24
common: 24
consumption: 24
contrast: 24
cornell: 24
correctly: 24
corresponding: 24
cs: 24
datacenters: 24
developed: 24
done: 24
entry: 24
equal: 24
front: 24
goal: 24
good: 24
infiltrating: 24
machines: 24
observe: 24
peers: 24
place: 24
ri: 24
transmission: 24
xi: 24
Block: 23
Communication: 23
Rate: 23
Reed: 23
There: 23
York: 23
bursty: 23
called: 23
choose: 23
committed: 23
create: 23
duration: 23
hardware: 23
idea: 23
include: 23
layers: 23
likely: 23
limit: 23
lock: 23
logs: 23
member: 23
mostly: 23
ones: 23
participants: 23
powered: 23
processing: 23
properties: 23
queue: 23
streams: 23
sufficient: 23
sync: 23
technologies: 23
third: 23
traditional: 23
view: 23
vol: 23
ways: 23
42: 22
All: 22
MTU: 22
Note: 22
Recovered: 22
USENIX: 22
Workshop: 22
always: 22
approaches: 22
avoid: 22
built: 22
classical: 22
collaboration: 22
connected: 22
context: 22
critical: 22
datacenter: 22
demand: 22
detected: 22
development: 22
distance: 22
flows: 22
framework: 22
generate: 22
handle: 22
individual: 22
made: 22
maintain: 22
occurs: 22
off: 22
parameters: 22
problems: 22
revision: 22
security: 22
solo: 22
starts: 22
strategies: 22
too: 22
via: 22
who: 22
00: 21
1999: 21
But: 21
Interleaving: 21
bandwidths: 21
basic: 21
buffers: 21
building: 21
certification: 21
configurations: 21
contains: 21
defined: 21
easily: 21
easy: 21
etc: 21
hand: 21
hash: 21
having: 21
identical: 21
line: 21
majority: 21
patterns: 21
response: 21
wait: 21
70: 20
Denote: 20
Figures: 20
Google: 20
III: 20
Miner: 20
Proc: 20
RACS: 20
Reliable: 20
Table: 20
They: 20
USA: 20
Unix: 20
Withholding: 20
alternative: 20
analyze: 20
benefits: 20
consists: 20
dependencies: 20
evaluation: 20
f: 20
garbage: 20
generates: 20
honest: 20
interfaces: 20
knowledge: 20
library: 20
machine: 20
manner: 20
minibrowser: 20
monitoring: 20
popular: 20
potentially: 20
presence: 20
reasons: 20
respectively: 20
short: 20
simply: 20
site: 20
stale: 20
structured: 20
traces: 20
updated: 20
video: 20
250: 19
31: 19
33: 19
34: 19
37: 19
Even: 19
Global: 19
Multicast: 19
OM: 19
SOC: 19
actually: 19
adding: 19
aggregate: 19
appropriate: 19
centralized: 19
changing: 19
course: 19
currently: 19
digital: 19
dilemma: 19
dynamic: 19
efficient: 19
expect: 19
explained: 19
extremely: 19
faster: 19
features: 19
index: 19
interest: 19
investigation: 19
issue: 19
kind: 19
know: 19
mashup: 19
mine: 19
natural: 19
old: 19
partition: 19
path: 19
potential: 19
product: 19
propagation: 19
proposed: 19
proxies: 19
quality: 19
query: 19
quickly: 19
reply: 19
report: 19
sequence: 19
significantly: 19
speeds: 19
subscribers: 19
taken: 19
works: 19
43: 18
90: 18
Also: 18
CPU: 18
Feb: 18
High: 18
Moreover: 18
Nash: 18
Of: 18
Once: 18
Pools: 18
R2: 18
Subversion: 18
aggregation: 18
background: 18
chooses: 18
clear: 18
difficult: 18
divided: 18
drop: 18
earn: 18
effectiveness: 18
employ: 18
extended: 18
failed: 18
foreground: 18
get: 18
identify: 18
includes: 18
instead: 18
itself: 18
known: 18
latencies: 18
month: 18
neither: 18
noted: 18
propose: 18
roundtrip: 18
s3vn: 18
serializability: 18
specific: 18
switching: 18
thus: 18
together: 18
tokens: 18
topics: 18
towards: 18
unique: 18
usage: 18
varied: 18
vary: 18
wireless: 18
writing: 18
2001: 17
38: 17
39: 17
Consequently: 17
Equation: 17
Error: 17
Layered: 17
RAPS: 17
Read: 17
SIGCOMM: 17
T1: 17
Z: 17
actual: 17
again: 17
automatically: 17
aware: 17
choosing: 17
chosen: 17
computer: 17
developer: 17
device: 17
devices: 17
enabled: 17
experimental: 17
focus: 17
forward: 17
four: 17
fraction: 17
functionality: 17
grows: 17
heavy: 17
highest: 17
history: 17
implement: 17
logging: 17
maintains: 17
milliseconds: 17
multi: 17
necessary: 17
offer: 17
online: 17
oriented: 17
participating: 17
partitioning: 17
periods: 17
queued: 17
ran: 17
rapidly: 17
re: 17
reach: 17
recipe: 17
remain: 17
setting: 17
social: 17
staleness: 17
successfully: 17
supported: 17
switch: 17
term: 17
transient: 17
transmitted: 17
van: 17
variety: 17
various: 17
years: 17
yet: 17
2011: 16
Analysis: 16
Burst: 16
By: 16
Coda: 16
Correction: 16
Here: 16
Research: 16
Using: 16
Vogels: 16
Work: 16
active: 16
acts: 16
already: 16
architectures: 16
axis: 16
believe: 16
buffering: 16
bytes: 16
compare: 16
configured: 16
connectivity: 16
contents: 16
created: 16
d: 16
databases: 16
demonstrate: 16
difference: 16
don: 16
environments: 16
exchange: 16
few: 16
fiber: 16
generation: 16
go: 16
hard: 16
highly: 16
i1: 16
illustrated: 16
inconsistent: 16
infiltrate: 16
infrastructure: 16
integration: 16
inter: 16
kinds: 16
lead: 16
main: 16
market: 16
modes: 16
modified: 16
module: 16
none: 16
operate: 16
periodically: 16
points: 16
private: 16
products: 16
publish: 16
publishes: 16
registered: 16
repositories: 16
serialization: 16
sharing: 16
simultaneously: 16
split: 16
standards: 16
status: 16
study: 16
sub: 16
subservice: 16
themselves: 16
timestamp: 16
topic: 16
transfer: 16
transparently: 16
typical: 16
whereas: 16
1998: 15
800: 15
ATP: 15
Adaptive: 15
Applications: 15
BC: 15
Chainsaw: 15
Communications: 15
How: 15
II: 15
June: 15
Linux: 15
Many: 15
OMs: 15
Protocol: 15
Sirer: 15
abort: 15
adapt: 15
adaptive: 15
arrive: 15
attempt: 15
clustering: 15
conference: 15
conflicts: 15
containing: 15
contribute: 15
convergence: 15
describes: 15
diff: 15
edge: 15
exists: 15
fire: 15
greater: 15
hosting: 15
impact: 15
included: 15
infiltrated: 15
initial: 15
latter: 15
linear: 15
maintaining: 15
namely: 15
optical: 15
optimize: 15
outgoing: 15
page: 15
paths: 15
progress: 15
queries: 15
regions: 15
regular: 15
represents: 15
respond: 15
sense: 15
slightly: 15
slow: 15
symmetric: 15
synthetic: 15
thousands: 15
underlying: 15
uniformly: 15
v: 15
1995: 14
Abstract: 14
After: 14
Auditing: 14
Bandwidth: 14
Consider: 14
FIFO: 14
GC: 14
Inconsistency: 14
Large: 14
Link: 14
References: 14
Renesse: 14
Scalable: 14
Size: 14
Way: 14
ZooKeeper: 14
accept: 14
accessing: 14
addition: 14
applied: 14
arise: 14
channels: 14
commits: 14
correction: 14
currency: 14
deliver: 14
destination: 14
detects: 14
distinct: 14
drops: 14
efficacy: 14
entries: 14
errors: 14
exactly: 14
explore: 14
fail: 14
finding: 14
growing: 14
guarantees: 14
help: 14
i2: 14
improves: 14
infiltrators: 14
interested: 14
involves: 14
malicious: 14
measurements: 14
option: 14
options: 14
perfect: 14
performing: 14
physical: 14
predictable: 14
prevent: 14
previously: 14
program: 14
published: 14
question: 14
requirements: 14
sabotage: 14
simulation: 14
singleton: 14
soon: 14
strong: 14
subsystem: 14
temporarily: 14
thread: 14
tolerate: 14
turn: 14
useful: 14
whole: 14
xj: 14
1500: 13
2005: 13
AFS: 13
Another: 13
Asynchronous: 13
Control: 13
Department: 13
Evaluation: 13
Layer: 13
NET: 13
NFM: 13
NTFS: 13
Pp: 13
RADIENT: 13
RTT: 13
Supercomputing: 13
achieved: 13
add: 13
although: 13
balance: 13
balancing: 13
bound: 13
bounded: 13
call: 13
class: 13
completely: 13
concurrent: 13
consisting: 13
conventional: 13
decrease: 13
decreases: 13
define: 13
denote: 13
depends: 13
determine: 13
determined: 13
did: 13
differentiated: 13
directory: 13
discarded: 13
dominant: 13
dynamically: 13
effects: 13
eventually: 13
exception: 13
face: 13
far: 13
field: 13
free: 13
frequently: 13
gives: 13
grained: 13
handling: 13
instances: 13
integrated: 13
interval: 13
join: 13
lack: 13
lambda: 13
limiting: 13
lines: 13
location: 13
magnitude: 13
measure: 13
mentioned: 13
missed: 13
normal: 13
paradigm: 13
parallel: 13
policies: 13
positive: 13
programming: 13
proportional: 13
providing: 13
public: 13
receipt: 13
registers: 13
remains: 13
robust: 13
search: 13
semantics: 13
setup: 13
shares: 13
situations: 13
static: 13
statistics: 13
stores: 13
structures: 13
thousand: 13
tier: 13
trade: 13
unbounded: 13
upon: 13
written: 13
z: 13
1996: 12
44: 12
54: 12
Appliance: 12
Architecture: 12
Balakrishnan: 12
Download: 12
Factor: 12
Further: 12
IX: 12
Jan: 12
LFS: 12
Left: 12
Li: 12
Life: 12
Management: 12
More: 12
Other: 12
Priorities: 12
Software: 12
TMs: 12
acceptable: 12
account: 12
buffer: 12
capable: 12
caused: 12
challenges: 12
classes: 12
collaborative: 12
collect: 12
combined: 12
conclude: 12
constraints: 12
copies: 12
creating: 12
decide: 12
dedicated: 12
delayed: 12
digests: 12
dissemination: 12
early: 12
efficiency: 12
element: 12
experience: 12
facts: 12
fairly: 12
fees: 12
financial: 12
footprint: 12
fundamental: 12
gigabit: 12
half: 12
he: 12
html: 12
idle: 12
implications: 12
info: 12
ing: 12
interactive: 12
intervals: 12
introduced: 12
keep: 12
leads: 12
locking: 12
maintained: 12
modern: 12
obtained: 12
orders: 12
overlap: 12
overview: 12
particularly: 12
primary: 12
project: 12
projects: 12
provided: 12
purposes: 12
push: 12
reason: 12
recently: 12
relatively: 12
relevant: 12
rescue: 12
sort: 12
specify: 12
spikes: 12
stack: 12
starting: 12
subscribe: 12
subset: 12
technique: 12
ten: 12
tool: 12
track: 12
tree: 12
unif: 12
you: 12
06: 11
2012: 11
ABORT: 11
And: 11
Any: 11
Existing: 11
Eyal: 11
Given: 11
Grep: 11
IV: 11
Ithaca: 11
Mobile: 11
November: 11
Number: 11
OSDI: 11
Orkut: 11
Proof: 11
Sep: 11
Some: 11
What: 11
XML: 11
academic: 11
accurate: 11
activity: 11
advance: 11
aggregated: 11
argue: 11
authors: 11
base: 11
basis: 11
begin: 11
callback: 11
complex: 11
computation: 11
concurrency: 11
crash: 11
date: 11
deal: 11
deployed: 11
detail: 11
embedded: 11
ensures: 11
equivalent: 11
examples: 11
fee: 11
fetches: 11
fetching: 11
finds: 11
focused: 11
generally: 11
generating: 11
growth: 11
heterogeneous: 11
hit: 11
hundreds: 11
identifiers: 11
illustrates: 11
improvements: 11
incorporates: 11
increasingly: 11
indicate: 11
insufficient: 11
intended: 11
interact: 11
interactions: 11
introduce: 11
involved: 11
largest: 11
layout: 11
let: 11
little: 11
logged: 11
maintenance: 11
modeless: 11
note: 11
opportunity: 11
pair: 11
parameter: 11
partitioned: 11
past: 11
practical: 11
practice: 11
prior: 11
processed: 11
prove: 11
providers: 11
randomly: 11
recovers: 11
replace: 11
replicas: 11
requiring: 11
responsible: 11
retransmission: 11
role: 11
roughly: 11
routine: 11
separate: 11
serve: 11
simplicity: 11
simultaneous: 11
special: 11
spread: 11
strictly: 11
summary: 11
supports: 11
true: 11
trusted: 11
try: 11
unlikely: 11
unshared: 11
valid: 11
ve: 11
whose: 11
2009: 10
95: 10
ACK: 10
Avg: 10
BAR: 10
Based: 10
Because: 10
Bitcoins: 10
CC: 10
CDN: 10
Corba: 10
Dec: 10
Forward: 10
GHash: 10
IO: 10
IPC: 10
Introduction: 10
Java: 10
Journal: 10
Let: 10
Minimum: 10
Model: 10
Most: 10
Nevertheless: 10
Now: 10
P2Pool: 10
Paxos: 10
Protocols: 10
Ratio: 10
Revenue: 10
Robbert: 10
So: 10
Transactional: 10
accuracy: 10
along: 10
appear: 10
arg: 10
assumption: 10
asynchronously: 10
becoming: 10
below: 10
bottleneck: 10
checks: 10
compares: 10
conflict: 10
constructed: 10
converge: 10
decision: 10
decisions: 10
degenerate: 10
degree: 10
depend: 10
desired: 10
detailed: 10
detecting: 10
discussed: 10
distinguish: 10
distributes: 10
efforts: 10
enable: 10
energy: 10
epidemics: 10
evaluated: 10
eventual: 10
examine: 10
expensive: 10
experienced: 10
figure: 10
final: 10
forms: 10
give: 10
gracefully: 10
his: 10
ideal: 10
infiltrates: 10
injected: 10
isn: 10
ken: 10
leverage: 10
limitations: 10
linearly: 10
m22: 10
maps: 10
methods: 10
minutes: 10
miss: 10
moving: 10
near: 10
networking: 10
node3: 10
novel: 10
o: 10
otherwise: 10
overlapping: 10
pending: 10
possibility: 10
prefetches: 10
presented: 10
presents: 10
prisoner: 10
proceed: 10
pushing: 10
rapid: 10
realistic: 10
really: 10
redundant: 10
remainder: 10
reported: 10
representing: 10
researchers: 10
rest: 10
return: 10
rise: 10
roles: 10
router: 10
routes: 10
runtime: 10
sample: 10
scales: 10
scheduling: 10
segment: 10
sensitive: 10
situation: 10
stock: 10
success: 10
supporting: 10
table: 10
tail: 10
team: 10
tests: 10
think: 10
thresholds: 10
tion: 10
unstable: 10
usually: 10
utility: 10
visible: 10
widely: 10
windows: 10
y: 10
zone: 10
05: 9
101: 9
110: 9
120: 9
1994: 9
350: 9
65: 9
700: 9
97: 9
99: 9
Annual: 9
August: 9
Bad: 9
Both: 9
Detection: 9
EC2: 9
Flow: 9
From: 9
GW: 9
Intel: 9
John: 9
Local: 9
Merlin: 9
Networking: 9
Notice: 9
ON: 9
Online: 9
Opportunistic: 9
Project: 9
RW: 9
Recall: 9
Right: 9
SIGMOD: 9
SOSP: 9
Server: 9
Split: 9
Streaming: 9
Traffic: 9
US: 9
VIII: 9
Werner: 9
Write: 9
aborted: 9
achieves: 9
achieving: 9
allowed: 9
apply: 9
assigned: 9
assigns: 9
assumed: 9
attackers: 9
bad: 9
behave: 9
beneficial: 9
break: 9
causing: 9
changed: 9
check: 9
co: 9
competing: 9
compute: 9
connections: 9
continue: 9
curves: 9
daily: 9
default: 9
degradation: 9
deterministic: 9
developing: 9
dirty: 9
discards: 9
effectively: 9
efficiently: 9
elements: 9
eliminate: 9
en: 9
engineering: 9
entirely: 9
epidemic: 9
especially: 9
execute: 9
explored: 9
fa: 9
fails: 9
fairness: 9
false: 9
fashion: 9
fd: 9
fine: 9
forming: 9
forwarding: 9
frequency: 9
fully: 9
functions: 9
grant: 9
grep: 9
grow: 9
hide: 9
hierarchy: 9
hold: 9
hot: 9
identifier: 9
implementing: 9
importance: 9
incentive: 9
inexpensive: 9
investigate: 9
iterative: 9
kept: 9
knows: 9
language: 9
leaving: 9
life: 9
loads: 9
lose: 9
lossless: 9
lowest: 9
map: 9
maximize: 9
metadata: 9
microbenchmarks: 9
minor: 9
modifications: 9
modify: 9
modules: 9
name: 9
owner: 9
pattern: 9
places: 9
plan: 9
profitable: 9
programmer: 9
prone: 9
proportion: 9
quite: 9
reached: 9
readers: 9
records: 9
reflect: 9
reports: 9
respect: 9
rings: 9
risk: 9
rounds: 9
routing: 9
sampled: 9
sampling: 9
sd: 9
sec: 9
segments: 9
selected: 9
session: 9
shall: 9
showing: 9
specialized: 9
specifically: 9
steps: 9
subject: 9
subsequent: 9
susceptible: 9
synchrony: 9
taking: 9
threads: 9
tolerant: 9
topology: 9
transfers: 9
transformation: 9
transitions: 9
transmitting: 9
unacknowledged: 9
variant: 9
vn: 9
volume: 9
want: 9
whenever: 9
000: 8
03: 8
07: 8
160: 8
4000: 8
9th: 8
AFRL: 8
API: 8
Additionally: 8
Average: 8
Back: 8
CA: 8
Collaboration: 8
Comparison: 8
Consistency: 8
David: 8
Dilemma: 8
Eligius: 8
GiB: 8
Indeed: 8
Instead: 8
Interleave: 8
J2EE: 8
JGroups: 8
Log: 8
MC: 8
NSDI: 8
Oct: 8
Order: 8
Rather: 8
Receiver: 8
Review: 8
Ricochet: 8
SDN: 8
SM: 8
Security: 8
September: 8
Specifically: 8
T2: 8
TTL: 8
Technologies: 8
Threshold: 8
Token: 8
Two: 8
Unlike: 8
VI: 8
WS: 8
Wang: 8
Wide: 8
abstract: 8
academia: 8
activities: 8
adds: 8
advantage: 8
advantages: 8
affect: 8
affects: 8
aggregates: 8
alarm: 8
allocation: 8
amounts: 8
appliances: 8
arXiv: 8
arbitrary: 8
aspects: 8
attributes: 8
auditor: 8
automated: 8
away: 8
backup: 8
behind: 8
breaks: 8
buffered: 8
byte: 8
central: 8
challenge: 8
charge: 8
checking: 8
clock: 8
collected: 8
combine: 8
come: 8
compile: 8
compromised: 8
concurrently: 8
confidence: 8
confirm: 8
congested: 8
connecting: 8
consecutive: 8
contributing: 8
coordinator: 8
corresponds: 8
creates: 8
currencies: 8
decentralized: 8
degrades: 8
densities: 8
depending: 8
deploy: 8
details: 8
differs: 8
disconnected: 8
domain: 8
drag: 8
earns: 8
employed: 8
engage: 8
equipment: 8
ever: 8
exact: 8
executed: 8
exist: 8
extensions: 8
extreme: 8
favor: 8
flexible: 8
greatly: 8
header: 8
honestly: 8
ideas: 8
improvement: 8
indeed: 8
initially: 8
input: 8
insight: 8
largely: 8
leader: 8
leading: 8
leave: 8
limits: 8
look: 8
masks: 8
match: 8
max: 8
measuring: 8
misbehaving: 8
mixed: 8
modal: 8
modeled: 8
models: 8
modification: 8
move: 8
neighbor: 8
net: 8
news: 8
nominal: 8
nor: 8
notifications: 8
offers: 8
oracle: 8
ordered: 8
ordering: 8
participation: 8
payoff: 8
permutation: 8
placed: 8
played: 8
plot: 8
portion: 8
prioritised: 8
purpose: 8
put: 8
react: 8
recovering: 8
refer: 8
refrain: 8
replaced: 8
reservation: 8
retransmissions: 8
returned: 8
reward: 8
series: 8
serves: 8
simulate: 8
sink: 8
solve: 8
span: 8
specified: 8
stacks: 8
strict: 8
studies: 8
suitable: 8
surprisingly: 8
terms: 8
timer: 8
transparent: 8
turns: 8
twice: 8
unusable: 8
upcall: 8
verify: 8
views: 8
weak: 8
weather: 8
wiki: 8
x01: 8
x02: 8
xc: 8
02: 7
140: 7
2010: 7
45: 7
46: 7
50ms: 7
51: 7
58: 7
6000: 7
76: 7
98: 7
Apache: 7
BSD: 7
Byzantine: 7
Caching: 7
Cluster: 7
Conclusion: 7
December: 7
Demers: 7
Dept: 7
Dolev: 7
EVICT: 7
Emulab: 7
Experimental: 7
Fixed: 7
Furthermore: 7
Gray: 7
He: 7
Hence: 7
INFOCOM: 7
January: 7
Joseph: 7
LBFS: 7
Memory: 7
MiCA: 7
NSF: 7
Networked: 7
Next: 7
Not: 7
October: 7
Optical: 7
Oriented: 7
Press: 7
Process: 7
RC: 7
RETRY: 7
San: 7
Scale: 7
Several: 7
Springer: 7
Summary: 7
Synchronous: 7
TeraGrid: 7
Third: 7
Though: 7
U: 7
Upload: 7
Usenix: 7
Users: 7
VII: 7
WAN: 7
Yu: 7
aborting: 7
abstraction: 7
acceptors: 7
acknowledged: 7
acknowledgments: 7
applying: 7
arises: 7
array: 7
arrays: 7
attractive: 7
belongs: 7
calculate: 7
card: 7
causes: 7
cleanup: 7
closer: 7
commercial: 7
companies: 7
comparing: 7
comparison: 7
completion: 7
complexity: 7
concept: 7
conflicting: 7
connection: 7
consistently: 7
construct: 7
continuous: 7
contributions: 7
counter: 7
creation: 7
cumulative: 7
dashed: 7
de: 7
degraded: 7
deployment: 7
derived: 7
designs: 7
deter: 7
discussion: 7
disrupt: 7
doesn: 7
downstream: 7
easier: 7
eliminated: 7
eliminates: 7
employs: 7
empty: 7
enterprise: 7
equally: 7
everything: 7
exchanges: 7
experiences: 7
explicit: 7
exploit: 7
exponential: 7
expression: 7
extend: 7
external: 7
feature: 7
feedback: 7
filesystem: 7
fires: 7
fit: 7
follow: 7
force: 7
handles: 7
hour: 7
huge: 7
ignored: 7
image: 7
implementations: 7
incoherent: 7
incorporate: 7
inferior: 7
interaction: 7
introduces: 7
inventory: 7
isolation: 7
keys: 7
latest: 7
light: 7
located: 7
mapping: 7
mashups: 7
media: 7
mesh: 7
method: 7
mines: 7
modifying: 7
multicasts: 7
nature: 7
negligible: 7
nothing: 7
oblivious: 7
observation: 7
obvious: 7
operator: 7
overloaded: 7
perhaps: 7
persistent: 7
perturbed: 7
plus: 7
poorly: 7
population: 7
pre: 7
prefer: 7
privacy: 7
programs: 7
propagated: 7
prototype: 7
rack: 7
rarely: 7
reaches: 7
reading: 7
reflects: 7
regarding: 7
register: 7
regularly: 7
relaying: 7
rely: 7
repeated: 7
represent: 7
represented: 7
requesting: 7
reservations: 7
resistant: 7
restart: 7
retailer: 7
retrieve: 7
rich: 7
route: 7
routers: 7
savings: 7
seamlessly: 7
self: 7
sharply: 7
she: 7
simplify: 7
simulator: 7
sites: 7
socket: 7
specification: 7
stop: 7
subservices: 7
substantially: 7
successful: 7
throughout: 7
tradeoff: 7
transition: 7
transmit: 7
treat: 7
treated: 7
trip: 7
ubiquitous: 7
unaware: 7
unchanged: 7
unless: 7
useless: 7
utilization: 7
variation: 7
vendor: 7
versus: 7
violating: 7
year: 7
yield: 7
yields: 7
1010: 6
1990: 6
450: 6
52: 6
75: 6
7th: 6
85: 6
96: 6
AFOSR: 6
AND: 6
Accordingly: 6
Again: 6
Application: 6
Attack: 6
Available: 6
Before: 6
Berkeley: 6
Bonneau: 6
Building: 6
CBCB: 6
CLR: 6
COM: 6
California: 6
Case: 6
Chen: 6
Cisco: 6
Codes: 6
Comput: 6
Consistent: 6
Corporation: 6
DARPA: 6
Debian: 6
Definition: 6
Despite: 6
Developers: 6
DiscusFish: 6
Dogecoin: 6
Druschel: 6
Due: 6
Earth: 6
Enhancing: 6
Execution: 6
Failure: 6
France: 6
Game: 6
Gbps: 6
Good: 6
Group: 6
Huang: 6
ID: 6
Ideally: 6
Internet2: 6
Isis: 6
Just: 6
Krzysztof: 6
LAN: 6
LCM: 6
LT: 6
Lamport: 6
Lemma: 6
Litecoin: 6
Long: 6
Luu: 6
ML: 6
Measurement: 6
Min: 6
Noble: 6
Object: 6
Ostrowski: 6
Per: 6
Percentage: 6
Power: 6
Prefetching: 6
Prisoner: 6
Related: 6
Reliability: 6
Report: 6
Ri: 6
Satyanarayanan: 6
Scalability: 6
Seattle: 6
Set: 6
Simultaneous: 6
Staggered: 6
Start: 6
Switzerland: 6
Their: 6
Then: 6
Theorem: 6
Twenty: 6
Unfortunately: 6
Upon: 6
VLDB: 6
Welsh: 6
World: 6
XXX: 6
Yahoo: 6
Zhang: 6
Zhu: 6
accommodate: 6
achievable: 6
acknowledgement: 6
acting: 6
addressed: 6
addresses: 6
adjustment: 6
administrators: 6
affected: 6
agreement: 6
alternatives: 6
analyzed: 6
apparently: 6
appropriately: 6
architectural: 6
argument: 6
arrives: 6
assumptions: 6
automatic: 6
averages: 6
avoids: 6
bars: 6
begins: 6
beyond: 6
bottom: 6
boundaries: 6
calls: 6
catch: 6
checkpoint: 6
checksum: 6
cleaning: 6
clearly: 6
closely: 6
clouds: 6
coherent: 6
collapse: 6
combination: 6
combines: 6
comes: 6
command: 6
communities: 6
competition: 6
comprised: 6
computers: 6
concave: 6
conducted: 6
conjunction: 6
consensus: 6
considerable: 6
considering: 6
contain: 6
contemporary: 6
controlled: 6
cooperative: 6
coordinate: 6
copying: 6
coupled: 6
crashed: 6
curve: 6
customize: 6
dates: 6
decoupling: 6
definition: 6
denotes: 6
dependent: 6
deployments: 6
despite: 6
detector: 6
detectors: 6
determines: 6
determining: 6
develop: 6
digest: 6
disjoint: 6
display: 6
disseminate: 6
divides: 6
document: 6
draw: 6
driven: 6
durability: 6
effort: 6
egress: 6
enabling: 6
established: 6
expressions: 6
feed: 6
feeds: 6
fill: 6
focuses: 6
forced: 6
former: 6
frame: 6
gaining: 6
guarantee: 6
hashcash: 6
hashing: 6
heartbeat: 6
hidden: 6
histories: 6
i0: 6
imposed: 6
indicates: 6
industry: 6
ingress: 6
intercepted: 6
intermediate: 6
io: 6
iperf: 6
issued: 6
l: 6
ledger: 6
locally: 6
manage: 6
massive: 6
master: 6
matching: 6
measured: 6
middleware: 6
minimize: 6
mission: 6
mod: 6
modifies: 6
moment: 6
motivation: 6
moves: 6
mp: 6
nearly: 6
necessarily: 6
node0: 6
nonce: 6
normalizes: 6
notification: 6
notion: 6
numbered: 6
numerous: 6
o2: 6
occurring: 6
older: 6
optimistic: 6
optimized: 6
oscillatory: 6
outlined: 6
outperforms: 6
output: 6
overload: 6
papers: 6
parties: 6
partitions: 6
pdf: 6
permit: 6
perspective: 6
phase: 6
play: 6
pooled: 6
possibly: 6
powerful: 6
predict: 6
prefix: 6
primarily: 6
prime: 6
principles: 6
produced: 6
publicly: 6
publisher: 6
publishing: 6
pulls: 6
punish: 6
questions: 6
queuing: 6
reachable: 6
red: 6
reduction: 6
redundancy: 6
reject: 6
repeatedly: 6
replay: 6
replicate: 6
resource: 6
resulted: 6
returns: 6
reveals: 6
rewards: 6
rig: 6
rj: 6
robin: 6
rules: 6
samples: 6
say: 6
sections: 6
seek: 6
seems: 6
select: 6
senders: 6
sensitivity: 6
serialized: 6
simulations: 6
six: 6
slowly: 6
snapshot: 6
sockets: 6
sometimes: 6
somewhat: 6
sorts: 6
standardized: 6
straightforward: 6
suggested: 6
summarize: 6
super: 6
supplies: 6
targeted: 6
tat: 6
thank: 6
timeout: 6
tit: 6
tracking: 6
transmits: 6
trees: 6
trigger: 6
trying: 6
txnID: 6
ultimately: 6
understand: 6
usability: 6
usable: 6
variability: 6
variations: 6
vast: 6
vector: 6
vendors: 6
w: 6
weekly: 6
white: 6
withheld: 6
won: 6
workers: 6
04: 5
1200: 5
13th: 5
2016: 5
3000: 5
55: 5
66: 5
7c: 5
900: 5
ACL: 5
Acknowledgements: 5
Basic: 5
Brewer: 5
CORBA: 5
Cheriton: 5
Client: 5
Clients: 5
Compile: 5
Cost: 5
Cumulative: 5
DB: 5
DSN: 5
Danny: 5
During: 5
EFERENCES: 5
ELATED: 5
Early: 5
Engineering: 5
Enterprise: 5
Ethernet: 5
Event: 5
Eventually: 5
Examples: 5
Experience: 5
Facebook: 5
Fast: 5
Fifth: 5
Foundation: 5
Framework: 5
FreeBSD: 5
Future: 5
Gigabit: 5
Gribble: 5
Groups: 5
Gu: 5
INFORMATIK: 5
INFORMATIQUE: 5
IPTV: 5
ISPs: 5
Importantly: 5
Improving: 5
Inc: 5
Information: 5
Johnson: 5
Katz: 5
Kleinberg: 5
LANs: 5
Lambda: 5
Like: 5
Little: 5
Liu: 5
Low: 5
Mar: 5
Maximum: 5
May: 5
Middle: 5
Mining: 5
Multiple: 5
NTRODUCTION: 5
National: 5
ONCLUSION: 5
Packets: 5
Pareto: 5
Percentile: 5
Private: 5
Q: 5
Replication: 5
Rizzo: 5
Schneider: 5
Soule: 5
Speed: 5
Stepwise: 5
Sun: 5
TOCS: 5
Technical: 5
Toolkit: 5
Transaction: 5
Updates: 5
Virtual: 5
Where: 5
White: 5
Within: 5
YCSB: 5
ability: 5
absence: 5
absolute: 5
acknowledgment: 5
act: 5
adapting: 5
adjusting: 5
ago: 5
agree: 5
ahead: 5
allocated: 5
anyone: 5
append: 5
approximate: 5
arbitrarily: 5
arrival: 5
assuming: 5
atomic: 5
atomicity: 5
attempts: 5
author: 5
avoiding: 5
backed: 5
band: 5
bar: 5
behalf: 5
behaves: 5
belong: 5
big: 5
bit: 5
bits: 5
blocking: 5
brevity: 5
broadcast: 5
broken: 5
broker: 5
buildings: 5
calculated: 5
calculates: 5
capture: 5
carry: 5
catches: 5
caught: 5
cb: 5
chains: 5
characteristics: 5
cheap: 5
claim: 5
classified: 5
cloned: 5
collapses: 5
comments: 5
communicate: 5
communicates: 5
compose: 5
concern: 5
conclusion: 5
consequence: 5
consequently: 5
considerations: 5
constitutes: 5
constrained: 5
consume: 5
consuming: 5
continuously: 5
cope: 5
costly: 5
count: 5
counts: 5
crashes: 5
credentials: 5
cross: 5
dark: 5
day: 5
dealing: 5
delaying: 5
demands: 5
demonstrated: 5
denial: 5
denoted: 5
depict: 5
df: 5
dialog: 5
direction: 5
dramatic: 5
eight: 5
elaborate: 5
elapsed: 5
eliminating: 5
email: 5
emerging: 5
employing: 5
encourage: 5
encryption: 5
enforce: 5
ephemeral: 5
essentially: 5
estimated: 5
exceed: 5
except: 5
exhibit: 5
explain: 5
explicitly: 5
exploring: 5
expressed: 5
falls: 5
faulty: 5
feasibility: 5
figures: 5
filtering: 5
finally: 5
firing: 5
flexibility: 5
forces: 5
forwarded: 5
frames: 5
frequent: 5
gap: 5
generality: 5
generic: 5
geo: 5
geographical: 5
gigabits: 5
ground: 5
highperformance: 5
hints: 5
hooks: 5
identification: 5
illustrate: 5
imagine: 5
implies: 5
improved: 5
improving: 5
inaccurate: 5
incorrect: 5
incrementally: 5
incurs: 5
independently: 5
instantly: 5
integrating: 5
intensive: 5
interesting: 5
interference: 5
internal: 5
international: 5
interoperability: 5
invalidate: 5
investigated: 5
investigator: 5
issuing: 5
joining: 5
joins: 5
languages: 5
layouts: 5
learn: 5
lengths: 5
loaded: 5
locality: 5
locations: 5
locks: 5
logic: 5
mask: 5
mean: 5
measurement: 5
minibrowsers: 5
minority: 5
modeling: 5
moved: 5
multicasting: 5
mutual: 5
my: 5
naturally: 5
node2: 5
observations: 5
obtains: 5
occasional: 5
occurred: 5
offered: 5
omit: 5
omitted: 5
optimization: 5
optimizes: 5
organization: 5
organizations: 5
organized: 5
orthogonal: 5
outside: 5
overcome: 5
pairs: 5
passed: 5
pay: 5
pays: 5
peerto: 5
percentages: 5
perfectly: 5
permitting: 5
pf: 5
phenomenon: 5
plots: 5
poison: 5
precisely: 5
predictor: 5
predictors: 5
price: 5
probabilistic: 5
proceeds: 5
progresses: 5
property: 5
proprietary: 5
protected: 5
proved: 5
punished: 5
raise: 5
rare: 5
reaching: 5
record: 5
references: 5
release: 5
reliance: 5
remaining: 5
remove: 5
repairs: 5
repeat: 5
requested: 5
requirement: 5
resilience: 5
resolve: 5
respects: 5
responsiveness: 5
restrictions: 5
reveal: 5
ring: 5
road: 5
satisfactory: 5
satisfied: 5
save: 5
scheduled: 5
seem: 5
sees: 5
separated: 5
separately: 5
sequenced: 5
serializable: 5
sessions: 5
shard: 5
shards: 5
simulated: 5
sized: 5
slower: 5
sophisticated: 5
speculative: 5
stated: 5
stay: 5
storing: 5
studied: 5
submit: 5
subsequently: 5
substantial: 5
subsystems: 5
suffer: 5
sum: 5
summarized: 5
suspicion: 5
switched: 5
switches: 5
symposium: 5
synchronization: 5
technical: 5
tell: 5
texture: 5
thinking: 5
tightly: 5
timely: 5
timeouts: 5
timestamps: 5
topologies: 5
tracks: 5
transform: 5
transformations: 5
trap: 5
trend: 5
trends: 5
triggers: 5
tune: 5
tuning: 5
turned: 5
udp: 5
unable: 5
unexpected: 5
unit: 5
unpredictable: 5
unreliable: 5
updating: 5
upper: 5
utilisation: 5
validate: 5
verifying: 5
viewed: 5
volatile: 5
vs: 5
waiting: 5
walk: 5
wall: 5
wants: 5
why: 5
wired: 5
worse: 5
writers: 5
001: 4
102: 4
14853: 4
155: 4
1987: 4
1992: 4
1KB: 4
1s: 4
2nd: 4
3232: 4
365: 4
3D: 4
47: 4
48: 4
5000: 4
550: 4
59: 4
61: 4
63: 4
6th: 4
7d: 4
89: 4
AB: 4
Abbadi: 4
Accessed: 4
Acknowledgments: 4
Agrawal: 4
Alvisi: 4
Amir: 4
Andrew: 4
AntPool: 4
April: 4
Architectures: 4
Archive: 4
Assume: 4
Athey: 4
Atkin: 4
B0: 4
Bahack: 4
Bernstein: 4
Biersack: 4
Buffer: 4
Buffering: 4
CMS: 4
Can: 4
Center: 4
City: 4
Classical: 4
Congestion: 4
Convergence: 4
Courtois: 4
Current: 4
Das: 4
Database: 4
Decentralized: 4
Deploying: 4
Do: 4
E2E: 4
ESB: 4
Edition: 4
Effect: 4
Electronic: 4
Emin: 4
Engineer: 4
Epidemic: 4
Equations: 4
Equilibrium: 4
Experiments: 4
Faber: 4
February: 4
Fees: 4
Foster: 4
Francis: 4
Francisco: 4
Freedman: 4
Ghash: 4
Graphs: 4
Grossklags: 4
Hacker: 4
Hakim: 4
Hashcash: 4
Histograms: 4
Host: 4
IDs: 4
ISBN: 4
Implications: 4
Increasing: 4
Inter: 4
Is: 4
Ittay: 4
JXTA: 4
Jelasity: 4
Jose: 4
July: 4
Jumbo: 4
KJ: 4
Kaashoek: 4
Kermarrec: 4
Knowledge: 4
Korn: 4
LRU: 4
Lakshman: 4
Last: 4
Laszka: 4
Length: 4
Lines: 4
Logsim: 4
MAID: 4
MTUs: 4
Ma: 4
Mahesh: 4
Metrics: 4
Miller: 4
Milliseconds: 4
Mode: 4
Moore: 4
NAK: 4
Namecoin: 4
None: 4
Numerical: 4
OOLS: 4
ORK: 4
Optimal: 4
Oracle: 4
Ordering: 4
Over: 4
Overheads: 4
Overlay: 4
PEPs: 4
PODC: 4
Participants: 4
Paul: 4
Permacoin: 4
Practice: 4
Prediction: 4
Privacy: 4
Product: 4
Projects: 4
Qi: 4
Qwest: 4
Recent: 4
Remote: 4
Reno: 4
Rev: 4
Robert: 4
Rosenfeld: 4
Round: 4
Rover: 4
Rowstron: 4
SHA: 4
SMR: 4
SRM: 4
STORE: 4
Scaling: 4
Send: 4
Shasha: 4
Shenker: 4
Similarly: 4
Sizes: 4
Social: 4
Solutions: 4
Source: 4
Staleness: 4
Storage: 4
Stratum: 4
Structured: 4
Substituting: 4
Tech: 4
Teragrid: 4
Terry: 4
Test: 4
That: 4
Today: 4
Tornado: 4
Towsley: 4
Trace: 4
USD: 4
Up: 4
VFS: 4
Vahdat: 4
Walsh: 4
Washington: 4
Weatherspoon: 4
Wireless: 4
Without: 4
XI: 4
__: 4
accepted: 4
action: 4
actions: 4
adopted: 4
advanced: 4
advances: 4
air: 4
alone: 4
alongside: 4
amortizable: 4
analogous: 4
annual: 4
answer: 4
applicable: 4
approximated: 4
arrow: 4
aspect: 4
assign: 4
assumes: 4
award: 4
awkward: 4
ballots: 4
banking: 4
behaviour: 4
bifurcations: 4
bitcointalk: 4
blade: 4
blend: 4
blocked: 4
blogspot: 4
boxes: 4
briefly: 4
broadcasts: 4
broader: 4
broadly: 4
bullet: 4
burstier: 4
ca: 4
calculator: 4
callbacks: 4
caller: 4
capitalization: 4
captures: 4
carefully: 4
certainly: 4
charts: 4
chat: 4
clean: 4
closes: 4
collecting: 4
collector: 4
coming: 4
comparable: 4
compensated: 4
compiles: 4
completed: 4
complicated: 4
componentized: 4
comprehensive: 4
compromise: 4
concerns: 4
conduct: 4
confirms: 4
consequences: 4
conservative: 4
consist: 4
contact: 4
contained: 4
continued: 4
converges: 4
cooperation: 4
correlated: 4
correlation: 4
crucial: 4
customers: 4
cutting: 4
cycle: 4
danger: 4
datagram: 4
debugging: 4
decades: 4
decided: 4
declining: 4
deep: 4
deferred: 4
defines: 4
demonstrates: 4
describing: 4
description: 4
desktop: 4
devastating: 4
di: 4
differences: 4
differentiate: 4
differently: 4
directed: 4
directions: 4
directories: 4
disaster: 4
discussions: 4
disloyal: 4
disrupted: 4
disruption: 4
distant: 4
distribute: 4
distributions: 4
division: 4
doing: 4
double: 4
e2epi: 4
ease: 4
elsewhere: 4
embedding: 4
emulate: 4
encoder: 4
encodes: 4
endhosts: 4
engages: 4
enhancing: 4
enormous: 4
enter: 4
equation: 4
equations: 4
erasure: 4
evenly: 4
evict: 4
evicting: 4
evidence: 4
ex: 4
examines: 4
exchanged: 4
exclusive: 4
executing: 4
existence: 4
export: 4
exposed: 4
extensive: 4
familiar: 4
faults: 4
filter: 4
firewalls: 4
five: 4
fixing: 4
flowing: 4
flush: 4
flushes: 4
focusing: 4
followed: 4
forks: 4
formula: 4
fragmentation: 4
gain: 4
games: 4
gathered: 4
gets: 4
globally: 4
goes: 4
going: 4
graceful: 4
grants: 4
grateful: 4
hackingdistributed: 4
heartbeats: 4
heavily: 4
helps: 4
hint: 4
histograms: 4
holds: 4
home: 4
honeypot: 4
hop: 4
hope: 4
human: 4
hweather: 4
identified: 4
identifying: 4
identity: 4
ignore: 4
images: 4
impacted: 4
implicit: 4
impossible: 4
inaccessible: 4
incorporating: 4
incurred: 4
incurring: 4
inform: 4
initiated: 4
initiative: 4
inquiry: 4
integral: 4
integrate: 4
intelligent: 4
intend: 4
interacts: 4
internet2: 4
interrupted: 4
intervening: 4
introducing: 4
invalidated: 4
invalidating: 4
keeping: 4
lacking: 4
lacks: 4
leaders: 4
learning: 4
leases: 4
leaves: 4
letting: 4
libraries: 4
lies: 4
lightweight: 4
linking: 4
listed: 4
lived: 4
ll: 4
looked: 4
malfunctioning: 4
marked: 4
marketing: 4
matched: 4
matrix: 4
maximizes: 4
maxx1: 4
measures: 4
merely: 4
messaging: 4
middle: 4
miles: 4
military: 4
mind: 4
minimal: 4
minted: 4
misconfigured: 4
misses: 4
mixture: 4
modulating: 4
monitor: 4
monitors: 4
monopoly: 4
msg7282674: 4
narrow: 4
normalized: 4
notable: 4
notice: 4
notify: 4
november: 4
numerically: 4
o1: 4
obstacles: 4
obtaining: 4
offering: 4
operates: 4
operators: 4
optimum: 4
organofcorti: 4
originally: 4
outstanding: 4
owners: 4
ownership: 4
packages: 4
parameterized: 4
participate: 4
parts: 4
passive: 4
patient: 4
payments: 4
payout: 4
payouts: 4
peak: 4
penalty: 4
people: 4
periodic: 4
permutations: 4
person: 4
personal: 4
phenomena: 4
php: 4
pi: 4
picked: 4
pictures: 4
placement: 4
placing: 4
player: 4
plotting: 4
polling: 4
positions: 4
positives: 4
practicality: 4
predicted: 4
preferred: 4
preprint: 4
pressure: 4
prevents: 4
principle: 4
probabilities: 4
probably: 4
promising: 4
provisioned: 4
pub: 4
pulled: 4
purely: 4
quantities: 4
quantum: 4
querying: 4
quick: 4
readSet: 4
reality: 4
realize: 4
reasonable: 4
reference: 4
reflected: 4
regardless: 4
regional: 4
rejoin: 4
relation: 4
relations: 4
reliably: 4
removed: 4
repairing: 4
replacement: 4
replicating: 4
repo: 4
reserved: 4
residing: 4
resolution: 4
resolved: 4
responding: 4
responses: 4
retailers: 4
review: 4
revoke: 4
rigs: 4
rk: 4
root: 4
routed: 4
rule: 4
saturate: 4
saving: 4
saw: 4
scan: 4
science: 4
secret: 4
secured: 4
selecting: 4
selfish: 4
sensors: 4
separates: 4
sequences: 4
sequentially: 4
shift: 4
shut: 4
signal: 4
silver: 4
similarly: 4
situational: 4
slack: 4
smart: 4
sold: 4
solving: 4
spanning: 4
spun: 4
standardization: 4
states: 4
statistically: 4
steadily: 4
steady: 4
stress: 4
stronger: 4
strongly: 4
subjected: 4
substituting: 4
successes: 4
suffers: 4
sums: 4
suspected: 4
symmetry: 4
synchronized: 4
tackle: 4
tailored: 4
tens: 4
teragrid: 4
terrain: 4
tested: 4
theorem: 4
theoretic: 4
things: 4
threaded: 4
took: 4
touched: 4
traced: 4
tradeoffs: 4
translate: 4
travel: 4
treating: 4
treats: 4
tremendous: 4
truly: 4
trust: 4
tuple: 4
understanding: 4
undesirable: 4
units: 4
unknown: 4
unusual: 4
upstream: 4
usual: 4
ut: 4
validation: 4
valuable: 4
variable: 4
variants: 4
varies: 4
viable: 4
von: 4
vulnerable: 4
waits: 4
weakly: 4
wish: 4
withhold: 4
worked: 4
worst: 4
worth: 4
writeSet: 4
wrong: 4
x0: 4
xk: 4
xkj: 4
yearly: 4
your: 4
08: 3
109: 3
1400: 3
1600: 3
172: 3
185: 3
192: 3
1980s: 3
1988: 3
19th: 3
1e: 3
201: 3
210: 3
24th: 3
260: 3
2609300: 3
276: 3
314: 3
43rd: 3
49: 3
4th: 3
506: 3
53: 3
56: 3
57: 3
58s: 3
5ms: 3
5s: 3
93: 3
A1: 3
ACIDRAIN: 3
ACKs: 3
AFEC: 3
Abadi: 3
Aborted: 3
Additional: 3
Against: 3
Aguilera: 3
Air: 3
Alan: 3
Alberta: 3
Area: 3
Arguably: 3
Arla: 3
Association: 3
Aug: 3
Automated: 3
B2: 3
B3: 3
BLOCKS: 3
Baltimore: 3
Bayou: 3
Behavior: 3
Bianchini: 3
Boston: 3
Boures: 3
Bus: 3
Call: 3
Cambridge: 3
Choosing: 3
Clement: 3
Code: 3
Colorado: 3
Commun: 3
Community: 3
Computers: 3
Concurrency: 3
Continuous: 3
Conventional: 3
Coolstreaming: 3
Copper: 3
Corbett: 3
Crossing: 3
Culler: 3
Cumulus: 3
Custer: 3
DNS: 3
Dahlin: 3
Dark: 3
Davis: 3
Decker: 3
Deering: 3
Defense: 3
Define: 3
Density: 3
Dependable: 3
Details: 3
Direct: 3
Discussion: 3
Disk: 3
Distance: 3
DoS: 3
Does: 3
Doing: 3
Edward: 3
Efficient: 3
Eicken: 3
El: 3
Empire: 3
Environment: 3
Errors: 3
Essentially: 3
Every: 3
Evil: 3
Example: 3
Exploiting: 3
Explorer: 3
FAST: 3
FG: 3
Fact: 3
Fifteenth: 3
Flexible: 3
Flows: 3
Four: 3
Franklin: 3
French: 3
Friendly: 3
G2: 3
GPS: 3
GUI: 3
Ganesh: 3
Gargamel: 3
Grossman: 3
Gt: 3
Gummadi: 3
Guo: 3
Gurumurthi: 3
HE: 3
Hall: 3
Handley: 3
Hauser: 3
Having: 3
Helland: 3
Hibernator: 3
His: 3
Horus: 3
Hosted: 3
Howard: 3
IBM: 3
ICDCS: 3
ICS: 3
IL: 3
Idle: 3
Inconsistent: 3
Independent: 3
Indiana: 3
Info: 3
Initial: 3
Iperf: 3
Its: 3
JBoss: 3
JavaScript: 3
Jim: 3
Joint: 3
Keidar: 3
Kenneth: 3
Key: 3
Kuenning: 3
Lakshmi: 3
Larson: 3
Lateral: 3
Limiting: 3
Lions: 3
List: 3
Lloyd: 3
Logical: 3
London: 3
Luby: 3
MA: 3
MainWin: 3
Malkhi: 3
Malo: 3
Marian: 3
Mazie: 3
McKusick: 3
Meanwhile: 3
Membership: 3
Message: 3
Messages: 3
Michael: 3
Middleware: 3
Modal: 3
Monitoring: 3
Monthly: 3
Montresor: 3
Mostly: 3
Mountain: 3
Much: 3
Multi: 3
Ninja: 3
Nonetheless: 3
ODEL: 3
Often: 3
Open: 3
Others: 3
Ousterhout: 3
PFLDNet: 3
Padhye: 3
Pai: 3
Parallel: 3
Pedone: 3
Perez: 3
Peter: 3
Pinheiro: 3
Pisa: 3
Platform: 3
Pleisch: 3
Poisson: 3
Popek: 3
Prabhakaran: 3
Previous: 3
Prior: 3
Procedure: 3
Queries: 3
Queued: 3
Rao: 3
Rapidly: 3
Raptor: 3
Reads: 3
Real: 3
Receiving: 3
Reducing: 3
Researchers: 3
Resort: 3
SOLO: 3
School: 3
Secure: 3
Seventeenth: 3
Sinfonia: 3
Sixteenth: 3
Smalltalk: 3
Song: 3
Sorrosal: 3
Spanner: 3
SplitStream: 3
Spread: 3
Sprint: 3
Sprite: 3
Subsequently: 3
Synthetic: 3
Syst: 3
TACC: 3
THE: 3
TON: 3
TPUT: 3
Taking: 3
Taylor: 3
Techniques: 3
Technological: 3
Technology: 3
Theory: 3
Thomson: 3
Throughout: 3
Trans: 3
Transfer: 3
Transport: 3
Tsunami: 3
Tudor: 3
UK: 3
UNBLOCKS: 3
UNIX: 3
Until: 3
User: 3
Utah: 3
VALUATION: 3
Validation: 3
Verlag: 3
Vigfusson: 3
Vol: 3
Walli: 3
Wefel: 3
Wei: 3
Whenever: 3
Whereas: 3
Whether: 3
Win32: 3
Wobber: 3
Workloads: 3
Writeback: 3
Writing: 3
YX: 3
Yet: 3
Zhao: 3
Zurich: 3
aborts: 3
accepting: 3
accepts: 3
accountable: 3
accumulated: 3
accusations: 3
acknowledge: 3
acm: 3
acquire: 3
actively: 3
adapts: 3
adequate: 3
adjust: 3
advancing: 3
advantageous: 3
advertisements: 3
affecting: 3
aggregating: 3
agnostic: 3
aimed: 3
aircraft: 3
albeit: 3
album: 3
alert: 3
alerts: 3
alleviate: 3
alternate: 3
amenable: 3
anomaly: 3
anything: 3
app: 3
apparent: 3
appears: 3
areas: 3
aren: 3
argued: 3
arising: 3
assertion: 3
assess: 3
assignment: 3
assure: 3
attribute: 3
auditable: 3
authorization: 3
avatars: 3
avg: 3
avoided: 3
backbone: 3
balancer: 3
banks: 3
batch: 3
beginning: 3
benchmark: 3
black: 3
boards: 3
bone: 3
breaking: 3
brief: 3
bring: 3
browser: 3
builds: 3
bulk: 3
busy: 3
calling: 3
camera: 3
cap: 3
care: 3
carries: 3
categories: 3
characteristic: 3
checked: 3
children: 3
chronous: 3
churn: 3
circumvent: 3
claims: 3
clones: 3
colleagues: 3
column: 3
combinations: 3
combining: 3
commands: 3
commonly: 3
communicating: 3
company: 3
compete: 3
competitive: 3
completes: 3
composed: 3
comprises: 3
compulsory: 3
computational: 3
computed: 3
con: 3
conceived: 3
concentrate: 3
concluded: 3
condition: 3
configurable: 3
confirmed: 3
conservation: 3
conserve: 3
consideration: 3
construction: 3
consumed: 3
consumes: 3
continues: 3
convinced: 3
convoy: 3
cooperate: 3
coordinates: 3
coordination: 3
corporate: 3
correlate: 3
criterion: 3
culprit: 3
curious: 3
custom: 3
customer: 3
customized: 3
cycles: 3
dangers: 3
deadline: 3
deadlines: 3
deals: 3
decade: 3
deciding: 3
decreasing: 3
degrade: 3
demonstrating: 3
deploying: 3
designers: 3
desire: 3
determinism: 3
deviation: 3
dialogue: 3
differentiation: 3
dis: 3
discover: 3
discrepancy: 3
disruptions: 3
disruptive: 3
disseminated: 3
distributing: 3
documentation: 3
documents: 3
dollars: 3
dominated: 3
drawn: 3
driver: 3
dropping: 3
dying: 3
ed: 3
eded: 3
editor: 3
election: 3
electronic: 3
elegant: 3
else: 3
emergence: 3
emotional: 3
encodings: 3
encouraging: 3
ending: 3
endpoint: 3
endto: 3
enhanced: 3
enhances: 3
ensuring: 3
entities: 3
entity: 3
essay: 3
essential: 3
evaluates: 3
evaluating: 3
evident: 3
evolution: 3
evolved: 3
exacerbates: 3
excellent: 3
excluding: 3
executes: 3
expands: 3
expelled: 3
experimentally: 3
experimented: 3
expires: 3
exposes: 3
extends: 3
extension: 3
extent: 3
extra: 3
extracted: 3
faced: 3
fascinating: 3
fewer: 3
fields: 3
finite: 3
firm: 3
flash: 3
flat: 3
fledged: 3
flight: 3
folder: 3
forest: 3
format: 3
formation: 3
friend: 3
gateways: 3
gather: 3
geographically: 3
gf: 3
gfgf: 3
goals: 3
gossiped: 3
government: 3
graphically: 3
great: 3
greatest: 3
grid: 3
guaranteed: 3
handlers: 3
happen: 3
happy: 3
haven: 3
health: 3
helping: 3
hierarchical: 3
hierarchically: 3
hierarchies: 3
highspeed: 3
homogeneous: 3
hypothesis: 3
id: 3
ideally: 3
ignores: 3
ignoring: 3
immersed: 3
implicitly: 3
impose: 3
imposes: 3
income: 3
incorporated: 3
incorrectly: 3
incremental: 3
incremented: 3
incur: 3
indicating: 3
inherits: 3
initiate: 3
innovation: 3
innovations: 3
insights: 3
instantaneous: 3
intent: 3
intercept: 3
intercepting: 3
intercepts: 3
interconnect: 3
interconnected: 3
internals: 3
internet: 3
intervene: 3
intrinsically: 3
inversion: 3
irregular: 3
iteration: 3
jitter: 3
keeps: 3
knowing: 3
landscape: 3
laptops: 3
late: 3
linkage: 3
linked: 3
logical: 3
longdistance: 3
looking: 3
looks: 3
losing: 3
lossy: 3
lot: 3
mahesh: 3
mainframe: 3
markets: 3
marks: 3
mashed: 3
matter: 3
methodology: 3
microbenchmark: 3
migration: 3
minimizing: 3
minute: 3
mirror: 3
mix: 3
modular: 3
motivated: 3
motivates: 3
movies: 3
multithreaded: 3
naive: 3
native: 3
networked: 3
node4: 3
normally: 3
noticeable: 3
null: 3
observing: 3
obstruction: 3
occasionally: 3
oneway: 3
onon: 3
onto: 3
opened: 3
operational: 3
opposed: 3
optimizations: 3
optionally: 3
organize: 3
orientation: 3
outbound: 3
outdated: 3
overflows: 3
overlapped: 3
overloads: 3
overwhelmed: 3
paid: 3
parameterize: 3
partially: 3
participant: 3
pause: 3
paying: 3
payload: 3
penalties: 3
perceived: 3
percent: 3
personalities: 3
personalization: 3
perspectives: 3
pervasive: 3
phone: 3
players: 3
poll: 3
polls: 3
poor: 3
popularity: 3
portions: 3
pose: 3
posed: 3
positioning: 3
prebuilt: 3
precedence: 3
predominate: 3
preferentially: 3
prefetched: 3
presentation: 3
preserved: 3
pricing: 3
primitives: 3
principal: 3
probing: 3
procedure: 3
produce: 3
productivity: 3
programmers: 3
promise: 3
propagate: 3
properly: 3
protect: 3
protection: 3
proves: 3
provider: 3
provisioning: 3
publication: 3
publishers: 3
punishment: 3
purchases: 3
pushed: 3
quanta: 3
queueing: 3
queues: 3
raises: 3
ranging: 3
raw: 3
reacting: 3
readonly: 3
realizing: 3
reasoning: 3
recipient: 3
recognize: 3
recommendations: 3
reconstruct: 3
recoveries: 3
reflecting: 3
rejoins: 3
relate: 3
relates: 3
released: 3
releases: 3
remarkably: 3
removing: 3
replaces: 3
replacing: 3
replayed: 3
replica: 3
replies: 3
representative: 3
res: 3
reserve: 3
resp: 3
respective: 3
responds: 3
restarting: 3
restored: 3
restricted: 3
retain: 3
retransmit: 3
retransmitted: 3
retrieves: 3
revisions: 3
rewarding: 3
rises: 3
risks: 3
robustness: 3
safely: 3
satisfies: 3
satisfy: 3
satisfying: 3
scaled: 3
scaling: 3
scene: 3
scope: 3
script: 3
secondary: 3
seeks: 3
sensor: 3
separation: 3
sequential: 3
served: 3
seventh: 3
severe: 3
sharded: 3
shepherd: 3
showed: 3
simpler: 3
simplest: 3
situated: 3
sleep: 3
slight: 3
smallest: 3
soft: 3
sound: 3
spare: 3
specifications: 3
speculatively: 3
splitting: 3
sporadic: 3
spot: 3
sr: 3
ssrn: 3
stability: 3
standardize: 3
started: 3
stays: 3
stems: 3
stepwise: 3
streamed: 3
struggled: 3
students: 3
subnet: 3
subscriber: 3
subsection: 3
substrate: 3
successive: 3
suddenly: 3
sufficiently: 3
suggests: 3
suite: 3
supplied: 3
surprising: 3
suspicions: 3
symbols: 3
synchronously: 3
t0: 3
tables: 3
tagging: 3
talk: 3
tape: 3
targets: 3
tcp: 3
teams: 3
television: 3
temporary: 3
tends: 3
terminating: 3
terminology: 3
testing: 3
text: 3
theory: 3
thin: 3
thinks: 3
thirty: 3
threat: 3
tight: 3
tiled: 3
tolerable: 3
train: 3
trans: 3
transferred: 3
transferring: 3
transmissions: 3
trial: 3
triggered: 3
triggering: 3
trivial: 3
trouble: 3
truncation: 3
tudorm: 3
turning: 3
unacceptable: 3
unavailable: 3
uncommittable: 3
undetected: 3
unexpectedly: 3
unfairly: 3
unmanaged: 3
unperturbed: 3
unrelated: 3
untrusted: 3
utilities: 3
utilize: 3
validated: 3
variables: 3
variance: 3
verified: 3
viewing: 3
visual: 3
visualization: 3
vogels: 3
vt: 3
wake: 3
wanted: 3
warns: 3
weight: 3
winner: 3
wisdom: 3
worlds: 3
worry: 3
zero: 3
010: 2
012: 2
0233: 2
0532: 2
106: 2
10ms: 2
10th: 2
1100: 2
1112: 2
1145: 2
11th: 2
1402: 2
154: 2
163: 2
1718: 2
174: 2
18313: 2
196: 2
1971: 2
1985: 2
1991: 2
1993: 2
1th: 2
2014weekly: 2
220: 2
22nd: 2
238: 2
23rd: 2
251: 2
264: 2
26th: 2
275: 2
294: 2
2PC: 2
2mi: 2
2q: 2
303: 2
304: 2
315: 2
329: 2
336: 2
337: 2
3500: 2
352: 2
379: 2
383: 2
3a: 2
3b: 2
441465: 2
4980: 2
512MB: 2
5500: 2
554: 2
5557910: 2
5a: 2
5b: 2
5c: 2
62: 2
67: 2
78: 2
7a: 2
7b: 2
8000: 2
81: 2
82: 2
86: 2
8679: 2
87: 2
9000: 2
92: 2
A3: 2
ABC: 2
ABSTRACT: 2
AC: 2
AICT: 2
AJAX: 2
AME: 2
APIs: 2
ASICs: 2
ATM: 2
ATTACKER: 2
Abilene: 2
Access: 2
Achieving: 2
Active: 2
Adam: 2
Adaptation: 2
Addison: 2
Akamai: 2
Alaska: 2
Alberto: 2
Algorithm: 2
Allow: 2
Alternative: 2
Alternatively: 2
Anchorage: 2
Anderson: 2
Andresen: 2
Antony: 2
Apart: 2
Apr: 2
Asia: 2
Attacking: 2
Auditors: 2
Aumann: 2
Automatic: 2
Availability: 2
B25: 2
B26: 2
BA: 2
BAA: 2
BTCGuild: 2
BTChine: 2
Babaoglu: 2
Bach: 2
Bailey: 2
Baker: 2
Ban: 2
Barb: 2
Barr: 2
Bases: 2
Basu: 2
Below: 2
Benjamin: 2
Berlin: 2
Bernoulli: 2
Besides: 2
Between: 2
Beyond: 2
Bhargava: 2
Big: 2
Bins: 2
Birrell: 2
Bjornstad: 2
Blast: 2
Bloomberg: 2
Blumenthal: 2
Bolton: 2
Border: 2
Broadly: 2
Bronson: 2
Bruijn: 2
Buf: 2
Bulpin: 2
Bursty: 2
Businesses: 2
Buterin: 2
C1: 2
CB: 2
CDNs: 2
CIDR: 2
CONCLUSION: 2
CPUs: 2
CT: 2
CUBIC: 2
Caches: 2
Caja: 2
Calvin: 2
Canada: 2
Carey: 2
Carolina: 2
Carrera: 2
Castro: 2
Centralized: 2
Chain: 2
Chandra: 2
Characteristics: 2
Chicago: 2
China: 2
Chowdhry: 2
Chubby: 2
Claim: 2
Clark: 2
Cloud: 2
Colarelli: 2
Combined: 2
Comer: 2
Commercial: 2
Commit: 2
Common: 2
Companies: 2
Compared: 2
Comparing: 2
Competition: 2
Compilers: 2
Composition: 2
Compound: 2
Computation: 2
Conclusions: 2
Configuration: 2
Conservative: 2
Consumption: 2
Content: 2
Cooper: 2
Corfu: 2
Countermeasures: 2
Cox: 2
Critical: 2
Croquet: 2
Cryptography: 2
Cryptology: 2
D1: 2
DATA: 2
DCE: 2
DD: 2
DENTICAL: 2
DRPM: 2
Dan: 2
Daniel: 2
Databases: 2
Datta: 2
Degradations: 2
Delay: 2
Delaying: 2
Delta: 2
Demand: 2
Dependencies: 2
Description: 2
Detectors: 2
Different: 2
Diot: 2
Discovery: 2
Dissemination: 2
DotBIT: 2
Ds: 2
Dugan: 2
Dutta: 2
Dynamic: 2
E3: 2
EBay: 2
ECOOP: 2
ED: 2
EDUCAUSE: 2
ESBs: 2
ESIGN: 2
EV: 2
Ebling: 2
Economic: 2
Economics: 2
Effective: 2
Effects: 2
Efficacy: 2
Einstein: 2
Elapsed: 2
ElasTraS: 2
Elmore: 2
Embedded: 2
Enforcing: 2
Ensemble: 2
Entry: 2
Envelope: 2
Erasure: 2
Escriva: 2
EthereumWhitePaper: 2
EuroSys: 2
European: 2
Eva: 2
Exchange: 2
Explicit: 2
Expression: 2
F2: 2
F2Pool: 2
F2pool: 2
F30602: 2
F49620: 2
FETCH: 2
FL: 2
FOR: 2
Failures: 2
Fekete: 2
Felser: 2
Felten: 2
Feng: 2
Ferguson: 2
Fetch: 2
Fiber: 2
Financial: 2
Firoiu: 2
Fluid: 2
Forbes: 2
Force: 2
Forks: 2
Foundations: 2
Fourth: 2
Fox: 2
Fraleigh: 2
Fred: 2
Freeloaders: 2
Friedman: 2
Frobenius: 2
Furman: 2
Gehrke: 2
General: 2
Generating: 2
Generation: 2
Gibbs: 2
Gibson: 2
Glick: 2
Gr: 2
Grapevine: 2
Greene: 2
Grid: 2
Grids: 2
Griner: 2
Growth: 2
Guerraoui: 2
Gupta: 2
Guruprasad: 2
HTTP: 2
Habel: 2
Hardware: 2
Harley: 2
HashCash: 2
Health: 2
Heidelberg: 2
Hereinafter: 2
Hey: 2
Hibler: 2
Hierarchical: 2
Histogram: 2
Hit: 2
Hobor: 2
Honeyman: 2
Hop: 2
Hosting: 2
Hot: 2
HotCloud: 2
Huitema: 2
Hurwitz: 2
Huston: 2
I3P: 2
ICIW: 2
ICMP: 2
IF: 2
IFIP: 2
IH: 2
INING: 2
INSTITUTE: 2
INTRODUCTION: 2
IPDPS: 2
ISCUSSION: 2
ITCOIN: 2
IZS: 2
Idit: 2
Illinois: 2
Impact: 2
Implementing: 2
Incentives: 2
Incoming: 2
Index: 2
Infiltration: 2
Infocom: 2
Injection: 2
Innovation: 2
Inside: 2
Intended: 2
Interconnects: 2
Into: 2
Invalidations: 2
Irish: 2
Island: 2
Italy: 2
JBossCache: 2
James: 2
JavaBeans: 2
Jimenez: 2
Joglekar: 2
Jonsson: 2
Juels: 2
KJKJ: 2
Kandemir: 2
Kansas: 2
Kapritsos: 2
Karlsson: 2
Katabi: 2
Kazar: 2
Kemme: 2
Kernel: 2
Kharif: 2
Kiawah: 2
Kilper: 2
Kim: 2
Kimsas: 2
KnCMiner: 2
Kncminer: 2
Kojo: 2
Kroll: 2
Kurose: 2
LM: 2
LOSS: 2
Lacking: 2
Lake: 2
LambdaRail: 2
Landing: 2
Landolsi: 2
Lastly: 2
Later: 2
Le: 2
Least: 2
Leigh: 2
Lepreau: 2
Leskovec: 2
Leslie: 2
Level: 2
Leveraging: 2
Levy: 2
Lightwave: 2
Likewise: 2
Links: 2
Liskov: 2
Locations: 2
Logically: 2
Looking: 2
Lossy: 2
Louise: 2
Ltd: 2
Lundqvist: 2
Lynch: 2
Lyon: 2
MD: 2
MDCC: 2
MIT: 2
MLML: 2
MRC: 2
MST: 2
MURI: 2
Madden: 2
Madhow: 2
Magharei: 2
Majority: 2
Managed: 2
Managing: 2
March: 2
Martinez: 2
Mashup: 2
Massive: 2
Matthews: 2
Measurements: 2
Mechanism: 2
Media: 2
Memcached: 2
Menees: 2
Mesh: 2
Microsystems: 2
Minimize: 2
Mitigate: 2
Mobility: 2
Modeless: 2
Modeling: 2
Modern: 2
Mohr: 2
Montenegro: 2
Monterey: 2
Moshe: 2
Multigrep: 2
Muthitacharoen: 2
NAT: 2
NCSA: 2
NE: 2
NIC: 2
Nakamoto: 2
Name: 2
Nandi: 2
Narayanan: 2
Naughton: 2
Navathe: 2
Near: 2
Needham: 2
Neighborhood: 2
Neil: 2
Nejdl: 2
Netfilter: 2
Newbold: 2
News: 2
Ngan: 2
Nichols: 2
Node: 2
Nonnenmacher: 2
Normally: 2
Notes: 2
Notification: 2
OLE: 2
OMi: 2
ONON: 2
OOL: 2
OOLED: 2
Old: 2
Olympics: 2
Operators: 2
Optimizations: 2
Optimizing: 2
Options: 2
Ostar: 2
Otherwise: 2
Overcast: 2
Overflow: 2
PAM: 2
PCB: 2
PDCS: 2
PERATION: 2
PREFETCH: 2
PSockets: 2
PVLDB: 2
Paper: 2
Papers: 2
Parameshwaran: 2
Parity: 2
Park: 2
Parno: 2
Partially: 2
Passive: 2
Path: 2
Patterson: 2
Pause: 2
PayPal: 2
Pentium: 2
Periods: 2
Peris: 2
Perron: 2
Petersen: 2
Ph: 2
Phanishayee: 2
Physical: 2
Plugging: 2
Popper: 2
Popular: 2
Porto: 2
Preiss: 2
President: 2
Prevention: 2
Probability: 2
Programming: 2
Progress: 2
Prominent: 2
Properties: 2
Proxies: 2
Proxy: 2
Publish: 2
QP: 2
Qin: 2
QuickSilver: 2
RACTICALITIES: 2
RADC: 2
RE: 2
RELIMINARIES: 2
RFC3135: 2
RT: 2
RTTs: 2
Ramakrishnan: 2
Ramamritham: 2
Ramp: 2
Random: 2
Rateless: 2
Raw: 2
Readers: 2
Reading: 2
Receive: 2
Recipe: 2
Record: 2
Recycling: 2
Redundant: 2
Rejaie: 2
Relatively: 2
Rep: 2
Repair: 2
Repurposing: 2
Requirements: 2
Resilient: 2
Results: 2
Reykjavik: 2
Reynolds: 2
Ricci: 2
Rimon: 2
Roberts: 2
Robin: 2
Robustness: 2
Rohrs: 2
Roles: 2
Rome: 2
Rosenblum: 2
Ross: 2
SABUL: 2
SCSI: 2
SEDA: 2
SIGACT: 2
SIGOPS: 2
SOAs: 2
SRS: 2
STRATUM: 2
SWIFT: 2
Safety: 2
Saha: 2
Saint: 2
Saxena: 2
Sci: 2
Sections: 2
Selective: 2
Self: 2
Seltzer: 2
Semantic: 2
Seminar: 2
Senders: 2
Sending: 2
Senior: 2
Sept: 2
Servers: 2
Shapley: 2
Shelby: 2
Shi: 2
Shieh: 2
Shokrollahi: 2
Sidebotham: 2
Significantly: 2
Sigurbjornsson: 2
Silverlight: 2
Similar: 2
Simple: 2
Simulation: 2
Singh: 2
Sivakumar: 2
Sixth: 2
Small: 2
Societies: 2
Solaris: 2
Solheim: 2
Solo: 2
Solving: 2
Sometimes: 2
Son: 2
Sonic: 2
Sons: 2
Soper: 2
SourceForge: 2
South: 2
Sovran: 2
Space: 2
Special: 2
Spreitzer: 2
Stable: 2
Starting: 2
State: 2
States: 2
Stefan: 2
Stephen: 2
Stoller: 2
Stores: 2
Strategies: 2
Stream: 2
Strong: 2
Studies: 2
Study: 2
Sturgis: 2
Subsequent: 2
Subsystem: 2
Support: 2
Suppose: 2
Swanson: 2
Swinehart: 2
Swiss: 2
Symmetric: 2
Synthesis: 2
TABLE: 2
TANDARD: 2
TC6: 2
TODS: 2
TR: 2
TRSU: 2
TRUST: 2
TV: 2
Tango: 2
Tardos: 2
Tauber: 2
Temporary: 2
Theimer: 2
Three: 2
Times: 2
Tirumala: 2
Tm: 2
Tock: 2
Together: 2
Too: 2
Total: 2
Toueg: 2
Trading: 2
Traditionally: 2
Transient: 2
Transparent: 2
Tremel: 2
Trip: 2
Tuft: 2
UMich: 2
UWIN: 2
Under: 2
Uniform: 2
United: 2
Unless: 2
Untrusted: 2
Usage: 2
Used: 2
VPN: 2
Vancouver: 2
VanderMeer: 2
Vandermonde: 2
Varying: 2
Vasek: 2
Venkataramani: 2
Very: 2
Vice: 2
Vogels03: 2
WLOG: 2
WO: 2
WV: 2
WVWV: 2
Wallace: 2
Watch: 2
Wattenhofer: 2
Welch: 2
Wesley: 2
West: 2
Wicker: 2
Wiley: 2
Will: 2
Williams: 2
Willner: 2
Wolfgang: 2
Wong: 2
Workload: 2
Writes: 2
X1: 2
X2: 2
X3: 2
XORed: 2
Xie: 2
Yang: 2
_: 2
abandon: 2
abstractions: 2
acceptance: 2
accessible: 2
accomplished: 2
accordance: 2
accordingly: 2
accounting: 2
accumulating: 2
accurately: 2
accusation: 2
accused: 2
acquisition: 2
additionally: 2
addressing: 2
adjusted: 2
adjusts: 2
administrative: 2
adopt: 2
advancements: 2
advent: 2
adversary: 2
advertises: 2
advice: 2
advocated: 2
aforementioned: 2
age: 2
agents: 2
aggregator: 2
aggressive: 2
aim: 2
albums: 2
alive: 2
alleviating: 2
allocates: 2
allocating: 2
alloscomp: 2
amortizes: 2
amounting: 2
analytical: 2
analytically: 2
analyzes: 2
anomalies: 2
anonymity: 2
anonymously: 2
anticipate: 2
appealing: 2
appeared: 2
appends: 2
applicationindependent: 2
approximately: 2
approximation: 2
approximations: 2
archetypal: 2
arguably: 2
argues: 2
arguing: 2
arithmetic: 2
arose: 2
arraystructured: 2
arrived: 2
art: 2
article: 2
articles: 2
asks: 2
asp: 2
aspx: 2
assigning: 2
assists: 2
assurance: 2
astonishingly: 2
asynch: 2
attach: 2
attached: 2
attain: 2
attempted: 2
attempting: 2
attestation: 2
attract: 2
attraction: 2
attractiveness: 2
audio: 2
authentication: 2
automate: 2
autonomous: 2
autotuning: 2
backlashed: 2
barrier: 2
basically: 2
began: 2
behaved: 2
behaving: 2
belonging: 2
bespoke: 2
bifurcation: 2
billion: 2
binary: 2
binomial: 2
bitcoinfoundation: 2
blank: 2
blast: 2
blow: 2
boils: 2
bonus: 2
border: 2
borders: 2
box: 2
bridge: 2
brings: 2
brokerage: 2
browsers: 2
burden: 2
burstiness: 2
bus: 2
business: 2
butterfly: 2
bypass: 2
c0: 2
calculation: 2
calculations: 2
came: 2
candidates: 2
canonical: 2
capabilities: 2
cash: 2
casual: 2
casually: 2
cation: 2
cbcb: 2
cease: 2
ceases: 2
ceived: 2
centralization: 2
century: 2
certainty: 2
cfm: 2
challenging: 2
chance: 2
characterise: 2
cheaply: 2
checksums: 2
chemical: 2
chief: 2
circle: 2
circulate: 2
circulates: 2
circumstances: 2
cited: 2
click: 2
closing: 2
coarse: 2
coexist: 2
coherence: 2
cohort: 2
colocated: 2
color: 2
combat: 2
commerce: 2
committing: 2
commons: 2
communicated: 2
communications: 2
compatible: 2
compelled: 2
compensate: 2
compensation: 2
compiling: 2
complement: 2
composing: 2
composition: 2
computes: 2
concentrated: 2
concert: 2
conclusive: 2
confident: 2
confined: 2
connect: 2
cons: 2
considerably: 2
considers: 2
constitute: 2
constraint: 2
constructing: 2
contacted: 2
contexts: 2
contract: 2
contradicts: 2
controlling: 2
conversion: 2
converted: 2
converting: 2
cooling: 2
cording: 2
corner: 2
correcting: 2
correctness: 2
correspond: 2
counterpart: 2
covered: 2
covering: 2
credited: 2
credits: 2
cripple: 2
cripples: 2
criticism: 2
crossing: 2
crypto: 2
cryptocurrencies: 2
cryptographic: 2
cryptography: 2
cypherspace: 2
cz: 2
damage: 2
dangerous: 2
dast: 2
datasets: 2
days: 2
ddos: 2
deadlocks: 2
dealbook: 2
deceased: 2
decodes: 2
deduce: 2
deeper: 2
deeply: 2
defects: 2
defense: 2
defer: 2
degrading: 2
deleting: 2
deliberately: 2
deliberation: 2
delivering: 2
demanding: 2
depList: 2
depListcurr: 2
department: 2
dependence: 2
depicted: 2
depicts: 2
derive: 2
descriptor: 2
designing: 2
deterministically: 2
deviate: 2
devise: 2
dfreedman: 2
didn: 2
differentiable: 2
diminish: 2
disabled: 2
disadvantage: 2
disadvantages: 2
discount: 2
discovery: 2
disincentivize: 2
disrupting: 2
disseminates: 2
distances: 2
distinction: 2
diverse: 2
dividing: 2
dogecoin: 2
doi: 2
dominate: 2
dot: 2
doubt: 2
downloaded: 2
downloads: 2
downside: 2
drives: 2
driving: 2
dry: 2
dubbed: 2
dumbbell: 2
duplex: 2
duplicating: 2
eBay: 2
ePrint: 2
eagerly: 2
earned: 2
earnings: 2
economic: 2
ecosystem: 2
educause: 2
eigenvalue: 2
elastic: 2
electrical: 2
embeddings: 2
emphasis: 2
enables: 2
encapsulate: 2
encapsulated: 2
encoders: 2
encounter: 2
encountered: 2
encrypted: 2
endhost: 2
ends: 2
enforcing: 2
enterprises: 2
enters: 2
envelope: 2
envision: 2
eprint: 2
equate: 2
equilibria: 2
ered: 2
establishing: 2
estimates: 2
ethereum: 2
ets: 2
eventbased: 2
eventing: 2
everyone: 2
eviction: 2
evictions: 2
evolve: 2
exabytes: 2
exceeds: 2
exceptionally: 2
excessively: 2
executions: 2
exerted: 2
exerting: 2
exhibits: 2
expectation: 2
expectations: 2
expedite: 2
explains: 2
exploiting: 2
explores: 2
exponentially: 2
exports: 2
express: 2
extensibility: 2
extremes: 2
eyes: 2
f2pool: 2
facilitate: 2
facto: 2
fair: 2
falling: 2
faulttolerance: 2
featuring: 2
fetched: 2
fills: 2
finer: 2
finished: 2
finishes: 2
fired: 2
fits: 2
fix: 2
flag: 2
flexibly: 2
fluctuations: 2
flushed: 2
fold: 2
folders: 2
forcing: 2
forgo: 2
formats: 2
formulae: 2
forrestv: 2
forth: 2
fourth: 2
fragment: 2
fragments: 2
frameworks: 2
friction: 2
friendliness: 2
fundamentally: 2
funded: 2
funds: 2
gained: 2
gaps: 2
garbled: 2
gateway: 2
generalize: 2
generations: 2
genesis: 2
geometric: 2
ghash: 2
gigabyte: 2
github: 2
globalcrossing: 2
globe: 2
gossips: 2
gotten: 2
gradually: 2
granted: 2
grasp: 2
guesses: 2
guessing: 2
hackers: 2
halfway: 2
halted: 2
handful: 2
hands: 2
happens: 2
harder: 2
harm: 2
hashrate: 2
headers: 2
healthy: 2
heap: 2
height: 2
held: 2
helpful: 2
hiccups: 2
hides: 2
highlight: 2
highvalue: 2
hinder: 2
hits: 2
hoarding: 2
holy: 2
honesty: 2
hops: 2
hundred: 2
hungry: 2
hurdle: 2
iacr: 2
ic: 2
identities: 2
ie: 2
ii: 2
ij: 2
ill: 2
im: 2
imagined: 2
immaterial: 2
immediate: 2
immersion: 2
imperatives: 2
imperfect: 2
imply: 2
importantly: 2
imposing: 2
impractical: 2
impulse: 2
incorporation: 2
indices: 2
inefficiency: 2
inefficient: 2
infer: 2
influence: 2
influenced: 2
informed: 2
infrastructures: 2
ings: 2
inherent: 2
initialized: 2
initiates: 2
injecting: 2
insertion: 2
inspect: 2
instability: 2
install: 2
installed: 2
instantiated: 2
institutions: 2
instruction: 2
instructs: 2
integer: 2
intentionally: 2
interacting: 2
interconnecting: 2
interconnection: 2
intermediary: 2
internetworks: 2
interplay: 2
interpret: 2
intersection: 2
intervention: 2
intimate: 2
intricacies: 2
invalid: 2
investment: 2
invoke: 2
invokes: 2
ith: 2
ity: 2
java: 2
jsp: 2
justify: 2
keycurr: 2
kncminer: 2
knob: 2
knobs: 2
krzys: 2
labelled: 2
lagged: 2
lagging: 2
lakshmi: 2
lambdas: 2
laptop: 2
lastOp: 2
launching: 2
laying: 2
lease: 2
legitimate: 2
lemma: 2
lesson: 2
leverages: 2
leveraging: 2
litecoin: 2
literally: 2
literature: 2
locate: 2
lockstep: 2
longest: 2
longhaul: 2
loop: 2
loops: 2
loosely: 2
lowering: 2
lowers: 2
lows: 2
luck: 2
m2i: 2
managers: 2
managing: 2
mance: 2
mandatory: 2
manifested: 2
manipulate: 2
manually: 2
marshaling: 2
matrices: 2
mature: 2
maximal: 2
maximized: 2
maxx2: 2
meant: 2
megabits: 2
memoryless: 2
mid: 2
migrating: 2
milestone: 2
mined: 2
minimising: 2
minimizes: 2
minus: 2
mirroring: 2
miscalculate: 2
mismatched: 2
mistakenly: 2
mitigated: 2
mitigation: 2
mobility: 2
modem: 2
moderately: 2
modest: 2
modulo: 2
multiples: 2
multiplied: 2
multithreading: 2
myriad: 2
n1: 2
named: 2
names: 2
naming: 2
negation: 2
netfilter: 2
newer: 2
newly: 2
nity: 2
nlanr: 2
node5: 2
nonequilibrium: 2
nontransactional: 2
normalization: 2
normalize: 2
normalizing: 2
normed: 2
notably: 2
notifies: 2
notifying: 2
noting: 2
numerical: 2
nutshell: 2
nytimes: 2
obsolete: 2
occasion: 2
occupies: 2
odd: 2
offices: 2
ofwork: 2
omits: 2
op: 2
opacity: 2
opening: 2
operated: 2
opportunities: 2
opt: 2
opted: 2
ordinarily: 2
organizational: 2
originate: 2
orities: 2
oscillates: 2
ourselves: 2
outs: 2
outsource: 2
outsources: 2
overlays: 2
p2pool: 2
pacific: 2
pairings: 2
panacea: 2
paradigms: 2
parameterizations: 2
parametrized: 2
pass: 2
passing: 2
pathway: 2
payloads: 2
pdfs: 2
penalised: 2
percentile: 2
perfor: 2
permits: 2
permitted: 2
perpetrators: 2
phases: 2
physically: 2
picture: 2
piggybacked: 2
piling: 2
pipeline: 2
pipelines: 2
plague: 2
plays: 2
plication: 2
plugged: 2
poolattacks: 2
port: 2
poses: 2
position: 2
power1: 2
powersaving: 2
practically: 2
preceding: 2
precise: 2
precludes: 2
predecessor: 2
predefined: 2
predetermined: 2
predictive: 2
predicts: 2
preferable: 2
prefers: 2
premise: 2
presenting: 2
preservation: 2
preserve: 2
preserving: 2
prevail: 2
preventing: 2
primitive: 2
prioritising: 2
prioritization: 2
prioritize: 2
prioritizing: 2
privately: 2
probabilistically: 2
probe: 2
problematic: 2
processor: 2
processors: 2
produces: 2
profile: 2
profiler: 2
profit: 2
projection: 2
prominent: 2
promote: 2
proper: 2
proposals: 2
pros: 2
proving: 2
pseudo: 2
pulling: 2
purchase: 2
purchasing: 2
pursue: 2
pushes: 2
putting: 2
qhuang: 2
qmi: 2
qp: 2
qpqp: 2
quasistatic: 2
quote: 2
qwest: 2
race: 2
radical: 2
radio: 2
raised: 2
randomness: 2
ranges: 2
rank: 2
ratios: 2
reacts: 2
readily: 2
ready: 2
realizes: 2
rears: 2
reasonably: 2
reception: 2
reciprocation: 2
recognition: 2
recompute: 2
reconciliation: 2
redesigned: 2
redesigning: 2
reductions: 2
referred: 2
refraining: 2
refreshed: 2
regard: 2
registration: 2
reimplemented: 2
reissue: 2
reissued: 2
rejected: 2
rejecting: 2
rejects: 2
relay: 2
relevance: 2
relied: 2
relies: 2
relying: 2
remarks: 2
remotely: 2
removes: 2
rendering: 2
repaired: 2
repeatable: 2
repetitions: 2
reporting: 2
representation: 2
reputation: 2
requisite: 2
resending: 2
resends: 2
reserves: 2
resident: 2
resilient: 2
resolutions: 2
respecting: 2
responsive: 2
restarted: 2
retaliate: 2
retried: 2
retries: 2
retry: 2
reverse: 2
reviewers: 2
revisiting: 2
rewarded: 2
rewritten: 2
rightful: 2
rigid: 2
rigorous: 2
rigorously: 2
rising: 2
rivalry: 2
roots: 2
rose: 2
rounding: 2
routinely: 2
row: 2
rp: 2
rsized: 2
ruling: 2
runners: 2
rvr: 2
s3fs: 2
safe: 2
safety: 2
said: 2
sake: 2
sales: 2
scans: 2
scatter: 2
scattered: 2
scheduler: 2
schedules: 2
scientist: 2
scratch: 2
seamless: 2
secondgeneration: 2
seeking: 2
seemingly: 2
seldom: 2
selective: 2
sensible: 2
serial: 2
serialize: 2
serially: 2
serious: 2
serverlets: 2
serverpull: 2
settle: 2
shape: 2
sharp: 2
sheds: 2
shell: 2
shifts: 2
shortcut: 2
shortly: 2
sight: 2
signs: 2
silicon: 2
simplification: 2
simplified: 2
simulating: 2
sinks: 2
sister: 2
slashes: 2
slows: 2
slush: 2
smooth: 2
soar: 2
soars: 2
solely: 2
solid: 2
solved: 2
something: 2
spatial: 2
specialists: 2
spend: 2
spending: 2
splits: 2
spreads: 2
sprint: 2
stabilizes: 2
staggered: 2
staggering: 2
standalone: 2
stands: 2
stat: 2
station: 2
statqwest: 2
stats: 2
steal: 2
stocks: 2
stops: 2
stories: 2
storms: 2
story: 2
stripe: 2
styles: 2
subgroup: 2
submits: 2
submitting: 2
subscribed: 2
subversive: 2
subverted: 2
succession: 2
successively: 2
sudden: 2
suffice: 2
suggest: 2
suggestions: 2
suited: 2
summation: 2
supercomputing: 2
supergame: 2
supergames: 2
supervisors: 2
supply: 2
sure: 2
surpass: 2
surprise: 2
survey: 2
susceptibility: 2
suspect: 2
sustain: 2
swallow: 2
swapping: 2
symbolic: 2
synchronize: 2
sys: 2
systematic: 2
tablets: 2
tagged: 2
tailer: 2
tation: 2
technically: 2
tempted: 2
tend: 2
termed: 2
testbed: 2
tgperf: 2
th: 2
thing: 2
thirtieth: 2
thumb: 2
tiered: 2
ties: 2
tional: 2
tions: 2
toappliance: 2
tolerates: 2
tolerating: 2
topologically: 2
totally: 2
touch: 2
touches: 2
tough: 2
toy: 2
tracing: 2
tracked: 2
trades: 2
trading: 2
tragedy: 2
tranmission: 2
transact: 2
transcoded: 2
transforming: 2
trapezoids: 2
treatment: 2
triangular: 2
tricky: 2
tries: 2
trivially: 2
trol: 2
trustworthy: 2
tuned: 2
twentieth: 2
twofold: 2
ugly: 2
un: 2
unaltered: 2
unattacked: 2
unattractive: 2
uncommitable: 2
uncommitted: 2
unexplored: 2
unfavorable: 2
uninvolved: 2
uniquely: 2
unlike: 2
unlucky: 2
unnecessarily: 2
unnecessary: 2
unreachable: 2
unresponsive: 2
uploading: 2
urgent: 2
userspace: 2
v2: 2
validating: 2
vcurr: 2
vein: 2
ver: 2
verby: 2
versioned: 2
vice: 2
vicinity: 2
viewers: 2
violations: 2
virtualized: 2
visibility: 2
visited: 2
volumes: 2
wakeup: 2
walks: 2
war: 2
warehouse: 2
warship: 2
watching: 2
weakens: 2
weaker: 2
weaknesses: 2
whatever: 2
whereby: 2
whom: 2
wider: 2
widespread: 2
width: 2
willing: 2
win: 2
winter: 2
withholding4: 2
withholds: 2
witnessed: 2
wizkid057: 2
words: 2
workflow: 2
worsening: 2
wrapping: 2
wrongdoing: 2
x21: 2
x2i: 2
xp: 2
xth: 2
yx: 2
yxyx: 2
zoom: 2
000001: 1
0001: 1
000343: 1
006: 1
0060: 1
0101: 1
0137: 1
0244: 1
025ms: 1
0424422: 1
09: 1
10000: 1
100000: 1
100MB: 1
100Mbit: 1
100Mbps: 1
100s: 1
103: 1
1037: 1
1037_3: 1
104: 1
107: 1
10MB: 1
10gigabit: 1
117: 1
118: 1
1223: 1
123: 1
1233: 1
1238: 1
124: 1
1246: 1
125: 1
1259: 1
1267308: 1
1267332: 1
12th: 1
130140ms: 1
133: 1
134: 1
143: 1
144: 1
145: 1
1466448: 1
14850: 1
14ms: 1
1514244: 1
15ms: 1
161: 1
164: 1
173: 1
178: 1
17th: 1
180: 1
182: 1
187: 1
188: 1
189: 1
18th: 1
191: 1
191839: 1
191915: 1
1963: 1
1968: 1
1970s: 1
198: 1
1980: 1
1981: 1
1982: 1
1984: 1
1986: 1
1989: 1
1Gbit: 1
1K: 1
1st: 1
200MB: 1
200ms: 1
202: 1
2025: 1
205: 1
2059: 1
2086: 1
209: 1
2098: 1
2100: 1
212: 1
215: 1
21Figure: 1
2220: 1
224: 1
225: 1
227: 1
22GB: 1
230: 1
23125: 1
2321: 1
2323: 1
233269: 1
2333: 1
233330: 1
2339: 1
239: 1
2409: 1
2429: 1
243: 1
244: 1
24Figure: 1
2500: 1
252: 1
2551: 1
2567: 1
259: 1
263: 1
274: 1
282: 1
289: 1
291: 1
292: 1
2952: 1
2ms: 1
2s: 1
301: 1
313: 1
3135: 1
323: 1
325: 1
326: 1
3312: 1
332: 1
333: 1
334: 1
338: 1
340: 1
341: 1
346: 1
349: 1
360MB: 1
362: 1
363: 1
370: 1
37031: 1
374: 1
377: 1
384: 1
385: 1
386: 1
394: 1
398: 1
3GHz: 1
3c: 1
3ms: 1
3s: 1
401: 1
4105a: 1
411: 1
454: 1
45ms: 1
473: 1
476: 1
476884: 1
477: 1
481: 1
497: 1
4K: 1
500ms: 1
545: 1
5454: 1
549794: 1
54s: 1
556: 1
55615: 1
563: 1
56789: 1
569: 1
571: 1
57231: 1
57398: 1
576: 1
584: 1
586: 1
587: 1
599: 1
60000: 1
60731160630: 1
610: 1
611: 1
625: 1
631: 1
636: 1
640: 1
644: 1
650: 1
669: 1
677: 1
68: 1
69: 1
6s: 1
7000: 1
700ms: 1
71: 1
726: 1
730: 1
731: 1
74: 1
750: 1
7500: 1
7676: 1
79: 1
7K: 1
7ms: 1
8192: 1
841: 1
850: 1
865: 1
8700: 1
8K: 1
8KB: 1
907: 1
90ms: 1
90s: 1
91: 1
94: 1
9518: 1
970116: 1
986: 1
986798: 1
9912010: 1
A2: 1
A4: 1
ACLs: 1
AEGIS: 1
AELSTROM: 1
AF: 1
AR: 1
ARCHITECTURAL: 1
AS: 1
ASE: 1
AT: 1
ATTR: 1
AUDITING: 1
Aaron: 1
About: 1
Abu: 1
Ac: 1
Academic: 1
Academics: 1
Accessing: 1
Accurate: 1
Aceves: 1
Act: 1
Adamic: 1
Address: 1
Adelaide: 1
Advantages: 1
Adya: 1
Aegis: 1
Affinity: 1
Afterward: 1
Agent: 1
Aggregate: 1
Agreements: 1
Ahnn: 1
Akkus: 1
Alarm: 1
Albatross: 1
Albrecht: 1
Alegre: 1
Alexander: 1
AllThingsDistributed: 1
Allavena: 1
Alloc: 1
Allowing: 1
Almost: 1
Alternate: 1
Altruistic: 1
Altrustic: 1
Amar: 1
America: 1
Amsden: 1
Analytical: 1
Analytically: 1
Anceaume: 1
Andersen: 1
Andre: 1
Andreas: 1
Ann: 1
Anne: 1
Annie: 1
Anti: 1
Antipolis: 1
Antonio: 1
Appendix: 1
Appleton: 1
Appliances: 1
Applied: 1
Approaches: 1
Approaching: 1
Apps: 1
Arbor: 1
Are: 1
Argus: 1
Arrays: 1
Assigning: 1
Astrolabe: 1
Atomic: 1
Attaching: 1
Attacks: 1
Attar: 1
Audit: 1
Australia: 1
Authorized: 1
Autodesk: 1
Avoidance: 1
Award: 1
Aware: 1
Azim: 1
B1: 1
BASE: 1
BAST: 1
BCC: 1
BCQ: 1
BITCOIN: 1
BOA: 1
BONN: 1
BQ: 1
BROADCAST: 1
Bakalova: 1
Balancing: 1
Banaei: 1
Banff: 1
Barcelona: 1
Barracuda: 1
Basically: 1
Bast: 1
Bayeux: 1
Behren: 1
Beijing: 1
Benchmarking: 1
Benoit: 1
Bernd: 1
Bershad: 1
Bertier: 1
Bhattacharjee: 1
Bill: 1
Bindel: 1
Bio: 1
Birman1: 1
Bologna: 1
Bond: 1
Borisov: 1
Bortnikov: 1
Bostic: 1
Bounded: 1
Boycott: 1
Brahms: 1
Brazil: 1
Bregni: 1
Breslau: 1
Brian: 1
Brief: 1
British: 1
Broadcast: 1
Broberg: 1
Brokers: 1
Bruno: 1
Bugs: 1
Bulk: 1
Bullet: 1
Bullet10: 1
Bureau: 1
Burgess: 1
Burrows: 1
Busnel: 1
C5: 1
CAS: 1
CASTOR: 1
CAiSE: 1
CBQPCB: 1
CCDF: 1
CDROM: 1
CISCO: 1
CMU: 1
CODA: 1
COMMUNITIES: 1
CONTROL: 1
COPS: 1
CORFU: 1
COST: 1
CS: 1
CSD: 1
CURRENT: 1
Cabrera: 1
Camargos: 1
Cannes: 1
Cantwell: 1
Caratti: 1
Cardoso: 1
Caribbean: 1
Carl: 1
Carzaniga: 1
Category: 1
Census: 1
Centers: 1
Central: 1
Certification: 1
ChainLink: 1
Chainsaw4: 1
Chakka: 1
Chakravorty: 1
Challenger: 1
Chang: 1
Changes: 1
Changing: 1
Changtao: 1
Chapman: 1
Chawathe: 1
Check: 1
Checkout: 1
Chenchu: 1
Cheslack: 1
Chien: 1
Chinese: 1
Ching: 1
Chockler: 1
Chow: 1
Christmas: 1
Chu: 1
Chuck: 1
Cincilla: 1
Clara: 1
Class: 1
Cleaning: 1
Clear: 1
Clearly: 1
Clements: 1
Clones: 1
Closing: 1
CloudViews: 1
Cloudifying: 1
Cloudviews: 1
Clustered: 1
Clustering: 1
Clusters: 1
CoNext: 1
CoRR: 1
Codaniques: 1
Coding: 1
Collaborative: 1
Collapse: 1
Columbia: 1
Combining: 1
Comm: 1
Commatic: 1
Commentary: 1
Commerce: 1
Committed: 1
Communal: 1
Comp: 1
Comparative: 1
Compatible: 1
Compiling: 1
Component: 1
Components: 1
Compositional: 1
Computfits: 1
Concentration: 1
Concerns: 1
Conditioned: 1
Confidence: 1
Conflicts: 1
ConitBased: 1
Conner: 1
Conserving: 1
Considering: 1
Consortium: 1
Constraints: 1
Constructing: 1
Contact: 1
Contemporary: 1
Converge: 1
Conversely: 1
CoolStreaming: 1
CoolStreaming5: 1
Cooperative: 1
Cornelldeveloped: 1
Correcting: 1
Correctness: 1
Costs: 1
Cotton: 1
Cross: 1
Crowcroft: 1
CustomUserServiceApp: 1
Customers: 1
Cutler: 1
Cyrus: 1
Czerwinski: 1
DATE: 1
DBMS: 1
DC: 1
DDoS: 1
DEFGH: 1
DEIT: 1
DEPSA: 1
DEXA: 1
DHS: 1
DHT: 1
DILEMMA: 1
DISCUSSION: 1
DLLs: 1
DONet: 1
DUR: 1
Dallas: 1
Danielsson: 1
Danilov: 1
Dantzig: 1
Datacenter: 1
Datagram: 1
Dave: 1
Davoli: 1
DeFanti: 1
DeWitt: 1
Dean: 1
Decoupling: 1
Decrease: 1
Defanti: 1
Deferplications: 1
Defined: 1
Delays: 1
Delegation: 1
Demonstration: 1
Denning: 1
Dennis: 1
Dependency: 1
Depending: 1
Deployment: 1
Describing: 1
Detected: 1
Detecting: 1
Determining: 1
Devlin: 1
Diamond: 1
Diego: 1
Differentiated: 1
Differentiating: 1
Digests: 1
Dimov: 1
Ding: 1
Dinosaur: 1
Directing: 1
Directory: 1
Disconnected: 1
Discounts: 1
Disks: 1
Display: 1
Dissertation: 1
Distribution: 1
Divide: 1
Division: 1
Domain: 1
Don: 1
Douceur: 1
Doug: 1
Dr: 1
Drain: 1
Drifting: 1
Driven: 1
Dummynet: 1
Dunn: 1
EDD: 1
EED: 1
ELIABILITY: 1
ENT: 1
ENTIRE: 1
EPOSITORIES: 1
ESPRIT: 1
ETUP: 1
EVALUATION: 1
EYAL: 1
Earlier: 1
Eastham: 1
Echo: 1
Ed: 1
Editor: 1
Eds: 1
Eduardo: 1
Edutella: 1
Effectively: 1
Egemen: 1
Eighteenth: 1
Einar: 1
Either: 1
Ekin: 1
Elastic: 1
Elastras: 1
Electrical: 1
Electronics: 1
Elements: 1
Eliminating: 1
Elkin: 1
Email: 1
Embedding: 1
Emmanuelle: 1
Empirical: 1
Enabling: 1
Endowment: 1
Energy: 1
Enhanced: 1
Enterprises: 1
Environments: 1
Epstein: 1
Equeation: 1
Era: 1
Especially: 1
Eugster: 1
Eve: 1
Eventing: 1
Events: 1
ExSmart: 1
Exercise: 1
Expected: 1
Expert: 1
Expurge: 1
Expurging: 1
Extended: 1
External: 1
Eyal1: 1
FA8750: 1
FA9550: 1
FFS: 1
FOCS: 1
FROM: 1
FS: 1
FTCS: 1
FaceBook: 1
Faced: 1
FailureDetectio: 1
Fairness: 1
Fall: 1
Faloutsos: 1
False: 1
Farber: 1
Farms: 1
Farnoush: 1
Faults: 1
Feasibility: 1
Federal: 1
Federica: 1
Feeds: 1
Feldman: 1
Fellow: 1
Ferris: 1
Fikes: 1
Filesystem: 1
Fine: 1
Fiorano: 1
Firewalling: 1
Fisher: 1
Flickr: 1
Folders: 1
Forecast: 1
Fort: 1
Franke: 1
Frans: 1
Freenix: 1
Fresh: 1
Fricano: 1
Frost: 1
Fugal: 1
Function: 1
Fund: 1
Fundamental: 1
G1: 1
GCHeap: 1
GHz: 1
GIS: 1
GLOBECOM: 1
GRADIENT: 1
GROUP: 1
GUIs: 1
Gabriel: 1
Gaining: 1
Gallager: 1
Garbinato: 1
Garcia: 1
Gates: 1
Gateways: 1
Gauthier: 1
Geambasu: 1
Generally: 1
Generic: 1
Geneva: 1
Geoff: 1
Geoffrey: 1
Gershinsky: 1
Ghemawat: 1
Gian: 1
Giardullo: 1
Gifford: 1
Gilbert: 1
Ginting: 1
Ginza: 1
Glade: 1
Goal: 1
Goel: 1
Goldberg: 1
Googles: 1
Gossip9: 1
Gossiper: 1
GrC: 1
Gradient: 1
Grande: 1
Graph: 1
Greece: 1
GrepWe: 1
Griffioen: 1
Grimm: 1
Grochowski: 1
Grove: 1
Gruber: 1
Grunwald: 1
Guadelope: 1
Guarantee: 1
Gubarev: 1
Guha: 1
Gunnar: 1
Gurevich: 1
Guy: 1
Guyadec: 1
HORUS: 1
HPCA: 1
Had: 1
Hadoop: 1
Haifa: 1
Halem: 1
Hanan: 1
Handling: 1
Handurukande: 1
Hansell: 1
Haridasan: 1
Haridasana: 1
Harnessing: 1
Harpaz: 1
Hartman: 1
Harwood: 1
Hawaii: 1
HeartbeatMonitor: 1
Heavy: 1
Hegde: 1
Hegedu: 1
Heidemann: 1
Heiser: 1
Helen: 1
Hellerstein: 1
Hersonissos: 1
Heterogenous: 1
Hiding: 1
Highbandwidth: 1
Higher: 1
Highly: 1
Hill: 1
Hisgen: 1
History: 1
Hjelm: 1
Hochschild: 1
Holbrook: 1
Holte: 1
Homepage: 1
Homes: 1
Honolulu: 1
Hoon: 1
Hopcroft: 1
Horn: 1
Hosts: 1
HotICE: 1
HotNets: 1
Hsieh: 1
Hu: 1
Huazhong: 1
Huberman: 1
Human: 1
Hungary: 1
Hurricane: 1
Hyder: 1
ICCCN: 1
ICOMP: 1
ICPADS: 1
ICSOC: 1
IIS: 1
IMC: 1
IN: 1
INDIVIDUAL: 1
IPTO: 1
IPTPS: 1
IPTPS03: 1
IRECTIONS: 1
ISCA: 1
ISIS: 1
ITCC: 1
ITTAY: 1
Iceland: 1
Idea: 1
Implemenfrom: 1
Implicit: 1
Impossibility: 1
Improved: 1
Inconsistencies: 1
Indirection: 1
Individual: 1
Indranil: 1
Informatics: 1
Informed: 1
Infrastructure: 1
Ingrid: 1
Initially: 1
Inspection: 1
Instability: 1
Institute: 1
Instrumenting: 1
Int: 1
Integrated: 1
Intensive: 1
InterProcess: 1
Interacting: 1
Interaction: 1
Interactions: 1
Interactive: 1
Interadditional: 1
Interest: 1
Interix: 1
Internetworks: 1
Interplay: 1
Intervals: 1
Intrinsically: 1
Intrusion: 1
Intuitively: 1
IperfThe: 1
Israel: 1
Issuing: 1
Istemi: 1
Istva: 1
Iyengar: 1
JADE: 1
JIT_NewArr1: 1
JK: 1
JMS: 1
Jacobson: 1
Jain: 1
Janne: 1
Jannotti: 1
Jansch: 1
Jared: 1
Jari: 1
Javascript: 1
Jerian: 1
Jesi: 1
Jian: 1
Jiang: 1
Jin: 1
Jini: 1
Jiun: 1
Jones: 1
Jong: 1
Jorge: 1
Jr: 1
Judicious: 1
Julkunen: 1
Juni: 1
Junqueira: 1
Justice: 1
KDE: 1
KK: 1
KLOC: 1
Kallman: 1
Kalogeras: 1
Kamilmani: 1
Kaminsky: 1
Kamra: 1
Kanthak: 1
Karamanolis: 1
Karels: 1
Karp: 1
Kashani: 1
Katti: 1
Kay: 1
Kb: 1
Keidar2: 1
Keidl: 1
Keith: 1
Keller: 1
Kempe: 1
Kent: 1
Kerberos: 1
Kevin: 1
Khan: 1
Khorlin: 1
KiB: 1
Kirk: 1
Kistler: 1
Kleiman: 1
Kliot: 1
Knights: 1
Kodali: 1
Kogan: 1
Korhonen: 1
Koskela: 1
Kostic: 1
Kouznetsov: 1
Kr: 1
Kramer: 1
Kraska: 1
Krishnakumar: 1
Krishnamurthy: 1
Kristjan: 1
Krohn: 1
Kubiatowicz: 1
Kulkarni: 1
Kumar: 1
Kupfer: 1
Kwiatkowski: 1
LA: 1
LADIS: 1
LESSON: 1
LIMITATIONS: 1
LKJJ: 1
LOUDIFYING: 1
LPDC: 1
LR: 1
Laboratories: 1
Laboratory: 1
Labs: 1
Lafayette: 1
Laing: 1
Language: 1
Larger: 1
Larry: 1
Lauderdale: 1
Leases: 1
Lecture: 1
Lee: 1
Leff: 1
Lenz: 1
Less: 1
Levin: 1
Libdeh: 1
Light: 1
Lightly: 1
Lightweight: 1
Limit: 1
Limitations: 1
Limited: 1
Lin: 1
Linden: 1
Lindsay: 1
Linearly: 1
Linking: 1
Livny: 1
Load: 1
Loaded: 1
Lock: 1
Logging: 1
Login: 1
Logs: 1
Lombard: 1
LongDistance: 1
Loop: 1
Lost: 1
Louisiana: 1
Lowney: 1
Lugano: 1
Luna: 1
Luo: 1
Lyles: 1
MANAGEMENT: 1
MAY: 1
MAZON: 1
METADATA: 1
MEsh: 1
MI: 1
MINER: 1
MISSING: 1
MMCN: 1
MOD: 1
MODEL: 1
MONTHLY: 1
MPI: 1
MPLEMENTATION: 1
MSN: 1
MTS: 1
MVS: 1
Maarten: 1
Mach: 1
Machines: 1
Macrobenchmarks: 1
Maffeis: 1
Magazine: 1
Maglaris: 1
Mahajan: 1
Maheshwari: 1
Mailoth: 1
MainSoft: 1
Mainsoft: 1
Major: 1
Maki: 1
Making: 1
Malloth: 1
Manager: 1
Managers: 1
Mandreoli: 1
Mangling: 1
Mann: 1
Mao: 1
Map: 1
Marandi: 1
Marchukov: 1
Marcon: 1
Margo: 1
Marie: 1
Marin: 1
Mario: 1
Marketing: 1
Markoff: 1
Markus: 1
Marshal: 1
Martignon: 1
Maryland: 1
Marzullo: 1
Mashups: 1
Massachussetts: 1
Matskin: 1
Matt: 1
MaxContiguous: 1
Maxim: 1
Maximizing: 1
Maya: 1
Maymounkov: 1
Mazieres: 1
McAuliffe: 1
McElroy: 1
McKenney: 1
Mean: 1
Megastore: 1
Meirong: 1
Melnik: 1
Memcache: 1
Memoryrelated: 1
Mendel: 1
Merchant: 1
Messaging: 1
MetaCDN: 1
Metacdn: 1
Metadata: 1
Method: 1
Methodology: 1
MiB: 1
Miami: 1
Miao: 1
Micro: 1
Microbenchmarks: 1
Mika: 1
Mike: 1
Mikhail: 1
Minh: 1
Minjun: 1
Minneapolis: 1
Minnesota: 1
Minobrowser: 1
Minute: 1
Miskin: 1
Mislove: 1
Misra: 1
Mitzenmacher: 1
Mix: 1
Modified: 1
Module: 1
Mofavouring: 1
Mohan: 1
Moll: 1
Monitors: 1
Monnet: 1
Mono: 1
Month: 1
Moon: 1
Morris: 1
Motivation: 1
Moving: 1
Multicast1: 1
Multicasting: 1
Multimedia: 1
Mummert: 1
Mwaura: 1
My: 1
NAKs: 1
NATs: 1
NCA: 1
NEC: 1
NFS: 1
NO: 1
NOSSDAV: 1
NP: 1
NSFC: 1
Na: 1
Naaman: 1
Nagle: 1
Nahrstedt: 1
Naive: 1
Napper: 1
Narada1: 1
Nate: 1
Naturally: 1
Naval: 1
Navy: 1
Nawab: 1
Needs: 1
Nelson: 1
Net: 1
NetEcon: 1
Neufeld: 1
Nick: 1
Nikolov: 1
Ninth: 1
Nishimura: 1
Nishita: 1
Nishtala: 1
Nomad: 1
Non: 1
NonBlockingTransport: 1
Noncongestion: 1
Nutanong: 1
Nygren: 1
OF: 1
OFC: 1
OPENNT: 1
ORKS: 1
OST: 1
OTIVATION: 1
OURCE: 1
Obviously: 1
Only: 1
Ontology: 1
Opearting: 1
OpenAFS: 1
Operat: 1
Operation: 1
Operations: 1
Opportunity: 1
Optimistic: 1
Or: 1
Organizing: 1
Origin: 1
Originally: 1
Orleans: 1
Orma: 1
Outages: 1
OverQOS: 1
Overall: 1
Overhead: 1
Overqos: 1
Oversight12: 1
Overview: 1
Ozalp: 1
Ozsu: 1
PA: 1
PAPER: 1
PAST: 1
PC: 1
PDC: 1
PFLDnet: 1
POLICIES: 1
PPENDIX: 1
PPLive: 1
PRICING: 1
PRIME: 1
PROBLEM: 1
PROCESS: 1
PROJECTS: 1
PROTOCOL: 1
PTIONS: 1
PUBLICATION: 1
PULSE: 1
PVM: 1
Pacific: 1
Packs: 1
Page: 1
Paleczny: 1
Pallickara: 1
Pang: 1
Paolo: 1
Papazoglou: 1
Paritybased: 1
Parsa: 1
Participating: 1
Partitionable: 1
Partitions: 1
Passing: 1
Pastry: 1
Pat: 1
Paterson: 1
Patin: 1
Patino: 1
Patterns: 1
Pavlo: 1
Peek: 1
Peep: 1
Peking: 1
Pennsylvania: 1
Penzo: 1
People: 1
Perdichizzi: 1
Perfectly: 1
Performing: 1
Perhaps: 1
Periodically: 1
Permutation: 1
Personal: 1
Perspectives: 1
Perturbed: 1
Perv: 1
Petko: 1
Petrov: 1
PfHSN: 1
Phani: 1
Philadelphia: 1
Pianese: 1
Picconi: 1
Pirahesh: 1
Pitfalls: 1
Pittsburgh: 1
Placement: 1
Plain: 1
Platforms: 1
Poirier: 1
Poll: 1
Portability: 1
Portob: 1
Ports: 1
Positives: 1
Posix: 1
Postava: 1
Powell: 1
Practical: 1
Pratt: 1
Predictability: 1
Predictable: 1
Prefetches: 1
Preguica: 1
Preliminary: 1
Preventing: 1
Price: 1
Primitives: 1
Princehouse: 1
Priority: 1
ProceedIn: 1
Processes: 1
Processing: 1
Processor: 1
Products: 1
Professor: 1
Profiles: 1
Prometheus: 1
Protect: 1
Protocols1: 1
Provided: 1
Providing: 1
Provisioning: 1
Psockets: 1
Publishing: 1
Pull: 1
Pullbased: 1
Pulse15: 1
Purdue: 1
Pure: 1
Putting: 1
Puzar: 1
Pvt: 1
QNX: 1
QPQP: 1
QoS: 1
Qu: 1
Quality: 1
Quarterman: 1
Quema: 1
Queue: 1
Quick: 1
Quinlan: 1
RADI: 1
RAID: 1
RBUDP: 1
RCHITECTURE: 1
RECENT: 1
REFERENCES: 1
RELATED: 1
REPLICATION: 1
REPOSITORIES: 1
RFC: 1
RPCS: 1
Raab: 1
Rachid: 1
Rago: 1
Rates: 1
Rational: 1
Ratnasamy: 1
Ratner: 1
Rayfield: 1
Reader: 1
Realistic: 1
Reardon: 1
Reasons: 1
Recursive: 1
ReedSolomon: 1
Regardless: 1
Region: 1
Reid: 1
Reiher: 1
Relative: 1
Relaxed: 1
Relevant: 1
Ren: 1
Renderers: 1
Renesse1: 1
Renessea: 1
Replicated: 1
Reports: 1
Repositories: 1
Repository: 1
Representation: 1
Reservations: 1
Resources: 1
Result: 1
Retransmit: 1
Rexford: 1
Rhee: 1
Ricardo: 1
Ring: 1
Rio: 1
Ro: 1
Rockell: 1
Rodrigues: 1
Rodriguez: 1
Rolig: 1
Roselli: 1
Rotations: 1
Roughly: 1
Roussel: 1
Routing: 1
Roy: 1
Rubenstein: 1
Ruichuan: 1
Running: 1
Rx: 1
SAINT: 1
SATA: 1
SCALABILITY: 1
SDNs: 1
SI: 1
SIG: 1
SIGKDD: 1
SO: 1
SOA: 1
STANDARDS: 1
STATEMENT: 1
STORING: 1
STREAMING: 1
STRUCTURE: 1
SUNDR: 1
SWWS: 1
SYSTEM: 1
SaTPEP: 1
Saab: 1
Saikat: 1
Saito: 1
Sakoda: 1
Salt: 1
Sambamurthy: 1
Same: 1
Samet: 1
Sample: 1
Sampling: 1
Sandber: 1
Sankaran: 1
Santa: 1
Sarana: 1
Saroiu: 1
Satellite: 1
Savage: 1
Saving: 1
Scaleable: 1
Scenarios: 1
Scheduling: 1
Schematic: 1
Schiper: 1
Schlosser: 1
Schmidt: 1
Schroeder: 1
Schuh: 1
Schultz: 1
Sciascia: 1
Scott: 1
Seagate: 1
Search: 1
Sears: 1
Secondary: 1
Seek: 1
Seeks: 1
Seely: 1
Selecting: 1
SemanticWeb: 1
Sender: 1
Sensitive: 1
Sensors: 1
Separating: 1
Sequencing: 1
Serializability: 1
Sericola: 1
Serverless: 1
Sets: 1
Shacham: 1
Shah: 1
Shahabi: 1
Shalunov: 1
Shao: 1
Shapiro: 1
Sharing: 1
Shenghua: 1
Shim: 1
Shirriff: 1
Shoring: 1
Short: 1
Shown: 1
Shraer: 1
Shrideep: 1
Shupp: 1
Siegenthaler: 1
Siena: 1
Sigcomm: 1
Sight: 1
Sigmetrics: 1
Signal: 1
Silberstein: 1
Simon: 1
Simulaneous: 1
Simulations: 1
Singhai: 1
Sintek: 1
Sitaraman: 1
Sivasubramaniam: 1
Slack: 1
Slashdot: 1
SmallTalk: 1
Smith: 1
SoCC: 1
Society: 1
Sockets: 1
Softway: 1
Solution: 1
Sophia: 1
Spain: 1
Spatial: 1
Specification: 1
Speedups: 1
Spielman: 1
Spix: 1
SplitX: 1
SquirrelMail: 1
Sridharan: 1
St: 1
Stafford: 1
Standard: 1
Stanford: 1
Stanislav: 1
Stanton: 1
Statistics: 1
Steen: 1
Steere: 1
Stepping: 1
Stock: 1
Stodolsky: 1
Stoica: 1
Storing: 1
Strategy: 1
Streams: 1
Striping: 1
Structure: 1
Students: 1
Style: 1
Su: 1
Subject: 1
Submission: 1
Submitted: 1
Subramanian: 1
Subscribe: 1
Subsection: 1
Subservice: 1
SubserviceControl: 1
SubserviceProcess: 1
Sul: 1
Summer: 1
SunOS: 1
SuperBowl: 1
Supporting: 1
Surtani: 1
Suryanarayana: 1
Sussman: 1
Swart: 1
Sybil: 1
Symbolic: 1
Sympo: 1
Synchronization: 1
Synchrony: 1
Sys: 1
Systemic: 1
Szeged: 1
Szymaniak: 1
TAO: 1
TDI: 1
TECHNOLOGIES: 1
TIMEOUT: 1
TOPLAS: 1
TR95: 1
TTLs: 1
TUDY: 1
TWEB: 1
TX: 1
Take: 1
Tam: 1
Tamma: 1
Tan: 1
Tanin: 1
Tari: 1
Te: 1
Technion: 1
Technologists: 1
Telecommunications: 1
Terminology: 1
Terms: 1
Testimony: 1
Th: 1
Thick: 1
Thorsten: 1
Through: 1
TiB: 1
Tier: 1
Tier1: 1
Timeline: 1
Timeouts: 1
Timo: 1
Todd: 1
Tokyo: 1
Tom: 1
Tool: 1
Towards: 1
Track: 1
Transactypes: 1
Transis: 1
Transmission: 1
Treating: 1
Tree: 1
Trees: 1
Tri: 1
Trial: 1
Truong: 1
Trust: 1
Tsatalos: 1
Tung: 1
Turing: 1
Twelth: 1
Twitter: 1
TxCache: 1
Typically: 1
UBICOMM: 1
UBLCS: 1
UC: 1
UCB: 1
URL: 1
URLs: 1
UT: 1
UTRSUT: 1
UTT: 1
UTURE: 1
UUT: 1
UUTT: 1
Univ: 1
Univerisity: 1
Universita: 1
Unreliable: 1
Unusually: 1
Upcall: 1
Upson: 1
Use: 1
Utilities: 1
VALIDATE: 1
VB: 1
VMM: 1
VMS: 1
Value: 1
Values: 1
Variant: 1
Various: 1
Vasilatos: 1
Veitch: 1
Velenis: 1
Vendors: 1
Venkataraman: 1
Video: 1
Vidhyashankar: 1
View: 1
ViewSynchronous: 1
Vishnumurthy: 1
Vista: 1
Visual: 1
Vivek: 1
VoIP: 1
Voelker: 1
Vogel: 1
Vollset: 1
Volume: 1
Vrable: 1
WA: 1
WANs: 1
WCNC: 1
WEB: 1
WG6: 1
WORK: 1
WORKING: 1
WSDL: 1
WSPDS: 1
WSTransactions: 1
WScompatible: 1
Wallach: 1
Warp: 1
WebSphere: 1
Webfs: 1
Websphere: 1
Weight: 1
Well: 1
Weng: 1
Were: 1
Wes: 1
Westerlund: 1
Whatever: 1
WhatsNew: 1
Wheeler: 1
Whole: 1
Why: 1
WiCom: 1
Widespread: 1
Wieloch: 1
Wilma: 1
Winnie: 1
Wolf: 1
Woo: 1
Woodford: 1
Working: 1
Worse: 1
Wu: 1
XCP: 1
XISTING: 1
XPERIMENTAL: 1
XYX: 1
Xi: 1
Xiao: 1
Xu: 1
YXXY: 1
YXYX: 1
Yaghmazadeh: 1
Yann: 1
Yee: 1
Yin: 1
Ylianttila: 1
Ymir: 1
YouTube: 1
Yuanchao: 1
Yuanyuan: 1
Yum: 1
Yushprakh: 1
Zahorjan: 1
Zdonik: 1
Zelenka: 1
Zephyr: 1
Zettabyte: 1
Zhen: 1
Zhenqi: 1
Zhou: 1
Zhuang: 1
Zou: 1
Zuck: 1
Zwilling: 1
a1: 1
a2: 1
aR: 1
abbreviations: 1
abilalso: 1
absorbs: 1
abstracts: 1
ac: 1
acThe: 1
academics: 1
accompanied: 1
accomplish: 1
accomplishing: 1
accounted: 1
accumulate: 1
acheive: 1
acheived: 1
acks: 1
acmula: 1
acomplish: 1
acquires: 1
acquiring: 1
activated: 1
actuators: 1
acwe: 1
adaptable: 1
adapted: 1
adaptively: 1
adaptivity: 1
addi: 1
addiserver: 1
additive: 1
addressable: 1
adhere: 1
adjacent: 1
administers: 1
admit: 1
admits: 1
adoption: 1
ads: 1
advertise: 1
advertised: 1
aerospace: 1
affinity: 1
affirmative: 1
afford: 1
afforded: 1
afraid: 1
agenda: 1
ager: 1
aggregat: 1
aggregrate: 1
aggressively: 1
aggressiveness: 1
aid: 1
aims: 1
airplane: 1
ak: 1
alarms: 1
alfetching: 1
allegedly: 1
alleviated: 1
alleviates: 1
allo: 1
allocations: 1
alof: 1
alpha: 1
altogether: 1
altruistic: 1
am: 1
ambitious: 1
amortized: 1
ample: 1
amplify: 1
analogy: 1
analyse: 1
analyst: 1
analytics: 1
ance: 1
anddrop: 1
annotated: 1
announce: 1
announcement: 1
announces: 1
annoyingly: 1
annually: 1
anonymous: 1
answered: 1
answers: 1
anti: 1
anticipated: 1
anytime: 1
anyway: 1
anywhere: 1
ap: 1
apa: 1
appeal: 1
applaud: 1
appli4: 1
appliIt: 1
applicationdropped: 1
applior: 1
applithe: 1
approximates: 1
apthe: 1
ar: 1
architects: 1
archival: 1
archives: 1
arisen: 1
armies: 1
arranging: 1
arrivals: 1
artifact: 1
artificial: 1
aside: 1
ask: 1
asking: 1
assembling: 1
assist: 1
assisted: 1
associates: 1
associating: 1
aswhen: 1
asynMicrobenchmarks: 1
asynMobile: 1
asynbandwidth: 1
asynchrony: 1
asynqueue: 1
ata: 1
atkin: 1
atomically: 1
atr: 1
att: 1
attaches: 1
attention: 1
attracted: 1
attributable: 1
attributed: 1
atypical: 1
audits: 1
authoritative: 1
automation: 1
autotion: 1
autowriteback: 1
autumn: 1
ava: 1
availTo: 1
avatar: 1
avoidance: 1
awaiting: 1
awareness: 1
axes: 1
b1: 1
b2: 1
backNT: 1
backgound: 1
backgrounds: 1
backlog: 1
backlogs: 1
backups: 1
balancers: 1
ballance: 1
ballot: 1
bandHowever: 1
bandWhile: 1
bandcontain: 1
bandwidthdelay: 1
bandwidthsensitive: 1
bare: 1
barriers: 1
baseline: 1
bases: 1
bastion: 1
batched: 1
batches: 1
batching: 1
batkin: 1
battle: 1
battlefield: 1
battleground: 1
became: 1
befriended: 1
belief: 1
believed: 1
beloved: 1
benchmarks: 1
benenational: 1
bered: 1
berkeleydb: 1
bership: 1
bert: 1
biggest: 1
bile: 1
billed: 1
billing: 1
bills: 1
binds: 1
bitcoinmines: 1
bk: 1
blades: 1
blame: 1
ble: 1
bleeding: 1
blending: 1
blogs: 1
board: 1
body: 1
bold: 1
bonding: 1
book: 1
boosting: 1
boot: 1
boss: 1
bother: 1
bottlenecks: 1
bought: 1
boundary: 1
bounds: 1
boycott: 1
boys: 1
br: 1
breakthrough: 1
bridging: 1
bright: 1
broad: 1
brought: 1
budget: 1
budgets: 1
buffercould: 1
bug: 1
buggy: 1
builders: 1
burdens: 1
buried: 1
button: 1
buyer: 1
buying: 1
buys: 1
cacheable: 1
cacheserializability: 1
cal: 1
calability: 1
cameras: 1
cancelled: 1
candidate: 1
capability: 1
cappuccino: 1
captured: 1
carded: 1
cardinality: 1
career: 1
careers: 1
careful: 1
carried: 1
carrier: 1
carrying: 1
cart: 1
cascades: 1
cast: 1
catalog: 1
catastrophe: 1
catastrophic: 1
cated: 1
categorized: 1
category: 1
cations: 1
causal: 1
cbb: 1
cbbc: 1
ccb: 1
cdf: 1
ceives: 1
cell: 1
census: 1
centered: 1
ceremony: 1
certify: 1
certifying: 1
cess: 1
chained: 1
chair: 1
chandra: 1
characterised: 1
cheat: 1
checkout: 1
checkouts: 1
chicken: 1
chiefly: 1
choke: 1
chose: 1
christened: 1
chronously: 1
chrony: 1
chubby: 1
chunk: 1
chunks: 1
circuitous: 1
circulation: 1
circumvented: 1
circumvents: 1
citation: 1
cites: 1
cities: 1
clarity: 1
cleaned: 1
cleanly: 1
clearer: 1
clientserver: 1
clocks: 1
clone: 1
closest: 1
clutter: 1
coded: 1
coexistence: 1
coexists: 1
coherency: 1
coincide: 1
collaborate: 1
collaborating: 1
collapsing: 1
collateral: 1
collects: 1
collusions: 1
comings: 1
comitting: 1
commence: 1
commences: 1
commenting: 1
commercially: 1
commonplace: 1
communi: 1
commutative: 1
compact: 1
comparaOur: 1
comparably: 1
comparisons: 1
compelling: 1
compensating: 1
competitors: 1
compiler: 1
compilers: 1
complementary: 1
complementing: 1
complexities: 1
complimentary: 1
comply: 1
compound: 1
compressed: 1
compresses: 1
comprising: 1
compromises: 1
compromising: 1
computationally: 1
comsium: 1
conThe: 1
concentrates: 1
conception: 1
conceptually: 1
concludes: 1
conclusions: 1
concrete: 1
concurby: 1
concuruses: 1
configure: 1
conform: 1
conios: 1
conjecture: 1
connec: 1
connectionless: 1
consequent: 1
consisted: 1
consisupdate: 1
constantly: 1
constituent: 1
constrain: 1
constructs: 1
consulting: 1
consumer: 1
consumers: 1
contacting: 1
contally: 1
contemplate: 1
contemplated: 1
contemplating: 1
contending: 1
contingencies: 1
contolerate: 1
contractors: 1
contravention: 1
contributes: 1
controller: 1
controversy: 1
convenience: 1
convention: 1
converged: 1
converging: 1
converts: 1
conveyed: 1
convoys: 1
cooperating: 1
coordinated: 1
coordinating: 1
coordinators: 1
copied: 1
coping: 1
corrected: 1
corrections: 1
correspondingly: 1
corroborated: 1
corrupted: 1
costly2: 1
counters: 1
counting: 1
couple: 1
crashing: 1
crawl: 1
credit: 1
credo: 1
crete: 1
crippling: 1
criteria: 1
critically: 1
crossroads: 1
cstr: 1
culled: 1
cumulatively: 1
customised: 1
customizability: 1
customizable: 1
customization: 1
customizing: 1
cuts: 1
cybercaf: 1
dat: 1
datagrams: 1
dataset: 1
datastores: 1
daunting: 1
dazzling: 1
db: 1
dbms: 1
de4: 1
dead: 1
deadlock: 1
deadly: 1
deadtially: 1
debatable: 1
debate: 1
debated: 1
debilitating: 1
debris: 1
debugger: 1
declare: 1
declaring: 1
declines: 1
decode: 1
decoder: 1
decoding: 1
decoupled: 1
decouples: 1
decreased: 1
defend: 1
deferrable: 1
defers: 1
defining: 1
definitions: 1
degrees: 1
delete: 1
deleted: 1
delivers: 1
demanded: 1
demise: 1
denoting: 1
dep: 1
depart: 1
departments: 1
dependalso: 1
dependenrate: 1
depth: 1
dequeued: 1
der: 1
dered: 1
deregistered: 1
derivative: 1
derivatives: 1
derives: 1
deriving: 1
des: 1
descending: 1
deserver: 1
deshow: 1
designated: 1
designating: 1
designer: 1
desirability: 1
desirable: 1
desktops: 1
destabilize: 1
destinations: 1
detailing: 1
deterioration: 1
determinants: 1
detrimental: 1
dev: 1
developerhours: 1
deviations: 1
devised: 1
devoted: 1
devoting: 1
dictionaries: 1
dies: 1
differAny: 1
differential: 1
differentiates: 1
differenwith: 1
differing: 1
diffs: 1
dimensions: 1
dinosaur: 1
directs: 1
disable: 1
disables: 1
disappearance: 1
disappeared: 1
disastrous: 1
disbut: 1
discard: 1
disconnections: 1
disconnects: 1
discontinuing: 1
discotheque: 1
discovered: 1
discusses: 1
discussing: 1
disguise: 1
disinclined: 1
disjoing: 1
dislike: 1
dismiss: 1
dispatch: 1
dispatcher: 1
dispatching: 1
displayed: 1
displaying: 1
displays: 1
dispools: 1
disregarded: 1
dissatisfied: 1
disseminating: 1
dissertation: 1
distill: 1
distorted: 1
diverges: 1
diversion: 1
diversity: 1
divide: 1
dll: 1
do1: 1
do11: 1
do2: 1
do21: 1
do22: 1
do32: 1
documented: 1
dok: 1
dok1: 1
dol: 1
domains: 1
dominating: 1
door: 1
dou: 1
doubled: 1
downgraded: 1
downtime: 1
dozen: 1
drIven: 1
drained: 1
drawbacks: 1
drawing: 1
draws: 1
drift: 1
drive: 1
drivers: 1
drone: 1
drpm: 1
duals: 1
ducing: 1
dundant: 1
duplicates: 1
durably: 1
durations: 1
dwarf: 1
dx: 1
dynamics: 1
ear: 1
echo: 1
economics: 1
economies: 1
ecution: 1
edges: 1
edit: 1
edition: 1
editors: 1
edits: 1
educational: 1
efficacies: 1
eg: 1
egg: 1
eighteenth: 1
einar: 1
elasticity: 1
elect: 1
electric: 1
electricity: 1
eleventh: 1
elicit: 1
eliciting: 1
embark: 1
embody: 1
embrace: 1
embraces: 1
emerged: 1
emerges: 1
emitting: 1
empirical: 1
empted: 1
emto: 1
emulating: 1
encapsulates: 1
encod1: 1
encode: 1
encompass: 1
encourages: 1
encrypt: 1
encrypts: 1
end2end: 1
endeavor: 1
endpoints: 1
endtransaction: 1
endtransport: 1
enemy: 1
enforced: 1
engine: 1
engineers: 1
enlarge: 1
enlarges: 1
enlarging: 1
enputing: 1
ent: 1
entail: 1
entering: 1
entitled: 1
entropy: 1
enumerate: 1
episodes: 1
equipped: 1
era: 1
erally: 1
erates: 1
erent: 1
ering: 1
erodes: 1
erred: 1
errorprone: 1
ery: 1
eschewing: 1
establish: 1
establishes: 1
evalappended: 1
eve: 1
eventoriented: 1
everyday: 1
everywhere: 1
evil: 1
evocative: 1
evolves: 1
evolving: 1
exacerbating: 1
exaggerated: 1
examined: 1
excellently: 1
excess: 1
exchanging: 1
excitement: 1
exclusively: 1
exec: 1
execusystem: 1
executable: 1
exemplifies: 1
exhibitThe: 1
existed: 1
exited: 1
exiting: 1
expects: 1
expedited: 1
expense: 1
experiencing: 1
expire: 1
explanation: 1
explanatory: 1
exploits: 1
exponentiated: 1
exporting: 1
expose: 1
exposition: 1
expunged: 1
expurging: 1
exspecified: 1
extending: 1
extensible: 1
extensively: 1
extracting: 1
extracts: 1
f2000: 1
facilitates: 1
facilitating: 1
facing: 1
factory: 1
failing: 1
failwhen: 1
faithful: 1
fall: 1
falsely: 1
faltered: 1
famous: 1
farewell: 1
farm: 1
farms: 1
farther: 1
fascination: 1
fashioned: 1
fastest: 1
featured: 1
fec: 1
feel: 1
feels: 1
feet: 1
fell: 1
fellowship: 1
felt: 1
ferred: 1
fic: 1
fidelity: 1
fifteen: 1
fifth: 1
figured: 1
filing: 1
filled: 1
films: 1
filtered: 1
filters: 1
finance: 1
findings: 1
finegrained: 1
finergrained: 1
finish: 1
firewall: 1
firewalling: 1
firms: 1
fitted: 1
fitting: 1
fixes: 1
flavors: 1
flaws: 1
flood: 1
flooding: 1
fluctuating: 1
fluke: 1
flushing: 1
fly: 1
football: 1
forcibly: 1
foregoing: 1
foremost: 1
foresee: 1
forgoing: 1
fork: 1
forked: 1
formal: 1
formally: 1
formed: 1
formerly: 1
fortune: 1
forwards: 1
foundation: 1
founder: 1
fractions: 1
freed: 1
freeloaders: 1
freeze: 1
fresh: 1
freshness: 1
friendly: 1
friends: 1
friendship: 1
frightening: 1
frontend: 1
fulfilling: 1
fulfillment: 1
functional: 1
funding: 1
furniture: 1
furthest: 1
gabit: 1
gathering: 1
gauge: 1
gave: 1
gc_heap_garbage_collect: 1
generalized: 1
genercache: 1
generous: 1
genit: 1
genuinely: 1
geodistributed: 1
gestion: 1
getting: 1
gi: 1
gist: 1
giving: 1
glance: 1
glitches: 1
globalized: 1
gold: 1
gossiper: 1
gov: 1
governed: 1
governmental: 1
grade: 1
gradual: 1
granting: 1
granularity: 1
graphical: 1
gree: 1
grids: 1
grips: 1
grounds: 1
groundwork: 1
gunnars10: 1
gurus: 1
h: 1
hacked: 1
hacker: 1
hacking: 1
halt: 1
halts: 1
halves: 1
hampering: 1
handing: 1
handled: 1
handler: 1
handoff: 1
hang: 1
happened: 1
harmful: 1
hasn: 1
hazard: 1
hazards: 1
headquarters: 1
heard: 1
heart: 1
heat: 1
heavyweight: 1
helped: 1
henceforth: 1
her: 1
herein: 1
hesitate: 1
heterogeneity: 1
heterogenous: 1
heuristic: 1
heuristics: 1
highassurance: 1
highbut: 1
highlights: 1
highlyavailable: 1
highpower: 1
highquality: 1
hik: 1
him: 1
himself: 1
holder: 1
holding: 1
homepage: 1
homomorphic: 1
honored: 1
hoping: 1
horizontal: 1
horrible: 1
hospital: 1
hotly: 1
hotzone: 1
hours: 1
howa: 1
hreshold: 1
hrs: 1
htm: 1
hurting: 1
hyper: 1
iDigest: 1
ically: 1
icons: 1
idempotence: 1
iden: 1
idenhigh: 1
identically: 1
ied: 1
igniting: 1
ignorance: 1
ihkj: 1
iii: 1
ilability: 1
illegal: 1
illusion: 1
imaginable: 1
imation: 1
imlocal: 1
immature: 1
immersive: 1
impacts: 1
implememted: 1
implemenshared: 1
implementapackages: 1
implosion: 1
implying: 1
import: 1
imported: 1
imporwriteback: 1
imposition: 1
imprecise: 1
impressive: 1
improv: 1
inNotice: 1
inadequacy: 1
inadequate: 1
inadequately: 1
inapplicable: 1
inappropriate: 1
inbound: 1
inc: 1
incentives: 1
incentivize: 1
inception: 1
inclusive: 1
incompatible: 1
incomplete: 1
incon1: 1
inconsisto: 1
incorpoas: 1
increasess: 1
incredibly: 1
incrementing: 1
indefinitely: 1
independence: 1
indexing: 1
indicators: 1
indictment: 1
indirect: 1
indirection: 1
indirectly: 1
indispensable: 1
individInterference: 1
individuals: 1
induced: 1
industrystandard: 1
inefficiencies: 1
inevitable: 1
inexpennet: 1
inf: 1
infect: 1
infected: 1
infection: 1
infectious: 1
inference: 1
inferred: 1
inflate: 1
inflexibility: 1
inflight: 1
informa: 1
informing: 1
informs: 1
infrequent: 1
infused: 1
ingrid: 1
inherited: 1
inhibit: 1
inhibiting: 1
inicontention: 1
initialised: 1
initialize: 1
initiating: 1
initiation: 1
inject: 1
injects: 1
inner: 1
innetwork: 1
innovate: 1
inprocess: 1
inputs: 1
insecure: 1
insensitive: 1
inseparable: 1
inserted: 1
inserts: 1
inside: 1
inspired: 1
instruct: 1
instructed: 1
instructing: 1
instructions: 1
instrument: 1
instrumented: 1
integers: 1
integrity: 1
intellection: 1
intellectual: 1
intelligence: 1
intem: 1
intense: 1
intensity: 1
intention: 1
intentions: 1
intercluster: 1
interdependence: 1
interests: 1
interfere: 1
interferes: 1
interix: 1
interject: 1
interlinked: 1
intermittent: 1
intermittently: 1
internally: 1
internships: 1
interoriginally: 1
interpreted: 1
interpreting: 1
interrogates: 1
interrupt: 1
interrupts: 1
interspersed: 1
intervenallows: 1
intra: 1
intractable: 1
intrapartition: 1
introduction: 1
intrusions: 1
invaluable: 1
invariant: 1
invest: 1
investigations: 1
invisible: 1
invocation: 1
invocations: 1
involve: 1
involvement: 1
involving: 1
ip: 1
irregularity: 1
irregularly: 1
irrelevant: 1
irrespectively: 1
isola: 1
isolate: 1
isolated: 1
isolating: 1
items: 1
iterators: 1
ities: 1
itors: 1
iv: 1
j2ee: 1
jboss: 1
jbosscache: 1
je: 1
jmc279: 1
job: 1
jobs: 1
joined: 1
joint: 1
jostling: 1
jump: 1
jumpy: 1
justification: 1
kernels: 1
kernilized: 1
keyboard: 1
kick: 1
kicking: 1
kicks: 1
kid: 1
kilobyte: 1
kilobytes: 1
kit: 1
knowledgments: 1
kristjanvj: 1
labs: 1
lacked: 1
lag: 1
lags: 1
laid: 1
lamb: 1
land: 1
landlord: 1
lands: 1
landscapes: 1
lans: 1
lapping: 1
lasting: 1
launch: 1
law: 1
lay: 1
layed: 1
layering: 1
lazy: 1
leaks: 1
learned: 1
learners: 1
leasing: 1
led: 1
legends: 1
lenient: 1
lesser: 1
lets: 1
lever: 1
liberal: 1
licensing: 1
lie: 1
lied: 1
lieu: 1
lighter: 1
lightly: 1
likelihood: 1
limitation: 1
lin: 1
linearizable: 1
linger: 1
linkages: 1
linux: 1
lion: 1
listen: 1
listening: 1
littered: 1
liveness: 1
lives: 1
livestreaming: 1
ln: 1
lo: 1
localized: 1
localizes: 1
lockfree: 1
log2: 1
logarithmic: 1
logarithmically: 1
logically: 1
logics: 1
logstructured: 1
longput: 1
longrunning: 1
lookup: 1
loose: 1
loses: 1
lowMFS: 1
lowand: 1
lowbandwidth: 1
lowerfile: 1
lowpower: 1
lying: 1
magnetic: 1
magnified: 1
magninetwork_performance_current: 1
mailing: 1
mainly: 1
mainsoft: 1
mainteConsequently: 1
mak: 1
man: 1
manM: 1
mandated: 1
mangling: 1
manipulates: 1
marrying: 1
mash: 1
mashing: 1
masking: 1
mass: 1
matches: 1
materials: 1
matically: 1
maya: 1
mea: 1
meaning: 1
meaningfully: 1
meantime: 1
mechacies: 1
medians: 1
mediarich: 1
mediately: 1
medical: 1
meet: 1
meetings: 1
megabytes: 1
mem: 1
memcached: 1
memcopy: 1
men: 1
menus: 1
mer: 1
merging: 1
meta: 1
metaphor: 1
metrics: 1
micro: 1
microsoft: 1
mid1990s: 1
migrate: 1
mile: 1
milestones: 1
millions: 1
mimic: 1
mindset: 1
minimise: 1
minimised: 1
minimized: 1
minitransactions: 1
mirrored: 1
mirrors: 1
misFinally: 1
misbehavior: 1
mistakes: 1
mitigate: 1
mitigates: 1
mitigating: 1
mixes: 1
ml: 1
moderate: 1
modewell: 1
modularity: 1
moments: 1
monetary: 1
monin: 1
monolithic: 1
monotonically: 1
monthly: 1
morning: 1
mostlyreads: 1
motes: 1
motivating: 1
motivations: 1
mounted: 1
mouse: 1
movement: 1
mscorwks: 1
msgs: 1
msiegen: 1
mulsequence: 1
multiframed: 1
multigigabit: 1
multiin: 1
multiplexing: 1
multispeed: 1
multitier: 1
multitude: 1
multiversioning: 1
multiwas: 1
multo: 1
murderous: 1
music: 1
mutexes: 1
nPapers: 1
naively: 1
namespace: 1
nance: 1
nary: 1
nat: 1
navigate: 1
ndi: 1
neatly: 1
nec: 1
necessity: 1
needing: 1
needless: 1
needlessly: 1
negative: 1
negatively: 1
negligibly: 1
negotiate: 1
nei: 1
neighborhood: 1
neighboring: 1
ness: 1
neting: 1
newscasts: 1
nextgeneration: 1
ngas: 1
nicely: 1
niche: 1
night: 1
nimble: 1
nishtala: 1
nism: 1
nization: 1
node6: 1
noisy: 1
nondeterministic: 1
nonempty: 1
noteworthy: 1
notified: 1
notoriously: 1
nowadays: 1
nsdi13: 1
ntserver: 1
num: 1
numB: 1
numeric: 1
obeyed: 1
obeys: 1
obfuscates: 1
objective: 1
objectivity: 1
objectoriented: 1
obligation: 1
obliging: 1
observable: 1
observes: 1
obsessively: 1
obsoleted: 1
obstacle: 1
obstructs: 1
obviously: 1
occurrence: 1
offenders: 1
offerings: 1
officially: 1
offline: 1
offset: 1
oforder: 1
ogy: 1
oltp: 1
omitting: 1
oneanother: 1
ongoing: 1
onwards: 1
opens: 1
opensketch: 1
operaMFS: 1
operafrom: 1
operatencies: 1
opposing: 1
opposition: 1
optimality: 1
optimally: 1
optimisation: 1
optimisations: 1
optimise: 1
optimised: 1
optimistically: 1
optimizing: 1
optional: 1
orchestrating: 1
orderings: 1
organised: 1
organizing: 1
originated: 1
originating: 1
originator: 1
ority: 1
ormance_benchmark: 1
ortributed: 1
orts: 1
oscillate: 1
oscillating: 1
ous: 1
outages: 1
outcontent: 1
outlook: 1
outof: 1
outperform: 1
outperformed: 1
outright: 1
outweighs: 1
over13: 1
overcomes: 1
overhaul: 1
overlaid: 1
overlayed: 1
overloading: 1
overpredict: 1
overprediction: 1
overreach: 1
overrequest: 1
overrequesting: 1
oversold: 1
overweigh: 1
overwritten: 1
owing: 1
owned: 1
p0: 1
pack: 1
package: 1
packparently: 1
pain: 1
pairing: 1
palossless: 1
pan: 1
panel: 1
panning: 1
pans: 1
par: 1
par6: 1
parallelising: 1
parallelism: 1
parallelized: 1
parallelizing: 1
parameterizing: 1
parent: 1
parents: 1
pareto: 1
parliament: 1
partners: 1
party: 1
passes: 1
peculiar: 1
penalized: 1
perceive: 1
perceiving: 1
percentiles: 1
perception: 1
perf: 1
performances: 1
perience: 1
periments: 1
permission: 1
permute: 1
permutes: 1
permuting: 1
perobject: 1
persisted: 1
persistence: 1
persisting: 1
personalize: 1
personalized: 1
personally: 1
pertaining: 1
perturbances: 1
perturbation: 1
perturbations: 1
pervasively: 1
pervasiveness: 1
philosophical: 1
phones: 1
phpMyAdmin: 1
physician: 1
picking: 1
picks: 1
piece: 1
pieces: 1
piles: 1
pitfalls: 1
pivotal: 1
planes: 1
planned: 1
planning: 1
plans: 1
plants: 1
plasma: 1
plat1: 1
plausible: 1
please: 1
plentiful: 1
plotted: 1
pluggable: 1
plugins: 1
pn42: 1
podcasts: 1
pointer: 1
pointers: 1
police: 1
political: 1
portal: 1
porting: 1
portray: 1
ports: 1
post: 1
posted: 1
posters: 1
postponed: 1
postponing: 1
potencan: 1
potholes: 1
powerhouse: 1
powering: 1
powers: 1
powersavings: 1
pplive: 1
practices: 1
pray: 1
preallocating: 1
precede: 1
precious: 1
precision: 1
preclude: 1
predetermining: 1
predicting: 1
predictions: 1
predominant: 1
predominantly: 1
preempted: 1
preexisting: 1
preferences: 1
prefetchThe: 1
preliminary: 1
preparation: 1
prepare: 1
prepared: 1
prepares: 1
preprocessing: 1
president: 1
pressing: 1
presume: 1
pretend: 1
pretending: 1
pretends: 1
prevailing: 1
prevalent: 1
previWhen: 1
previewer: 1
priIn: 1
prices: 1
prifetched: 1
primaldual: 1
priorTwo: 1
priorispeedup: 1
prioritized: 1
prito: 1
priviledges: 1
privileged: 1
pro1: 1
probed: 1
probes: 1
procedures: 1
professional: 1
professor: 1
profiled: 1
profiling: 1
programmed: 1
progressed: 1
prohibited: 1
prohibitive: 1
prohibitively: 1
prohibits: 1
projected: 1
proliferating: 1
prominently: 1
promised: 1
promotional: 1
promptly: 1
pronounced: 1
propagates: 1
propagating: 1
propel: 1
propgated: 1
proportionally: 1
proposal: 1
proposes: 1
prothe: 1
prototypes: 1
protruding: 1
proverbial: 1
provers: 1
provisions: 1
provoke: 1
proximity: 1
prudently: 1
pruned: 1
publications: 1
publishsubscribe: 1
pubsub: 1
pullbased: 1
punishing: 1
purchased: 1
pure: 1
putation: 1
puter: 1
puts: 1
pvalues: 1
qos: 1
qppq: 1
quadrant: 1
quadrants: 1
qualities: 1
quantified: 1
quantity: 1
quarters: 1
quasi: 1
questionable: 1
queue10: 1
quired: 1
quorum: 1
quotes: 1
races: 1
racing: 1
radar: 1
rails: 1
raincloud: 1
raisepriority: 1
raising: 1
rameter: 1
ramifications: 1
ramp: 1
randomised: 1
ranks: 1
rational: 1
rePrefetch: 1
reThese: 1
reaction: 1
reactive: 1
readiness: 1
readwrite: 1
realistically: 1
realities: 1
realization: 1
realtime: 1
reboot: 1
rebuilt: 1
recall: 1
recast: 1
rechecks: 1
recipients: 1
recite: 1
reclamation: 1
recode: 1
recommendation: 1
recommended: 1
recommends: 1
reconfigurable: 1
reconfiguration: 1
reconfigure: 1
reconfiguring: 1
reconnect: 1
reconstruction: 1
recorded: 1
recov10: 1
recovrequests: 1
recovthroughput: 1
recurrence: 1
recursively: 1
redesign: 1
redirect: 1
redirected: 1
redirecting: 1
redisplaying: 1
referenced: 1
referring: 1
refers: 1
refines: 1
reflectsan: 1
refuse: 1
reg: 1
regenerate: 1
registering: 1
registry: 1
regularity: 1
reimplementing: 1
relational: 1
relationship: 1
relationships: 1
relaxed: 1
relayed: 1
religion: 1
relinked: 1
remained: 1
removal: 1
rename: 1
rency: 1
render: 1
renders: 1
rendezvous: 1
renewed: 1
rently: 1
reordering: 1
replacements: 1
replicates: 1
reportedly: 1
repre9: 1
representa: 1
reproduced: 1
rescuer: 1
researcher: 1
resembles: 1
resend: 1
resetting: 1
reside: 1
resides: 1
resiliency: 1
resolves: 1
resolving: 1
resorting: 1
respected: 1
restarts: 1
restrict: 1
restructured: 1
results2: 1
resumed: 1
retained: 1
retaining: 1
retains: 1
rethink: 1
retriev: 1
retrieving: 1
reusability: 1
reusable: 1
reuse: 1
reused: 1
revalidate: 1
revealed: 1
revealing: 1
reverts: 1
revisited: 1
revived: 1
revoked: 1
revolution: 1
rewind: 1
ricochet: 1
ride: 1
righteous: 1
rive: 1
rlogin: 1
roads: 1
rocky: 1
roll: 1
rollback: 1
rolled: 1
rollout: 1
room: 1
rooted: 1
rotating: 1
rough: 1
roundrobin: 1
rss: 1
ru: 1
rush: 1
rushed: 1
sacrificing: 1
sarcasm: 1
satellite: 1
saturated: 1
saturates: 1
saturating: 1
saturation: 1
saying: 1
says: 1
scale2: 1
scarce: 1
scenarMobile: 1
scenes: 1
schedule: 1
schizophrenic: 1
screen: 1
screenshots: 1
scripting: 1
scripts: 1
scrutiny: 1
searchers: 1
searching: 1
searing: 1
secretly: 1
secure: 1
seeing: 1
seemed: 1
selec: 1
selection: 1
selectively: 1
selfinterested: 1
selforganizing: 1
semantic: 1
senses: 1
sensitivFigure: 1
sentative: 1
ser: 1
serialised: 1
server1: 1
servicing: 1
serving: 1
setceiver: 1
setups: 1
seven: 1
severely: 1
shaded: 1
shaped: 1
shapers: 1
sharding: 1
shayee: 1
shed: 1
shelf: 1
shielding: 1
shipping: 1
shop: 1
shopping: 1
shortcomings: 1
shorter: 1
shouldn: 1
shtml: 1
shutdown: 1
sidebar: 1
signature: 1
signed: 1
signers: 1
significance: 1
signing: 1
silently: 1
similarities: 1
similarity: 1
simulates: 1
sishim: 1
sistencies: 1
sits: 1
sive: 1
sixteenth: 1
sixth: 1
sjsu: 1
skepticism: 1
sky: 1
skyrocketing: 1
slash: 1
slashdot: 1
slay: 1
sleeps: 1
slide: 1
slip: 1
slowdowns: 1
sluggish: 1
smartphone: 1
smoothly: 1
snapshots: 1
snooping: 1
society: 1
sock: 1
solicited: 1
solves: 1
somefound: 1
sonicmq: 1
sonicsoftware: 1
sooner: 1
sorted: 1
sought: 1
spanningtree: 1
spans: 1
spared: 1
spawned: 1
speaking: 1
speaks: 1
speci: 1
specializes: 1
specificity: 1
specifies: 1
specifying: 1
spectrum: 1
speech: 1
speedups: 1
spends: 1
spiked: 1
spinning: 1
spirits: 1
splay: 1
sponse: 1
sporting: 1
sports: 1
sprays: 1
spring: 1
squash: 1
srrs: 1
srsr: 1
ssh: 1
stabilizing: 1
stadrops: 1
stand: 1
standardizing: 1
startup: 1
starved: 1
starving: 1
state1: 1
statemachine: 1
statement: 1
statems: 1
stationary: 1
statistic: 1
statistics_tp: 1
stead: 1
steep: 1
steer: 1
stipulating: 1
sto: 1
stochastic: 1
stocking: 1
stor: 1
stordifferent: 1
straight: 1
street: 1
strengths: 1
stretagy: 1
strike: 1
strikes: 1
strikingly: 1
strings: 1
striping: 1
stripped: 1
strongest: 1
stronglytraces: 1
structuring: 1
studying: 1
stumbled: 1
stylistic: 1
su: 1
suba: 1
subing: 1
subintervals: 1
subj: 1
subkilobyte: 1
submitted: 1
subnetworks: 1
subordinate: 1
subscribes: 1
subscription: 1
subscriptions: 1
subsections: 1
subsequence: 1
substandard: 1
substitute: 1
substreams: 1
subtitles: 1
subtransactions: 1
succeeds: 1
successor: 1
sue: 1
suffix: 1
suggestive: 1
summarise: 1
summarised: 1
summarizes: 1
summarizing: 1
sun: 1
sundr: 1
sup: 1
superceded: 1
superfluous: 1
superimpose: 1
superior: 1
superlinearly: 1
supersede: 1
superseded: 1
supervisory: 1
supplementing: 1
supposed: 1
suppressing: 1
surely: 1
surements: 1
surge: 1
surplus: 1
surrounding: 1
survivors: 1
suspended: 1
sw: 1
swivel: 1
sybil: 1
syn94: 1
synchro1: 1
synchronise: 1
synchronizing: 1
systemic: 1
systraditional: 1
tablet: 1
tag: 1
tags: 1
tailor: 1
tains: 1
talking: 1
talks: 1
tance: 1
tapped: 1
tapping: 1
taught: 1
tech: 1
technetwork: 1
technicalsessions: 1
technological: 1
telemetry: 1
templates: 1
tempt: 1
tems: 1
tency: 1
tended: 1
tension: 1
tent: 1
tentative: 1
tention: 1
terleaving: 1
terlinked: 1
terminal: 1
terminate: 1
terminated: 1
textures: 1
thanks: 1
thefly: 1
ther: 1
thereafter: 1
thereby: 1
thick: 1
thrashing: 1
threading: 1
threetier: 1
throughXORs: 1
throughputs: 1
tial: 1
tiate: 1
ticast: 1
ticipant: 1
ticker: 1
ticular: 1
tied: 1
tightrope: 1
timecritical: 1
timeframe: 1
timers: 1
timescales: 1
timesharing: 1
tination: 1
tings: 1
tiple: 1
titles: 1
tity: 1
tively: 1
tivity: 1
toPeer: 1
todisincentivize: 1
toend: 1
toknow: 1
tolercations: 1
tone: 1
tons: 1
toolkit: 1
topeer: 1
topoltifiers: 1
tortures: 1
touted: 1
tp: 1
traceroute: 1
traditionally: 1
traf: 1
trafficshaping: 1
trailing: 1
tranasctions: 1
trans4: 1
transactionally: 1
transcoding: 1
transformed: 1
transit: 1
transitional: 1
translated: 1
translates: 1
translation: 1
translators: 1
translucence: 1
transportation: 1
transporting: 1
travels: 1
treatments: 1
trials: 1
tricks: 1
tried: 1
trillion: 1
trim: 1
trips: 1
truncate: 1
truncated: 1
ttl: 1
tude: 1
tunable: 1
tuples: 1
tus: 1
twenty: 1
twin: 1
twotier: 1
txn: 1
typed: 1
typwrite: 1
ual: 1
ubiquitously: 1
ufrgs: 1
ular: 1
ulate: 1
ultrareliable: 1
unRPCs: 1
unacceptably: 1
unavoidable: 1
unbe: 1
uncertain: 1
unclustered: 1
unconstrained: 1
uncontrolled: 1
uncorrelated: 1
underdog: 1
undergo: 1
underlies: 1
underneath: 1
underperform: 1
understandable: 1
understanddevices: 1
understands: 1
understood: 1
undertake: 1
undertaken: 1
underutilisation: 1
underway: 1
undesired: 1
undisturbed: 1
unencrypted: 1
unflattering: 1
unfortunate: 1
unidirectional: 1
unifies: 1
uniformally: 1
unilaterally: 1
unintended: 1
uninterference: 1
unites: 1
universal: 1
universe: 1
unjustified: 1
unlink: 1
unlinking: 1
unmodified: 1
unnoticed: 1
unparalleled: 1
unrealistic: 1
unreasonable: 1
unrecoverable: 1
unreserved: 1
unresilient: 1
unresolved: 1
unresponsiveness: 1
unspecified: 1
unsuitable: 1
untouched: 1
unwilling: 1
upReader: 1
upStudies: 1
upThe: 1
upare: 1
upcalls: 1
upcan: 1
upcations: 1
upcoming: 1
upgrade: 1
upgrades: 1
upisting: 1
uploads: 1
upwidth: 1
ures: 1
urgently: 1
usages: 1
usdoj: 1
usefully: 1
usenix: 1
usermakes: 1
uservisible: 1
utilising: 1
utilized: 1
utilizes: 1
utilizing: 1
uwin: 1
v1: 1
v1o: 1
v1o1: 1
v2o: 1
v2o1: 1
v2o2: 1
v3o2: 1
validations: 1
vanilla: 1
vanishes: 1
var: 1
variances: 1
vectored: 1
vehicles: 1
vercurr: 1
verification: 1
versa: 1
vertical: 1
vi: 1
victims: 1
videos: 1
viding: 1
viewer: 1
violated: 1
violates: 1
viral: 1
vironment: 1
virtually: 1
virtue: 1
visibly: 1
vision: 1
visit: 1
visiting: 1
visualize: 1
visualizing: 1
vital: 1
vko: 1
vko1: 1
vlo2: 1
voluntary: 1
w3: 1
waived: 1
wasn: 1
wasteful: 1
wastes: 1
wasting: 1
watch: 1
watches: 1
wave: 1
weaken: 1
weakness: 1
weblogs: 1
webtier: 1
weeks: 1
weighed: 1
welldefined: 1
whatsoever: 1
whereupon: 1
whiteboard: 1
whitepaper: 1
whithout: 1
wholefile: 1
widens: 1
windowsnt5: 1
wires: 1
withuated: 1
witnesses: 1
wonder: 1
worker: 1
workshop: 1
worries: 1
worrisome: 1
worthwhile: 1
wouldn: 1
wrapped: 1
wraps: 1
writclassification: 1
writeaccesses: 1
writeinvalidations: 1
writequirement: 1
writeserver: 1
writetive: 1
writewrite: 1
wrote: 1
wv: 1
xaxis: 1
yaxes: 1
yielded: 1
ymir: 1
yxxy: 1
zeal: 1
zealous: 1
zoomed: 1
zooming: 1
zooms: 1
