web technologies can web |
technologies can web services |
can web services scale |
web services scale up |
cornell university i n |
university i n the |
i n the past |
only major internet players |
major internet players such |
internet players such as |
players such as amazon |
and google were interested |
google were interested in |
were interested in deploying |
implementing high performance multicast |
enforcing fairness in a |
fairness in a live |
high performance multicast in |
interested in deploying large |
performance multicast in a |
multicast in a managed |
in a managed environment |
a managed environment krzysztof |
managed environment krzysztof ostrowski |
streaming system maya haridasana |
environment krzysztof ostrowski cornell |
krzysztof ostrowski cornell university |
ostrowski cornell university ken |
cornell university ken birman |
university ken birman cornell |
ken birman cornell university |
this is changing rapidly |
birman cornell university abstract |
is changing rapidly all |
portob and robbert van |
changing rapidly all sorts |
cornell university abstract motes |
rapidly all sorts of |
transparent error correction for |
university abstract motes end |
and robbert van renessea |
all sorts of companies |
an adaptive distributed file |
error correction for communication |
robbert van renessea a |
sorts of companies and |
adaptive distributed file system |
correction for communication between |
van renessea a dept |
of companies and governmental |
user application development using |
distributed file system for |
for communication between data |
companies and governmental organizations |
application development using c |
file system for mobile |
communication between data centers |
and governmental organizations are |
system for mobile hosts |
governmental organizations are suddenly |
between data centers mahesh |
for mobile hosts benjamin |
organizations are suddenly looking |
data centers mahesh balakrishnan |
new york b institute |
mobile hosts benjamin atkin |
are suddenly looking towards |
york b institute of |
the company s own |
suddenly looking towards web |
hosts benjamin atkin and |
b institute of informatics |
company s own products |
looking towards web services |
benjamin atkin and kenneth |
s own products are |
towards web services as |
atkin and kenneth p |
own products are still |
federal university of rio |
web services as a |
products are still implemented |
university of rio grande |
services as a platform |
are still implemented primarily |
of rio grande do |
as a platform that |
still implemented primarily in |
birman department of computer |
a platform that might |
rio grande do sul |
implemented primarily in unmanaged |
department of computer science |
platform that might support |
grande do sul porto |
primarily in unmanaged c |
of computer science cornell |
that might support a |
do sul porto alegre |
computer science cornell university |
might support a wide |
support a wide range |
a wide range of |
wide range of demanding |
range of demanding applications |
by building xyx in |
examples of such systems |
building xyx in the |
of such systems include |
xyx in the recommended |
such systems include big |
in the recommended manner |
systems include big banking |
include big banking and |
big banking and brokerage |
banking and brokerage data |
and brokerage data centers |
we found ourselves breaking |
found ourselves breaking new |
ourselves breaking new ground |
online service centers for |
service centers for companies |
edu abstract mfs using |
the multicast protocols employed |
centers for companies that |
abstract mfs using file |
multicast protocols employed by |
for companies that operate |
mfs using file access |
protocols employed by qsm |
companies that operate on |
using file access traces |
employed by qsm were |
that operate on a |
file access traces from |
by qsm were designed |
operate on a global |
access traces from windows |
qsm were designed for |
on a global scale |
traces from windows nt |
were designed for performance |
from windows nt and |
designed for performance and |
windows nt and unix |
edu abstract we describe |
for performance and scalability |
systems to operate critical |
abstract we describe a |
to operate critical infrastructures |
abstract the global network |
we describe a practical |
operate critical infrastructures like |
and a synthetic workload |
the global network of |
incorporating a mixture of |
describe a practical auditing |
critical infrastructures like electric |
a synthetic workload designed |
global network of data |
a mixture of new |
a practical auditing approach |
infrastructures like electric power |
synthetic workload designed to |
network of data centers |
mixture of new ideas |
practical auditing approach designed |
like electric power and |
workload designed to emulate |
of data centers is |
of new ideas and |
auditing approach designed to |
electric power and transportation |
designed to emulate sharing |
data centers is emerging |
new ideas and ideas |
approach designed to encourage |
to emulate sharing patterns |
and government and military |
ideas and ideas drawn |
designed to encourage fairness |
centers is emerging as |
emulate sharing patterns seen |
is emerging as an |
and ideas drawn from |
to encourage fairness in |
government and military systems |
sharing patterns seen in |
emerging as an important |
ideas drawn from prior |
encourage fairness in peer |
and military systems responsible |
patterns seen in mobility |
as an important distributed |
drawn from prior systems |
military systems responsible for |
seen in mobility is |
an important distributed systems |
systems responsible for everything |
in mobility is a |
important distributed systems paradigm |
responsible for everything from |
mobility is a critical |
distributed systems paradigm commodity |
for everything from intelligence |
is a critical feature |
the aspects on which |
systems paradigm commodity clusters |
auditing is employed to |
everything from intelligence gathering |
a critical feature of |
aspects on which we |
paradigm commodity clusters running |
is employed to ensure |
from intelligence gathering to |
critical feature of computer |
on which we focus |
commodity clusters running high |
employed to ensure that |
intelligence gathering to issuing |
feature of computer systems |
which we focus here |
to ensure that correct |
gathering to issuing social |
we focus here reflect |
ensure that correct nodes |
and while collaborative engineering |
focus here reflect architectural |
to issuing social security |
that correct nodes are |
while collaborative engineering systems |
here reflect architectural responses |
issuing social security checks |
correct nodes are able |
speed lambda networks across |
reflect architectural responses to |
nodes are able to |
lambda networks across hundreds |
this emerging trend presents |
architectural responses to scheduling |
are able to receive |
wireless networks are common |
networks across hundreds of |
emerging trend presents developers |
responses to scheduling delays |
able to receive streams |
across hundreds of milliseconds |
trend presents developers with |
to receive streams even |
hundreds of milliseconds of |
overheads associated with threads |
presents developers with a |
receive streams even in |
most applications that run |
of milliseconds of network |
applications that run on |
and costs arising in |
that run on existing |
milliseconds of network latency |
streams even in the |
developers with a new |
costs arising in the |
run on existing work |
even in the presence |
packet loss on long |
in the presence of |
on existing work in |
with a new challenge |
arising in the memory |
the presence of nodes |
existing work in cache |
in the memory management |
presence of nodes that |
building web services solutions |
of nodes that do |
haul networks can cripple |
work in cache management |
the memory management subsystem |
web services solutions that |
nodes that do not |
networks can cripple the |
in cache management for |
can cripple the performance |
that do not upload |
services solutions that scale |
over the period during |
cache management for mobile |
cripple the performance of |
do not upload enough |
the performance of applications |
management for mobile file |
performance of applications and |
not upload enough data |
a scalable system is |
for mobile file systems |
of applications and protocols |
the period during which |
scalable system is one |
mobile file systems mobile |
applications and protocols a |
period during which qsm |
system is one that |
file systems mobile hosts |
and protocols a loss |
during which qsm was |
is one that can |
systems mobile hosts lack |
protocols a loss rate |
which qsm was developed |
and scales well when |
one that can flexibly |
mobile hosts lack flexible |
a loss rate as |
scales well when compared |
that can flexibly accommodate |
hosts lack flexible mechanisms |
loss rate as low |
well when compared to |
can flexibly accommodate growth |
lack flexible mechanisms for |
rate as low as |
these had pervasive consequences |
flexibly accommodate growth in |
flexible mechanisms for data |
when compared to previous |
accommodate growth in its |
mechanisms for data access |
forcing us to redesign |
growth in its client |
compared to previous solutions |
for data access in |
us to redesign and |
in its client base |
to previous solutions that |
data access in an |
to redesign and recode |
previous solutions that rely |
access in an en |
redesign and recode one |
such systems typically run |
solutions that rely on |
systems typically run on |
and recode one layer |
typically run on a |
that rely on tit |
run on a clustered |
recode one layer of |
on a clustered computer |
one layer of the |
is sufficient to reduce |
a clustered computer or |
layer of the system |
clustered computer or in |
sufficient to reduce tcp |
computer or in a |
of the system after |
or in a large |
tat style of data |
the system after another |
in a large data |
style of data exchange |
ip throughput by an |
a large data center |
throughput by an order |
large data center and |
by an order of |
auditing involves two roles |
data center and must |
an order of magnitude |
the original system was |
center and must be |
order of magnitude on |
original system was multithreaded |
and must be able |
of magnitude on a |
must be able to |
untrusted local auditors run |
be able to handle |
local auditors run on |
able to handle high |
auditors run on all |
to handle high loads |
run on all nodes |
handle high loads or |
o calls and was |
on all nodes in |
high loads or sudden |
calls and was rather |
all nodes in the |
loads or sudden demand |
and was rather casual |
nodes in the system |
or sudden demand bursts |
was rather casual about |
sudden demand bursts and |
rather casual about buffering |
incorporates mechanisms for making |
demand bursts and a |
casual about buffering and |
maelstrom is an edge |
and are responsible for |
mechanisms for making efficient |
bursts and a vast |
about buffering and caching |
is an edge appliance |
are responsible for collecting |
for making efficient vironment |
and a vast number |
an edge appliance that |
the current system is |
making efficient vironment with |
a vast number of |
responsible for collecting and |
edge appliance that masks |
current system is single |
efficient vironment with large |
vast number of users |
for collecting and maintaining |
appliance that masks packet |
vironment with large and |
collecting and maintaining accountable |
that masks packet loss |
with large and frequent |
and maintaining accountable information |
masks packet loss transparently |
they must reliably respond |
large and frequent variations |
maintaining accountable information regarding |
packet loss transparently and |
must reliably respond even |
and frequent variations in |
accountable information regarding data |
loss transparently and quickly |
reliably respond even in |
frequent variations in network |
information regarding data sent |
and obsessively minimizes memory |
transparently and quickly from |
respond even in the |
variations in network connec |
regarding data sent and |
obsessively minimizes memory consumption |
and quickly from inter |
even in the event |
data sent and received |
use of available bandwidth |
in the event of |
sent and received by |
the event of failures |
and received by each |
event of failures or |
received by each node |
of failures or reconfiguration |
aggregating traffic for high |
it has mostly focused |
has mostly focused on |
mostly focused on tivity |
speed encoding and using |
encoding and using a |
performs well and is |
and using a new |
well and is stable |
one or more trusted |
using a new forward |
and is stable at |
or more trusted global |
a new forward error |
is stable at high |
managed and automate as |
more trusted global auditors |
in collaborative work adapting |
stable at high data |
and automate as many |
new forward error correction |
trusted global auditors periodically |
collaborative work adapting existing |
at high data rates |
automate as many routine |
forward error correction scheme |
global auditors periodically sample |
work adapting existing systems |
as many routine services |
error correction scheme to |
auditors periodically sample the |
adapting existing systems to |
large scale and under |
many routine services such |
correction scheme to handle |
periodically sample the state |
existing systems to cope |
scale and under stress |
routine services such as |
scheme to handle bursty |
sample the state of |
systems to cope with |
services such as backups |
the finished system achieves |
the state of participating |
to cope with periods |
to handle bursty loss |
such as backups and |
finished system achieves extremely |
state of participating nodes |
cope with periods of |
as backups and component |
system achieves extremely high |
with periods of low |
backups and component upgrades |
estimate whether the streaming |
periods of low bandwidth |
achieves extremely high performance |
and component upgrades as |
whether the streaming quality |
extremely high performance with |
component upgrades as possible |
the streaming quality is |
high performance with relatively |
streaming quality is satisfactory |
performance with relatively modest |
with relatively modest cpu |
many settings also require |
particularly when wireless and |
relatively modest cpu and |
and decide whether any |
when wireless and wired |
settings also require security |
modest cpu and memory |
decide whether any actions |
wireless and wired users |
also require security against |
cpu and memory loads |
whether any actions are |
and wired users share |
require security against attempted |
any actions are required |
wired users share in |
security against attempted intrusions |
users share in a |
although our paper is |
against attempted intrusions and |
share in a style |
our paper is not |
attempted intrusions and distributed |
we demonstrate through simulation |
in a style which |
demonstrate through simulation that |
intrusions and distributed denial |
i ntroduction t a |
a style which we |
through simulation that our |
paper is not about |
ntroduction t a conference |
style which we will |
simulation that our approach |
is not about setting |
t a conference version |
which we will refer |
that our approach can |
not about setting performance |
a conference version of |
we will refer to |
our approach can successfully |
about setting performance records |
conference version of this |
will refer to as |
approach can successfully detect |
setting performance records the |
version of this paper |
refer to as modal |
can successfully detect and |
performance records the absolute |
of this paper appeared |
to as modal adaptation |
successfully detect and react |
records the absolute numbers |
this paper appeared in |
detect and react to |
the absolute numbers are |
when files or databases |
and react to the |
paper appeared in nsdi |
absolute numbers are good |
react to the presence |
to the presence of |
the presence of opportunistic |
presence of opportunistic nodes |
of opportunistic nodes in |
opportunistic nodes in streaming |
nodes in streaming sessions |
we describe some techniques |
qsm outperforms the multicast |
the second builds on |
describe some techniques bandwidth |
outperforms the multicast platforms |
second builds on the |
some techniques bandwidth is |
the multicast platforms we |
it incurs low network |
techniques bandwidth is high |
builds on the first |
multicast platforms we ve |
incurs low network and |
on the first and |
fifth usenix symposium on |
the application communicates normally |
low network and computational |
the first and supports |
platforms we ve worked |
usenix symposium on networked |
network and computational overheads |
first and supports a |
we ve worked with |
symposium on networked systems |
when for adapting data |
and supports a way |
ve worked with in |
on networked systems design |
for adapting data access |
which remain fixed as |
supports a way to |
worked with in the |
networked systems design and |
adapting data access to |
remain fixed as the |
a way to build |
with in the past |
systems design and implementation |
data access to network |
fixed as the system |
as the system scales |
in the past systems |
access to network variability |
way to build scripts |
the past systems that |
to network variability in |
to build scripts of |
past systems that run |
network variability in the |
build scripts of simpler |
systems that run in |
variability in the context |
scripts of simpler transactions |
that run in unmanaged |
introduction video and audio |
in the context of |
run in unmanaged settings |
video and audio streaming |
the context of bandwidth |
some might argue that |
and audio streaming account |
context of bandwidth falls |
this paper won t |
audio streaming account for |
might argue that all |
of bandwidth falls below |
paper won t tell |
streaming account for a |
ms index terms data |
argue that all reliability |
bandwidth falls below a |
won t tell the |
account for a large |
index terms data centers |
that all reliability needs |
falls below a threshold |
t tell the blow |
for a large percentage |
all reliability needs can |
a large percentage of |
reliability needs can be |
large percentage of content |
the application enters a |
needs can be recast |
percentage of content accessed |
application enters a lowmfs |
can be recast in |
of content accessed over |
be recast in terms |
content accessed over the |
recast in terms of |
a client cache manager |
he emergence of commodity |
accessed over the web |
emergence of commodity clusters |
client cache manager for |
in terms of transactions |
we use qsm in |
of commodity clusters and |
cache manager for a |
commodity clusters and data |
one popular style of |
use qsm in a |
manager for a distributed |
clusters and data centers |
popular style of streaming |
qsm in a series |
the past three decades |
and data centers has |
style of streaming on |
for a distributed file |
in a series of |
past three decades have |
data centers has enabled |
of streaming on the |
a distributed file system |
a series of experiments |
three decades have seen |
centers has enabled a |
streaming on the web |
series of experiments that |
decades have seen one |
we bandwidth mode in |
have seen one failed |
bandwidth mode in which |
has enabled a new |
on the web is |
of experiments that highlight |
seen one failed attempt |
mode in which communication |
enabled a new class |
the web is on |
experiments that highlight fundamental |
one failed attempt after |
in which communication is |
a new class of |
web is on demand |
that highlight fundamental factors |
failed attempt after another |
which communication is restricted |
new class of globally |
attempt after another to |
in which users access |
class of globally distributed |
these reveal linkages between |
after another to build |
which users access pre |
communication is restricted or |
of globally distributed highperformance |
reveal linkages between achievable |
another to build everything |
is restricted or deshow |
globally distributed highperformance applications |
linkages between achievable performance |
to build everything over |
stored content at will |
restricted or deshow how |
distributed highperformance applications that |
between achievable performance and |
build everything over a |
or deshow how mfs |
highperformance applications that coordinate |
achievable performance and the |
everything over a database |
another style requires streams |
deshow how mfs is |
applications that coordinate over |
performance and the costs |
over a database system |
style requires streams to |
how mfs is able |
that coordinate over vast |
and the costs and |
requires streams to be |
mfs is able to |
coordinate over vast geographical |
the costs and characteristics |
and it s now |
streams to be generated |
is able to adapt |
over vast geographical distances |
costs and characteristics of |
it s now clear |
to be generated and |
able to adapt to |
and characteristics of the |
s now clear that |
be generated and disseminated |
to adapt to widely |
characteristics of the managed |
now clear that many |
generated and disseminated in |
adapt to widely varying |
a financial firm s |
of the managed framework |
clear that many kinds |
and disseminated in real |
to widely varying bandwidth |
financial firm s new |
that many kinds of |
widely varying bandwidth ferred |
doing so sheds light |
many kinds of systems |
so sheds light on |
firm s new york |
sheds light on the |
kinds of systems just |
s new york city |
light on the challenges |
of systems just don |
this may be the |
new york city data |
an application has a |
systems just don t |
may be the case |
on the challenges of |
york city data center |
the challenges of working |
just don t match |
be the case with |
application has a small |
city data center may |
challenges of working in |
don t match the |
the case with important |
has a small number |
data center may receive |
of working in a |
t match the model |
case with important social |
a small number of |
center may receive real |
working in a kind |
small number of levels |
in a kind of |
these intrinsically distributed systems |
number of levels through |
a kind of environment |
time updates from a |
intrinsically distributed systems make |
of levels through the |
kind of environment that |
updates from a stock |
an important property of |
levels through the use |
of environment that will |
distributed systems make use |
from a stock exchange |
important property of live |
through the use of |
environment that will be |
systems make use of |
a stock exchange in |
the use of modeless |
streaming is that data |
make use of direct |
stock exchange in switzerland |
that will be more |
use of modeless adaptation |
is that data is |
use of direct communication |
will be more and |
that data is not |
of direct communication between |
conduct financial transactions with |
be more and more |
data is not available |
direct communication between programs |
and evaluate the possible |
financial transactions with banks |
evaluate the possible modes |
is not available in |
the possible modes and |
more and more prevalent |
transactions with banks in |
communication between programs via |
not available in advance |
possible modes and chooses |
and more prevalent in |
with banks in asia |
between programs via the |
modes and chooses the |
more prevalent in years |
programs via the trans |
being generated just before |
and chooses the appropriate |
prevalent in years to |
cache data in london |
generated just before transmission |
current web services standards |
in years to come |
data in london for |
chooses the appropriate one |
just before transmission at |
web services standards have |
in london for locality |
the appropriate one based |
before transmission at the |
services standards have many |
london for locality and |
our insights should be |
appropriate one based on |
transmission at the sender |
standards have many critical |
for locality and mirror |
insights should be of |
one based on the |
have many critical limitations |
locality and mirror it |
should be of value |
based on the benefit |
and mirror it to |
be of value to |
interested users ideally want |
mirror it to kansas |
on the benefit of |
of value to developers |
today s web services |
it to kansas for |
the benefit of mechanisms |
users ideally want to |
value to developers of |
s web services standards |
to kansas for disaster |
benefit of mechanisms for |
ideally want to receive |
to developers of other |
web services standards seem |
of mechanisms for improving |
want to receive the |
developers of other high |
services standards seem to |
mechanisms for improving file |
to receive the stream |
standards seem to answer |
to interconnect these bandwidth |
performance communication and event |
receive the stream without |
seem to answer these |
for improving file system |
the stream without much |
to answer these needs |
improving file system performance |
hungry data centers across |
stream without much delay |
file system performance currently |
data centers across the |
without much delay from |
system performance currently available |
centers across the globe |
much delay from its |
performance currently available bandwidth |
a more probing analysis |
delay from its original |
more probing analysis reveals |
from its original transmission |
organizations are increasingly deploying |
probing analysis reveals many |
are increasingly deploying private |
analysis reveals many critical |
we propose a new |
increasingly deploying private lambda |
reveals many critical limitations |
propose a new positioning |
in the coda file |
deploying private lambda networks |
streaming systems now allow |
the coda file and |
a new positioning of |
systems now allow large |
coda file and cache |
raw bandwidth is ubiquitous |
now allow large numbers |
new positioning of multicast |
file and cache consistency |
bandwidth is ubiquitous and |
allow large numbers of |
positioning of multicast technology |
and cache consistency using |
is ubiquitous and cheaply |
the major web services |
large numbers of interested |
cache consistency using microbenchmarks |
as an extension of |
major web services standards |
numbers of interested users |
ubiquitous and cheaply available |
consistency using microbenchmarks and |
an extension of the |
web services standards dealing |
of interested users to |
and cheaply available in |
using microbenchmarks and file |
extension of the component |
services standards dealing with |
interested users to receive |
cheaply available in the |
microbenchmarks and file system |
of the component integration |
standards dealing with reliability |
users to receive streamed |
available in the form |
and file system system |
the component integration features |
to receive streamed data |
in the form of |
component integration features of |
receive streamed data in |
the form of existing |
integration features of the |
streamed data in near |
form of existing dark |
features of the microsoft |
data in near real |
of existing dark fiber |
in near real time |
reliability provides for reliable |
net managed runtime environment |
provides for reliable handoff |
without requiring extensive amounts |
for reliable handoff between |
requiring extensive amounts of |
reliable handoff between a |
extensive amounts of resources |
the cache manager operates |
handoff between a client |
cache manager operates in |
between a client system |
manager operates in either |
running and maintaining high |
a client system and |
although we started with |
client system and a |
these systems are based |
operates in either a |
we started with a |
system and a queuing |
systems are based on |
in either a stronglytraces |
free networks over this |
and a queuing system |
are based on the |
started with a sophisticated |
networks over this fiber |
a queuing system residing |
based on the peer |
with a sophisticated multicast |
over this fiber is |
queuing system residing between |
a sophisticated multicast protocol |
this fiber is difficult |
system residing between the |
fiber is difficult and |
residing between the client |
is difficult and expensive |
between the client and |
experiments reveal a series |
the client and some |
reveal a series of |
client and some service |
a series of problematic |
which affects the policy |
series of problematic interactions |
capacity optical links are |
affects the policy for |
of problematic interactions between |
optical links are almost |
the policy for writing |
problematic interactions between its |
links are almost never |
policy for writing changes |
where nodes interested in |
interactions between its high |
are almost never congested |
the standard isn t |
for writing changes to |
nodes interested in receiving |
standard isn t nearly |
writing changes to files |
interested in receiving data |
they drop packets for |
isn t nearly as |
changes to files back |
in receiving data also |
drop packets for numerous |
t nearly as comprehensive |
processing logic and the |
to files back to |
receiving data also help |
packets for numerous reasons |
nearly as comprehensive as |
logic and the properties |
files back to the |
data also help disseminate |
for numerous reasons dirty |
as comprehensive as the |
and the properties of |
back to the server |
also help disseminate it |
comprehensive as the name |
the properties of the |
help disseminate it to |
as the name implies |
modal adaptation schemes are |
disseminate it to each |
properties of the managed |
adaptation schemes are well |
it to each other |
of the managed framework |
it s limited to |
alleviating the bottleneck at |
s limited to pipelines |
the bottleneck at the |
introduction in which changes |
limited to pipelines that |
bottleneck at the source |
in which changes in |
to pipelines that include |
which changes in bandwidth |
pipelines that include queuing |
changes in bandwidth are |
that include queuing subsystems |
initial protocols were based |
in bandwidth are relatively |
protocols were based on |
we addressed these and |
bandwidth are relatively predictable |
were based on building |
addressed these and achieved |
based on building a |
these and achieved high |
on building a tree |
such as switching network |
and achieved high performance |
reliability boils down to |
as switching network access |
boils down to a |
based overlay of nodes |
down to a few |
achieved high performance by |
switching network access from |
overlay of nodes through |
to a few options |
high performance by making |
network access from an |
of nodes through which |
a few options that |
performance by making some |
access from an ethernet |
nodes through which data |
few options that a |
by making some unusual |
from an ethernet to |
through which data would |
options that a client |
making some unusual architectural |
an ethernet to a |
which data would be |
that a client can |
some unusual architectural decisions |
ethernet to a modem |
data would be pushed |
for example and in |
a client can use |
example and in different |
client can use to |
and in different patterns |
which we distill into |
but mobility is now |
can use to tell |
we distill into general |
mobility is now an |
ranging from singleton drops |
distill into general insights |
use to tell the |
is now an major |
from singleton drops to |
to tell the queuing |
now an major feature |
component integration environments such |
such as chainsaw and |
singleton drops to extended |
an major feature of |
tell the queuing system |
integration environments such as |
as chainsaw and coolstreaming |
drops to extended bursts |
major feature of computer |
the queuing system whether |
environments such as microsoft |
feature of computer systems |
have shown that the |
queuing system whether or |
shown that the use |
system whether or not |
that the use of |
whether or not to |
over the not as |
the use of a |
or not to reissue |
the not as appropriate |
use of a mesh |
ee have become widely |
not to reissue a |
not as appropriate in |
of a mesh of |
have become widely popular |
to reissue a request |
as appropriate in for |
a mesh of connected |
become widely popular with |
reissue a request if |
appropriate in for wireless |
mesh of connected nodes |
widely popular with application |
a request if a |
in for wireless networks |
of connected nodes and |
popular with application developers |
request if a failure |
connected nodes and a |
noncongestion loss has been |
if a failure occurs |
nodes and a pull |
in which bandwidth past |
loss has been observed |
who benefit from standardized |
which bandwidth past decade |
has been observed on |
benefit from standardized memory |
and a way to |
been observed on long |
based data dissemination approach |
from standardized memory management |
a way to timestamp |
data dissemination approach can |
way to timestamp requests |
haul networks as well |
dissemination approach can provide |
to timestamp requests so |
held devices capable of |
approach can provide similar |
timestamp requests so that |
devices capable of wireless |
can provide similar results |
requests so that a |
capable of wireless availability |
provide similar results with |
so that a service |
of wireless availability is |
similar results with better |
that a service can |
wireless availability is less |
and performance analysis tools |
results with better resilience |
a service can detect |
availability is less predictable |
performance analysis tools that |
with better resilience to |
service can detect duplicates |
is less predictable and |
analysis tools that operate |
better resilience to failures |
less predictable and varies |
tools that operate across |
resilience to failures and |
predictable and varies over |
that operate across component |
to failures and churn |
and varies over a |
operate across component boundaries |
transactions actually consists of |
varies over a larger |
actually consists of two |
nodes joining and leaving |
over a larger possible |
this paper describes quicksilver |
joining and leaving the |
consists of two side |
a larger possible network |
paper describes quicksilver scalable |
and leaving the system |
larger possible network access |
describes quicksilver scalable multicast |
possible network access have |
network access have become |
access have become common |
and wireless networks are |
one is aimed at |
wireless networks are range |
is aimed at applications |
aimed at applications that |
at applications that perform |
applications that perform database |
that perform database transactions |
the notion of insufficient |
perform database transactions with |
a new multicast platform |
notion of insufficient bandwidth |
database transactions with the |
new multicast platform designed |
of insufficient bandwidth can |
transactions with the usual |
multicast platform designed to |
insufficient bandwidth can vary |
nodes notify each other |
with the usual acid |
platform designed to achieve |
bandwidth can vary dependalso |
notify each other of |
designed to achieve high |
can vary dependalso proliferating |
each other of receipt |
the inadequacy of commodity |
to achieve high performance |
other of receipt of |
inadequacy of commodity tcp |
achieve high performance in |
of receipt of data |
applications that run on |
high performance in managed |
receipt of data packets |
ip in high bandwidthdelay |
performance in managed environments |
that run on hosts |
in high bandwidthdelay product |
run on hosts in |
high bandwidthdelay product networks |
and request packets from |
on hosts in wireless |
bandwidthdelay product networks is |
memoryrelated overheads and phenomena |
request packets from their |
hosts in wireless neting |
product networks is extensively |
overheads and phenomena related |
or the remote procedure |
in wireless neting on |
networks is extensively documented |
packets from their neighbors |
and phenomena related to |
the remote procedure call |
wireless neting on how |
from their neighbors based |
phenomena related to scheduling |
remote procedure call and |
neting on how much |
their neighbors based on |
related to scheduling are |
procedure call and that |
on how much data |
neighbors based on the |
to scheduling are shown |
call and that can |
how much data the |
based on the received |
scheduling are shown to |
and that can t |
much data the application |
on the received notifications |
are shown to dominate |
that can t tolerate |
data the application is |
shown to dominate the |
can t tolerate delay |
practical systems based on |
to dominate the behavior |
the application is trying |
systems based on pull |
dominate the behavior of |
application is trying to |
the behavior of the |
these systems lack databases |
is trying to send |
behavior of the system |
systems lack databases clean |
based streaming now exist |
lack databases clean separation |
streaming now exist in |
databases clean separation of |
we discuss techniques that |
so that works must |
now exist in china |
that works must cope |
discuss techniques that helped |
works must cope with |
clean separation of stored |
must cope with constraints |
techniques that helped us |
cope with constraints on |
where they are used |
with constraints on access |
that helped us to |
constraints on access to |
they are used to |
separation of stored data |
helped us to alleviate |
on access to data |
ip has three major |
access to data that |
us to alleviate these |
are used to disseminate |
of stored data from |
has three major problems |
to data that are |
to alleviate these problems |
used to disseminate television |
stored data from code |
three major problems when |
data that are genit |
to disseminate television channels |
major problems when used |
that are genit may |
and argue that they |
disseminate television channels to |
and any attempt to |
problems when used over |
are genit may make |
argue that they reveal |
television channels to thousands |
any attempt to force |
when used over such |
genit may make sense |
that they reveal general |
channels to thousands of |
attempt to force them |
used over such networks |
may make sense to |
they reveal general principles |
to thousands of users |
to force them into |
make sense to adjust |
reveal general principles applicable |
force them into that |
sense to adjust network |
general principles applicable to |
them into that model |
to adjust network usage |
principles applicable to other |
into that model results |
adjust network usage when |
even though the p |
applicable to other kinds |
that model results in |
ip suffers throughput collapse |
network usage when the |
to other kinds of |
model results in unacceptable |
suffers throughput collapse if |
usage when the bandwidth |
p paradigm allows systems |
other kinds of high |
results in unacceptable loss |
throughput collapse if the |
when the bandwidth erally |
paradigm allows systems to |
in unacceptable loss of |
collapse if the network |
the bandwidth erally not |
allows systems to scale |
unacceptable loss of performance |
rate protocols and applications |
bandwidth erally not present |
systems to scale with |
if the network is |
protocols and applications in |
intrinsically distributed systems are |
to scale with the |
the network is even |
erally not present in |
and applications in managed |
distributed systems are common |
scale with the number |
network is even slightly |
not present in wired |
applications in managed settings |
with the number of |
is even slightly prone |
present in wired networks |
and web services will |
the number of users |
even slightly prone to |
web services will need |
slightly prone to packet |
prone to packet loss |
services will need to |
distance from a base |
introduction a component integration |
will need to support |
need to support them |
from a base stadrops |
a component integration revolution |
it also leaves them |
conservative flow control mechanisms |
a base stadrops by |
component integration revolution is |
also leaves them vulnerable |
flow control mechanisms designed |
base stadrops by half |
the existing reliability options |
leaves them vulnerable to |
control mechanisms designed to |
integration revolution is transforming |
existing reliability options simply |
rather than just when |
mechanisms designed to deal |
revolution is transforming the |
them vulnerable to opportunistic |
reliability options simply don |
than just when it |
designed to deal with |
is transforming the development |
vulnerable to opportunistic behavior |
options simply don t |
just when it falls |
to deal with the |
transforming the development of |
simply don t address |
when it falls to |
deal with the systematic |
the development of desktop |
don t address the |
opportunistic nodes attempt to |
it falls to modem |
with the systematic congestion |
development of desktop applications |
t address the requirement |
nodes attempt to receive |
the systematic congestion of |
attempt to receive a |
systematic congestion of the |
to receive a stream |
congestion of the commodity |
platforms such as windows |
receive a stream without |
a lesson from the |
of the commodity internet |
a stream without uploading |
contention with other hosts |
the commodity internet react |
lesson from the past |
stream without uploading their |
with other hosts or |
commodity internet react too |
from the past what |
without uploading their fair |
other hosts or processes |
ee promote an application |
internet react too sharply |
the past what sorts |
uploading their fair share |
hosts or processes on |
promote an application development |
react too sharply to |
past what sorts of |
their fair share of |
or processes on the |
an application development style |
what sorts of scaling |
fair share of data |
processes on the same |
application development style in |
sorts of scaling and |
on the same host |
development style in which |
reducing the overall upload |
of scaling and reliability |
style in which components |
the overall upload capacity |
scaling and reliability features |
in which components are |
overall upload capacity of |
selecting a mode according |
and reliability features are |
which components are implemented |
upload capacity of the |
a mode according to |
reliability features are lacking |
ms w n s |
w n s e |
n s e fig |
mode according to the |
features are lacking in |
components are implemented independently |
capacity of the system |
according to the available |
are lacking in web |
are implemented independently and |
to the available bandwidth |
lacking in web services |
implemented independently and heavily |
the available bandwidth can |
despite the damage that |
in web services standards |
independently and heavily reused |
example lambda network ephemeral |
available bandwidth can uninterference |
the damage that they |
web services standards today |
lambda network ephemeral loss |
damage that they may |
network ephemeral loss on |
by standardizing memory management |
that they may cause |
a good example is |
and switching between different |
standardizing memory management and |
ephemeral loss on over |
good example is data |
switching between different wireless |
memory management and type |
not much work has |
provisioned links a single |
between different wireless media |
management and type checking |
example is data replication |
much work has been |
links a single packet |
work has been done |
different wireless media all |
a single packet in |
these platforms enable safe |
has been done in |
wireless media all necessarily |
building a server that |
single packet in ten |
platforms enable safe and |
been done in studying |
media all necessarily constrain |
a server that scales |
packet in ten thousand |
enable safe and efficient |
done in studying mechanisms |
all necessarily constrain communication |
server that scales to |
in ten thousand is |
safe and efficient cross |
in studying mechanisms to |
that scales to handle |
ten thousand is enough |
since it ignores what |
scales to handle load |
studying mechanisms to avoid |
thousand is enough to |
is enough to reduce |
to handle load often |
mechanisms to avoid their |
avoiding overheads associated with |
enough to reduce tcp |
handle load often requires |
it ignores what data |
to avoid their presence |
overheads associated with protection |
ip throughput to a |
ignores what data compound |
avoid their presence in |
load often requires replicating |
associated with protection boundaries |
throughput to a third |
what data compound the |
their presence in live |
often requires replicating data |
to a third over |
data compound the variability |
requires replicating data on |
a third over a |
compound the variability in |
replicating data on multiple |
the variability in network |
data on multiple nodes |
variability in network performance |
on multiple nodes of |
the goal of this |
in network performance to |
multiple nodes of a |
goal of this the |
network performance to which |
nodes of a cluster |
of this the authors |
and one in a |
performance to which apthe |
this the authors were |
one in a thousand |
our project is interested |
to which apthe application |
project is interested in |
in a thousand drops |
is interested in leveraging |
which apthe application actually |
interested in leveraging these |
a thousand drops it |
in leveraging these benefits |
the authors were supported |
apthe application actually wants |
another example is guaranteed |
thousand drops it by |
leveraging these benefits to |
authors were supported by |
application actually wants to |
example is guaranteed real |
drops it by an |
these benefits to help |
were supported by afrl |
actually wants to send |
it by an order |
benefits to help developers |
supported by afrl award |
wants to send over |
by an order of |
to help developers implement |
a company that buys |
help developers implement robust |
an order of magnitude |
by afrl award fa |
to send over the |
company that buys a |
developers implement robust and |
that buys a cluster |
implement robust and scalable |
send over the network |
buys a cluster probably |
robust and scalable computing |
a cluster probably wants |
and scalable computing services |
cluster probably wants to |
scalable computing services that |
time or interactive applications |
deferplications must adapt if |
probably wants to guarantee |
computing services that will |
or interactive applications are |
must adapt if they |
wants to guarantee that |
services that will run |
interactive applications are impacted |
adapt if they are |
to guarantee that some |
that will run on |
applications are impacted by |
if they are to |
guarantee that some service |
will run on clusters |
are impacted by the |
they are to perform |
that some service will |
run on clusters or |
impacted by the reliance |
are to perform well |
some service will be |
on clusters or in |
by the reliance of |
service will be responsive |
clusters or in datacenters |
the reliance of reliability |
will be responsive enough |
ring writing back all |
reliance of reliability mechanisms |
be responsive enough to |
writing back all modifications |
of reliability mechanisms on |
back all modifications to |
early users of our |
all modifications to files |
reliability mechanisms on acknowledgments |
modifications to files may |
users of our platform |
to files may not |
mechanisms on acknowledgments and |
files may not be |
of our platform are |
may not be a |
on acknowledgments and retransmissions |
not be a sensible |
our platform are creating |
responsive enough to keep |
be a sensible this |
platform are creating applications |
enough to keep its |
limiting the latency of |
a sensible this paper |
are creating applications in |
to keep its customers |
the latency of packet |
sensible this paper focuses |
creating applications in areas |
keep its customers happy |
latency of packet recovery |
this paper focuses on |
applications in areas such |
its customers happy even |
of packet recovery to |
paper focuses on adaptation |
in areas such as |
customers happy even when |
packet recovery to at |
focuses on adaptation techniques |
areas such as parallelized |
happy even when demand |
recovery to at least |
on adaptation techniques for |
such as parallelized data |
even when demand is |
to at least the |
adaptation techniques for management |
as parallelized data mining |
when demand is high |
at least the round |
techniques for management policy |
least the round trip |
for management policy if |
the round trip time |
event stream filtering software |
management policy if those |
the missing technologies don |
policy if those are |
missing technologies don t |
and scalable web services |
if those are the |
technologies don t stop |
those are the only |
don t stop there |
are the only messages |
developers of clustered services |
if delivery is sequenced |
the only messages available |
of clustered services need |
only messages available to |
clustered services need reliable |
messages available to send |
services need reliable multicast |
cycle services that can |
need reliable multicast protocols |
services that can launch |
reliable multicast protocols for |
that can launch an |
of data accessed and |
multicast protocols for data |
each lost packet acts |
can launch an application |
data accessed and modified |
protocols for data replication |
lost packet acts as |
launch an application on |
accessed and modified by |
packet acts as a |
an application on demand |
and modified by mobile |
acts as a virtual |
application on demand or |
and in light of |
modified by mobile hosts |
as a virtual road |
on demand or restart |
in light of our |
demand or restart a |
light of our broader |
or restart a failed |
we investigate we describe |
block in the fifo |
of our broader goal |
restart a failed component |
investigate we describe mfs |
in the fifo channel |
our broader goal of |
the fifo channel until |
broader goal of leveraging |
fifo channel until it |
goal of leveraging the |
or load balancers and |
channel until it is |
of leveraging the power |
load balancers and technology |
until it is recovered |
leveraging the power and |
balancers and technology to |
the power and component |
a flexible cache adaptation |
and technology to automate |
power and component integration |
flexible cache adaptation in |
technology to automate management |
and component integration features |
cache adaptation in the |
to automate management of |
component integration features of |
ip requires massive buffers |
automate management of a |
adaptation in the context |
integration features of a |
requires massive buffers at |
management of a machine |
in the context of |
features of a managed |
massive buffers at the |
of a machine cluster |
the context of mfs |
of a managed framework |
buffers at the communicating |
a machine cluster running |
at the communicating endhosts |
machine cluster running web |
a client cache manager |
the communicating endhosts to |
cluster running web services |
the multicast technology must |
client cache manager for |
communicating endhosts to fully |
running web services applications |
multicast technology must run |
cache manager for a |
endhosts to fully exploit |
technology must run in |
manager for a manager |
to fully exploit the |
must run in a |
for a manager for |
fully exploit the bandwidth |
run in a managed |
a manager for a |
exploit the bandwidth of |
in a managed setting |
manager for a distributed |
the bandwidth of a |
working groups within the |
for a distributed file |
bandwidth of a long |
groups within the world |
a distributed file system |
but little is known |
within the world wide |
distributed file system client |
little is known about |
the world wide web |
is known about highperformance |
world wide web consortium |
known about highperformance protocols |
which differs from distributed |
about highperformance protocols in |
even in the absence |
differs from distributed file |
highperformance protocols in managed |
in the absence of |
from distributed file system |
protocols in managed environments |
the absence of packet |
absence of packet loss |
we concentrate on distributed |
it is interesting to |
concentrate on distributed file |
is interesting to realize |
on distributed file systraditional |
interesting to realize that |
resistant alternatives to tcp |
distributed file systraditional cache |
to realize that although |
file systraditional cache manager |
realize that although microsoft |
ip is not feasible |
systraditional cache manager design |
that although microsoft pro |
the primary organization developing |
is not feasible in |
cache manager design in |
primary organization developing web |
not feasible in corporate |
manager design in two |
this research was supported |
organization developing web services |
feasible in corporate data |
design in two important |
research was supported by |
developing web services standards |
in corporate data centers |
in two important respects |
was supported by afrl |
where standardization is the |
standardization is the key |
is the key to |
if with additional support |
not one is addressing |
the key to low |
with additional support from |
the views and conclusions |
key to low and |
one is addressing these |
additional support from afosr |
views and conclusions herein |
tems because systems in |
to low and predictable |
because systems in this |
and conclusions herein are |
is addressing these kinds |
low and predictable maintenance |
systems in this area |
conclusions herein are those |
addressing these kinds of |
and predictable maintenance costs |
in this area are |
herein are those of |
these kinds of issues |
this area are highly |
are those of the |
area are highly developed |
neither is eliminating loss |
those of the authors |
are highly developed and |
a similar dynamic played |
is eliminating loss events |
highly developed and have |
similar dynamic played out |
eliminating loss events on |
developed and have mfs |
dynamic played out in |
loss events on a |
department of computer science |
and have mfs uses |
played out in the |
events on a network |
have mfs uses an |
out in the early |
on a network that |
mfs uses an rpc |
a network that could |
uses an rpc library |
network that could span |
an rpc library supporting |
that could span thousands |
rpc library supporting priorities |
could span thousands of |
library supporting priorities to |
span thousands of miles |
supporting priorities to enable |
priorities to enable modewell |
to enable modewell understood |
enable modewell understood semantics |
there is a need |
although the techniques we |
is a need to |
the techniques we describe |
a need to mask |
techniques we describe less |
need to mask loss |
server computing was touted |
we describe less adaptation |
to mask loss on |
computing was touted as |
mask loss on the |
was touted as the |
loss on the link |
touted as the next |
on the link from |
as the next big |
the link from the |
the next big thing |
link from the commodity |
from the commodity protocols |
the commodity protocols running |
commodity protocols running at |
protocols running at end |
a silver bullet to |
silver bullet to solve |
bullet to solve every |
to solve every problem |
which allocates available bandwidth |
solve every problem related |
allocates available bandwidth based |
every problem related to |
available bandwidth based should |
and to do so |
problem related to older |
bandwidth based should be |
to do so rapidly |
related to older mainframe |
based should be broadly |
do so rapidly and |
to older mainframe and |
should be broadly applicable |
so rapidly and transparently |
older mainframe and batch |
be broadly applicable in |
mainframe and batch systems |
broadly applicable in other |
applicable in other application |
in other application environments |
companies rushed to move |
because recovery delays for |
rushed to move everything |
on the types of |
recovery delays for lost |
to move everything from |
the types of messages |
delays for lost packets |
move everything from mainframe |
types of messages being |
for lost packets translate |
everything from mainframe settings |
of messages being sent |
lost packets translate into |
from mainframe settings to |
the embedding of qsm |
packets translate into dramatic |
mainframe settings to client |
by assigning priorities such |
translate into dramatic reductions |
embedding of qsm into |
assigning priorities such as |
into dramatic reductions in |
of qsm into windows |
priorities such as caching |
dramatic reductions in application |
qsm into windows yielded |
such as caching dynamic |
there were notable successes |
into windows yielded an |
as caching dynamic internet |
windows yielded an unexpected |
caching dynamic internet content |
yielded an unexpected benefit |
but it quickly became |
dynamic internet content or |
it quickly became apparent |
internet content or caching |
quickly became apparent that |
because applications and os |
content or caching to |
it enables what we |
became apparent that the |
applications and os networking |
or caching to improve |
enables what we are |
apparent that the early |
and os networking stacks |
caching to improve appropriately |
what we are calling |
that the early platforms |
os networking stacks in |
we are calling live |
the early platforms were |
networking stacks in commodity |
are calling live distributed |
early platforms were strikingly |
stacks in commodity data |
calling live distributed objects |
platforms were strikingly immature |
minimum and average download |
such as retrieving files |
in commodity data centers |
and average download rates |
commodity data centers cannot |
processes needed to be |
average download rates across |
can the performance of |
data centers cannot be |
needed to be automated |
as the term suggests |
download rates across all |
the performance of interactions |
centers cannot be rewritten |
to be automated and |
rates across all nodes |
these are abstract data |
cannot be rewritten from |
be automated and standardized |
performance of interactions with |
across all nodes when |
are abstract data types |
be rewritten from scratch |
of interactions with web |
all nodes when using |
abstract data types in |
and the early generations |
interactions with web services |
nodes when using the |
data types in which |
the early generations of |
when using the bar |
types in which content |
early generations of client |
using the bar gossip |
in which content evolves |
we evaluate proceed concurrently |
the bar gossip and |
evaluate proceed concurrently with |
which content evolves over |
bar gossip and chainsaw |
server systems cost a |
proceed concurrently with background |
content evolves over time |
gossip and chainsaw protocols |
systems cost a fortune |
side appliance receiver buffer |
concurrently with background activities |
cost a fortune to |
when an application binds |
with background activities such |
appliance receiver buffer overflow |
a fortune to build |
an application binds to |
paper is to propose |
background activities such as |
local recovery locations of |
activities such as writing |
application binds to a |
is to propose and |
recovery locations of packet |
such as writing the |
required armies of systems |
to propose and evaluate |
locations of packet loss |
binds to a live |
as writing the authors |
armies of systems administrators |
propose and evaluate a |
of packet loss receive |
to a live object |
writing the authors were |
of systems administrators and |
and evaluate a mechanism |
the authors were supported |
side appliance receiving end |
evaluate a mechanism that |
systems administrators and specialists |
authors were supported in |
the current state of |
a mechanism that can |
were supported in part |
current state of the |
mechanism that can defend |
and were extremely insecure |
supported in part by |
state of the object |
that can defend against |
in part by darpa |
of the object is |
the total cost of |
part by darpa under |
can defend against this |
the object is imported |
maelstrom communication path forward |
by darpa under afrl |
defend against this problem |
total cost of ownership |
object is imported and |
communication path forward error |
darpa under afrl grant |
whithout incurring large overheads |
is imported and the |
path forward error correction |
cost of ownership proved |
under afrl grant radc |
the approach that most |
of ownership proved to |
imported and the object |
afrl grant radc back |
approach that most closely |
ownership proved to be |
and the object can |
grant radc back changes |
that most closely relates |
proved to be unexpectedly |
the object can send |
is a promising solution |
most closely relates to |
to be unexpectedly and |
object can send and |
a promising solution for |
closely relates to our |
under the assurance that |
be unexpectedly and unacceptably |
can send and receive |
promising solution for reliability |
relates to our work |
the assurance that if |
unexpectedly and unacceptably high |
send and receive updates |
solution for reliability over |
to our work is |
assurance that if bandwidth |
and receive updates at |
the lesson of the |
our work is the |
that if bandwidth becomes |
for reliability over long |
receive updates at high |
lesson of the client |
work is the bar |
if bandwidth becomes f |
updates at high data |
is the bar gossip |
at high data rates |
server era is that |
the bar gossip protocol |
era is that incomplete |
is that incomplete platforms |
that incomplete platforms can |
incomplete platforms can t |
an object could be |
platforms can t support |
object could be a |
can t support major |
could be a place |
which employs a tit |
be a place in |
a place in a |
place in a game |
in a game like |
a game like second |
packet recovery latency is |
game like second life |
recovery latency is independent |
tat approach for encouraging |
my concern is that |
latency is independent of |
approach for encouraging nodes |
concern is that the |
is independent of the |
for encouraging nodes to |
is that the web |
independent of the rtt |
encouraging nodes to contribute |
that the web services |
of the rtt of |
the web services community |
the rtt of the |
a node only sends |
web services community is |
rtt of the link |
node only sends as |
services community is about |
only sends as much |
community is about to |
sends as much data |
is about to face |
as much data to |
while fec codes have |
about to face the |
much data to another |
fec codes have been |
to face the same |
data to another node |
live objects are a |
codes have been used |
face the same problem |
to another node as |
objects are a natural |
have been used for |
another node as it |
are a natural and |
been used for decades |
node as it receives |
a natural and powerful |
platform developers are racing |
used for decades within |
as it receives back |
natural and powerful idea |
developers are racing forward |
for decades within link |
are racing forward at |
and by afosr under |
racing forward at top |
and we plan to |
by afosr under muri |
it provides an elegant |
forward at top speed |
we plan to pursue |
afosr under muri grant |
provides an elegant solution |
plan to pursue the |
under muri grant f |
faster commodity processors have |
an elegant solution shown |
to pursue the concept |
jostling for position with |
commodity processors have enabled |
elegant solution shown to |
pursue the concept in |
for position with ever |
processors have enabled packet |
solution shown to tolerate |
the concept in future |
position with ever more |
shown to tolerate both |
concept in future work |
level fec at end |
to tolerate both opportunistic |
with ever more exaggerated |
tolerate both opportunistic behavior |
ever more exaggerated claims |
both opportunistic behavior and |
opportunistic behavior and other |
behavior and other malicious |
and other malicious attacks |
while closing their eyes |
this use of qsm |
closing their eyes to |
use of qsm raises |
their eyes to the |
of qsm raises performance |
eyes to the dangerous |
qsm raises performance and |
to the dangerous potholes |
raises performance and scalability |
the dangerous potholes in |
performance and scalability issues |
dangerous potholes in the |
tat does present a |
and scalability issues beyond |
potholes in the road |
does present a few |
scalability issues beyond the |
in the road ahead |
present a few undesirable |
issues beyond the ones |
a few undesirable requirements |
beyond the ones seen |
the ones seen in |
ones seen in our |
architectural standards for scalability |
seen in our original |
standards for scalability to |
in our original target |
for scalability to properly |
the data source should |
our original target domain |
scalability to properly address |
data source should ensure |
to properly address scalability |
source should ensure that |
properly address scalability in |
should ensure that packets |
address scalability in web |
ensure that packets are |
scalability in web services |
that packets are evenly |
we leave detailed discussion |
packets are evenly spread |
leave detailed discussion of |
are evenly spread across |
detailed discussion of the |
end fec is very |
evenly spread across the |
we need more than |
discussion of the idea |
fec is very attractive |
spread across the system |
need more than a |
rather than the foreground |
is very attractive for |
across the system by |
of the idea for |
more than a long |
than the foreground ones |
very attractive for communication |
the system by sending |
the idea for the |
than a long list |
attractive for communication between |
additional support from microsoft |
idea for the future |
a long list of |
system by sending data |
for communication between data |
support from microsoft research |
long list of reliability |
qsm has been available |
communication between data centers |
from microsoft research and |
by sending data to |
list of reliability and |
has been available for |
microsoft research and from |
sending data to a |
of reliability and management |
been available for free |
research and from the |
data to a fixed |
reliability and management standards |
available for free download |
and from the intel |
easy to deploy and |
to a fixed proportion |
for free download since |
from the intel corporation |
to deploy and customize |
a fixed proportion of |
free download since mid |
we need a new |
fixed proportion of nodes |
need a new methodology |
a new methodology suitable |
and does not require |
new methodology suitable for |
and by sending different |
does not require specialized |
application programs background processing |
by sending different packets |
methodology suitable for supporting |
not require specialized equipment |
sending different packets to |
programs background processing incoming |
suitable for supporting a |
require specialized equipment in |
different packets to different |
background processing incoming traffic |
for supporting a scalable |
specialized equipment in the |
packets to different nodes |
processing incoming traffic cache |
supporting a scalable data |
equipment in the network |
a scalable data center |
incoming traffic cache consistency |
in the network linking |
traffic cache consistency demand |
and it has a |
cache consistency demand fetch |
the network linking the |
it requires the source |
scalable data center architecture |
it has a number |
consistency demand fetch access |
network linking the data |
demand fetch access monitoring |
has a number of |
fetch access monitoring prefetch |
linking the data centers |
access monitoring prefetch outgoing |
a number of users |
monitoring prefetch outgoing traffic |
requires the source and |
prefetch outgoing traffic synchronous |
the source and all |
outgoing traffic synchronous writeback |
source and all nodes |
traffic synchronous writeback update |
most working on clustered |
and all nodes to |
synchronous writeback update logging |
working on clustered computing |
endhost fec has two |
all nodes to have |
writeback update logging asynchronous |
fec has two major |
nodes to have full |
update logging asynchronous writeback |
has two major issues |
to have full membership |
logging asynchronous writeback mfs |
one large project is |
have full membership knowledge |
two major issues first |
asynchronous writeback mfs server |
large project is pairing |
along with pat helland |
writeback mfs server adaptive |
project is pairing qsm |
these restrictions affect scalability |
with pat helland and |
mfs server adaptive rpc |
it s not transparent |
is pairing qsm with |
restrictions affect scalability when |
pat helland and dennis |
server adaptive rpc library |
pairing qsm with high |
affect scalability when the |
helland and dennis shasha |
adaptive rpc library mfs |
requiring modification of the |
scalability when the data |
rpc library mfs cache |
modification of the end |
when the data source |
speed event stream filtering |
library mfs cache manager |
recommends that developers think |
the data source has |
event stream filtering and |
mfs cache manager will |
that developers think in |
data source has bounded |
stream filtering and data |
cache manager will be |
developers think in terms |
source has bounded upload |
filtering and data mining |
manager will be penalised |
think in terms of |
has bounded upload bandwidth |
and data mining system |
will be penalised first |
in terms of a |
it s not necessarily |
to illustrate this problem |
terms of a reliable |
data mining system to |
s not necessarily rapid |
of a reliable arraystructured |
we fixed the upload |
modeless adaptation using prioritised |
mining system to obtain |
adaptation using prioritised communication |
fixed the upload capacity |
using prioritised communication also |
system to obtain a |
prioritised communication also allows |
a reliable arraystructured partitioned |
the upload capacity of |
fec works best over |
to obtain a scalable |
communication also allows mfs |
reliable arraystructured partitioned service |
upload capacity of a |
works best over high |
also allows mfs to |
capacity of a data |
allows mfs to be |
of a data source |
mfs to be more |
a data source at |
hosted service capable of |
stable traffic rates and |
to be more flexible |
service capable of handling |
traffic rates and performs |
implemented as a set |
be more flexible in |
capable of handling very |
mbps and simulated bar |
rates and performs poorly |
as a set of |
more flexible in response |
of handling very high |
and simulated bar gossip |
and performs poorly if |
a set of reliable |
flexible in response to |
handling very high event |
simulated bar gossip when |
performs poorly if the |
set of reliable arraystructured |
in response to bandwidth |
very high event rates |
bar gossip when streaming |
poorly if the data |
of reliable arraystructured clustered |
response to bandwidth variations |
if the data rate |
reliable arraystructured clustered servers |
to bandwidth variations than |
the data rate in |
bandwidth variations than would |
data rate in the |
group used for system |
variations than would be |
rate in the channel |
used for system management |
than would be possible |
in the channel is |
kbps with increasing numbers |
for system management service |
would be possible with |
the channel is low |
with increasing numbers of |
system management service b |
be possible with a |
this architecture offers scalability |
channel is low and |
increasing numbers of receivers |
management service b x |
possible with a modal |
architecture offers scalability and |
is low and sporadic |
service b x y |
with a modal scheme |
offers scalability and reliability |
varied between one and |
b x y z |
scalability and reliability at |
between one and thirty |
x y z x |
and reliability at two |
y z x y |
one and thirty thousand |
reliability at two levels |
z x y z |
and thirty thousand nodes |
mfs incorporates a new |
x y z x |
incorporates a new cache |
the top level uses |
y z x y |
a new cache consistency |
top level uses some |
we compare its scalability |
z x y z |
new cache consistency algorithm |
level uses some sort |
as in a single |
compare its scalability against |
x y z a |
cache consistency algorithm to |
uses some sort of |
in a single end |
its scalability against the |
y z a b |
consistency algorithm to efficiently |
some sort of application |
scalability against the chainsaw |
z a b service |
algorithm to efficiently provide |
against the chainsaw protocol |
a b service c |
specific key to partition |
to efficiently provide a |
b service c a |
key to partition the |
efficiently provide a high |
service c a b |
to partition the service |
provide a high degree |
c a b w |
partition the service into |
for which we fixed |
we present the maelstrom |
a high degree of |
a b w figure |
the service into subservices |
which we fixed the |
present the maelstrom error |
high degree of consistency |
we fixed the source |
the maelstrom error correction |
degree of consistency for |
fixed the source s |
the lower level implements |
maelstrom error correction appliance |
of consistency for access |
the source s upload |
lower level implements subservices |
if sets of components |
error correction appliance a |
consistency for access to |
source s upload bandwidth |
level implements subservices using |
sets of components are |
correction appliance a rack |
for access to shared |
s upload bandwidth to |
implements subservices using groups |
of components are replicated |
appliance a rack of |
access to shared files |
subservices using groups of |
a rack of proxies |
using groups of programs |
rack of proxies residing |
the associated multicast groups |
which is required for |
of proxies residing between |
groups of programs that |
associated multicast groups overlap |
is required for collaborative |
proxies residing between a |
of programs that run |
multicast groups overlap hierarchically |
required for collaborative work |
residing between a data |
programs that run on |
we present the average |
for collaborative work applications |
between a data center |
that run on multiple |
present the average and |
the foregoing is the |
a data center and |
run on multiple machines |
the average and minimum |
foregoing is the primary |
the rest of this |
data center and its |
average and minimum download |
is the primary use |
rest of this paper |
center and its wan |
and minimum download rates |
perhaps in a cluster |
the primary use scenario |
of this paper is |
and its wan link |
in a cluster computer |
primary use scenario for |
this paper is organised |
as ratios of the |
use scenario for qsm |
paper is organised as |
ratios of the stream |
is organised as follows |
the groups replicate data |
of the stream rate |
but may not be |
groups replicate data so |
may not be the |
replicate data so that |
not be the only |
data so that each |
of both protocols when |
describes the mfs design |
so that each can |
be the only one |
both protocols when the |
maelstrom encodes fec packets |
the mfs design and |
that each can handle |
protocols when the number |
encodes fec packets over |
mfs design and differences |
each can handle any |
when the number of |
one could imagine an |
fec packets over traffic |
design and differences from |
can handle any incoming |
the number of nodes |
could imagine an approach |
packets over traffic flowing |
and differences from existing |
handle any incoming query |
number of nodes is |
imagine an approach to |
over traffic flowing through |
differences from existing distributed |
any incoming query for |
of nodes is increased |
an approach to laying |
traffic flowing through it |
from existing distributed and |
incoming query for its |
approach to laying out |
flowing through it and |
existing distributed and mobile |
query for its range |
to laying out components |
through it and routes |
distributed and mobile file |
for its range within |
bar gossip is not |
laying out components on |
it and routes them |
and mobile file systems |
its range within the |
gossip is not able |
out components on a |
and routes them to |
range within the keys |
as well as giving |
components on a cluster |
routes them to a |
is not able to |
well as giving an |
not able to sustain |
them to a corresponding |
on a cluster that |
as giving an overview |
enabling updates to reach |
able to sustain its |
to a corresponding appliance |
a cluster that would |
giving an overview of |
updates to reach all |
to sustain its performance |
a corresponding appliance at |
cluster that would result |
an overview of the |
to reach all the |
sustain its performance without |
corresponding appliance at the |
that would result in |
overview of the mfs |
reach all the replicas |
its performance without scaling |
appliance at the destination |
would result in irregular |
of the mfs rpc |
performance without scaling the |
at the destination data |
result in irregular layouts |
the mfs rpc library |
without scaling the upload |
the destination data center |
in irregular layouts of |
scaling the upload capacity |
irregular layouts of groups |
a raps that an |
the upload capacity of |
raps that an e |
which decodes them and |
describes the use of |
upload capacity of the |
decodes them and recovers |
the use of prioritised |
qsm can support such |
capacity of the source |
them and recovers lost |
tailer such as amazon |
use of prioritised communication |
can support such layouts |
of the source proportionally |
and recovers lost data |
such as amazon might |
of prioritised communication in |
the source proportionally with |
as amazon might use |
prioritised communication in mfs |
at least to a |
source proportionally with the |
amazon might use to |
maelstrom is completely transparent |
communication in mfs and |
least to a degree |
proportionally with the size |
might use to personalize |
is completely transparent it |
in mfs and experiments |
with the size of |
use to personalize a |
completely transparent it does |
mfs and experiments to |
the size of the |
but for reasons of |
to personalize a product |
transparent it does not |
and experiments to evaluate |
size of the system |
for reasons of brevity |
personalize a product recommendation |
it does not require |
experiments to evaluate its |
reasons of brevity the |
does not require modification |
to evaluate its effectiveness |
of brevity the discussion |
not require modification of |
depending on the customer |
brevity the discussion in |
require modification of end |
chainsaw is able to |
on the customer s |
the discussion in the |
is able to scale |
the customer s profile |
host software and is |
presents and explains experimental |
able to scale well |
discussion in the remainder |
software and is agnostic |
and explains experimental results |
to scale well even |
in the remainder of |
and is agnostic to |
the service ranks matching |
explains experimental results for |
scale well even with |
the remainder of the |
is agnostic to the |
service ranks matching products |
experimental results for the |
well even with a |
remainder of the paper |
agnostic to the network |
ranks matching products differently |
results for the mfs |
even with a fixed |
of the paper focuses |
to the network connecting |
matching products differently to |
for the mfs prefetching |
with a fixed lower |
the paper focuses on |
the network connecting the |
products differently to maximize |
the mfs prefetching mechanism |
a fixed lower upload |
paper focuses on regular |
network connecting the data |
differently to maximize the |
fixed lower upload bandwidth |
connecting the data centers |
to maximize the chance |
lower upload bandwidth at |
hierarchically structured communication groups |
does the same for |
upload bandwidth at the |
maximize the chance of |
structured communication groups with |
the same for the |
bandwidth at the source |
the chance of a |
communication groups with extensive |
same for the cache |
it eliminates the dependence |
chance of a purchase |
groups with extensive and |
for the cache consistency |
eliminates the dependence of |
but cannot handle the |
with extensive and regular |
the cache consistency algorithm |
the dependence of fec |
cannot handle the presence |
if the product is |
extensive and regular overlap |
dependence of fec recovery |
handle the presence of |
of fec recovery latency |
the presence of opportunistic |
fec recovery latency on |
presence of opportunistic nodes |
recovery latency on the |
initial users of our |
latency on the data |
concludes and describes future |
users of our system |
on the data rate |
and describes future work |
we propose to use |
of our system haven |
the data rate in |
as shown in figure |
propose to use auditing |
our system haven t |
data rate in any |
to use auditing to |
system haven t had |
rate in any single |
use auditing to encourage |
haven t had any |
in any single node |
auditing to encourage data |
t had any difficulty |
the service assigns the |
had any difficulty with |
the most important part |
service assigns the search |
any difficulty with this |
most important part of |
assigns the search request |
node channel by encoding |
important part of mfs |
difficulty with this constraint |
streaming systems like chainsaw |
the search request to |
channel by encoding over |
part of mfs is |
search request to the |
by encoding over the |
of mfs is the |
knowing qsm is particularly |
request to the racs |
encoding over the aggregated |
our auditing approach establishes |
mfs is the cache |
qsm is particularly effective |
to the racs handling |
over the aggregated traffic |
auditing approach establishes a |
is the cache manager |
is particularly effective with |
the racs handling all |
the aggregated traffic leaving |
approach establishes a minimum |
particularly effective with regular |
racs handling all ds |
aggregated traffic leaving the |
establishes a minimum threshold |
effective with regular layouts |
which intercepts file system |
traffic leaving the data |
intercepts file system operations |
a minimum threshold for |
leaving the data center |
they just design to |
minimum threshold for the |
file system operations from |
just design to favor |
such as the customer |
threshold for the amount |
system operations from application |
design to favor regularity |
as the customer s |
for the amount of |
operations from application programs |
maelstrom uses a new |
the customer s name |
the amount of data |
from application programs and |
uses a new encoding |
customer s name are |
amount of data sent |
application programs and resolves |
a new encoding scheme |
s name are equally |
usage cases architecture reliable |
of data sent by |
programs and resolves them |
new encoding scheme called |
name are equally plausible |
cases architecture reliable multicast |
data sent by any |
and resolves them into |
encoding scheme called layered |
architecture reliable multicast is |
sent by any node |
resolves them into accesses |
scheme called layered interleaving |
reliable multicast is a |
by any node in |
them into accesses to |
multicast is a mature |
the load balancer then |
any node in the |
into accesses to its |
is a mature area |
designed especially for time |
load balancer then routes |
node in the system |
accesses to its local |
balancer then routes the |
to its local mfs |
but a review of |
then routes the request |
sensitive packet recovery in |
its local mfs cache |
a review of prior |
and removes nodes that |
routes the request to |
packet recovery in the |
local mfs cache or |
review of prior systems |
removes nodes that upload |
the request to the |
recovery in the presence |
mfs cache or rpcs |
of prior systems convinced |
nodes that upload less |
request to the appropriate |
in the presence of |
cache or rpcs to |
prior systems convinced us |
that upload less data |
to the appropriate program |
the presence of bursty |
or rpcs to a |
systems convinced us that |
upload less data than |
the appropriate program for |
presence of bursty loss |
rpcs to a server |
convinced us that no |
less data than the |
appropriate program for processing |
us that no existing |
data than the threshold |
program for processing in |
maelstrom s positioning as |
that no existing system |
the cache manager has |
for processing in this |
instead of relying on |
no existing system would |
cache manager has a |
s positioning as a |
processing in this case |
positioning as a network |
existing system would work |
manager has a number |
of relying on a |
as a network appliance |
system would work well |
has a number of |
relying on a tit |
a network appliance reflects |
would work well in |
with support for this |
network appliance reflects the |
a number of components |
work well in the |
support for this basic |
appliance reflects the physical |
well in the scenarios |
for this basic layout |
reflects the physical infrastructure |
in the scenarios targeted |
those in solid boxes |
the physical infrastructure of |
the scenarios targeted by |
in solid boxes are |
we focus on encouraging |
physical infrastructure of modern |
it s possible to |
scenarios targeted by our |
solid boxes are part |
focus on encouraging nodes |
infrastructure of modern data |
s possible to tackle |
targeted by our project |
boxes are part of |
on encouraging nodes to |
of modern data centers |
possible to tackle a |
are part of the |
encouraging nodes to respect |
modern data centers clean |
to tackle a wide |
part of the core |
this forced us to |
nodes to respect the |
data centers clean insertion |
tackle a wide range |
of the core system |
forced us to build |
to respect the established |
centers clean insertion points |
a wide range of |
us to build a |
respect the established protocol |
clean insertion points for |
wide range of secondary |
to build a new |
those in dashed boxes |
insertion points for proxy |
range of secondary issues |
build a new system |
in dashed boxes are |
points for proxy devices |
nodes are forced to |
a new system that |
dashed boxes are optional |
for proxy devices exist |
are forced to provide |
new system that combines |
boxes are optional extensions |
proxy devices exist on |
forced to provide accountable |
system that combines features |
we could create standards |
are optional extensions which |
devices exist on the |
to provide accountable information |
that combines features from |
could create standards for |
optional extensions which are |
exist on the high |
provide accountable information regarding |
combines features from a |
create standards for a |
extensions which are described |
accountable information regarding packets |
features from a number |
standards for a self |
which are described in |
speed lambda links that |
information regarding packets sent |
from a number of |
are described in subsequent |
lambda links that interconnect |
regarding packets sent to |
a number of prior |
managed raps of racs |
described in subsequent sections |
links that interconnect individual |
packets sent to and |
number of prior systems |
that interconnect individual data |
sent to and received |
or for one that |
interconnect individual data centers |
to and received from |
for one that guarantees |
individual data centers to |
and received from neighbors |
our decision not to |
one that guarantees real |
data centers to each |
mfs overview mfs differs |
decision not to use |
centers to each other |
overview mfs differs from |
not to use some |
and the auditing system |
mfs differs from earlier |
to use some existing |
the auditing system is |
differs from earlier mobile |
maelstrom can operate as |
such a basic architecture |
use some existing multicast |
auditing system is responsible |
from earlier mobile file |
can operate as either |
a basic architecture is |
some existing multicast system |
system is responsible for |
earlier mobile file systems |
operate as either a |
basic architecture is effectively |
existing multicast system reflects |
is responsible for detecting |
mobile file systems in |
as either a passive |
architecture is effectively a |
multicast system reflects a |
responsible for detecting and |
file systems in adjusting |
either a passive or |
is effectively a framework |
system reflects a number |
for detecting and removing |
systems in adjusting to |
a passive or active |
effectively a framework to |
reflects a number of |
detecting and removing misbehaving |
in adjusting to changing |
passive or active device |
a framework to resolve |
a number of issues |
and removing misbehaving nodes |
adjusting to changing network |
or active device on |
framework to resolve other |
to changing network conditions |
active device on these |
to resolve other related |
changing network conditions using |
most prior multicast systems |
device on these links |
notice that identifying the |
resolve other related issues |
network conditions using modeless |
prior multicast systems were |
that identifying the misbehaving |
conditions using modeless adaptation |
of the three problems |
multicast systems were designed |
identifying the misbehaving nodes |
the three problems of |
systems were designed to |
it comprises a core |
three problems of tcp |
the misbehaving nodes is |
group replication web services |
were designed to replicate |
comprises a core client |
misbehaving nodes is not |
replication web services currently |
designed to replicate state |
nodes is not a |
web services currently lacks |
to replicate state within |
is not a trivial |
maelstrom solves the first |
services currently lacks support |
replicate state within just |
not a trivial task |
solves the first two |
currently lacks support for |
the first two throughput |
state within just a |
first two throughput collapse |
lacks support for building |
and a number of |
within just a single |
two throughput collapse and |
support for building scalable |
since there is no |
a number of subsystems |
just a single group |
throughput collapse and realtime |
for building scalable services |
there is no fixed |
number of subsystems that |
a single group at |
collapse and realtime recovery |
is no fixed minimum |
of subsystems that perform |
single group at a |
and realtime recovery delays |
the architecture makes it |
no fixed minimum amount |
subsystems that perform different |
group at a time |
realtime recovery delays while |
architecture makes it easy |
fixed minimum amount of |
that perform different kinds |
recovery delays while operating |
makes it easy to |
minimum amount of data |
perform different kinds of |
delays while operating as |
it easy to build |
for example a single |
amount of data that |
different kinds of adaptation |
while operating as a |
easy to build a |
to build a single |
of data that nodes |
operating as a passive |
example a single distributed |
data that nodes should |
as a passive device |
a single distributed service |
that nodes should contribute |
node server that responds |
and can be selectively |
a passive device that |
nodes should contribute to |
server that responds to |
can be selectively enabled |
passive device that does |
should contribute to the |
contribute to the system |
that responds to requests |
device that does not |
some don t support |
responds to requests from |
that does not intervene |
if we assume a |
to requests from some |
don t support multiple |
does not intervene in |
we assume a model |
requests from some set |
t support multiple groups |
not intervene in the |
shows the structure of |
assume a model where |
from some set of |
support multiple groups at |
intervene in the critical |
the structure of the |
a model where misbehaving |
some set of clients |
multiple groups at all |
in the critical communication |
structure of the system |
model where misbehaving nodes |
the critical communication path |
where misbehaving nodes simply |
but there s no |
misbehaving nodes simply did |
there s no way |
nodes simply did not |
s no way to |
while others have overheads |
simply did not upload |
in this section we |
others have overheads linear |
no way to turn |
did not upload any |
this section we describe |
maelstrom handles the additional |
have overheads linear in |
way to turn that |
not upload any data |
section we describe the |
handles the additional problem |
overheads linear in the |
to turn that single |
we describe the core |
the additional problem of |
linear in the number |
turn that single server |
detecting them would be |
describe the core system |
additional problem of massive |
in the number of |
that single server into |
them would be an |
problem of massive buffering |
the number of groups |
while subsequent sections do |
would be an easier |
of massive buffering requirements |
single server into a |
number of groups to |
subsequent sections do the |
be an easier task |
massive buffering requirements as |
server into a racs |
of groups to which |
sections do the same |
buffering requirements as well |
into a racs or |
groups to which a |
do the same for |
a racs or turn |
once we assume that |
the same for the |
to which a node |
racs or turn a |
at the cost of |
we assume that misbehaving |
same for the three |
which a node belongs |
or turn a set |
the cost of adding |
assume that misbehaving nodes |
for the three main |
turn a set of |
cost of adding a |
that misbehaving nodes may |
the three main subsystems |
a set of racs |
of adding a point |
misbehaving nodes may adjust |
set of racs into |
adding a point of |
nodes may adjust their |
we looked at jgroups |
a point of failure |
of racs into a |
may adjust their contribution |
point of failure in |
racs into a raps |
adjust their contribution level |
we begin with an |
of failure in the |
their contribution level based |
failure in the network |
begin with an overview |
contribution level based on |
in the network path |
with an overview of |
level based on the |
it would be easy |
an overview of mobile |
the contributions of this |
overview of mobile file |
based on the policy |
of mobile file system |
contributions of this paper |
mobile file system design |
on the policy used |
file system design and |
of this paper are |
system design and the |
the policy used by |
would be easy to |
this paper are as |
design and the relation |
policy used by an |
a component of the |
be easy to bridge |
paper are as follows |
and the relation of |
used by an auditing |
component of the jboss |
easy to bridge the |
the relation of mfs |
by an auditing system |
of the jboss platform |
to bridge the gap |
relation of mfs to |
the jboss platform which |
bridge the gap if |
of mfs to previous |
end fec for long |
a more elaborate approach |
the gap if vendors |
jboss platform which runs |
mfs to previous work |
more elaborate approach is |
gap if vendors and |
platform which runs in |
distance communication between data |
elaborate approach is required |
if vendors and platform |
which runs in a |
communication between data centers |
vendors and platform builders |
then briefly describe the |
runs in a managed |
and platform builders wanted |
this paper presents and |
and argue that the |
in a managed java |
platform builders wanted to |
briefly describe the adaptive |
paper presents and evaluates |
argue that the rate |
presents and evaluates an |
that the rate sensitivity |
describe the adaptive rpc |
a managed java framework |
builders wanted to do |
and evaluates an auditing |
the rate sensitivity of |
evaluates an auditing model |
rate sensitivity of fec |
an auditing model based |
wanted to do so |
auditing model based on |
sensitivity of fec codes |
model based on sampling |
of fec codes and |
based on sampling the |
the adaptive rpc library |
jgroups wasn t designed |
fec codes and the |
on sampling the system |
adaptive rpc library used |
wasn t designed to |
codes and the opacity |
structured partitioned service reliable |
sampling the system and |
rpc library used in |
t designed to support |
and the opacity of |
partitioned service reliable array |
the system and using |
library used in mfs |
designed to support large |
the opacity of their |
system and using the |
to support large numbers |
opacity of their implementations |
and using the sampled |
support large numbers of |
of their implementations present |
using the sampled information |
and the current mfs |
large numbers of overlapping |
their implementations present major |
the sampled information to |
the current mfs implementation |
numbers of overlapping groups |
implementations present major obstacles |
sampled information to build |
present major obstacles to |
information to build a |
major obstacles to their |
to build a global |
obstacles to their usage |
build a global view |
a global view of |
x y z search |
global view of how |
and if configured to |
y z search for |
view of how the |
if configured to do |
z search for digital |
of how the system |
configured to do so |
a gateway appliance that |
search for digital camera |
how the system is |
gateway appliance that transparently |
for digital camera figure |
the system is currently |
appliance that transparently aggregates |
system is currently behaving |
that transparently aggregates traffic |
transparently aggregates traffic and |
aggregates traffic and encodes |
traffic and encodes over |
and encodes over the |
encodes over the resulting |
mfs design and related |
example of raps of |
over the resulting high |
of raps of racs |
there has been a |
auditors employ strategies to |
design and related work |
has been a great |
employ strategies to identify |
and related work the |
the service assigns a |
been a great deal |
strategies to identify the |
related work the core |
we describe layered interleaving |
service assigns a digital |
a great deal of |
to identify the misbehaving |
work the core of |
assigns a digital camera |
great deal of work |
identify the misbehaving nodes |
the core of mfs |
a digital camera search |
a new fec scheme |
deal of work on |
the misbehaving nodes that |
core of mfs follows |
digital camera search request |
new fec scheme used |
of work on p |
misbehaving nodes that should |
of mfs follows a |
camera search request to |
mfs follows a design |
nodes that should be |
fec scheme used by |
search request to the |
follows a design common |
that should be punished |
scheme used by maelstrom |
p pubsub and content |
request to the clustered |
a design common to |
used by maelstrom where |
pubsub and content delivery |
to the clustered server |
design common to many |
the paper is organized |
by maelstrom where for |
and content delivery platforms |
the clustered server handling |
common to many mobile |
paper is organized as |
maelstrom where for constant |
content delivery platforms in |
clustered server handling all |
to many mobile file |
is organized as follows |
where for constant encoding |
delivery platforms in recent |
server handling all ds |
many mobile file systems |
for constant encoding overhead |
platforms in recent years |
constant encoding overhead the |
and a load balancer |
encoding overhead the latency |
a load balancer routes |
overhead the latency of |
load balancer routes it |
the latency of packet |
we state the exact |
balancer routes it to |
latency of packet recovery |
state the exact problem |
routes it to the |
of packet recovery degrades |
often oriented towards content |
the exact problem that |
it to the appropriate |
packet recovery degrades gracefully |
oriented towards content filtering |
exact problem that we |
to the appropriate process |
recovery degrades gracefully as |
towards content filtering in |
problem that we aim |
degrades gracefully as losses |
content filtering in document |
that we aim to |
gracefully as losses get |
old and familiar technologies |
filtering in document streams |
we aim to solve |
as losses get burstier |
and familiar technologies the |
aim to solve and |
familiar technologies the most |
to solve and the |
technologies the most standard |
we discuss implementation considerations |
solve and the assumptions |
the most standard form |
a good example is |
and the assumptions considered |
most standard form of |
good example is siena |
the assumptions considered in |
we built two versions |
standard form of system |
assumptions considered in this |
built two versions of |
form of system support |
considered in this work |
two versions of maelstrom |
of system support for |
system support for building |
a system that has |
support for building a |
one runs in user |
runs in user mode |
for building a raps |
system that has become |
building a raps of |
that has become popular |
while the other runs |
has become popular in |
a raps of racs |
we review the pull |
the other runs within |
become popular in wan |
raps of racs would |
other runs within the |
popular in wan settings |
of racs would draw |
runs within the linux |
based streaming protocol employed |
racs would draw on |
within the linux kernel |
streaming protocol employed in |
would draw on virtual |
protocol employed in our |
draw on virtual synchrony |
employed in our system |
we evaluate maelstrom on |
evaluate maelstrom on emulab |
followed by a description |
by a description of |
a description of our |
description of our novel |
group computing model developed |
of our novel auditing |
which use techniques such |
computing model developed at |
our novel auditing approach |
use techniques such as |
model developed at cornell |
novel auditing approach in |
techniques such as wholefile |
developed at cornell in |
auditing approach in section |
and show that it |
such as wholefile caching |
at cornell in the |
show that it provides |
that it provides near |
it provides near lossless |
provides near lossless tcp |
systems in this class |
ip throughput and latency |
in this class incur |
throughput and latency over |
and update logging combined |
this class incur steep |
and latency over lossy |
update logging combined with |
we evaluate the proposed |
class incur steep overheads |
latency over lossy links |
logging combined with asynchronous |
s and used today |
evaluate the proposed approach |
incur steep overheads associated |
combined with asynchronous writes |
and used today to |
steep overheads associated with |
and recovers packets with |
used today to run |
we then discuss the |
overheads associated with content |
to cope with disconnections |
today to run the |
then discuss the costs |
recovers packets with latency |
associated with content filtering |
cope with disconnections or |
to run the new |
discuss the costs of |
packets with latency independent |
with disconnections or intermittent |
run the new york |
the costs of auditing |
with latency independent of |
disconnections or intermittent connectivity |
the new york and |
latency independent of the |
new york and swiss |
and briefly describe how |
independent of the rtt |
messages often follow circuitous |
briefly describe how to |
york and swiss stock |
of the rtt of |
often follow circuitous routes |
describe how to extend |
and swiss stock exchange |
the design of mfs |
the rtt of the |
follow circuitous routes from |
how to extend our |
swiss stock exchange systems |
design of mfs is |
rtt of the link |
circuitous routes from source |
to extend our model |
of mfs is closest |
of the link and |
routes from source to |
extend our model for |
the french air traffic |
mfs is closest in |
the link and the |
from source to destination |
our model for heterogeneous |
french air traffic control |
is closest in structure |
link and the rate |
model for heterogeneous systems |
air traffic control system |
closest in structure to |
and the rate in |
in structure to that |
the rate in any |
structure to that of |
and the us navy |
rate in any single |
to that of coda |
the us navy s |
us navy s aegis |
in any single channel |
in high performance settings |
we present related work |
present related work in |
related work in section |
ibm s websphere platform |
m odel loss model |
s websphere platform and |
websphere platform and the |
these factors would degrade |
platform and the windows |
packet loss typically occurs |
and conclude in section |
factors would degrade the |
and the windows vista |
loss typically occurs at |
would degrade the performance |
the windows vista clustering |
typically occurs at two |
degrade the performance of |
windows vista clustering system |
occurs at two points |
the performance of the |
vista clustering system also |
at two points in |
performance of the replicated |
clustering system also use |
two points in an |
points in an end |
system also use versions |
problem statement our approach |
of the replicated application |
also use versions of |
statement our approach focuses |
use versions of the |
our approach focuses on |
versions of the model |
approach focuses on a |
end communication path between |
focuses on a target |
communication path between two |
on a target streaming |
path between two data |
although developers can t |
a target streaming system |
the spread multicast system |
between two data centers |
developers can t access |
target streaming system consisting |
spread multicast system implements |
can t access the |
streaming system consisting of |
multicast system implements lightweight |
as shown in figure |
t access the internal |
system consisting of one |
system implements lightweight groups |
access the internal mechanisms |
consisting of one data |
the internal mechanisms directly |
of one data source |
area network connecting them |
network connecting them and |
connecting them and at |
the other popular standard |
them and at the |
other popular standard uses |
and at the receiving |
popular standard uses a |
at the receiving end |
standard uses a state |
machine approach to guarantee |
a host acting as |
approach to guarantee stronger |
host acting as a |
which disseminates data at |
to guarantee stronger durability |
loss in the lambda |
acting as a client |
disseminates data at a |
in the lambda link |
leslie lamport s paxos |
data at a fixed |
as a client of |
the lambda link can |
lamport s paxos algorithm |
at a fixed rate |
a client of an |
lambda link can occur |
a fixed rate to |
client of an mfs |
link can occur for |
fixed rate to a |
which is implemented in |
of an mfs file |
can occur for many |
rate to a dynamic |
the groups seen by |
is implemented in scalable |
an mfs file system |
occur for many reasons |
to a dynamic set |
groups seen by applications |
implemented in scalable file |
mfs file system runs |
a dynamic set of |
seen by applications are |
in scalable file systems |
file system runs a |
dynamic set of receivers |
by applications are an |
scalable file systems and |
system runs a user |
applications are an illusion |
file systems and other |
dirty or degraded fiber |
the source has limited |
systems and other ultrareliable |
source has limited upload |
and other ultrareliable server |
has limited upload bandwidth |
malfunctioning or misconfigured equipment |
other ultrareliable server designs |
there is really only |
and hence can only |
low receiver power and |
hence can only send |
is really only one |
which receives file system |
really only one use |
receiver power and burst |
one architecture could support |
can only send data |
receives file system operations |
only one use of |
power and burst switching |
architecture could support both |
only send data directly |
file system operations intercepted |
one use of qsm |
and burst switching contention |
could support both of |
send data directly to |
system operations intercepted by |
use of qsm in |
burst switching contention are |
support both of these |
data directly to a |
operations intercepted by a |
of qsm in our |
switching contention are some |
both of these powerful |
of these powerful technologies |
intercepted by a kernel |
qsm in our target |
contention are some reasons |
directly to a small |
by a kernel module |
in our target settings |
to a small subset |
our target settings gives |
a natural option would |
a small subset of |
target settings gives rise |
natural option would be |
small subset of interested |
interacting with the vfs |
settings gives rise to |
option would be to |
subset of interested receivers |
with the vfs layer |
gives rise to potentially |
would be to offer |
the vfs layer of |
rise to potentially large |
be to offer them |
vfs layer of the |
to potentially large numbers |
participating nodes are consequently |
to offer them in |
layer of the local |
potentially large numbers of |
nodes are consequently required |
offer them in the |
of the local file |
large numbers of overlapping |
are consequently required to |
them in the context |
the local file system |
numbers of overlapping communication |
consequently required to forward |
in the context of |
of overlapping communication groups |
required to forward packets |
we adopt the same |
the context of ws |
to forward packets to |
adopt the same approach |
as we have seen |
forward packets to their |
the same approach to |
packets to their neighbors |
same approach to intercepting |
approach to intercepting vfs |
to intercepting vfs operations |
the primary goal is |
intercepting vfs operations as |
primary goal is to |
helping disseminate all packets |
vfs operations as lbfs |
goal is to support |
disseminate all packets across |
is to support data |
all packets across the |
to support data replication |
packets across the system |
support data replication in |
making use of the |
data replication in scalable |
if you re replicating |
use of the kernel |
you re replicating data |
the streamed data should |
of the kernel module |
re replicating data within |
streamed data should be |
the kernel module provided |
replicating data within some |
in which sets of |
kernel module provided as |
data should be received |
data within some form |
which sets of components |
module provided as part |
should be received by |
within some form of |
sets of components are |
provided as part of |
be received by all |
some form of group |
of components are interconnected |
as part of the |
received by all nodes |
components are interconnected and |
part of the arla |
by all nodes within |
are interconnected and cooperate |
of the arla afs |
loss can also occur |
all nodes within a |
interconnected and cooperate to |
you can just as |
the arla afs client |
can also occur at |
nodes within a fixed |
and cooperate to perform |
can just as easily |
also occur at receiving |
within a fixed latency |
cooperate to perform requests |
just as easily imagine |
occur at receiving end |
a fixed latency from |
as easily imagine that |
fixed latency from the |
easily imagine that it |
latency from the source |
imagine that it has |
hosts within the destination |
from the source s |
that it has a |
within the destination data |
the source s original |
it has a subject |
the destination data center |
source s original transmission |
components sets are normally |
has a subject name |
the cache manager maintains |
sets are normally colocated |
a subject name in |
cache manager maintains a |
these are usually cheap |
subject name in a |
even in the presence |
manager maintains a cache |
are usually cheap commodity |
when a service is |
usually cheap commodity machines |
in the presence of |
maintains a cache of |
a service is replicated |
name in a publish |
cheap commodity machines prone |
the presence of opportunistic |
a cache of recently |
each of its constituent |
presence of opportunistic nodes |
commodity machines prone to |
of its constituent components |
machines prone to temporary |
its constituent components will |
accessed mfs files on |
prone to temporary overloads |
advantages with this type |
constituent components will need |
mfs files on the |
we first assume a |
with this type of |
components will need to |
to temporary overloads that |
files on the local |
on the local disk |
this type of process |
will need to replicate |
temporary overloads that cause |
first assume a system |
need to replicate its |
when a vfs operation |
assume a system in |
overloads that cause packets |
to replicate its portion |
a vfs operation is |
data can be anything |
that cause packets to |
replicate its portion of |
a system in which |
vfs operation is intercepted |
cause packets to be |
its portion of the |
system in which all |
operation is intercepted for |
packets to be dropped |
portion of the service |
in which all nodes |
is intercepted for a |
to be dropped by |
of the service state |
intercepted for a file |
be dropped by the |
for a file that |
dropped by the kernel |
a file that is |
by the kernel in |
if qsm is used |
have similar upload and |
the kernel in bursts |
file that is not |
qsm is used to |
similar upload and download |
that is not in |
is used to disseminate |
upload and download bandwidths |
is not in the |
used to disseminate updates |
not in the cache |
w e b te |
this results in a |
e b te c |
it is retrieved in |
b te c h |
results in a pattern |
is retrieved in full |
this loss mode occurs |
te c h n |
in a pattern of |
retrieved in full from |
loss mode occurs with |
c h n o |
a pattern of communication |
in full from the |
mode occurs with udp |
h n o l |
pattern of communication groups |
n o l o |
full from the appropriate |
o l o g |
of communication groups that |
based traffic but not |
from the appropriate server |
l o g i |
we briefly discuss how |
communication groups that are |
traffic but not with |
o g i e |
briefly discuss how to |
groups that are exactly |
but not with tcp |
and the vfs operation |
g i e s |
discuss how to extend |
that are exactly overlapped |
the vfs operation is |
i e s concerns |
how to extend our |
vfs operation is then |
e s concerns experience |
to extend our model |
operation is then resumed |
each replicated component will |
which advertises receiver windows |
s concerns experience with |
extend our model to |
mfs uses the writeback |
advertises receiver windows to |
concerns experience with corba |
replicated component will have |
our model to work |
receiver windows to prevent |
experience with corba even |
component will have one |
model to work in |
close semantics first implemented |
with corba even good |
will have one or |
windows to prevent end |
to work in heterogeneous |
semantics first implemented in |
corba even good ideas |
have one or more |
work in heterogeneous scenarios |
first implemented in the |
even good ideas can |
one or more associated |
implemented in the andrew |
good ideas can be |
or more associated groups |
we assume that malicious |
ideas can be used |
what are typical loss |
in the andrew file |
assume that malicious nodes |
can be used in |
are typical loss rates |
the andrew file system |
delivering update streams to |
that malicious nodes exhibit |
be used in ways |
typical loss rates on |
update streams to its |
malicious nodes exhibit byzantine |
used in ways that |
loss rates on long |
streams to its replicas |
nodes exhibit byzantine behavior |
in ways that developers |
ways that developers dislike |
that developers dislike and |
developers dislike and ultimately |
dislike and ultimately reject |
while correct nodes follow |
correct nodes follow the |
the answer to this |
nodes follow the protocol |
answer to this question |
follow the protocol as |
a datacenter will typically |
to this question is |
a good example of |
when a dirty file |
the protocol as defined |
datacenter will typically host |
this question is surprisingly |
a dirty file is |
good example of this |
will typically host many |
question is surprisingly hard |
dirty file is closed |
typically host many services |
example of this occurred |
is surprisingly hard to |
requesting data as needed |
of this occurred when |
data as needed and |
this occurred when the |
as needed and sending |
each with a disjoint |
occurred when the corba |
the entire file contents |
needed and sending data |
with a disjoint set |
entire file contents are |
when the corba community |
and sending data as |
the corba community decided |
file contents are transferred |
a disjoint set of |
sending data as requested |
corba community decided to |
contents are transferred to |
disjoint set of components |
data as requested from |
community decided to tackle |
are transferred to the |
as requested from them |
and often deployed on |
transferred to the server |
decided to tackle replication |
often deployed on disjoint |
to tackle replication for |
deployed on disjoint sets |
altrustic nodes are a |
tackle replication for fault |
on disjoint sets of |
nodes are a subgroup |
replication for fault tolerance |
disjoint sets of nodes |
are a subgroup of |
for fault tolerance but |
a subgroup of correct |
fault tolerance but then |
subgroup of correct nodes |
tolerance but then stumbled |
though scheme for minimising |
of correct nodes that |
in cases where two |
but then stumbled by |
scheme for minimising bandwidth |
correct nodes that are |
cases where two services |
then stumbled by presenting |
for minimising bandwidth utilisation |
nodes that are willing |
where two services are |
stumbled by presenting the |
minimising bandwidth utilisation when |
that are willing to |
two services are co |
by presenting the technology |
bandwidth utilisation when transferring |
are willing to upload |
presenting the technology to |
utilisation when transferring files |
willing to upload more |
located on the same |
on the same node |
when transferring files is |
to upload more data |
the technology to developers |
transferring files is not |
upload more data than |
technology to developers in |
files is not used |
we ll still see |
more data than required |
to developers in a |
is not used in |
ll still see heavy |
data than required from |
developers in a way |
not used in mfs |
still see heavy overlap |
than required from them |
in a way that |
a way that was |
way that was much |
that was much too |
was much too limiting |
much too limiting for |
but unless the degree |
although it is orthogonal |
too limiting for general |
unless the degree of |
it is orthogonal to |
limiting for general use |
the degree of replication |
is orthogonal to mfs |
we employ the term |
degree of replication is |
orthogonal to mfs adaptation |
of replication is identical |
employ the term opportunistic |
to mfs adaptation and |
the term opportunistic to |
mfs adaptation and could |
term opportunistic to refer |
tolerance mechanism is based |
opportunistic to refer to |
there may be two |
to refer to a |
mechanism is based on |
refer to a subgroup |
may be two cases |
to a subgroup of |
is based on the |
adaptation and could be |
a subgroup of byzantine |
based on the virtual |
subgroup of byzantine nodes |
nodes that host both |
of byzantine nodes that |
on the virtual synchrony |
and could be added |
that host both services |
byzantine nodes that attempt |
the virtual synchrony model |
could be added to |
nodes that attempt to |
be added to further |
and hence both sets |
that attempt to give |
but the programming tools |
attempt to give less |
the programming tools built |
hence both sets of |
added to further improve |
to give less data |
programming tools built over |
both sets of qsm |
to further improve performance |
give less data than |
tools built over this |
sets of qsm groups |
less data than they |
built over this model |
data than they would |
the server that stores |
over this model prevent |
server that stores a |
than they would if |
that stores a file |
this model prevent developers |
stores a file is |
they would if they |
model prevent developers from |
and nodes that just |
would if they behaved |
a file is responsible |
prevent developers from using |
nodes that just host |
if they behaved as |
file is responsible for |
developers from using threads |
that just host one |
they behaved as correct |
is responsible for maintaining |
just host one of |
behaved as correct nodes |
responsible for maintaining the |
host one of them |
for maintaining the mutual |
maintaining the mutual consistency |
the mutual consistency of |
mutual consistency of the |
with the intention of |
consistency of the copies |
guis or other direct |
the intention of obtaining |
cluster management systems use |
of the copies cached |
or other direct end |
intention of obtaining as |
management systems use groups |
the copies cached by |
of obtaining as much |
systems use groups for |
copies cached by clients |
obtaining as much data |
of lost packets fig |
use groups for purposes |
as much data as |
groups for purposes other |
much data as possible |
it records which clients |
for purposes other than |
data as possible at |
records which clients cache |
purposes other than component |
as possible at least |
which clients cache the |
other than component replication |
possible at least feasible |
clients cache the file |
loss rates on teragrid |
at least feasible cost |
rates on teragrid determine |
such as tracking node |
as tracking node status |
tracking node status and |
or even prebuilt libraries |
and is responsible for |
node status and launching |
these may employ a |
perhaps because such links |
is responsible for notifying |
in the corba approach |
because such links are |
status and launching applications |
responsible for notifying them |
may employ a simple |
such links are a |
for notifying them of |
employ a simple strategy |
these groups will span |
links are a relatively |
notifying them of changes |
are a relatively recent |
groups will span large |
a developer who obeys |
a relatively recent addition |
will span large numbers |
mfs implements a variation |
developer who obeys this |
relatively recent addition to |
such as refuse to |
span large numbers of |
implements a variation of |
who obeys this long |
recent addition to the |
as refuse to contribute |
large numbers of nodes |
a variation of the |
obeys this long list |
addition to the networking |
refuse to contribute any |
variation of the scheme |
this long list of |
to the networking landscape |
perhaps the entire cluster |
the networking landscape and |
of the scheme used |
long list of constraints |
to contribute any upload |
networking landscape and their |
the scheme used by |
landscape and their ownership |
contribute any upload resources |
such groups overlap with |
list of constraints can |
scheme used by coda |
and their ownership is |
groups overlap with everything |
of constraints can do |
their ownership is still |
constraints can do lockstep |
or a more elaborate |
ownership is still mostly |
can do lockstep replication |
when a file is |
a more elaborate strategy |
is still mostly restricted |
the result is an |
do lockstep replication of |
a file is retrieved |
more elaborate strategy that |
still mostly restricted to |
result is an environment |
lockstep replication of a |
file is retrieved from |
elaborate strategy that allows |
mostly restricted to commercial |
is an environment in |
replication of a program |
is retrieved from the |
strategy that allows them |
restricted to commercial organizations |
an environment in which |
of a program for |
retrieved from the server |
that allows them to |
to commercial organizations disinclined |
environment in which there |
a program for tolerance |
allows them to cheat |
commercial organizations disinclined to |
in which there will |
program for tolerance of |
the server issues a |
them to cheat without |
organizations disinclined to reveal |
which there will be |
for tolerance of hardware |
server issues a limited |
to cheat without being |
disinclined to reveal such |
there will be a |
tolerance of hardware faults |
cheat without being easily |
to reveal such information |
will be a hierarchy |
without being easily detected |
obliging it to inform |
it to inform the |
one source of information |
to inform the client |
source of information is |
notice that our model |
inform the client through |
the scheme doesn t |
the client through a |
of information is teragrid |
that our model diverges |
scheme doesn t protect |
client through a callback |
our model diverges from |
doesn t protect against |
through a callback if |
model diverges from the |
t protect against software |
a callback if another |
diverges from the one |
callback if another host |
from the one used |
if another host modifies |
the one used in |
another host modifies the |
one used in bar |
host modifies the file |
used in bar gossip |
an optical network interconnecting |
optical network interconnecting major |
developers regard the standard |
qsm is highly effective |
network interconnecting major supercomputing |
regard the standard as |
is highly effective in |
interconnecting major supercomputing sites |
if the callback promise |
the standard as rigid |
highly effective in supporting |
major supercomputing sites in |
the callback promise expires |
standard as rigid and |
effective in supporting this |
in which nodes are |
supercomputing sites in the |
callback promise expires without |
as rigid and limited |
in supporting this style |
which nodes are classified |
sites in the us |
promise expires without a |
supporting this style of |
they need fault tolerance |
expires without a callback |
this style of use |
nodes are classified as |
without a callback being |
teragrid has a monitoring |
are classified as byzantine |
but not in this |
a callback being issued |
has a monitoring framework |
not in this very |
a monitoring framework within |
in this very narrow |
monitoring framework within which |
this very narrow form |
framework within which ten |
within which ten sites |
the client must revalidate |
which ten sites periodically |
client must revalidate the |
ten sites periodically send |
must revalidate the file |
sites periodically send each |
revalidate the file before |
periodically send each other |
recover in y inter |
systems like the isis |
like the isis toolkit |
the file before using |
gbps streams of udp |
rational nodes attempt to |
file before using it |
streams of udp packets |
region protocol y intra |
nodes attempt to maximize |
popular during the early |
attempt to maximize their |
of udp packets and |
during the early and |
to maximize their utility |
the cache consistency algorithm |
udp packets and measure |
the early and mid |
maximize their utility while |
consisting of a small |
packets and measure the |
cache consistency algorithm is |
their utility while still |
of a small set |
and measure the resulting |
consistency algorithm is described |
utility while still following |
a small set of |
measure the resulting loss |
algorithm is described in |
while still following the |
small set of servers |
the resulting loss rate |
is described in more |
still following the defined |
set of servers to |
described in more detail |
following the defined protocol |
of servers to which |
in more detail in |
servers to which client |
also used virtual synchrony |
more detail in section |
to which client systems |
our model is actually |
used virtual synchrony but |
which client systems connect |
model is actually less |
virtual synchrony but had |
is actually less lenient |
each site measures the |
synchrony but had fewer |
site measures the loss |
but had fewer limitations |
measures the loss rate |
the loss rate to |
loss rate to every |
rate to every other |
nodes employing strategies to |
to every other site |
they supported many of |
employing strategies to maximize |
level multicast is vectored |
supported many of the |
every other site once |
strategies to maximize their |
multicast is vectored through |
many of the mechanisms |
other site once an |
site once an hour |
is vectored through a |
of the mechanisms needed |
to maximize their utility |
vectored through a server |
adaptive rpc library the |
the mechanisms needed to |
resulting in a total |
in a total of |
rpc library the fundamental |
mechanisms needed to build |
which multicasts it to |
maximize their utility are |
library the fundamental difference |
needed to build and |
multicasts it to its |
loss rate measurements collected |
the fundamental difference between |
to build and manage |
it to its peers |
their utility are classified |
rate measurements collected across |
fundamental difference between mfs |
build and manage a |
these filter the ordered |
measurements collected across the |
difference between mfs and |
utility are classified as |
and manage a raps |
filter the ordered multicast |
collected across the network |
between mfs and other |
are classified as byzantine |
manage a raps of |
the ordered multicast stream |
across the network every |
mfs and other file |
a raps of racs |
ordered multicast stream and |
the network every hour |
and other file systems |
multicast stream and relay |
so that we can |
other file systems we |
and their successes have |
that we can build |
stream and relay messages |
file systems we have |
their successes have clearly |
we can build a |
shows that between nov |
and relay messages back |
systems we have described |
successes have clearly demonstrated |
can build a practical |
relay messages back out |
we have described is |
have clearly demonstrated the |
build a practical punishment |
messages back out to |
have described is in |
clearly demonstrated the model |
back out to receivers |
described is in the |
demonstrated the model s |
is in the communication |
the model s effectiveness |
based system in which |
in the communication between |
system in which any |
this approach can support |
the communication between the |
in which any node |
approach can support huge |
which any node not |
communication between the cache |
isis is no longer |
can support huge numbers |
any node not contributing |
between the cache manager |
is no longer available |
support huge numbers of |
node not contributing its |
the cache manager and |
no longer available as |
huge numbers of groups |
not contributing its fair |
cache manager and servers |
longer available as a |
numbers of groups with |
contributing its fair share |
available as a product |
of groups with irregular |
while lbfs uses a |
its fair share of |
groups with irregular overlap |
lbfs uses a variant |
fair share of data |
yet many critical systems |
with irregular overlap patterns |
uses a variant of |
share of data may |
many critical systems continue |
a variant of the |
of data may be |
but the servers are |
variant of the nfs |
critical systems continue to |
of all such measurements |
data may be expelled |
the servers are a |
of the nfs rpc |
systems continue to use |
all such measurements were |
may be expelled from |
servers are a point |
the nfs rpc protocol |
continue to use isis |
such measurements were over |
be expelled from the |
are a point of |
expelled from the system |
a point of contention |
based solutions or other |
solutions or other virtual |
or other virtual synchrony |
other virtual synchrony implementations |
throughout the paper we |
and the indirect communication |
the paper we use |
the indirect communication pathway |
paper we use the |
indirect communication pathway introduces |
machine approach as used |
we use the terms |
communication pathway introduces potentially |
approach as used in |
use the terms upload |
pathway introduces potentially high |
as used in the |
the terms upload factor |
introduces potentially high latencies |
terms upload factor and |
used in the paxos |
upload factor and download |
of them were over |
factor and download factor |
in the paxos algorithm |
and download factor to |
the paxos algorithm is |
uses a customised rpc |
these considerations convinced us |
download factor to refer |
paxos algorithm is also |
considerations convinced us that |
factor to refer to |
algorithm is also becoming |
convinced us that a |
to refer to the |
is also becoming more |
refer to the ratio |
after eliminating a single |
to the ratio between |
also becoming more popular |
unlike coda s rpc |
us that a new |
eliminating a single site |
the ratio between an |
that a new system |
ratio between an upload |
a new system was |
the key insight is |
between an upload or |
the rpc used in |
new system was needed |
key insight is that |
an upload or download |
that dropped incoming packets |
rpc used in mfs |
insight is that these |
used in mfs incorporates |
is that these successes |
qsm implements a approach |
upload or download rate |
dropped incoming packets steadily |
in mfs incorporates novel |
that these successes use |
implements a approach similar |
or download rate and |
incoming packets steadily at |
mfs incorporates novel features |
these successes use similar |
a approach similar to |
download rate and the |
packets steadily at a |
incorporates novel features to |
successes use similar ideas |
approach similar to spread |
rate and the original |
steadily at a rate |
novel features to allow |
use similar ideas but |
similar to spread s |
and the original stream |
at a rate of |
features to allow it |
similar ideas but in |
to spread s lightweight |
the original stream rate |
to allow it to |
ideas but in ways |
spread s lightweight group |
allow it to adapt |
but in ways very |
s lightweight group abstraction |
it to adapt to |
in ways very different |
to adapt to network |
ways very different from |
adapt to network variability |
very different from what |
different from what the |
but without a separate |
given a stream rate |
from what the corba |
the mfs rpc library |
a stream rate of |
without a separate server |
what the corba fault |
mfs rpc library is |
a separate server group |
rpc library is implemented |
library is implemented on |
is implemented on top |
of the remainder were |
implemented on top of |
the remainder were over |
we define a region |
on top of the |
define a region of |
what we need today |
top of the adaptive |
a region of overlap |
of the adaptive transport |
we need today is |
region of overlap to |
the adaptive transport protocol |
need today is a |
of overlap to be |
a download rate of |
today is a modern |
overlap to be a |
is a modern revisiting |
to be a set |
a modern revisiting of |
be a set of |
modern revisiting of this |
a set of nodes |
revisiting of this technology |
set of nodes with |
of this technology that |
in discussing mfs rpc |
of nodes with approximately |
this technology that draws |
nodes with approximately the |
technology that draws on |
kbps corresponds to a |
that draws on group |
with approximately the same |
draws on group communication |
we give an overview |
approximately the same group |
corresponds to a download |
on group communication but |
give an overview of |
the same group membership |
to a download factor |
these numbers may look |
group communication but packages |
an overview of the |
a download factor of |
numbers may look small |
communication but packages it |
overview of the parts |
but packages it in |
may look small in |
of the parts of |
packages it in a |
look small in absolute |
it in a way |
the parts of atp |
small in absolute terms |
in a way that |
parts of atp which |
under the assumptions of |
a way that developers |
of atp which are |
way that developers perceive |
but they are sufficient |
that developers perceive as |
atp which are most |
developers perceive as solving |
they are sufficient to |
perceive as solving their |
which are most relevant |
as solving their most |
are sufficient to bring |
the assumptions of section |
are most relevant to |
solving their most pressing |
sufficient to bring tcp |
most relevant to mfs |
their most pressing scalability |
most pressing scalability problems |
ip throughput crashing down |
streaming system model our |
pressing scalability problems and |
throughput crashing down on |
crashing down on high |
scalability problems and that |
our cluster should be |
atp and its design |
system model our auditing |
problems and that flexibly |
cluster should be nicely |
and its design motivations |
model our auditing approach |
and that flexibly matches |
should be nicely tiled |
conventional wisdom states that |
our auditing approach is |
that flexibly matches their |
its design motivations have |
be nicely tiled by |
wisdom states that optical |
auditing approach is used |
flexibly matches their preferred |
design motivations have been |
nicely tiled by regions |
states that optical links |
approach is used over |
matches their preferred styles |
motivations have been described |
that optical links do |
is used over the |
their preferred styles and |
qsm uses regions for |
have been described in |
optical links do not |
links do not drop |
preferred styles and tools |
uses regions for multicast |
been described in more |
used over the chainsaw |
do not drop packets |
regions for multicast dissemination |
described in more detail |
over the chainsaw protocol |
other kinds of persistent |
for multicast dissemination and |
in more detail in |
kinds of persistent objects |
multicast dissemination and for |
more detail in our |
grade optical equipment is |
dissemination and for recovery |
detail in our earlier |
optical equipment is configured |
and for recovery of |
in our earlier work |
all nodes participating in |
for recovery of lost |
equipment is configured to |
nodes participating in the |
recovery of lost packets |
is configured to shut |
participating in the system |
configured to shut down |
in the system are |
the user simply designs |
to shut down beyond |
the system are organized |
user simply designs a |
employing different protocols for |
system are organized into |
shut down beyond bit |
simply designs a data |
different protocols for each |
are organized into a |
down beyond bit error |
designs a data structure |
protocols for each purpose |
the hypothesis underlying atp |
beyond bit error rates |
a data structure and |
organized into a fully |
bit error rates of |
protocol node x recover |
data structure and employs |
into a fully connected |
hypothesis underlying atp is |
node x recover in |
underlying atp is that |
a fully connected mesh |
structure and employs multicast |
x recover in x |
atp is that adapting |
fully connected mesh overlay |
and employs multicast technology |
recover in x region |
is that adapting to |
employs multicast technology to |
one out of a |
in x region figure |
that adapting to network |
multicast technology to transmit |
adapting to network variation |
where each node has |
out of a trillion |
of a trillion bits |
to network variation by |
each node has the |
network variation by structuring |
technology to transmit updates |
node has the same |
hierarchical recovery in qsm |
to transmit updates to |
variation by structuring applications |
has the same number |
transmit updates to the |
the reliability of the |
by structuring applications according |
the same number of |
updates to the group |
a group spans multiple |
reliability of the lambda |
structuring applications according to |
same number of neighbors |
to the group members |
group spans multiple regions |
of the lambda network |
applications according to modes |
the lambda network is |
according to modes is |
lambda network is clearly |
the source is randomly |
each region has an |
which apply them in |
network is clearly not |
to modes is not |
source is randomly connected |
region has an associated |
apply them in the |
is clearly not equal |
modes is not always |
is randomly connected to |
has an associated structure |
them in the same |
clearly not equal to |
is not always appropriate |
randomly connected to a |
an associated structure of |
in the same order |
not equal to the |
connected to a small |
and can sometimes lead |
the same order everywhere |
equal to the sum |
associated structure of token |
to a small subset |
can sometimes lead to |
to the sum of |
structure of token rings |
a small subset of |
sometimes lead to poor |
the sum of its |
small subset of the |
lead to poor performance |
sum of its optical |
of its optical parts |
subset of the nodes |
can be done on |
be done on any |
done on any desired |
on any desired copy |
it s less reliable |
the streaming process starts |
s less reliable by |
streaming process starts at |
less reliable by orders |
process starts at the |
reliable by orders of |
by orders of magnitude |
to recover from packet |
examples of updates include |
shows the results of |
starts at the source |
recover from packet loss |
of updates include a |
the results of an |
updates include a stock |
applications and protocols such |
include a stock trade |
results of an experiment |
and protocols such as |
of an experiment in |
qsm uses a hierarchical |
an experiment in which |
protocols such as tcp |
experiment in which modeless |
which breaks the data |
uses a hierarchical structure |
breaks the data stream |
in which modeless adaptation |
the data stream into |
which modeless adaptation over |
data stream into packets |
a stock trade or |
a hierarchical structure of |
ip which expect extreme |
modeless adaptation over atp |
stream into packets and |
stock trade or stock |
hierarchical structure of token |
which expect extreme reliability |
adaptation over atp achieves |
into packets and sends |
trade or stock market |
structure of token rings |
expect extreme reliability from |
over atp achieves higher |
packets and sends notifications |
atp achieves higher bandwidth |
extreme reliability from the |
or stock market quote |
and sends notifications to |
reliability from the high |
achieves higher bandwidth utilisation |
we considered using other |
sends notifications to its |
higher bandwidth utilisation than |
a new object detected |
speed network are instead |
notifications to its neighbors |
considered using other structures |
bandwidth utilisation than we |
new object detected by |
network are instead subjected |
to its neighbors as |
utilisation than we will |
object detected by radar |
are instead subjected to |
its neighbors as soon |
than we will concentrate |
detected by radar in |
instead subjected to unexpectedly |
neighbors as soon as |
we will concentrate on |
but token rings produce |
by radar in an |
subjected to unexpectedly high |
as soon as it |
will concentrate on a |
token rings produce a |
radar in an air |
to unexpectedly high loss |
soon as it has |
concentrate on a system |
rings produce a more |
in an air traffic |
unexpectedly high loss rates |
as it has packets |
on a system with |
produce a more predictable |
an air traffic control |
it has packets to |
a system with a |
a more predictable traffic |
these numbers reflect the |
has packets to disseminate |
system with a single |
air traffic control system |
more predictable traffic pattern |
numbers reflect the loss |
with a single server |
reflect the loss rate |
the loss rate specifically |
these notifications are small |
loss rate specifically experienced |
notifications are small messages |
a communication to or |
rate specifically experienced by |
are small messages used |
communication to or from |
the importance of this |
specifically experienced by udp |
mfs is designed to |
small messages used only |
to or from an |
importance of this will |
experienced by udp traffic |
is designed to support |
messages used only to |
or from an aircraft |
of this will become |
by udp traffic on |
designed to support multiple |
used only to inform |
this will become clear |
udp traffic on an |
to support multiple mfs |
only to inform neighbors |
will become clear later |
to inform neighbors of |
or the addition of |
support multiple mfs file |
traffic on an end |
inform neighbors of the |
the addition of a |
multiple mfs file servers |
neighbors of the availability |
addition of a node |
of the availability of |
the basic structure is |
of a node to |
the availability of new |
end path and may |
basic structure is illustrated |
a node to a |
availability of new packets |
path and may not |
structure is illustrated in |
node to a distributed |
modal adaptation modeless adaptation |
and may not generalize |
is illustrated in figure |
based on the received |
may not generalize to |
to a distributed data |
on the received notifications |
not generalize to tcp |
a distributed data structure |
generalize to tcp packets |
distributed data structure containing |
data structure containing an |
structure containing an index |
each node requests missing |
containing an index of |
node requests missing packets |
an index of pending |
we do not know |
at the highest level |
index of pending orders |
do not know if |
of pending orders in |
and the source satisfies |
true bandwidth bandwidth used |
not know if packets |
pending orders in an |
the source satisfies as |
know if packets were |
qsm circulates tokens around |
orders in an online |
source satisfies as many |
if packets were dropped |
circulates tokens around sets |
in an online warehouse |
satisfies as many requests |
packets were dropped within |
tokens around sets of |
as many requests as |
were dropped within the |
around sets of regions |
many requests as allowed |
data replication can be |
dropped within the optical |
requests as allowed by |
replication can be remarkably |
within the optical network |
as allowed by its |
can be remarkably cheap |
the optical network or |
allowed by its upload |
aggregating information that can |
optical network or at |
by its upload capacity |
network or at intermediate |
information that can be |
with modern technology and |
that can be used |
or at intermediate devices |
modern technology and small |
can be used by |
at intermediate devices within |
technology and small updates |
be used by a |
intermediate devices within either |
devices within either data |
with chainsaw the upload |
within either data center |
used by a group |
chainsaw the upload capacity |
by a group sender |
the upload capacity of |
though it s unlikely |
upload capacity of the |
a group sender to |
it s unlikely that |
group sender to retransmit |
capacity of the source |
s unlikely that they |
sender to retransmit packets |
of the source does |
unlikely that they were |
to retransmit packets that |
the source does not |
that they were dropped |
computer chrony service can |
retransmit packets that were |
source does not need |
they were dropped at |
chrony service can run |
packets that were missed |
does not need to |
were dropped at the |
service can run at |
that were missed by |
not need to increase |
dropped at the end |
can run at rates |
were missed by entire |
need to increase with |
run at rates well |
missed by entire regions |
to increase with the |
at rates well in |
many of the measurements |
increase with the size |
rates well in excess |
of the measurements lost |
with the size of |
well in excess of |
the measurements lost just |
the size of the |
measurements lost just one |
size of the system |
lost just one or |
just one or two |
one or two packets |
or two packets whereas |
two packets whereas kernel |
even an upload capacity |
nic losses are known |
an upload capacity of |
losses are known to |
upload capacity of twice |
are known to be |
capacity of twice the |
known to be bursty |
of twice the stream |
twice the stream rate |
the stream rate is |
stream rate is sufficient |
rate is sufficient to |
a token circulates to |
is sufficient to ensure |
token circulates to provide |
sufficient to ensure that |
circulates to provide loss |
to ensure that the |
to provide loss recovery |
ensure that the system |
provide loss recovery at |
that the system performs |
loss recovery at the |
loss occurred on paths |
recovery at the level |
the system performs and |
occurred on paths where |
at the level of |
system performs and scales |
on paths where levels |
the level of nodes |
performs and scales well |
paths where levels of |
level of nodes belonging |
where levels of optical |
ordered updates per second |
of nodes belonging to |
levels of optical link |
as nodes receive packets |
nodes belonging to the |
of optical link utilization |
even if an update |
belonging to the region |
if an update requires |
an update requires a |
they mimic the role |
update requires a large |
mimic the role of |
requires a large message |
the role of the |
role of the source |
it s possible to |
s possible to maintain |
were consistently lower than |
sending notifications to their |
possible to maintain rates |
notifications to their own |
to maintain rates of |
to their own neighbors |
maintain rates of thousands |
their own neighbors in |
rates of thousands per |
if regions become large |
own neighbors in the |
ruling out congestion as |
of thousands per second |
neighbors in the mesh |
out congestion as a |
thousands per second on |
congestion as a possible |
qsm partitions them into |
per second on typical |
as a possible cause |
partitions them into smaller |
allowing packets to be |
second on typical hardware |
them into smaller rings |
packets to be propagated |
a conclusion supported by |
to be propagated through |
conclusion supported by dialogue |
be propagated through the |
the virtual synchrony and |
supported by dialogue with |
virtual synchrony and statemachine |
this is illustrated in |
propagated through the system |
by dialogue with the |
synchrony and statemachine models |
is illustrated in figure |
dialogue with the network |
and statemachine models show |
with the network administrators |
statemachine models show how |
models show how a |
show how a tremendous |
based approach to acquisition |
how a tremendous range |
approach to acquisition of |
a tremendous range of |
to acquisition of packets |
tremendous range of application |
in the experiments reported |
range of application requirements |
the experiments reported in |
of application requirements can |
experiments reported in this |
application requirements can map |
what are some possible |
reported in this paper |
requirements can map down |
are some possible causes |
can map down to |
some possible causes for |
map down to a |
possible causes for such |
down to a rigorously |
no token ring ever |
causes for such high |
provides some resilience to |
token ring ever grows |
to a rigorously precise |
for such high loss |
some resilience to failure |
ring ever grows larger |
a rigorously precise execution |
such high loss rates |
resilience to failure or |
ever grows larger than |
rigorously precise execution model |
high loss rates on |
to failure or malicious |
grows larger than about |
loss rates on teragrid |
failure or malicious behavior |
which in turn can |
in turn can be |
turn can be used |
can be used to |
a likely hypothesis is |
be used to validate |
since a participant will |
likely hypothesis is device |
used to validate a |
a participant will have |
hypothesis is device clutter |
to validate a platform |
participant will have multiple |
is device clutter the |
will have multiple possible |
device clutter the critical |
have multiple possible sources |
clutter the critical communication |
multiple possible sources for |
because the models have |
and the system uses |
the critical communication path |
possible sources for each |
the models have formal |
the system uses single |
critical communication path between |
true bandwidth bandwidth used |
sources for each packet |
models have formal specifications |
system uses single and |
communication path between nodes |
uses single and two |
path between nodes in |
between nodes in different |
the mesh overlay defines |
you can test the |
nodes in different data |
mesh overlay defines a |
can test the correctness |
in different data centers |
overlay defines a predetermined |
test the correctness of |
different data centers is |
defines a predetermined set |
the correctness of an |
data centers is littered |
a predetermined set of |
correctness of an implementation |
centers is littered with |
predetermined set of neighbors |
is littered with multiple |
set of neighbors for |
littered with multiple electronic |
of neighbors for each |
with multiple electronic devices |
we plan to experiment |
and even use theorem |
neighbors for each peer |
plan to experiment with |
even use theorem provers |
each of which represents |
to experiment with larger |
use theorem provers to |
of which represents a |
which also makes it |
theorem provers to assist |
also makes it hard |
which represents a potential |
experiment with larger configurations |
provers to assist developers |
makes it hard for |
represents a potential point |
with larger configurations and |
to assist developers in |
a potential point of |
it hard for malicious |
larger configurations and will |
assist developers in testing |
potential point of failure |
hard for malicious peers |
configurations and will work |
for malicious peers to |
developers in testing their |
malicious peers to round |
and will work with |
another possibility is that |
in testing their most |
peers to round up |
will work with deeper |
possibility is that such |
testing their most critical |
to round up on |
work with deeper hierarchies |
is that such loss |
their most critical application |
round up on individual |
that such loss rates |
up on individual peers |
most critical application components |
such loss rates may |
on individual peers since |
the qsm recovery protocol |
loss rates may be |
individual peers since attackers |
qsm recovery protocol uses |
rates may be typical |
peers since attackers lack |
recovery protocol uses tokens |
one reason that we |
may be typical for |
reason that we lack |
protocol uses tokens to |
that we lack this |
be typical for any |
since attackers lack a |
uses tokens to track |
typical for any large |
we lack this sort |
attackers lack a deterministic |
lack this sort of |
tokens to track message |
this sort of support |
scale network where the |
to track message status |
lack a deterministic means |
sort of support today |
network where the cost |
of support today is |
a deterministic means of |
where the cost of |
support today is that |
deterministic means of acquiring |
the cost of immediately |
today is that vendors |
means of acquiring control |
cost of immediately detecting |
is that vendors and |
of acquiring control of |
of immediately detecting and |
that vendors and platform |
acquiring control of all |
immediately detecting and fixing |
vendors and platform developers |
control of all of |
detecting and fixing failures |
and platform developers worry |
of all of its |
and fixing failures is |
platform developers worry that |
all of its neighbors |
fixing failures is prohibitively |
developers worry that these |
failures is prohibitively high |
worry that these forms |
that these forms of |
these forms of replication |
all nodes with exception |
forms of replication haven |
nodes with exception of |
of replication haven t |
with exception of the |
replication haven t achieved |
we found through dialogue |
exception of the source |
haven t achieved huge |
the token carries ack |
found through dialogue with |
of the source have |
t achieved huge market |
token carries ack and |
through dialogue with the |
the source have a |
achieved huge market success |
carries ack and nak |
dialogue with the administrators |
source have a fixed |
ack and nak information |
with the administrators that |
have a fixed upper |
the administrators that the |
as the experience with |
a fixed upper limit |
administrators that the steady |
the experience with corba |
fixed upper limit on |
that the steady loss |
experience with corba sidebar |
aggregated over the nodes |
upper limit on their |
the steady loss rate |
with corba sidebar describes |
over the nodes below |
limit on their upload |
steady loss rate experienced |
the nodes below each |
on their upload contribution |
the common object request |
nodes below each ring |
loss rate experienced by |
common object request broker |
rate experienced by the |
object request broker architecture |
experienced by the indiana |
request broker architecture offers |
by the indiana university |
broker architecture offers a |
the indiana university site |
token rings avoid the |
architecture offers a fault |
indiana university site was |
rings avoid the kinds |
university site was due |
avoid the kinds of |
site was due to |
the kinds of ack |
was due to a |
tolerant groups mechanism that |
due to a faulty |
groups mechanism that was |
to a faulty line |
mechanism that was based |
a faulty line card |
that was based on |
was based on the |
nak implosion problems with |
times the stream rate |
implosion problems with which |
based on the virtual |
and the measurements showed |
problems with which reliable |
on the virtual synchrony |
the measurements showed that |
with which reliable multicast |
the virtual synchrony model |
measurements showed that the |
which reliable multicast protocols |
showed that the error |
defined by the protocol |
reliable multicast protocols traditionally |
that the error persisting |
multicast protocols traditionally have |
the error persisting over |
protocols traditionally have struggled |
error persisting over at |
the corba standard is |
persisting over at least |
corba standard is widely |
over at least a |
standard is widely viewed |
this upper limit is |
at least a three |
is widely viewed as |
upper limit is not |
but problems of their |
least a three month |
modal versus modeless adaptation |
widely viewed as rigid |
limit is not respected |
problems of their own |
a three month period |
versus modeless adaptation with |
viewed as rigid and |
is not respected by |
modeless adaptation with atp |
as rigid and limited |
not respected by opportunistic |
if a message is |
points for loss rates |
respected by opportunistic nodes |
a message is lost |
for loss rates on |
i believe that the |
loss rates on high |
the left graph shows |
believe that the corba |
left graph shows performance |
that the corba community |
the sender may not |
who attempt to reduce |
graph shows performance with |
the corba community erred |
sender may not find |
attempt to reduce it |
shows performance with modal |
haul networks are provided |
corba community erred by |
may not find out |
to reduce it with |
performance with modal adaptation |
networks are provided by |
community erred by embedding |
not find out for |
reduce it with the |
are provided by the |
and the right graph |
find out for quite |
it with the goal |
erred by embedding a |
provided by the back |
the right graph shows |
out for quite a |
with the goal of |
by embedding a powerful |
bone networks of tier |
embedding a powerful solution |
the goal of uploading |
right graph shows a |
for quite a while |
a powerful solution into |
goal of uploading less |
graph shows a scheme |
powerful solution into a |
of uploading less data |
global crossing reports average |
shows a scheme in |
solution into a tool |
crossing reports average loss |
a scheme in which |
this isn t a |
on the course of |
reports average loss rates |
scheme in which there |
into a tool mismatched |
isn t a major |
the course of a |
average loss rates between |
in which there are |
a tool mismatched to |
t a major issue |
course of a streaming |
which there are four |
tool mismatched to developer |
a major issue because |
of a streaming session |
there are four classes |
mismatched to developer needs |
major issue because most |
are four classes of |
issue because most message |
four classes of messages |
because most message losses |
each node stores packets |
classes of messages being |
node stores packets and |
web services move beyond |
most message losses can |
of messages being sent |
stores packets and forwards |
services move beyond corba |
message losses can be |
messages being sent simultaneously |
packets and forwards them |
move beyond corba in |
losses can be corrected |
and forwards them to |
beyond corba in many |
can be corrected locally |
forwards them to other |
corba in many ways |
them to other peers |
to other peers only |
on four of its |
other peers only while |
four of its six |
the lowest line corresponds |
through cooperation among receivers |
peers only while the |
of its six inter |
but the corba community |
lowest line corresponds to |
only while the packet |
the basic idea is |
the corba community s |
line corresponds to the |
corba community s failed |
basic idea is to |
community s failed effort |
corresponds to the highest |
s failed effort to |
idea is to perform |
failed effort to implement |
while the packet is |
haul links for the |
to the highest priority |
is to perform recovery |
effort to implement virtual |
the packet is within |
links for the month |
to perform recovery as |
to implement virtual synchrony |
packet is within its |
dark horizontal lines represent |
perform recovery as locally |
implement virtual synchrony carries |
for the month of |
is within its availability |
horizontal lines represent operating |
recovery as locally as |
virtual synchrony carries an |
the month of december |
within its availability window |
lines represent operating modes |
as locally as possible |
synchrony carries an important |
represent operating modes on |
carries an important lesson |
operating modes on the |
usually spanning a few |
an important lesson to |
modes on the left |
spanning a few seconds |
important lesson to current |
lesson to current researchers |
each node also maintains |
and the highest priority |
node also maintains an |
any technology offered to |
the highest priority of |
also maintains an interest |
if a message is |
technology offered to developers |
highest priority of data |
maintains an interest window |
a message is available |
offered to developers must |
priority of data being |
message is available within |
to developers must support |
of data being sent |
is available within the |
developers must support the |
data being sent during |
which represents the set |
available within the same |
must support the programming |
being sent during a |
represents the set of |
within the same token |
the same token ring |
sent during a second |
qwest reports loss rates |
the set of packets |
support the programming styles |
during a second on |
reports loss rates of |
set of packets in |
some process that has |
the programming styles they |
a second on the |
second on the right |
process that has a |
programming styles they prefer |
of packets in which |
that has a a |
packets in which the |
has a a a |
the modeless scheme achieves |
in which the peer |
a a a c |
modeless scheme achieves higher |
which the peer is |
management policies a scalable |
a a c ac |
policies a scalable services |
the peer is currently |
scheme achieves higher utilisation |
a c ac ab |
a scalable services architecture |
peer is currently interested |
c ac ab abc |
scalable services architecture for |
ac ab abc bc |
services architecture for building |
ab abc bc b |
architecture for building raps |
abc bc b c |
nodes choose packets to |
for building raps of |
bc b c b |
choose packets to request |
building raps of racs |
b c b figure |
packets to request from |
raps of racs alone |
to request from each |
mb of data sent |
of racs alone isn |
request from each of |
racs alone isn t |
from each of its |
in either direction on |
alone isn t enough |
each of its neighbors |
groups overlap to form |
overlap to form regions |
because it always has |
either direction on its |
it always has messages |
direction on its trans |
always has messages to |
nodes belong to the |
respecting a maximum limit |
has messages to send |
belong to the same |
a maximum limit l |
scale systems that will |
to the same region |
pacific link for the |
while the modal scheme |
systems that will likely |
the same region if |
maximum limit l on |
link for the same |
the modal scheme is |
that will likely soon |
same region if they |
limit l on the |
for the same month |
modal scheme is dependent |
will likely soon rely |
region if they have |
l on the number |
scheme is dependent on |
likely soon rely on |
if they have similar |
on the number of |
is dependent on a |
soon rely on standardized |
they have similar group |
the number of outstanding |
dependent on a rapid |
rely on standardized web |
have similar group membership |
number of outstanding requests |
on a rapid and |
on standardized web services |
of outstanding requests to |
a rapid and accurate |
standardized web services including |
outstanding requests to each |
qsm currently uses an |
web services including global |
rapid and accurate estimate |
currently uses an unreliable |
we expect privately managed |
services including global banks |
and accurate estimate of |
uses an unreliable ip |
expect privately managed lambdas |
accurate estimate of the |
an unreliable ip multicast |
privately managed lambdas to |
estimate of the available |
the entire us air |
managed lambdas to exhibit |
of the available bandwidth |
entire us air force |
since a single group |
lambdas to exhibit higher |
the available bandwidth in |
a single group may |
to exhibit higher loss |
available bandwidth in order |
single group may span |
and the supervisory control |
bandwidth in order to |
exhibit higher loss rates |
group may span multiple |
may span multiple regions |
in order to select |
higher loss rates due |
the supervisory control and |
order to select its |
supervisory control and data |
to send to group |
control and data acquisition |
to select its correct |
and data acquisition systems |
loss rates due to |
send to group g |
select its correct operating |
data acquisition systems that |
rates due to the |
its correct operating mode |
a node multicasts a |
due to the inherent |
acquisition systems that operate |
node multicasts a message |
to the inherent tradeoff |
systems that operate the |
multicasts a message to |
the inherent tradeoff between |
that operate the us |
a message to each |
inherent tradeoff between fiber |
operate the us power |
message to each of |
the us power grid |
to each of the |
us power grid will |
equipment quality and cost |
each of the regions |
power grid will also |
of the regions separately |
grid will also require |
will also require policies |
also require policies to |
require policies to manage |
policies to manage security |
to manage security keys |
these graphs are reproduced |
graphs are reproduced from |
as well as the |
well as the difficulty |
as the difficulty of |
the difficulty of performing |
our approach makes it |
difficulty of performing routine |
approach makes it easy |
of performing routine maintenance |
automated tools for monitoring |
makes it easy to |
performing routine maintenance on |
tools for monitoring large |
an equivalent modal scheme |
it easy to aggregate |
routine maintenance on longdistance |
for monitoring large complex |
easy to aggregate messages |
maintenance on longdistance links |
monitoring large complex systems |
other experiments have shown |
to aggregate messages across |
large complex systems will |
experiments have shown that |
aggregate messages across different |
complex systems will be |
have shown that modeless |
messages across different groups |
systems will be needed |
shown that modeless adaptation |
will be needed as |
that modeless adaptation can |
be needed as well |
modeless adaptation can achieve |
adaptation can achieve improvements |
can achieve improvements of |
end paths as dropping |
paths as dropping packets |
as dropping packets at |
dropping packets at rates |
packets at rates of |
if a node has |
researchers must think about |
a node has two |
must think about how |
node has two messages |
think about how monitoring |
has two messages to |
about how monitoring and |
two messages to send |
how monitoring and management |
messages to send to |
monitoring and management policies |
to send to a |
and management policies in |
send to a pair |
management policies in different |
to a pair of |
policies in different organizations |
a pair of groups |
in different organizations should |
pair of groups g |
different organizations should talk |
organizations should talk to |
should talk to one |
talk to one another |
to one another when |
and it is possible |
one another when web |
it is possible to |
another when web services |
which overlap in region |
is possible to construct |
when web services interactions |
overlap in region r |
to capture a wide |
web services interactions cross |
possible to construct cases |
capture a wide range |
then while transmitting to |
to construct cases in |
services interactions cross boundaries |
a wide range of |
while transmitting to r |
construct cases in which |
wide range of deployed |
cases in which the |
range of deployed networks |
these are tough problems |
in which the improvement |
the node can batch |
which the improvement is |
node can batch these |
the improvement is even |
can batch these messages |
improvement is even greater |
but they can be |
batch these messages together |
they can be solved |
e xisting r eliability |
xisting r eliability o |
r eliability o ptions |
work on adaptation in |
eliability o ptions tcp |
on adaptation in mobile |
apps send to a |
adaptation in mobile file |
send to a send |
in mobile file systems |
to a send to |
mobile file systems has |
a send to b |
at cornell we recently |
upload factor download factor |
ip is the default |
file systems has generally |
send to b group |
cornell we recently developed |
is the default reliable |
systems has generally relied |
to b group senders |
we recently developed astrolabe |
the default reliable communication |
has generally relied on |
b group senders a |
default reliable communication option |
generally relied on modal |
group senders a b |
reliable communication option for |
relied on modal schemes |
senders a b c |
a scalable technology for |
communication option for contemporary |
a b c region |
scalable technology for distributed |
option for contemporary networked |
b c region senders |
technology for distributed monitoring |
for contemporary networked applications |
c region senders a |
for distributed monitoring and |
region senders a ab |
distributed monitoring and control |
senders a ab ac |
monitoring and control that |
a ab ac abc |
and control that has |
ab ac abc b |
exclusive embeddings in commodity |
control that has attracted |
ac abc b c |
embeddings in commodity operating |
that has attracted tremendous |
abc b c bc |
in commodity operating systems |
has attracted tremendous interest |
b c bc region |
commodity operating systems and |
but our evaluation of |
attracted tremendous interest and |
c bc region leader |
operating systems and networking |
our evaluation of atp |
tremendous interest and attention |
bc region leader figure |
systems and networking apis |
evaluation of atp demonstrated |
of atp demonstrated that |
atp demonstrated that it |
demonstrated that it could |
researchers at other institutions |
that it could also |
at other institutions are |
it could also improve |
other institutions are working |
to multicast to a |
could also improve the |
most applications requiring reliable |
institutions are working on |
multicast to a group |
also improve the performance |
applications requiring reliable communication |
are working on other |
improve the performance of |
qsm sends a copy |
working on other promising |
requiring reliable communication over |
the performance of file |
sends a copy to |
on other promising solutions |
reliable communication over any |
performance of file system |
a copy to each |
communication over any form |
copy to each of |
over any form of |
to each of the |
calability isn t just |
any form of network |
each of the underlying |
isn t just a |
we discuss the implementation |
of the underlying regions |
form of network use |
t just a technology |
discuss the implementation of |
of network use tcp |
the implementation of modeless |
implementation of modeless adaptation |
of modeless adaptation in |
it s also a |
modeless adaptation in mfs |
s also a mindset |
partition leader token intrapartition |
adaptation in mfs further |
also a mindset with |
leader token intrapartition token |
in mfs further in |
a mindset with ramifications |
token intrapartition token partition |
mfs further in section |
mindset with ramifications at |
intrapartition token partition figure |
with ramifications at many |
ramifications at many levels |
ip has three major |
has three major problems |
three major problems when |
major problems when used |
problems when used over |
when used over high |
to ensure true scalability |
atp is implemented at |
is implemented at user |
implemented at user level |
a hierarchy of token |
hierarchy of token rings |
web services platforms must |
services platforms must begin |
on top of kernel |
platforms must begin to |
top of kernel udp |
must begin to standardize |
begin to standardize application |
to standardize application architectures |
standardize application architectures that |
it has a message |
application architectures that promote |
architectures that promote reliability |
that promote reliability and |
throughput collapse in lossy |
promote reliability and interoperability |
collapse in lossy networks |
oriented interface for communication |
reliability and interoperability when |
naks ack through upcalls |
and interoperability when developers |
interoperability when developers build |
when developers build systems |
in which messages of |
developers build systems of |
which messages of an |
qsm is also registered |
messages of an arbitrary |
ip is unable to |
build systems of systems |
is also registered as |
of an arbitrary size |
is unable to distinguish |
also registered as a |
an arbitrary size can |
unable to distinguish between |
registered as a shell |
work with intrinsically distributed |
to distinguish between ephemeral |
arbitrary size can be |
as a shell extension |
with intrinsically distributed programs |
distinguish between ephemeral loss |
size can be reliably |
intrinsically distributed programs that |
between ephemeral loss modes |
can be reliably transmitted |
making it possible to |
distributed programs that don |
ephemeral loss modes due |
be reliably transmitted with |
it possible to access |
programs that don t |
loss modes due to |
reliably transmitted with their |
possible to access the |
that don t fit |
modes due to transient |
transmitted with their boundaries |
to access the communication |
don t fit a |
due to transient congestion |
with their boundaries preserved |
access the communication subsystem |
t fit a transactional |
their boundaries preserved at |
the communication subsystem directly |
fit a transactional model |
boundaries preserved at the |
communication subsystem directly from |
preserved at the receiver |
subsystem directly from the |
at the receiver s |
the receiver s side |
directly from the windows |
or bad fiber and |
and must provide responsiveness |
from the windows gui |
bad fiber and persistent |
must provide responsiveness guarantees |
an application can send |
fiber and persistent congestion |
provide responsiveness guarantees to |
application can send a |
responsiveness guarantees to their |
can send a message |
the user can store |
maximum upload factor figure |
guarantees to their users |
send a message synchronously |
the loss of one |
user can store a |
a message synchronously or |
loss of one packet |
can store a shortcut |
message synchronously or asynchronously |
applications with these sorts |
store a shortcut to |
of one packet out |
download and upload factors |
with these sorts of |
a shortcut to a |
these sorts of requirements |
in the latter case |
sorts of requirements are |
one packet out of |
shortcut to a qsm |
and upload factors of |
the latter case the |
of requirements are already |
packet out of ten |
to a qsm stream |
upload factors of nodes |
latter case the sender |
requirements are already in |
out of ten thousand |
a qsm stream in |
factors of nodes in |
case the sender provides |
are already in the |
of ten thousand is |
qsm stream in the |
of nodes in an |
the sender provides a |
already in the pipeline |
ten thousand is sufficient |
stream in the file |
in the file system |
sender provides a function |
in the pipeline and |
thousand is sufficient to |
nodes in an ideal |
provides a function to |
the pipeline and even |
is sufficient to reduce |
in an ideal system |
a function to be |
pipeline and even more |
sufficient to reduce tcp |
click to attach a |
function to be executed |
and even more of |
an ideal system where |
to attach a previewer |
to be executed when |
even more of them |
ideal system where all |
attach a previewer or |
be executed when transmission |
ip throughput to a |
more of them are |
system where all nodes |
a previewer or a |
executed when transmission of |
throughput to a third |
of them are on |
where all nodes behave |
previewer or a viewer |
when transmission of the |
to a third of |
them are on drawing |
all nodes behave correctly |
or a viewer to |
transmission of the message |
a third of its |
are on drawing boards |
a viewer to an |
of the message completes |
third of its lossless |
on drawing boards in |
viewer to an event |
of its lossless maximum |
and the send operation |
to an event stream |
drawing boards in government |
this limit not only |
the send operation itself |
if one packet is |
the overall architecture is |
send operation itself is |
limit not only improves |
one packet is lost |
overall architecture is summarized |
operation itself is non |
not only improves the |
packet is lost out |
architecture is summarized in |
only improves the general |
is lost out of |
is summarized in figure |
improves the general flow |
lost out of a |
the general flow of |
this is similar to |
the only option for |
out of a thousand |
general flow of packets |
is similar to the |
only option for the |
similar to the queued |
the system is single |
throughput collapses to a |
to the queued rpc |
option for the web |
collapses to a thirtieth |
but also makes it |
the queued rpc developed |
for the web services |
to a thirtieth of |
also makes it harder |
queued rpc developed for |
we use a windows |
use a windows i |
makes it harder for |
the web services community |
rpc developed for rover |
a thirtieth of the |
it harder for malicious |
web services community is |
thirtieth of the maximum |
harder for malicious peers |
services community is to |
henceforth referred to as |
for malicious peers to |
community is to take |
referred to as an |
malicious peers to overrequest |
the root cause of |
is to take on |
to as an i |
peers to overrequest packets |
root cause of throughput |
to take on the |
to overrequest packets from |
cause of throughput collapse |
take on the challenge |
overrequest packets from their |
of throughput collapse is |
packets from their neighbors |
throughput collapse is tcp |
atp also allows the |
to collect all asynchronous |
collect all asynchronous i |
also allows the sender |
peers maintain a queue |
if they do so |
allows the sender to |
maintain a queue of |
ip s fundamental reliance |
the sender to attach |
a queue of non |
including notifications of any |
sender to attach a |
s fundamental reliance on |
solutions will be readily |
notifications of any received |
to attach a priority |
fundamental reliance on loss |
will be readily available |
satisfied requests from its |
of any received messages |
attach a priority to |
reliance on loss as |
requests from its neighbors |
a priority to each |
web services are going |
on loss as a |
priority to each message |
services are going to |
loss as a signal |
keeping only the l |
are going to be |
as a signal of |
only the l most |
going to be the |
a signal of congestion |
the l most recent |
to control the order |
a single core thread |
l most recent ones |
to be the ubiquitous |
control the order in |
single core thread synchronously |
while recent approaches have |
be the ubiquitous platform |
the order in which |
core thread synchronously polls |
recent approaches have sought |
the ubiquitous platform technology |
order in which the |
thread synchronously polls the |
approaches have sought to |
ubiquitous platform technology for |
in which the queued |
synchronously polls the i |
have sought to replace |
platform technology for next |
which the queued messages |
sought to replace loss |
expected behavior our first |
the queued messages are |
o queue to retrieve |
to replace loss with |
behavior our first goal |
queued messages are transmitted |
queue to retrieve incoming |
generation critical computing systems |
replace loss with delay |
our first goal is |
to retrieve incoming messages |
loss with delay as |
messages are queued at |
first goal is to |
with delay as a |
are queued at the |
and we ve no |
goal is to explore |
the core thread also |
delay as a congestion |
queued at the sender |
we ve no one |
is to explore the |
core thread also maintains |
as a congestion signal |
at the sender according |
ve no one but |
to explore the typical |
thread also maintains an |
the sender according to |
no one but ourselves |
explore the typical signature |
also maintains an alarm |
sender according to their |
according to their receivers |
the typical signature of |
maintains an alarm queue |
one but ourselves to |
typical signature of the |
but ourselves to blame |
and each queue is |
signature of the system |
ourselves to blame if |
each queue is ordered |
implemented as a splay |
as a splay tree |
queue is ordered by |
to blame if these |
is ordered by priority |
or to specifically identify |
since an understanding of |
blame if these systems |
an understanding of the |
to specifically identify loss |
understanding of the behavior |
if these systems don |
messages of the same |
of the behavior of |
specifically identify loss caused |
the behavior of pullbased |
and a request queue |
behavior of pullbased dissemination |
identify loss caused by |
of pullbased dissemination in |
these systems don t |
of the same priority |
loss caused by non |
implemented as a lockfree |
pullbased dissemination in the |
systems don t work |
the same priority within |
as a lockfree queue |
dissemination in the absence |
don t work properly |
same priority within a |
a lockfree queue with |
lockfree queue with cas |
priority within a queue |
in the absence of |
within a queue are |
the absence of opportunistic |
a queue are transmitted |
do we really want |
absence of opportunistic nodes |
queue are transmitted in |
for requests from the |
of opportunistic nodes will |
we really want to |
are transmitted in first |
really want to create |
opportunistic nodes will turn |
want to create a |
nodes will turn out |
to create a world |
older variants prominently reno |
create a world in |
will turn out to |
a world in which |
variants prominently reno remain |
turn out to be |
world in which minor |
prominently reno remain ubiquitously |
in which minor computer |
the core thread polls |
out to be important |
reno remain ubiquitously deployed |
which minor computer glitches |
atp also allows a |
to be important when |
core thread polls all |
minor computer glitches shut |
also allows a sender |
computer glitches shut down |
thread polls all queues |
glitches shut down massive |
allows a sender to |
shut down massive critical |
polls all queues in |
down massive critical applications |
a sender to specify |
massive critical applications and |
all queues in a |
queues in a round |
recovery delays for real |
sender to specify a |
critical applications and in |
be important when we |
to specify a send |
applications and in which |
robin fashion and processes |
important when we set |
specify a send timeout |
and in which hackers |
fashion and processes the |
when we set out |
a send timeout for |
send timeout for a |
and processes the events |
we set out to |
in which hackers can |
timeout for a message |
processes the events sequentially |
set out to introduce |
ip uses positive acknowledgments |
which hackers can readily |
out to introduce auditing |
uses positive acknowledgments and |
hackers can readily disrupt |
events of the same |
positive acknowledgments and retransmissions |
which causes the transmission |
acknowledgments and retransmissions to |
we conducted experiments using |
and retransmissions to ensure |
causes the transmission to |
retransmissions to ensure reliability |
can readily disrupt access |
conducted experiments using an |
of the same type |
the transmission to be |
to ensure reliability the |
readily disrupt access to |
experiments using an event |
the same type are |
transmission to be suspended |
ensure reliability the sender |
disrupt access to banking |
same type are processed |
to be suspended if |
reliability the sender buffers |
access to banking records |
type are processed in |
be suspended if it |
the sender buffers packets |
are processed in batches |
which is described in |
suspended if it expires |
air traffic control systems |
is described in more |
sender buffers packets until |
up to the limit |
described in more detail |
buffers packets until their |
to the limit determined |
in more detail in |
so that the sender |
and even shut down |
packets until their receipt |
the limit determined by |
more detail in section |
that the sender can |
even shut down the |
until their receipt is |
limit determined by a |
the sender can react |
sender can react to |
can react to it |
determined by a quantum |
shut down the power |
their receipt is acknowledged |
down the power grid |
receipt is acknowledged by |
is acknowledged by the |
acknowledged by the receiver |
an analogous mechanism is |
analogous mechanism is available |
mechanism is available for |
time is running out |
we evaluate the performance |
is available for receive |
evaluate the performance of |
and resends if an |
available for receive operations |
resends if an acknowledgment |
current halfway solutions will |
if an acknowledgment is |
halfway solutions will tempt |
an acknowledgment is not |
solutions will tempt developers |
acknowledgment is not received |
besides detecting when a |
will tempt developers to |
is not received within |
detecting when a remote |
tempt developers to embark |
not received within some |
when a remote host |
nodes during an ideal |
received within some time |
developers to embark on |
a remote host is |
to embark on a |
during an ideal execution |
within some time period |
there is no limit |
remote host is inaccessible |
embark on a path |
an ideal execution of |
is no limit for |
on a path that |
send timeouts do not |
a path that will |
ideal execution of chainsaw |
no limit for local |
timeouts do not play |
path that will soon |
limit for local push |
a lost packet is |
do not play a |
that will soon lead |
where all the nodes |
lost packet is received |
not play a major |
will soon lead many |
pull data sender inter |
all the nodes behave |
packet is received in |
play a major role |
soon lead many of |
the nodes behave correctly |
is received in the |
a major role in |
major role in mfs |
received in the form |
pull region partition figure |
we fixed the upload |
in the form of |
lead many of them |
fixed the upload factor |
the form of a |
many of them into |
the upload factor of |
an additional use for |
form of a retransmission |
additional use for timeouts |
upload factor of the |
of them into real |
recovery inside and across |
inside and across partitions |
use for timeouts would |
factor of the source |
for timeouts would be |
of a retransmission that |
timeouts would be to |
of the source at |
would be to detect |
a retransmission that arrives |
them into real trouble |
copy will forward it |
be to detect prefetches |
retransmission that arrives no |
will forward it to |
to detect prefetches which |
that arrives no earlier |
forward it to the |
detect prefetches which are |
arrives no earlier than |
the entire industry clients |
it to the process |
prefetches which are not |
to the process missing |
which are not making |
the process missing the |
are not making progress |
process missing the message |
not making progress and |
making progress and reissue |
progress and reissue a |
and reissue a prefetch |
and vendors as well |
reissue a prefetch for |
rtts after the original |
vendors as well as |
a prefetch for a |
qsm implements a scheme |
as well as the |
prefetch for a different |
well as the government |
implements a scheme originally |
for a different file |
and the stream rate |
as the government have |
a scheme originally proposed |
the stream rate to |
the sender has to |
the government have a |
scheme originally proposed by |
sender has to buffer |
government have a shared |
originally proposed by zhao |
has to buffer each |
have a shared obligation |
to buffer each packet |
a shared obligation to |
buffer each packet until |
shared obligation to make |
atp administers priorities by |
each packet until it |
obligation to make web |
administers priorities by deriving |
packet until it s |
to make web services |
we varied the maximum |
priorities by deriving an |
until it s acknowledged |
make web services better |
varied the maximum upload |
by deriving an estimate |
the maximum upload factor |
even in a large |
maximum upload factor of |
deriving an estimate for |
upload factor of nodes |
in a large ring |
factor of nodes to |
an estimate for the |
rtt in lossless operation |
of nodes to see |
s ken birman is |
estimate for the bandwidth |
nodes to see how |
no more than five |
ken birman is a |
for the bandwidth available |
to see how it |
more than five nodes |
and it has to |
birman is a professor |
the bandwidth available between |
see how it affected |
than five nodes cache |
it has to perform |
is a professor in |
bandwidth available between the |
how it affected both |
five nodes cache any |
has to perform additional |
a professor in the |
available between the sender |
it affected both the |
nodes cache any given |
cache any given message |
professor in the department |
between the sender and |
affected both the download |
to perform additional work |
qsm also uses this |
the sender and receiver |
both the download and |
in the department of |
perform additional work to |
also uses this idea |
the download and upload |
the department of computer |
additional work to retransmit |
in order to minimise |
uses this idea at |
download and upload factors |
department of computer science |
work to retransmit the |
order to minimise the |
this idea at the |
and upload factors of |
of computer science at |
to retransmit the packet |
to minimise the transmission |
idea at the level |
upload factors of nodes |
computer science at cornell |
retransmit the packet if |
minimise the transmission delay |
at the level of |
factors of nodes across |
science at cornell university |
the packet if it |
the transmission delay when |
the level of partitions |
of nodes across the |
packet if it does |
transmission delay when a |
contact him at ken |
if it does not |
nodes across the system |
delay when a new |
each message is cached |
it does not receive |
when a new message |
message is cached in |
does not receive the |
a new message is |
is cached in a |
cached in a single |
in a single partition |
new message is sent |
not receive the acknowledgment |
the maximum upload factor |
maximum upload factor is |
upload factor is a |
factor is a fixed |
is a fixed parameter |
atp uses a form |
a fixed parameter which |
uses a form of |
fixed parameter which defines |
any packets that arrive |
a form of rate |
packets that arrive with |
if some partition is |
parameter which defines the |
that arrive with higher |
some partition is missing |
which defines the maximum |
arrive with higher sequence |
partition is missing a |
defines the maximum rate |
with higher sequence numbers |
is missing a message |
each second is divided |
the maximum rate at |
higher sequence numbers than |
second is divided into |
maximum rate at which |
department of computer engineering |
sequence numbers than that |
is divided into twenty |
numbers than that of |
rate at which a |
the partition caching it |
divided into twenty send |
san jose state university |
at which a node |
partition caching it steps |
than that of a |
into twenty send periods |
which a node will |
caching it steps in |
that of a lost |
twenty send periods of |
a node will upload |
it steps in to |
of a lost packet |
node will upload data |
steps in to resend |
a lost packet must |
will upload data to |
in to resend it |
lost packet must be |
upload data to all |
packet must be queued |
data to all its |
must be queued while |
to all its neighbors |
and at most one |
be queued while the |
queued while the receiver |
while the receiver waits |
the receiver waits for |
for fairness in nodes |
receiver waits for the |
fairness in nodes bandwidth |
if an entire region |
twentieth of the available |
waits for the lost |
in nodes bandwidth consumption |
an entire region is |
of the available bandwidth |
for the lost packet |
entire region is missing |
the available bandwidth is |
the lost packet to |
region is missing a |
available bandwidth is used |
we would like all |
lost packet to arrive |
is missing a message |
bandwidth is used during |
would like all nodes |
is used during a |
like all nodes to |
used during a single |
all nodes to upload |
during a single send |
the sender becomes involved |
nodes to upload data |
a single send period |
sender becomes involved and |
to upload data at |
throughput financial banking application |
becomes involved and re |
upload data at a |
financial banking application running |
without such a constraint |
data at a factor |
banking application running in |
at a factor as |
application running in a |
a factor as close |
running in a data |
factor as close as |
in a data center |
atp would send as |
as close as possible |
qsm tokens also carry |
a data center in |
would send as much |
close as possible to |
tokens also carry other |
data center in new |
send as much data |
also carry other information |
center in new york |
as much data as |
in new york city |
much data as it |
data as it could |
as it could on |
including data used to |
it could on receipt |
sending updates to a |
data used to perform |
could on receipt of |
updates to a sister |
used to perform rate |
on receipt of a |
to a sister site |
we varied the maximum |
to perform rate control |
receipt of a low |
a sister site in |
varied the maximum upload |
perform rate control and |
sister site in switzerland |
the maximum upload factor |
rate control and information |
maximum upload factor of |
control and information used |
upload factor of nodes |
and this data could |
and information used to |
the rtt value between |
factor of nodes from |
this data could then |
information used to trigger |
rtt value between these |
data could then be |
used to trigger garbage |
value between these two |
could then be buffered |
to trigger garbage collection |
between these two centers |
then be buffered at |
these two centers is |
be buffered at an |
two centers is typically |
buffered at an intermediate |
the overall system configuration |
at an intermediate link |
overall system configuration is |
system configuration is managed |
configuration is managed by |
is managed by what |
managed by what we |
by what we call |
delaying the transmission of |
what we call the |
the transmission of any |
we call the configuration |
transmission of any high |
call the configuration management |
the left graph shows |
the configuration management service |
left graph shows the |
graph shows the minimum |
priority message which might |
message which might be |
which might be sent |
might be sent later |
average and maximum download |
and maximum download factors |
maximum download factors across |
download factors across the |
factors across the nodes |
across the nodes when |
in the case of |
the disadvantage of this |
the nodes when the |
disadvantage of this scheme |
the case of a |
which handles join and |
of this scheme is |
nodes when the maximum |
case of a lost |
handles join and leave |
this scheme is that |
when the maximum upload |
of a lost packet |
join and leave requests |
scheme is that heavy |
the maximum upload factor |
is that heavy contention |
maximum upload factor of |
that heavy contention at |
upload factor of nodes |
all packets received within |
heavy contention at the |
factor of nodes is |
packets received within the |
contention at the sender |
of nodes is increased |
and uses these to |
at the sender may |
uses these to generate |
the sender may delay |
these to generate a |
sender may delay a |
to generate a sequence |
may delay a new |
generate a sequence of |
delay a new message |
a sequence of membership |
by increasing the maximum |
a new message by |
sequence of membership views |
increasing the maximum upload |
new message by as |
milliseconds or more between |
of membership views for |
or more between the |
message by as much |
more between the original |
membership views for each |
between the original packet |
by as much as |
the original packet send |
views for each multicast |
the maximum upload factor |
original packet send and |
maximum upload factor of |
packet send and the |
upload factor of nodes |
send and the receipt |
and the receipt of |
the cms also determines |
the receipt of its |
we increase the global |
cms also determines and |
receipt of its retransmission |
increase the global upload |
also determines and continuously |
of its retransmission have |
regardless of its priority |
the global upload capacity |
determines and continuously updates |
its retransmission have to |
global upload capacity of |
and continuously updates region |
retransmission have to be |
upload capacity of the |
this inefficiency of the |
continuously updates region boundaries |
inefficiency of the atp |
capacity of the system |
of the atp implementation |
have to be buffered |
the atp implementation is |
to be buffered at |
atp implementation is most |
maintains sequences of region |
leading to a better |
be buffered at the |
implementation is most visible |
sequences of region views |
to a better flow |
buffered at the receiver |
is most visible when |
of region views for |
a better flow of |
most visible when there |
region views for each |
better flow of packets |
visible when there is |
views for each region |
when there is contention |
there is contention between |
is contention between different |
the loss of a |
contention between different priorities |
loss of a single |
between different priorities at |
and tracks the mapping |
the discrepancy among the |
of a single packet |
different priorities at high |
a single packet stops |
discrepancy among the upload |
tracks the mapping from |
priorities at high bandwidth |
single packet stops all |
among the upload factors |
the mapping from group |
packet stops all traffic |
the upload factors of |
like it or not |
mapping from group views |
stops all traffic in |
upload factors of individual |
from group views to |
all traffic in the |
factors of individual nodes |
group views to region |
traffic in the channel |
of individual nodes also |
web services are distributed |
views to region views |
mfs implementation the version |
in the channel to |
implementation the version of |
services are distributed objects |
individual nodes also increases |
the channel to the |
the version of mfs |
channel to the application |
version of mfs described |
to the application for |
the cms runs on |
as seen in the |
of mfs described in |
the application for a |
cms runs on a |
seen in the graph |
mfs described in this |
application for a seventh |
runs on a single |
on a single node |
described in this paper |
for a seventh of |
in the graph to |
cornell university within the |
in this paper is |
university within the community |
the graph to the |
but we intend to |
a seventh of a |
this paper is implemented |
within the community developing |
graph to the right |
we intend to replace |
seventh of a second |
paper is implemented in |
the community developing the |
when the maximum upload |
is implemented in c |
intend to replace this |
community developing the web |
the maximum upload factor |
implemented in c and |
to replace this with |
replace this with a |
this with a state |
maximum upload factor is |
in c and runs |
a sequence of such |
developing the web services |
machine replicated version in |
c and runs on |
sequence of such blocks |
upload factor is increased |
the web services architecture |
replicated version in the |
and runs on freebsd |
of such blocks can |
web services architecture and |
version in the future |
such blocks can have |
some nodes participate more |
services architecture and products |
in the future to |
blocks can have devastating |
nodes participate more actively |
the future to eliminate |
can have devastating effect |
participate more actively in |
future to eliminate the |
have devastating effect on |
an increasingly schizophrenic message |
more actively in dissemination |
to eliminate the risk |
both the client and |
devastating effect on a |
increasingly schizophrenic message is |
actively in dissemination while |
eliminate the risk of |
the client and server |
effect on a high |
schizophrenic message is emerging |
in dissemination while others |
the risk of single |
client and server have |
dissemination while others end |
and server have multiple |
while others end up |
marketing materials assure us |
server have multiple threads |
throughput system where every |
others end up contributing |
materials assure us that |
have multiple threads to |
system where every spare |
end up contributing less |
in the longer term |
assure us that web |
multiple threads to cope |
where every spare cycle |
the longer term we |
even though all of |
threads to cope with |
every spare cycle counts |
us that web services |
longer term we will |
though all of them |
to cope with simultaneous |
that web services are |
term we will move |
all of them are |
cope with simultaneous file |
web services are a |
we will move to |
of them are behaving |
with simultaneous file system |
simultaneous file system requests |
will move to a |
in applications with many |
them are behaving correctly |
services are a breakthrough |
and the rpc library |
applications with many fine |
move to a hierarchically |
the rpc library has |
to a hierarchically structured |
this is an important |
rpc library has its |
a hierarchically structured cms |
is an important consideration |
offering unparalleled interoperability and |
library has its own |
unparalleled interoperability and comprehensive |
has its own thread |
a lost packet can |
interoperability and comprehensive standards |
when we introduce auditing |
lost packet can potentially |
and comprehensive standards for |
packet can potentially trigger |
comprehensive standards for associated |
therefore there are two |
can potentially trigger a |
standards for associated technologies |
we do not want |
there are two mandatory |
potentially trigger a butterfly |
do not want to |
are two mandatory thread |
trigger a butterfly effect |
not want to punish |
two mandatory thread context |
a butterfly effect of |
alarm queue application thread |
mandatory thread context switches |
want to punish nodes |
butterfly effect of missed |
queue application thread operating |
thread context switches on |
they portray web services |
to punish nodes that |
effect of missed deadlines |
application thread operating system |
context switches on any |
portray web services as |
punish nodes that are |
of missed deadlines along |
thread operating system kernel |
switches on any message |
web services as a |
nodes that are willing |
missed deadlines along a |
operating system kernel implementation |
on any message send |
services as a seamless |
that are willing to |
deadlines along a distributed |
system kernel implementation qsm |
any message send or |
as a seamless interconnection |
are willing to contribute |
along a distributed workflow |
kernel implementation qsm qsm |
message send or receive |
send or receive operation |
willing to contribute but |
implementation qsm qsm request |
a seamless interconnection layer |
to contribute but cannot |
as we shall describe |
contribute but cannot do |
qsm qsm request queue |
seamless interconnection layer that |
we shall describe in |
but cannot do so |
qsm request queue core |
interconnection layer that will |
shall describe in subsequent |
cannot do so because |
request queue core thread |
queue core thread i |
layer that will propel |
describe in subsequent sections |
do so because of |
overloaded networks and end |
that will propel computer |
so because of factors |
o queue socket figure |
some subsystems have additional |
because of factors such |
subsystems have additional threads |
hosts can exhibit continuous |
of factors such as |
have additional threads to |
can exhibit continuous packet |
factors such as their |
additional threads to carry |
exhibit continuous packet loss |
such as their physical |
qsm uses a single |
computer commerce to a |
threads to carry out |
as their physical positioning |
commerce to a previously |
to carry out background |
their physical positioning in |
with each lost packet |
to a previously inaccessible |
carry out background processing |
physical positioning in the |
with a core thread |
each lost packet driving |
a previously inaccessible level |
positioning in the system |
a core thread that |
lost packet driving the |
core thread that controls |
packet driving the system |
our experiments were conducted |
and they use language |
thread that controls three |
driving the system further |
experiments were conducted with |
in all our future |
they use language evocative |
that controls three queues |
the system further and |
were conducted with a |
all our future experiments |
use language evocative of |
system further and further |
conducted with a default |
our future experiments we |
language evocative of marketing |
further and further out |
with a default client |
future experiments we set |
evocative of marketing for |
and further out of |
a default client cache |
experiments we set the |
of marketing for distributed |
further out of sync |
default client cache size |
we set the maximum |
marketing for distributed object |
out of sync with |
and requests from the |
client cache size of |
set the maximum upload |
for distributed object middleware |
of sync with respect |
requests from the possibly |
the maximum upload factor |
sync with respect to |
from the possibly multithreaded |
maximum upload factor to |
with respect to its |
technologists are sending a |
the possibly multithreaded application |
respect to its real |
are sending a somewhat |
sending a somewhat different |
a somewhat different message |
when we set out |
we set out to |
set out to implement |
out to implement qsm |
rpcs with priorities mfs |
in an essay entitled |
with priorities mfs rpcs |
an essay entitled web |
priorities mfs rpcs are |
our intent was to |
essay entitled web services |
mfs rpcs are implemented |
intent was to leverage |
massive buffering needs for |
entitled web services are |
rpcs are implemented on |
was to leverage the |
buffering needs for high |
web services are not |
are implemented on top |
to leverage the component |
needs for high throughput |
services are not distributed |
implemented on top of |
effect of opportunistic behavior |
leverage the component integration |
of opportunistic behavior our |
are not distributed objects |
opportunistic behavior our next |
for high throughput applications |
the component integration tools |
on top of atp |
behavior our next goal |
component integration tools available |
our next goal was |
werner vogels argues that |
top of atp in |
integration tools available on |
next goal was to |
vogels argues that web |
ip uses fixed size |
tools available on the |
goal was to understand |
of atp in the |
was to understand the |
uses fixed size buffers |
available on the windows |
atp in the natural |
argues that web services |
to understand the expected |
fixed size buffers at |
on the windows platform |
in the natural way |
that web services will |
understand the expected behavior |
size buffers at receivers |
web services will work |
the expected behavior of |
buffers at receivers to |
services will work well |
an rpc request constitutes |
at receivers to prevent |
we didn t expect |
didn t expect that |
t expect that co |
rpc request constitutes one |
receivers to prevent overflows |
expected behavior of correct |
will work well for |
existence with the managed |
behavior of correct nodes |
request constitutes one message |
work well for important |
with the managed environment |
of correct nodes under |
the sender never pushes |
and its reply another |
sender never pushes more |
correct nodes under different |
well for important classes |
the managed environment would |
priorities are used to |
nodes under different scenarios |
for important classes of |
never pushes more unacknowledged |
managed environment would require |
are used to differentiate |
under different scenarios where |
important classes of applications |
pushes more unacknowledged data |
environment would require any |
used to differentiate types |
different scenarios where opportunistic |
more unacknowledged data into |
would require any special |
to differentiate types of |
scenarios where opportunistic nodes |
but he also cites |
unacknowledged data into the |
require any special architectural |
differentiate types of rpcs |
where opportunistic nodes compromise |
he also cites significant |
data into the network |
any special architectural features |
types of rpcs to |
opportunistic nodes compromise the |
also cites significant limits |
into the network than |
qsm is implemented much |
nodes compromise the system |
of rpcs to improve |
the network than the |
as vogels sees it |
rpcs to improve performance |
is implemented much like |
network than the receiver |
implemented much like any |
than the receiver is |
we therefore studied how |
the architecture is so |
the receiver is capable |
therefore studied how the |
architecture is so centered |
receiver is capable of |
studied how the download |
is so centered on |
is capable of holding |
how the download and |
so centered on document |
the download and contribution |
centered on document exchange |
the system is coded |
download and contribution rates |
or those which would |
system is coded in |
is coded in c |
those which would cause |
and contribution rates of |
the size of the |
which would cause an |
and at its core |
contribution rates of correct |
size of the fluctuating |
would cause an interactive |
at its core is |
rates of correct nodes |
of the fluctuating window |
cause an interactive client |
its core is so |
of correct nodes are |
the fluctuating window at |
an interactive client to |
core is so simple |
correct nodes are affected |
fluctuating window at the |
interactive client to block |
nodes are affected under |
window at the sender |
are affected under these |
that many features taken |
at the sender is |
affected under these conditions |
many features taken for |
the sender is bounded |
are given high priority |
features taken for granted |
sender is bounded by |
taken for granted in |
is bounded by the |
opportunistic nodes may contribute |
for granted in object |
bounded by the size |
nodes may contribute with |
rpcs for background activities |
by the size of |
may contribute with some |
the size of the |
oriented systems are fundamentally |
contribute with some data |
size of the buffer |
systems are fundamentally lacking |
with some data in |
of the buffer at |
such as writing back |
some data in an |
the buffer at the |
as writing back files |
examples include dynamic object |
data in an attempt |
buffer at the receiver |
writing back files to |
include dynamic object creation |
in an attempt to |
back files to the |
dynamic object creation and |
an attempt to disguise |
files to the server |
object creation and garbage |
attempt to disguise their |
creation and garbage collection |
to disguise their opportunistic |
disguise their opportunistic behavior |
to interface to the |
interface to the native |
to the native windows |
the native windows asynchronous |
native windows asynchronous i |
the quantity of inflight |
dynamically created object references |
are performed at low |
quantity of inflight unacknowledged |
we considered different rates |
performed at low priority |
of inflight unacknowledged data |
considered different rates of |
and a variety of |
inflight unacknowledged data has |
different rates of contribution |
a variety of reliability |
unacknowledged data has to |
rates of contribution for |
variety of reliability and |
so that they do |
data has to be |
and is accessible from |
is accessible from any |
of reliability and transactional |
that they do not |
has to be extremely |
of contribution for opportunistic |
reliability and transactional mechanisms |
they do not slow |
to be extremely high |
contribution for opportunistic nodes |
do not slow down |
not slow down high |
windows understands qsm to |
be extremely high for |
understands qsm to be |
extremely high for the |
qsm to be the |
high for the flow |
to be the handler |
for the flow to |
be the handler for |
the flow to saturate |
the handler for operations |
flow to saturate the |
handler for operations on |
to saturate the network |
for operations on new |
operations on new kind |
both perspectives can t |
on new kind of |
shows the priority levels |
perspectives can t be |
new kind of event |
the priority levels for |
since the size of |
can t be correct |
kind of event stream |
priority levels for different |
the size of the |
levels for different types |
size of the receiver |
it s easy to |
for different types of |
of the receiver window |
s easy to see |
an application can obtain |
different types of rpcs |
the receiver window limits |
easy to see how |
application can obtain handles |
receiver window limits the |
to see how this |
can obtain handles from |
window limits the sending |
assigning priorities to rpcs |
see how this situation |
obtain handles from these |
handles from these qsm |
priorities to rpcs allows |
how this situation arose |
limits the sending envelope |
to rpcs allows mfs |
rpcs allows mfs to |
allows mfs to adapt |
mfs to adapt to |
it plays a major |
web services are the |
to adapt to bandwidth |
plays a major role |
and can then invoke |
services are the most |
adapt to bandwidth variation |
a major role in |
can then invoke methods |
are the most recent |
to bandwidth variation in |
major role in determining |
then invoke methods on |
the most recent in |
bandwidth variation in a |
role in determining tcp |
invoke methods on those |
most recent in a |
variation in a straightforward |
methods on those handles |
recent in a long |
in a straightforward way |
on those handles to |
in a long series |
those handles to send |
a long series of |
the default receiver buffer |
handles to send events |
long series of object |
default receiver buffer sizes |
series of object oriented |
receiver buffer sizes in |
of object oriented interoperability |
all rpcs complete quickly |
buffer sizes in many |
object oriented interoperability platforms |
incoming messages are delivered |
sizes in many standard |
presents the average and |
messages are delivered application |
with or without priorities |
in many standard tcp |
the average and minimum |
and mixes ideas from |
are delivered application requests |
average and minimum download |
mixes ideas from corba |
ip implementations are in |
and minimum download factors |
implementations are in the |
minimum download factors among |
are in the range |
download factors among all |
in the range of |
factors among all correct |
the range of tens |
among all correct nodes |
range of tens of |
all correct nodes under |
of tens of kilobytes |
correct nodes under different |
nodes under different configurations |
while exploiting xml and |
o event representing a |
exploiting xml and other |
the stream rate was |
event representing a received |
and consequently inadequate receiver |
xml and other web |
stream rate was fixed |
representing a received packet |
consequently inadequate receiver buffering |
rate was fixed at |
a received packet is |
inadequate receiver buffering is |
received packet is retrieved |
receiver buffering is the |
packet is retrieved for |
developers using popular middleware |
buffering is the first |
is retrieved for a |
using popular middleware platforms |
is the first hurdle |
retrieved for a given |
corresponding rpc types fetch |
popular middleware platforms can |
the first hurdle faced |
for a given socket |
rpc types fetch attributes |
middleware platforms can transform |
and all correct nodes |
first hurdle faced by |
platforms can transform a |
all correct nodes had |
hurdle faced by most |
the socket is drained |
correct nodes had a |
can transform a program |
faced by most practical |
callbacks fetch file data |
socket is drained to |
nodes had a maximum |
transform a program object |
by most practical deployments |
is drained to minimize |
had a maximum upload |
a program object into |
directory contents write back |
drained to minimize the |
contents write back directory |
program object into a |
a maximum upload factor |
a natural solution is |
to minimize the probability |
write back directory and |
object into a web |
maximum upload factor of |
natural solution is to |
minimize the probability of |
back directory and metadata |
into a web services |
solution is to increase |
the probability of loss |
directory and metadata updates |
a web services object |
and metadata updates write |
is to increase the |
metadata updates write back |
to increase the size |
updates write back shared |
or access a remote |
several aspects of the |
increase the size of |
write back shared files |
access a remote ws |
aspects of the architecture |
the size of the |
back shared files write |
a remote ws object |
of the architecture are |
size of the receiver |
shared files write back |
the architecture are noteworthy |
of the receiver buffers |
at the touch of |
architecture are noteworthy because |
files write back unshared |
the touch of a |
are noteworthy because of |
write back unshared files |
touch of a button |
noteworthy because of their |
back unshared files prefetch |
because of their performance |
unshared files prefetch file |
in many cases the |
of their performance implications |
files prefetch file data |
performance leaves something to |
many cases the receiving |
we ran experiments with |
prefetch file data section |
leaves something to be |
cases the receiving end |
something to be desired |
qsm assigns priorities to |
assigns priorities to different |
priorities to different types |
host may not have |
to different types of |
but computers and networks |
may not have the |
different types of i |
computers and networks have |
not have the spare |
and networks have become |
have the spare memory |
networks have become astonishingly |
nodes and increasing percentages |
the spare memory capacity |
have become astonishingly fast |
and increasing percentages of |
spare memory capacity to |
increasing percentages of opportunistic |
memory capacity to buffer |
percentages of opportunistic nodes |
the basic idea is |
capacity to buffer the |
major application providers are |
of opportunistic nodes in |
independently and may compete |
to buffer the entire |
application providers are planning |
basic idea is that |
opportunistic nodes in the |
buffer the entire bandwidth |
providers are planning to |
idea is that when |
nodes in the system |
are planning to offer |
is that when an |
by making writes asynchronous |
planning to offer ws |
delay product of the |
that when an i |
to offer ws interfaces |
product of the long |
update logging pushes read |
offer ws interfaces to |
ws interfaces to their |
interfaces to their products |
write contention into the |
so it makes perfect |
the need for larger |
we retrieve all events |
contention into the future |
it makes perfect sense |
need for larger buffers |
retrieve all events from |
makes perfect sense that |
to occur at the |
all events from the |
for larger buffers is |
perfect sense that the |
occur at the next |
events from the i |
larger buffers is orthogonal |
sense that the marketing |
at the next log |
buffers is orthogonal to |
that the marketing community |
the next log flush |
is orthogonal to the |
we vary the percentage |
the marketing community would |
orthogonal to the flow |
determine the type of |
the type of each |
marketing community would feel |
the designers of little |
to the flow control |
vary the percentage of |
and then place it |
designers of little work |
the flow control mechanisms |
community would feel that |
the percentage of opportunistic |
then place it in |
of little work incorporated |
flow control mechanisms used |
would feel that finally |
percentage of opportunistic nodes |
place it in an |
little work incorporated a |
work incorporated a low |
it in an appropriate |
control mechanisms used within |
in an appropriate priority |
they ve reached the |
mechanisms used within tcp |
level priority mechanism at |
ve reached the promised |
an appropriate priority queue |
priority mechanism at the |
we can observe that |
reached the promised land |
ip and impacts all |
can observe that the |
mechanism at the ip |
and impacts all variants |
at the ip packet |
observe that the download |
the ip packet level |
the system processes queued |
ip packet level to |
impacts all variants equally |
that the download factors |
system processes queued events |
packet level to further |
the download factors of |
processes queued events in |
level to further reduce |
download factors of correct |
queued events in priority |
has an understandable emphasis |
to further reduce interference |
factors of correct nodes |
events in priority order |
an understandable emphasis on |
further reduce interference between |
understandable emphasis on facts |
reduce interference between writeback |
emphasis on facts on |
of correct nodes decreases |
on facts on the |
interference between writeback traffic |
facts on the ground |
between writeback traffic and |
on the ground and |
fec fec encoders are |
the ground and the |
correct nodes decreases since |
ground and the vogels |
fec encoders are typically |
by prioritizing incoming i |
nodes decreases since the |
and the vogels essay |
writeback traffic and other |
encoders are typically parameterized |
decreases since the aggregated |
the vogels essay reflects |
traffic and other network |
are typically parameterized with |
since the aggregated upload |
vogels essay reflects the |
and other network traffic |
typically parameterized with an |
the aggregated upload capacity |
essay reflects the realities |
other network traffic sent |
aggregated upload capacity in |
reflects the realities of |
network traffic sent by |
upload capacity in the |
the realities of an |
traffic sent by the |
and by prioritizing control |
realities of an architecture |
capacity in the system |
sent by the client |
by prioritizing control packets |
of an architecture focused |
tuple for each outgoing |
prioritizing control packets over |
in the system becomes |
an architecture focused at |
for each outgoing sequence |
control packets over data |
architecture focused at its |
each outgoing sequence of |
packets over data we |
focused at its core |
outgoing sequence of r |
over data we reduce |
at its core on |
sequence of r data |
data we reduce delays |
its core on using |
of r data packets |
we reduce delays in |
core on using document |
reduce delays in reacting |
on using document exchange |
delays in reacting to |
a total of r |
priority levels for mfs |
levels for mfs rpcs |
in reacting to packet |
using document exchange to |
reacting to packet loss |
document exchange to access |
to packet loss or |
symbolic names are given |
exchange to access backend |
c data and error |
packet loss or other |
avg download factor min |
names are given for |
to access backend servers |
data and error correction |
loss or other control |
download factor min download |
are given for the |
and error correction packets |
factor min download factor |
given for the priority |
error correction packets are |
this core has been |
for the priority levels |
correction packets are sent |
we will see that |
core has been extended |
packets are sent over |
listed from highest to |
has been extended with |
will see that this |
are sent over the |
from highest to lowest |
highest to lowest priority |
see that this slashes |
sent over the channel |
been extended with such |
that this slashes system |
extended with such mechanisms |
the third column gives |
with such mechanisms as |
third column gives the |
such mechanisms as rpc |
column gives the section |
mechanisms as rpc and |
gives the section in |
redundancy information cannot be |
as rpc and asynchronous |
the section in which |
information cannot be generated |
the pros and cons |
rpc and asynchronous messaging |
section in which the |
cannot be generated and |
pros and cons of |
in which the corresponding |
be generated and sent |
and cons of using |
which the corresponding rpc |
generated and sent until |
cons of using threads |
the corresponding rpc types |
and sent until all |
of using threads in |
corresponding rpc types are |
sent until all r |
using threads in eventoriented |
rpc types are described |
until all r data |
threads in eventoriented systems |
in eventoriented systems are |
all r data packets |
types are described in |
eventoriented systems are hotly |
systems are hotly debated |
a variety of roll |
r data packets are |
are described in detail |
data packets are available |
packets are available for |
are available for sending |
forward and rendezvous options |
threads turned out to |
turned out to be |
out to be a |
to be a bad |
be a bad idea |
but the primary usage |
the latency of packet |
the primary usage case |
latency of packet recovery |
primary usage case remains |
of packet recovery is |
although we used threads |
usage case remains that |
packet recovery is determined |
we used threads rather |
case remains that of |
asynchronous writeback though it |
recovery is determined by |
used threads rather casually |
remains that of a |
writeback though it reduces |
is determined by the |
threads rather casually in |
that of a client |
though it reduces bandwidth |
determined by the rate |
rather casually in the |
of a client sending |
it reduces bandwidth consumption |
by the rate at |
casually in the first |
a client sending documents |
the rate at which |
update logging is fundamentally |
client sending documents to |
in the first year |
rate at which the |
logging is fundamentally unsuitable |
sending documents to a |
the first year of |
at which the sender |
is fundamentally unsuitable for |
documents to a back |
first year of our |
year of our effort |
fundamentally unsuitable for use |
which the sender transmits |
unsuitable for use at |
the sender transmits data |
end service in a |
for use at high |
that version of the |
service in a client |
use at high bandwidth |
version of the system |
of the system was |
generating error correction packets |
the system was annoyingly |
error correction packets from |
system was annoyingly process |
since it imposes a |
was annoyingly process requests |
correction packets from less |
it imposes a delay |
annoyingly process requests incoming |
packets from less than |
imposes a delay on |
process requests incoming control |
the assumption is that |
from less than r |
a delay on transmitting |
less than r data |
assumption is that the |
than r data packets |
delay on transmitting updates |
r data packets at |
is that the application |
requests incoming control outgoing |
on transmitting updates to |
data packets at the |
that the application can |
incoming control outgoing control |
transmitting updates to the |
control outgoing control outgoing |
the application can tolerate |
packets at the sender |
updates to the server |
at the sender is |
application can tolerate substantial |
outgoing control outgoing data |
the sender is not |
can tolerate substantial delay |
control outgoing data feed |
systems using update logging |
sender is not a |
tolerate substantial delay before |
outgoing data feed sink |
using update logging must |
is not a viable |
substantial delay before a |
data feed sink limit |
update logging must therefore |
not a viable option |
delay before a response |
feed sink limit sending |
logging must therefore switch |
a viable option even |
before a response arrives |
sink limit sending rate |
must therefore switch to |
viable option even though |
limit sending rate limit |
therefore switch to a |
option even though the |
and mechanisms capable of |
switch to a synchronous |
sending rate limit concurrency |
even though the data |
mechanisms capable of introducing |
to a synchronous writes |
rate limit concurrency limit |
though the data rate |
capable of introducing delays |
a synchronous writes when |
limit concurrency limit window |
the data rate in |
of introducing delays are |
synchronous writes when bandwidth |
concurrency limit window size |
data rate in this |
introducing delays are scattered |
writes when bandwidth is |
limit window size figure |
rate in this channel |
delays are scattered throughout |
when bandwidth is high |
in this channel is |
are scattered throughout the |
this channel is low |
scattered throughout the architecture |
with a threshold controlling |
in a pull protocol |
a threshold controlling switches |
a pull protocol a |
threshold controlling switches between |
the more basic assumption |
controlling switches between the |
more basic assumption is |
switches between the two |
between the two modes |
basic assumption is that |
assumption is that it |
is that it all |
that it all boils |
registers the intent to |
it all boils down |
the mode switch also |
the intent to send |
all boils down to |
mode switch also changes |
intent to send with |
boils down to moving |
switch also changes the |
to send with a |
down to moving documents |
also changes the semantics |
send with a sink |
to moving documents around |
changes the semantics of |
moving documents around whereas |
with a sink that |
documents around whereas the |
the semantics of the |
around whereas the most |
a sink that may |
semantics of the file |
whereas the most basic |
h a b c |
sink that may be |
of the file system |
the most basic assumption |
a b c d |
that may be controlled |
most basic assumption of |
b c d x |
may be controlled by |
and the developers of |
basic assumption of a |
c d x x |
be controlled by a |
the developers of coda |
assumption of a distributed |
d x x e |
controlled by a policy |
developers of coda have |
of a distributed object |
x x e f |
by a policy limiting |
of coda have noted |
a distributed object system |
x e f g |
a policy limiting the |
coda have noted that |
distributed object system is |
e f g h |
policy limiting the send |
have noted that undetected |
object system is that |
f g h x |
limiting the send rate |
noted that undetected mode |
system is that the |
g h x x |
that undetected mode changes |
is that the world |
h x x a |
undetected mode changes can |
that the world consists |
x x a c |
mode changes can surprise |
the world consists of |
when the sink is |
x a c b |
changes can surprise the |
world consists of programs |
the sink is ready |
a c b e |
can surprise the user |
consists of programs and |
sink is ready to |
c b e d |
surprise the user in |
of programs and data |
is ready to send |
b e d a |
the user in undesirable |
user in undesirable ways |
active and passive objects |
it issues an upcall |
the gist of vogel |
gist of vogel s |
of vogel s essay |
app elements of the |
vogel s essay is |
elements of the protocol |
g g x x |
s essay is that |
of the protocol stack |
g x x f |
essay is that even |
the protocol stack f |
x x f h |
is that even with |
x f h x |
that even with all |
f h x x |
even with all the |
h x x b |
such as cache inconsistencies |
with all the contemplated |
as cache inconsistencies arising |
all the contemplated extensions |
cache inconsistencies arising due |
o events according to |
inconsistencies arising due to |
events according to priorities |
arising due to unexpectedly |
according to priorities incoming |
web services are deeply |
due to unexpectedly delayed |
to priorities incoming data |
services are deeply mismatched |
to unexpectedly delayed writes |
of opportunistic nodes figure |
priorities incoming data policy |
are deeply mismatched with |
incoming data policy get |
deeply mismatched with distributed |
data policy get messages |
rather than relying on |
mismatched with distributed object |
policy get messages pre |
than relying on a |
minimum and average download |
with distributed object computing |
relying on a modal |
and average download factors |
on a modal adaptation |
average download factors across |
a modal adaptation scheme |
download factors across all |
the dilemma underlying the |
modal adaptation scheme incorporating |
factors across all correct |
o events process timer |
dilemma underlying the debate |
adaptation scheme incorporating a |
underlying the debate is |
events process timer events |
the debate is that |
scheme incorporating a transition |
separate encoding for odd |
across all correct nodes |
process timer events register |
debate is that the |
incorporating a transition to |
encoding for odd and |
all correct nodes when |
timer events register to |
is that the platforms |
a transition to update |
for odd and even |
correct nodes when opportunistic |
events register to send |
that the platforms one |
transition to update logging |
odd and even packets |
nodes when opportunistic nodes |
register to send app |
the platforms one uses |
to update logging when |
and even packets could |
when opportunistic nodes are |
to send app app |
send app app f |
update logging when bandwidth |
even packets could be |
opportunistic nodes are present |
platforms one uses to |
logging when bandwidth is |
packets could be operating |
one uses to create |
when bandwidth is low |
each curve corresponds to |
uses to create wscompatible |
could be operating at |
curve corresponds to a |
to create wscompatible objects |
be operating at near |
corresponds to a different |
create wscompatible objects impose |
operating at near full |
mfs uses a modeless |
to a different contribution |
wscompatible objects impose no |
one can think of |
uses a modeless asynchronous |
a different contribution rate |
at near full capacity |
objects impose no such |
can think of qsm |
a modeless asynchronous writeback |
different contribution rate used |
near full capacity with |
impose no such restrictions |
think of qsm as |
modeless asynchronous writeback mechanism |
contribution rate used by |
full capacity with data |
of qsm as a |
rate used by opportunistic |
qsm as a collection |
there is nothing in |
capacity with data from |
used by opportunistic nodes |
which is active at |
as a collection of |
is nothing in j |
with data from other |
is active at all |
a collection of protocol |
data from other senders |
active at all bandwidth |
collection of protocol stacks |
at all bandwidth levels |
of protocol stacks in |
protocol stacks in which |
net that warns a |
stacks in which components |
fec is also very |
that warns a user |
in which components act |
just as with update |
as with update logging |
warns a user that |
which components act as |
is also very susceptible |
a user that an |
when an application performs |
also very susceptible to |
components act as both |
user that an intended |
an application performs an |
very susceptible to bursty |
act as both feeds |
that an intended use |
application performs an operation |
susceptible to bursty losses |
as both feeds and |
an intended use of |
performs an operation that |
both feeds and as |
feeds and as sinks |
an operation that changes |
intended use of the |
operation that changes a |
use of the architecture |
that changes a file |
of the architecture may |
the overall structure is |
the architecture may be |
overall structure is of |
architecture may be inappropriate |
structure is of a |
such as a write |
is of a forest |
as a write or |
of a forest of |
a write or metadata |
a forest of trees |
write or metadata update |
much of the excitement |
of the excitement reflects |
the excitement reflects the |
excitement reflects the realization |
reflects the realization that |
the realization that with |
realization that with web |
that with web services |
interoperability really is easier |
create directory and so |
o was to reduce |
directory and so on |
developers have long struggled |
is a standard encoding |
was to reduce staleness |
have long struggled with |
to reduce staleness by |
a standard encoding technique |
reduce staleness by postponing |
avg upload factor min |
long struggled with program |
standard encoding technique used |
staleness by postponing the |
upload factor min upload |
encoding technique used to |
the update is then |
factor min upload factor |
by postponing the creation |
technique used to combat |
update is then passed |
postponing the creation of |
used to combat bursty |
is then passed to |
program interconnection and integration |
the creation of control |
to combat bursty loss |
then passed to the |
creation of control messages |
passed to the writeback |
of control messages until |
to the writeback subsystem |
and it is natural |
control messages until the |
where error correction packets |
it is natural to |
which sends it to |
error correction packets are |
is natural to applaud |
messages until the time |
sends it to the |
correction packets are generated |
natural to applaud a |
until the time when |
it to the server |
packets are generated from |
to applaud a widely |
the time when transmission |
to the server when |
are generated from alternate |
applaud a widely adopted |
time when transmission is |
the server when there |
generated from alternate disjoint |
a widely adopted advance |
when transmission is actually |
server when there is |
from alternate disjoint sub |
transmission is actually about |
when there is sufficient |
is actually about to |
like it or not |
there is sufficient bandwidth |
actually about to take |
about to take place |
streams of data rather |
of data rather than |
web services are becoming |
data rather than from |
services are becoming a |
asynchronous writeback therefore only |
rather than from consecutive |
are becoming a de |
writeback therefore only delays |
than from consecutive packets |
therefore only delays updates |
only delays updates when |
delays updates when there |
facto standard for everything |
updates when there is |
time information is more |
when there is foreground |
information is more accurate |
there is foreground traffic |
that s not all |
with an interleave index |
an interleave index of |
and this makes qsm |
when bandwidth is high |
this makes qsm more |
makes qsm more stable |
based direct sales systems |
direct sales systems are |
sales systems are turning |
the encoder would create |
systems are turning to |
the performance of asynchronous |
an unintended benefit is |
encoder would create correction |
are turning to the |
performance of asynchronous writeback |
unintended benefit is that |
would create correction packets |
turning to the ws |
of asynchronous writeback should |
benefit is that the |
create correction packets separately |
to the ws architecture |
asynchronous writeback should be |
is that the pull |
correction packets separately from |
the ws architecture as |
writeback should be comparable |
that the pull architecture |
packets separately from three |
ws architecture as a |
should be comparable to |
the pull architecture slashes |
separately from three disjoint |
architecture as a means |
be comparable to purely |
pull architecture slashes buffering |
from three disjoint sub |
as a means of |
comparable to purely synchronous |
architecture slashes buffering and |
a means of enlarging |
to purely synchronous writes |
slashes buffering and memory |
means of enlarging their |
buffering and memory overheads |
of enlarging their markets |
the first containing data |
but when bandwidth is |
first containing data packets |
when bandwidth is insufficient |
containing data packets numbered |
as we shall demonstrate |
asynchronous writes will improve |
writes will improve the |
com has developed a |
will improve the performance |
has developed a web |
improve the performance non |
turns out to have |
out to have an |
to have an enormous |
have an enormous impact |
access library whereby third |
an enormous impact on |
enormous impact on performance |
party application developers can |
application developers can access |
developers can access their |
can access their datacenters |
in qsm each element |
access their datacenters from |
qsm each element of |
their datacenters from a |
an implementation without priorities |
each element of a |
datacenters from a diversity |
implementation without priorities will |
element of a protocol |
from a diversity of |
without priorities will result |
of a protocol stack |
a diversity of end |
priorities will result in |
a protocol stack acts |
will result in the |
protocol stack acts as |
result in the completion |
stack acts as a |
in the completion times |
acts as a feed |
the completion times for |
as a feed that |
completion times for all |
a feed that has |
an application could order |
times for all rpcs |
feed that has data |
application could order thus |
for all rpcs increasing |
that has data to |
has data to send |
all rpcs increasing uniformly |
could order thus supplies |
order thus supplies directly |
thus supplies directly from |
supplies directly from amazon |
or a sink that |
a sink that can |
when priorities are used |
sink that can send |
that can send it |
query the fulfillment system |
the fulfillment system to |
the second with data |
a backlog of low |
fulfillment system to track |
second with data packets |
system to track order |
with data packets numbered |
to track order status |
track order status or |
order status or billing |
priority rpcs will accumulate |
status or billing data |
while the time taken |
and many play both |
the time taken for |
many play both roles |
both the vendor and |
time taken for high |
the vendor and the |
vendor and the application |
and the application developer |
the application developer benefit |
priority rpcs to complete |
rpcs to complete will |
to complete will increase |
complete will increase more |
will increase more gradually |
com enlarges its client |
enlarges its client base |
of opportunistic nodes figure |
our design is based |
while the developer avoids |
design is based on |
the developer avoids duplicating |
is based on the |
developer avoids duplicating an |
minimum and average upload |
based on the assumption |
avoids duplicating an enormous |
rather than creating a |
and average upload factors |
on the assumption that |
duplicating an enormous technology |
than creating a message |
average upload factors across |
the assumption that when |
an enormous technology investment |
creating a message and |
upload factors across all |
assumption that when bandwidth |
a message and handing |
factors across all correct |
that when bandwidth is |
message and handing it |
across all correct nodes |
when bandwidth is low |
and handing it down |
all correct nodes when |
handing it down to |
correct nodes when opportunistic |
web service components will |
it down to the |
nodes when opportunistic nodes |
an assignment of differentiated |
service components will play |
down to the sink |
when opportunistic nodes are |
assignment of differentiated priorities |
components will play a |
opportunistic nodes are present |
of differentiated priorities will |
will play a critical |
a feed registers the |
differentiated priorities will improve |
play a critical role |
feed registers the intent |
each curve corresponds to |
priorities will improve the |
a critical role in |
registers the intent to |
curve corresponds to a |
will improve the response |
critical role in tremendous |
and the third with |
the intent to send |
corresponds to a different |
improve the response times |
role in tremendous numbers |
the third with data |
intent to send a |
to a different contribution |
the response times for |
in tremendous numbers of |
third with data packets |
to send a message |
a different contribution rate |
response times for interactive |
tremendous numbers of end |
with data packets numbered |
send a message with |
different contribution rate used |
times for interactive tasks |
a message with the |
contribution rate used by |
message with the sink |
rate used by opportunistic |
used by opportunistic nodes |
the challenge is to |
challenge is to make |
insufficient to provide all |
if a task which |
is to make such |
to provide all nodes |
the message can be |
a task which predominantly |
to make such systems |
provide all nodes with |
message can be created |
task which predominantly performs |
make such systems work |
all nodes with all |
can be created at |
which predominantly performs reads |
such systems work reliably |
nodes with all data |
be created at this |
predominantly performs reads executes |
created at this time |
performs reads executes in |
at this time and |
outages that plague human |
reads executes in parallel |
this time and buffered |
that plague human users |
executes in parallel to |
the extent of the |
time and buffered in |
plague human users of |
in parallel to a |
extent of the impact |
and buffered in the |
human users of web |
parallel to a task |
of the impact may |
buffered in the feed |
users of web browsers |
to a task which |
the impact may be |
of web browsers don |
a task which performs |
but the creation may |
web browsers don t |
impact may be surprising |
task which performs many |
the creation may also |
browsers don t cause |
which performs many writes |
creation may also be |
don t cause much |
may also be postponed |
t cause much harm |
also be postponed until |
be postponed until the |
postponed until the time |
until the time when |
the time when the |
time when the sink |
when the sink polls |
performance drops by as |
the first task will |
outages could disrupt a |
the sink polls the |
drops by as much |
first task will receive |
could disrupt a computer |
sink polls the feed |
by as much as |
task will receive a |
polls the feed for |
will receive a higher |
the feed for messages |
receive a higher share |
feed for messages to |
a higher share of |
for messages to transmit |
higher share of the |
computer pathway buried deep |
share of the bandwidth |
pathway buried deep within |
buried deep within an |
deep within an application |
the sink determines its |
within an application on |
interleaving adds burst tolerance |
sink determines its readiness |
an application on which |
presents the average and |
adds burst tolerance to |
determines its readiness to |
application on which an |
the average and minimum |
many applications have patterns |
burst tolerance to fec |
its readiness to send |
tolerance to fec but |
average and minimum upload |
to fec but exacerbates |
on which an enterprise |
fec but exacerbates its |
applications have patterns of |
but exacerbates its sensitivity |
readiness to send based |
exacerbates its sensitivity to |
and minimum upload factors |
its sensitivity to sending |
which an enterprise has |
to send based on |
have patterns of interactive |
minimum upload factors among |
sensitivity to sending rate |
an enterprise has become |
send based on a |
patterns of interactive file |
upload factors among all |
to sending rate with |
enterprise has become dependent |
based on a control |
of interactive file access |
factors among all correct |
sending rate with an |
on a control policy |
interactive file access involving |
it is too easy |
rate with an interleave |
among all correct nodes |
file access involving both |
is too easy to |
with an interleave index |
access involving both reads |
too easy to dismiss |
an interleave index of |
involving both reads and |
easy to dismiss these |
interleave index of i |
both reads and writes |
to dismiss these concerns |
axis we vary the |
index of i and |
dismiss these concerns by |
we vary the percentage |
of i and an |
these concerns by arguing |
vary the percentage of |
i and an encoding |
when the socket at |
concerns by arguing that |
the percentage of opportunistic |
and an encoding rate |
the socket at the |
compiling source files involves |
by arguing that the |
percentage of opportunistic nodes |
an encoding rate of |
socket at the root |
source files involves interspersed |
arguing that the web |
at the root of |
files involves interspersed reads |
and on the y |
that the web is |
the root of the |
involves interspersed reads and |
interspersed reads and writes |
root of the tree |
the web is extremely |
axis we present the |
of the tree is |
web is extremely scalable |
but does not issue |
the tree is ready |
we present the upload |
is extremely scalable and |
does not issue concurrent |
tree is ready for |
present the upload factors |
extremely scalable and robust |
not issue concurrent rpcs |
is ready for transmission |
the upload factors of |
upload factors of nodes |
issue concurrent rpcs frequently |
the sender would have |
but this ignores the |
which can vary up |
messages will be recursively |
this ignores the way |
can vary up to |
sender would have to |
will be recursively pulled |
ignores the way we |
such an application will |
would have to wait |
be recursively pulled from |
the way we use |
an application will have |
have to wait for |
recursively pulled from the |
way we use the |
application will have improved |
to wait for i |
pulled from the tree |
it is interesting to |
will have improved read |
we use the web |
from the tree of |
is interesting to note |
have improved read performance |
the tree of protocol |
interesting to note that |
improved read performance when |
tree of protocol stack |
to note that the |
a human can deal |
read performance when there |
of protocol stack components |
note that the average |
human can deal with |
performance when there is |
that the average upload |
can deal with the |
when there is contention |
the average upload factor |
deal with the many |
there is contention with |
average upload factor among |
with the many error |
is contention with other |
packets before sending any |
the many error conditions |
upload factor among correct |
contention with other applications |
feeds that no longer |
before sending any redundancy |
many error conditions the |
factor among correct nodes |
that no longer have |
sending any redundancy information |
error conditions the web |
among correct nodes initially |
but will correspondingly be |
no longer have data |
conditions the web exposes |
correct nodes initially increases |
these two obstacles to |
longer have data to |
will correspondingly be penalised |
two obstacles to using |
have data to send |
correspondingly be penalised on |
and then starts falling |
obstacles to using fec |
handling those conditions in |
data to send are |
be penalised on writes |
then starts falling when |
to using fec in |
those conditions in a |
to send are automatically |
starts falling when the |
using fec in time |
conditions in a seamless |
this does not match |
send are automatically deregistered |
falling when the percentage |
does not match our |
when the percentage of |
not match our design |
sensitive settings rate sensitivity |
the percentage of opportunistic |
automated manner is an |
match our design goal |
settings rate sensitivity and |
percentage of opportunistic nodes |
manner is an entirely |
our design goal of |
rate sensitivity and burst |
of opportunistic nodes increases |
is an entirely different |
design goal of having |
sensitivity and burst susceptibility |
opportunistic nodes increases significantly |
an entirely different challenge |
sharing and priority i |
goal of having interactive |
and burst susceptibility are |
this behavior can be |
burst susceptibility are interlinked |
behavior can be explained |
susceptibility are interlinked through |
can be explained by |
are interlinked through the |
be explained by the |
when we take what |
explained by the fact |
we take what was |
by the fact that |
take what was once |
interlinked through the tuning |
what was once a |
through the tuning knobs |
read applications obtain a |
was once a batch |
and prone to oscillatory |
applications obtain a larger |
once a batch service |
correct nodes start contributing |
prone to oscillatory throughput |
obtain a larger share |
an interleave of i |
a batch service or |
nodes start contributing more |
to oscillatory throughput when |
a larger share of |
interleave of i and |
batch service or a |
start contributing more to |
oscillatory throughput when scaled |
larger share of bandwidth |
of i and a |
service or a web |
contributing more to compensate |
throughput when scaled up |
i and a rate |
or a web site |
more to compensate for |
we have implemented two |
and a rate of |
a web site and |
to compensate for the |
have implemented two solutions |
when we decided to |
web site and transform |
compensate for the lack |
implemented two solutions to |
we decided to take |
site and transform it |
for the lack of |
two solutions to this |
decided to take control |
and transform it into |
the lack of data |
solutions to this problem |
provides tolerance to a |
transform it into a |
lack of data provided |
to take control over |
tolerance to a burst |
it into a web |
of data provided by |
based on making writes |
take control over event |
to a burst of |
into a web service |
data provided by a |
on making writes asynchronous |
control over event processing |
a burst of up |
provided by a small |
over event processing order |
there is no way |
by a small percentage |
burst of up to |
is no way to |
a small percentage of |
of up to c |
no way to enforce |
small percentage of opportunistic |
up to c i |
way to enforce appropriate |
we also eliminated multithreading |
percentage of opportunistic nodes |
used in several existing |
to c i consecutive |
to enforce appropriate patterns |
in several existing systems |
c i consecutive packets |
enforce appropriate patterns of |
several existing systems and |
once the effect of |
appropriate patterns of use |
existing systems and incorporated |
the effect of opportunistic |
grained scheduling eliminated convoy |
systems and incorporated in |
effect of opportunistic nodes |
scheduling eliminated convoy behavior |
the burst tolerance of |
eliminated convoy behavior and |
burst tolerance of an |
convoy behavior and oscillatory |
of opportunistic nodes becomes |
what s to stop |
and incorporated in mfs |
tolerance of an fec |
behavior and oscillatory throughput |
opportunistic nodes becomes significant |
s to stop a |
incorporated in mfs for |
of an fec code |
and oscillatory throughput of |
the system collapses and |
oscillatory throughput of the |
an fec code can |
to stop a web |
in mfs for the |
system collapses and correct |
throughput of the sort |
fec code can be |
stop a web client |
mfs for the purposes |
collapses and correct nodes |
of the sort that |
code can be changed |
a web client from |
for the purposes of |
and correct nodes are |
the sort that can |
can be changed by |
web client from trying |
the purposes of comparison |
correct nodes are not |
sort that can disrupt |
be changed by modulating |
that can disrupt reliable |
nodes are not able |
can disrupt reliable multicast |
changed by modulating either |
client from trying to |
are not able to |
disrupt reliable multicast systems |
by modulating either the |
from trying to download |
not able to keep |
reliable multicast systems when |
which is new to |
is new to mfs |
trying to download amazon |
able to keep contributing |
multicast systems when they |
modulating either the c |
systems when they run |
an alternative approach is |
either the c or |
another important point to |
when they run at |
alternative approach is to |
com s entire catalog |
the c or the |
important point to note |
they run at high |
approach is to retain |
c or the i |
point to note is |
run at high data |
is to retain synchronous |
or the i parameters |
to note is that |
the only answer is |
to retain synchronous writes |
at high data rates |
note is that the |
high data rates on |
is that the minimum |
increasing c enhances burst |
data rates on a |
but assign priorities according |
c enhances burst tolerance |
that the minimum upload |
rates on a large |
assign priorities according to |
enhances burst tolerance at |
the minimum upload factor |
on a large scale |
priorities according to some |
burst tolerance at the |
minimum upload factor does |
according to some notion |
tolerance at the cost |
one might argue that |
to some notion of |
upload factor does not |
at the cost of |
might argue that none |
some notion of relative |
factor does not follow |
the cost of network |
the last aspect relates |
argue that none of |
notion of relative importance |
does not follow a |
cost of network and |
last aspect relates to |
that none of these |
of relative importance of |
not follow a clearly |
of network and encoding |
aspect relates to the |
none of these uses |
relative importance of processes |
follow a clearly defined |
a clearly defined pattern |
relates to the creation |
of these uses are |
network and encoding overhead |
to the creation of |
these uses are what |
making it hard to |
the creation of new |
uses are what the |
potentially worsening the packet |
creation of new messages |
existing operating systems and |
it hard to estimate |
operating systems and applications |
worsening the packet loss |
are what the architecture |
hard to estimate the |
systems and applications generally |
the packet loss experienced |
particularly by qsm itself |
what the architecture is |
to estimate the minimum |
and applications generally do |
packet loss experienced and |
the architecture is intended |
estimate the minimum contribution |
applications generally do not |
loss experienced and reducing |
readers who have implemented |
architecture is intended to |
the minimum contribution of |
generally do not provide |
experienced and reducing throughput |
who have implemented multicast |
is intended to support |
minimum contribution of correct |
do not provide this |
have implemented multicast protocols |
contribution of correct nodes |
not provide this information |
implemented multicast protocols will |
not so many years |
multicast protocols will know |
of correct nodes under |
so many years ago |
increasing i trades off |
protocols will know that |
correct nodes under compromised |
so we have not |
i trades off recovery |
will know that most |
nodes under compromised scenarios |
we have not investigated |
trades off recovery latency |
know that most existing |
have not investigated it |
off recovery latency for |
that most existing systems |
server architectures faltered over |
not investigated it further |
recovery latency for better |
most existing systems are |
by applying thresholds to |
architectures faltered over precisely |
latency for better burst |
existing systems are push |
applying thresholds to punish |
faltered over precisely this |
the cache manager s |
for better burst tolerance |
thresholds to punish opportunistic |
over precisely this type |
cache manager s writeback |
better burst tolerance without |
to punish opportunistic nodes |
precisely this type of |
manager s writeback thread |
some layer initiates a |
burst tolerance without adding |
this type of situation |
s writeback thread divides |
layer initiates a new |
correct nodes may also |
tolerance without adding overhead |
writeback thread divides updates |
initiates a new message |
nodes may also be |
without adding overhead as |
thread divides updates into |
a new message at |
server technologies of the |
adding overhead as mentioned |
divides updates into metadata |
may also be unfairly |
new message at will |
updates into metadata operations |
also be unfairly penalized |
for higher values of |
higher values of i |
and lower layers then |
such as directory modifications |
lower layers then buffer |
the encoder has to |
as directory modifications and |
auditing protocol our idea |
layers then buffer that |
s were widely seen |
directory modifications and file |
protocol our idea for |
encoder has to wait |
then buffer that message |
were widely seen as |
modifications and file status |
our idea for auditing |
has to wait for |
buffer that message until |
widely seen as a |
and file status changes |
idea for auditing the |
to wait for more |
that message until it |
seen as a kind |
for auditing the described |
wait for more data |
message until it can |
as a kind of |
auditing the described live |
for more data packets |
until it can be |
it can be sent |
the two types of |
streaming system against opportunistic |
two types of operations |
more data packets to |
a kind of panacea |
system against opportunistic behavior |
types of operations are |
data packets to be |
this makes sense under |
against opportunistic behavior is |
of operations are queued |
packets to be transmitted |
makes sense under the |
opportunistic behavior is motivated |
a silver bullet that |
operations are queued and |
to be transmitted before |
sense under the assumption |
behavior is motivated by |
silver bullet that would |
are queued and replayed |
be transmitted before it |
under the assumption that |
is motivated by the |
bullet that would slay |
queued and replayed to |
transmitted before it can |
the assumption that senders |
motivated by the graphs |
that would slay evil |
and replayed to the |
before it can send |
assumption that senders often |
by the graphs presented |
would slay evil mainframe |
replayed to the server |
it can send error |
that senders often generate |
the graphs presented in |
slay evil mainframe architectures |
to the server separately |
can send error correction |
senders often generate bursts |
graphs presented in the |
send error correction packets |
often generate bursts of |
so that a metadata |
enterprises fell over themselves |
presented in the previous |
generate bursts of packets |
that a metadata rpc |
fell over themselves in |
in the previous section |
a metadata rpc can |
over themselves in a |
metadata rpc can proceed |
themselves in a kind |
we propose to employ |
rpc can proceed in |
once the fec encoding |
in a kind of |
propose to employ auditing |
can proceed in parallel |
the communication subsystem can |
the fec encoding is |
a kind of technology |
to employ auditing to |
proceed in parallel with |
communication subsystem can smooth |
fec encoding is parameterized |
kind of technology gold |
employ auditing to ensure |
in parallel with a |
subsystem can smooth the |
encoding is parameterized with |
of technology gold rush |
auditing to ensure that |
parallel with a file |
can smooth the traffic |
is parameterized with a |
to ensure that all |
with a file writeback |
only to discover that |
parameterized with a rate |
ensure that all nodes |
smooth the traffic flow |
to discover that the |
when an rpc from |
that all nodes in |
the traffic flow and |
with a rate and |
discover that the technology |
an rpc from a |
all nodes in the |
traffic flow and keep |
a rate and an |
that the technology had |
rpc from a particular |
nodes in the system |
flow and keep the |
rate and an interleave |
the technology had been |
from a particular queue |
in the system contribute |
and keep the network |
and an interleave to |
technology had been oversold |
a particular queue completes |
the system contribute more |
keep the network interface |
an interleave to tolerate |
system contribute more than |
the network interface busy |
interleave to tolerate a |
contribute more than a |
we say that the |
to tolerate a certain |
more than a particular |
say that the update |
the total cost of |
tolerate a certain burst |
than a particular specified |
one consequence is that |
that the update has |
total cost of ownership |
a certain burst length |
a particular specified threshold |
consequence is that messages |
the update has been |
cost of ownership for |
certain burst length b |
is that messages can |
update has been committed |
of ownership for clientserver |
that messages can linger |
has been committed at |
ownership for clientserver systems |
messages can linger for |
been committed at the |
we illustrate the potential |
for clientserver systems remains |
can linger for a |
committed at the server |
illustrate the potential benefit |
clientserver systems remains excessively |
linger for a while |
the potential benefit from |
systems remains excessively high |
for a while before |
potential benefit from using |
the next update is |
a while before they |
benefit from using auditing |
next update is then |
while before they are |
from using auditing in |
update is then dequeued |
is then dequeued and |
before they are sent |
using auditing in a |
the number of system |
auditing in a system |
number of system administrators |
in a system where |
of system administrators remains |
system administrators remains roughly |
not only does this |
administrators remains roughly proportional |
only does this increase |
remains roughly proportional to |
does this increase memory |
roughly proportional to the |
of the nodes are |
this increase memory consumption |
update logging an asynchronous |
the nodes are correct |
proportional to the size |
to tolerate a burst |
logging an asynchronous rpc |
nodes are correct and |
to the size of |
tolerate a burst of |
but if a message |
an asynchronous rpc for |
the size of the |
a burst of length |
if a message contains |
asynchronous rpc for it |
size of the deployment |
a message contains current |
rpc for it is |
message contains current state |
for it is initiated |
the latter do not |
contains current state information |
latter do not upload |
do not upload any |
not upload any data |
a list like these |
separating the small update |
list like these comments |
the small update logging |
that state may be |
like these comments might |
state may be stale |
these comments might have |
may be stale by |
comments might have seemed |
be stale by the |
which is implemented in |
might have seemed like |
stale by the time |
is implemented in some |
no punishment was applied |
have seemed like an |
by the time it |
implemented in some mobile |
punishment was applied in |
seemed like an indictment |
the time it s |
in some mobile file |
was applied in an |
like an indictment of |
time it s sent |
some mobile file sys |
all losses occurring in |
applied in an attempt |
an indictment of the |
losses occurring in bursts |
in an attempt to |
indictment of the technology |
occurring in bursts of |
metadata rpcs from file |
in contrast to this |
an attempt to simulate |
in bursts of size |
rpcs from file writes |
bursts of size less |
attempt to simulate a |
of size less than |
contrast to this usual |
size less than or |
because we lacked solutions |
to simulate a system |
from file writes allows |
to this usual approach |
less than or equal |
simulate a system with |
than or equal to |
a system with no |
or equal to b |
qsm implements a pull |
system with no auditing |
file writes allows remote |
equal to b are |
implements a pull architecture |
writes allows remote clients |
to b are recovered |
allows remote clients to |
b are recovered with |
we know how to |
remote clients to see |
are recovered with the |
know how to implement |
clients to see statems |
recovered with the same |
how to implement management |
evaluation evaluation of qsm |
with the same latency |
to implement management tools |
evaluation of qsm could |
the same latency and |
auditing is enabled and |
implement management tools and |
of qsm could pursue |
same latency and this |
is enabled and opportunistic |
management tools and fault |
qsm could pursue many |
latency and this latency |
enabled and opportunistic nodes |
could pursue many directions |
and this latency depends |
and opportunistic nodes start |
this latency depends on |
opportunistic nodes start to |
latency depends on the |
nodes start to be |
how to replicate data |
depends on the i |
start to be expelled |
to replicate data and |
costs of the domain |
on the i parameter |
to be expelled from |
replicate data and functionality |
of the domain crossing |
be expelled from the |
the domain crossing between |
expelled from the system |
domain crossing between the |
and how to achieve |
from the system for |
crossing between the application |
how to achieve high |
we d like to |
between the application and |
the system for low |
to achieve high ava |
d like to parameterize |
the application and qsm |
system for low contribution |
achieve high ava ilability |
like to parameterize the |
tus changes to files |
to parameterize the encoding |
protocol design and scalability |
parameterize the encoding to |
changes to files without |
we ve had decades |
the minimum upload factor |
the encoding to tolerate |
and interactions between protocol |
encoding to tolerate a |
minimum upload factor for |
to tolerate a maximum |
ve had decades of |
interactions between protocol properties |
to files without having |
upload factor for nodes |
tolerate a maximum burst |
had decades of experience |
between protocol properties and |
files without having to |
factor for nodes to |
a maximum burst length |
decades of experience with |
protocol properties and the |
without having to wait |
for nodes to stay |
maximum burst length and |
of experience with large |
properties and the managed |
having to wait for |
nodes to stay in |
burst length and then |
and the managed framework |
to wait for intervening |
to stay in the |
length and then have |
scale system monitoring and |
wait for intervening writequirement |
stay in the system |
and then have recovery |
system monitoring and control |
here we focus on |
for intervening writequirement that |
in the system was |
then have recovery latency |
we focus on the |
intervening writequirement that processes |
the system was set |
system was set to |
focus on the latter |
and are beginning to |
writequirement that processes wait |
have recovery latency depend |
are beginning to understand |
that processes wait for |
recovery latency depend on |
beginning to understand how |
our goal is to |
processes wait for writes |
latency depend on the |
to understand how to |
goal is to arrive |
depend on the actual |
understand how to build |
rather than sending an |
is to arrive at |
on the actual burstiness |
how to build solutions |
than sending an back |
to arrive at a |
the actual burstiness of |
to build solutions on |
sending an back traffic |
arrive at a deep |
actual burstiness of the |
build solutions on an |
burstiness of the loss |
at a deep understanding |
solutions on an internet |
a deep understanding of |
a similar motivation underlies |
deep understanding of the |
similar motivation underlies the |
understanding of the performance |
motivation underlies the cache |
of the performance limits |
on an internet scale |
at the same time |
underlies the cache consisupdate |
the performance limits of |
the cache consisupdate to |
performance limits of qsm |
cache consisupdate to the |
we would like the |
limits of qsm when |
consisupdate to the server |
would like the encoding |
of qsm when operating |
to the server as |
like the encoding to |
qsm when operating at |
the server as soon |
the encoding to have |
peer file sharing turns |
when operating at high |
server as soon as |
encoding to have a |
file sharing turns out |
operating at high data |
as soon as a |
without auditing with auditing |
to have a constant |
sharing turns out to |
at high data rates |
soon as a file |
have a constant rate |
turns out to be |
high data rates with |
as a file is |
a constant rate for |
out to be illegal |
data rates with large |
a file is closed |
constant rate for network |
rates with large numbers |
rate for network provisioning |
with large numbers of |
and it doesn t |
for network provisioning and |
large numbers of overlapping |
the cache manager tency |
it doesn t work |
network provisioning and stability |
numbers of overlapping groups |
cache manager tency scheme |
doesn t work all |
manager tency scheme for |
t work all that |
tency scheme for high |
work all that well |
for reasons of brevity |
scheme for high read |
an fec scheme is |
fec scheme is required |
scheme is required where |
is required where latency |
required where latency of |
where latency of recovery |
we are unable to |
latency of recovery degrades |
write contention environments we |
are unable to undertake |
of recovery degrades gracefully |
contention environments we logs |
unable to undertake a |
recovery degrades gracefully as |
but spawned a new |
environments we logs the |
to undertake a detailed |
degrades gracefully as losses |
spawned a new generation |
we logs the update |
undertake a detailed analysis |
gracefully as losses get |
a new generation of |
logs the update and |
a detailed analysis of |
as losses get burstier |
new generation of technologies |
the update and periodically |
detailed analysis of oscillatory |
download factor of correct |
generation of technologies based |
even as the encoding |
analysis of oscillatory phenomena |
factor of correct nodes |
update and periodically flushes |
of technologies based on |
as the encoding overhead |
of oscillatory phenomena in |
of correct nodes during |
and periodically flushes logged |
technologies based on distributed |
the encoding overhead stays |
oscillatory phenomena in this |
correct nodes during a |
periodically flushes logged updates |
based on distributed hash |
encoding overhead stays constant |
phenomena in this paper |
flushes logged updates to |
on distributed hash tables |
logged updates to the |
distributed hash tables and |
updates to the describe |
hash tables and epidemic |
to the describe in |
also called convoys and |
tables and epidemic communication |
second streaming session with |
the describe in section |
called convoys and broadcast |
and epidemic communication protocols |
convoys and broadcast storms |
these offer remarkably stable |
these plague many multicast |
plague many multicast and |
many multicast and pub |
the chief complexity in |
scalable tools for dealing |
auditing is enabled in |
chief complexity in implementing |
tools for dealing with |
is enabled in the |
complexity in implementing asynchronous |
for dealing with enormous |
enabled in the last |
in implementing asynchronous writeserver |
dealing with enormous numbers |
with enormous numbers of |
enormous numbers of components |
numbers of components scattered |
event prioritization eliminated such |
of components scattered over |
prioritization eliminated such problems |
components scattered over a |
these systems enable logging |
eliminated such problems in |
scattered over a network |
systems enable logging when |
such problems in the |
enable logging when bandwidth |
we present the minimum |
problems in the configurations |
logging when bandwidth is |
not all the stories |
in the configurations tested |
when bandwidth is low |
all the stories are |
the configurations tested by |
average and maximum download |
the stories are positive |
configurations tested by our |
and maximum download factors |
to improve read performance |
tested by our experiments |
maximum download factors across |
improve read performance and |
download factors across correct |
read performance and reduce |
factors across correct nodes |
performance and reduce write |
end flow control x |
the web services community |
across correct nodes varying |
and reduce write traffic |
flow control x appliance |
web services community decided |
correct nodes varying along |
reduce write traffic by |
control x appliance appliance |
services community decided not |
write traffic by aggregat |
x appliance appliance end |
community decided not to |
on varying numbers of |
decided not to adapt |
varying numbers of nodes |
back lies in resolving |
not to adapt the |
to adapt the corba |
lies in resolving dependencies |
adapt the corba fault |
in resolving dependencies between |
as observed in this |
resolving dependencies between metadata |
we ll find that |
observed in this particular |
tolerance standard for their |
ll find that the |
dependencies between metadata operations |
in this particular example |
standard for their setting |
find that the experiments |
between metadata operations ing |
that the experiments have |
metadata operations ing updates |
auditing has the potential |
the experiments have a |
operations ing updates to |
this is a specification |
has the potential to |
experiments have a pattern |
ing updates to the |
is a specification i |
the potential to improve |
updates to the same |
a specification i know |
in scenario after scenario |
to the same file |
potential to improve the |
specification i know well |
the same file in |
to improve the quality |
split flow control fig |
same file in the |
improve the quality of |
the performance of qsm |
file in the log |
the quality of streamed |
it was based on |
performance of qsm is |
was based on the |
of qsm is ultimately |
in the log before |
quality of streamed sessions |
based on the virtual |
qsm is ultimately limited |
the log before they |
of streamed sessions significantly |
on the virtual synchrony |
is ultimately limited by |
log before they are |
flow control options in |
and at low cost |
ultimately limited by overheads |
before they are transmitted |
the virtual synchrony model |
control options in maelstrom |
limited by overheads associated |
virtual synchrony model colleagues |
one important concern is |
by overheads associated with |
and updates to the |
synchrony model colleagues of |
important concern is that |
overheads associated with memory |
updates to the same |
to the same file |
concern is that if |
associated with memory management |
model colleagues of mine |
is that if the |
with memory management in |
colleagues of mine and |
that if the specified |
lan mtu lambda jumbo |
of mine and i |
memory management in the |
if the specified threshold |
mtu lambda jumbo mtu |
a file may be |
file may be created |
management in the managed |
the specified threshold is |
lambda jumbo mtu recipe |
mine and i developed |
in the managed environment |
specified threshold is too |
jumbo mtu recipe list |
and i developed in |
update logging separates communication |
threshold is too high |
i developed in work |
logging separates communication with |
developed in work on |
separates communication with the |
the more memory in |
more memory in use |
more opportunistic nodes may |
communication with the server |
in work on the |
opportunistic nodes may be |
with the server into |
the higher the overheads |
nodes may be caught |
work on the isis |
the server into modified |
higher the overheads of |
on the isis toolkit |
server into modified and |
the overheads of the |
but correct nodes may |
into modified and closed |
overheads of the memory |
correct nodes may also |
the standard hasn t |
of the memory management |
nodes may also be |
standard hasn t been |
the memory management subsystem |
may also be unfairly |
and the length of |
hasn t been a |
memory management subsystem and |
also be unfairly punished |
the length of the |
t been a commercial |
management subsystem and the |
length of the metadata |
been a commercial success |
subsystem and the more |
of the metadata queue |
and the more cpu |
the metadata queue may |
the more cpu time |
no correct nodes were |
metadata queue may two |
more cpu time it |
cpu time it consumes |
correct nodes were mistakenly |
queue may two distinct |
but the corba standard |
nodes were mistakenly expelled |
may two distinct streams |
the corba standard limits |
were mistakenly expelled from |
leaving less time for |
corba standard limits itself |
updates to files and |
less time for qsm |
mistakenly expelled from the |
standard limits itself to |
to files and directories |
time for qsm to |
expelled from the system |
limits itself to lock |
for qsm to run |
and all be enough |
all be enough to |
state replication of a |
be enough to mean |
replication of a deterministic |
these aren t just |
enough to mean that |
of a deterministic server |
aren t just garbage |
to mean that the |
t just garbage collection |
auditing components we now |
mean that the file |
just garbage collection costs |
components we now give |
perhaps the issue is |
that the file update |
we now give some |
the issue is the |
the file update would |
now give some additional |
every aspect of memory |
issue is the way |
file update would be |
give some additional details |
aspect of memory management |
is the way the |
update would be initiated |
some additional details of |
of memory management gets |
the way the technology |
would be initiated first |
additional details of the |
memory management gets expensive |
way the technology was |
details of the auditing |
the technology was used |
of the auditing architecture |
and the costs grow |
focusing upon two aspects |
not the technology itself |
these two types of |
the costs grow linearly |
two types of communication |
costs grow linearly in |
types of communication are |
grow linearly in the |
of communication are scheduled |
linearly in the amount |
communication are scheduled this |
in the amount of |
repair packets are injected |
are scheduled this case |
used in other ways |
the amount of memory |
amount of memory in |
scheduled this case the |
packets are injected into |
collecting accountable information about |
of memory in use |
this case the file |
are injected into stream |
has been quite successful |
accountable information about the |
when qsm runs flat |
injected into stream transparently |
case the file update |
information about the download |
into stream transparently iv |
the file update must |
about the download and |
file update must wait |
the download and upload |
isis runs the new |
download and upload factors |
cpu cycles are a |
runs the new york |
m aelstrom d esign |
and upload factors of |
cycles are a precious |
the new york stock |
aelstrom d esign and |
upload factors of individual |
are a precious commodity |
new york stock exchange |
a file may be |
d esign and i |
factors of individual nodes |
york stock exchange quote |
esign and i mplementation |
of individual nodes in |
stock exchange quote and |
and i mplementation we |
individual nodes in the |
test activity gc grep |
i mplementation we describe |
activity gc grep compile |
nodes in the system |
exchange quote and trade |
minimizing the memory footprint |
mplementation we describe the |
gc grep compile grep |
quote and trade reporting |
the memory footprint turns |
we describe the maelstrom |
grep compile grep write |
and trade reporting system |
memory footprint turns out |
describe the maelstrom appliance |
compile grep write read |
footprint turns out to |
the maelstrom appliance as |
grep write read compile |
turns out to be |
establishing and applying the |
a role it has |
write read compile read |
maelstrom appliance as a |
out to be the |
and applying the best |
role it has played |
read compile read write |
appliance as a single |
to be the key |
applying the best threshold |
it has played since |
compile read write gw |
as a single machine |
be the key to |
the best threshold at |
read write gw rc |
a single machine later |
the key to high |
best threshold at any |
write gw rc rw |
key to high performance |
threshold at any given |
gw rc rw synchronous |
at any given time |
we will show how |
rc rw synchronous uniform |
all results reported here |
will show how more |
any given time during |
show how more machines |
results reported here come |
how more machines can |
given time during execution |
rw synchronous uniform priorities |
reported here come from |
more machines can be |
here come from experiments |
machines can be added |
we employ two types |
come from experiments on |
can be added to |
employ two types of |
from experiments on a |
be added to the |
two types of components |
and the french air |
added to the appliance |
types of components to |
the french air traffic |
to the appliance to |
of components to perform |
french air traffic control |
the appliance to balance |
components to perform these |
air traffic control system |
appliance to balance encoding |
to perform these two |
to balance encoding load |
perform these two roles |
balance encoding load and |
and the us naval |
encoding load and scale |
the us naval aegis |
load and scale to |
local and global auditors |
us naval aegis warship |
and scale to multiple |
naval aegis warship communication |
scale to multiple gigabits |
aegis warship communication system |
local auditors are executed |
to multiple gigabits per |
auditors are executed on |
multiple gigabits per second |
are executed on the |
gigabits per second of |
cluster of pentium iii |
to name just a |
executed on the nodes |
per second of traffic |
name just a few |
on the nodes participating |
the nodes participating in |
nodes participating in the |
participating in the system |
leslie lamport s paxos |
lamport s paxos protocol |
and therefore cannot be |
s paxos protocol has |
basic mechanism the basic |
therefore cannot be trusted |
paxos protocol has been |
mechanism the basic operation |
protocol has been used |
the basic operation of |
if a node is |
has been used to |
basic operation of maelstrom |
a node is malicious |
been used to build |
operation of maelstrom is |
used to build file |
of maelstrom is shown |
it might report false |
to build file systems |
maelstrom is shown in |
might report false data |
build file systems and |
is shown in figure |
file systems and scalable |
systems and scalable clusters |
global auditors are trusted |
auditors are trusted components |
are trusted components that |
trusted components that run |
components that run on |
that run on dedicated |
none of these examples |
run on dedicated external |
of these examples uses |
on dedicated external nodes |
these examples uses lock |
it intercepts outgoing data |
connected into a single |
intercepts outgoing data packets |
there can be just |
into a single broadcast |
outgoing data packets and |
can be just one |
step replication of the |
a single broadcast domain |
data packets and routes |
be just one or |
replication of the type |
single broadcast domain using |
packets and routes them |
just one or a |
of the type mandated |
broadcast domain using a |
and routes them to |
one or a few |
the type mandated by |
domain using a switched |
routes them to the |
or a few global |
type mandated by corba |
them to the destination |
a few global auditors |
to the destination data |
the destination data center |
every technology has its |
we describe their roles |
technology has its successes |
describe their roles and |
generating and injecting fec |
has its successes and |
their roles and interactions |
and injecting fec repair |
its successes and failures |
roles and interactions in |
injecting fec repair packets |
and interactions in detail |
fec repair packets into |
interactions in detail below |
repair packets into the |
packets into the stream |
into the stream in |
the stream in their |
stream in their wake |
nodes run windows server |
a repair packet consists |
repair packet consists of |
packet consists of a |
consists of a recipe |
of a recipe list |
a recipe list of |
these technologies could take |
recipe list of data |
technologies could take the |
list of data packet |
could take the web |
of data packet identifiers |
take the web services |
local auditors each node |
data packet identifiers and |
the web services architecture |
auditors each node n |
packet identifiers and fec |
web services architecture to |
each node n runs |
identifiers and fec information |
services architecture to a |
node n runs a |
and fec information generated |
architecture to a new |
n runs a local |
fec information generated from |
to a new level |
runs a local auditor |
information generated from these |
generated from these packets |
which interacts with other |
interacts with other local |
in the example in |
with other local auditors |
the example in figure |
other local auditors and |
doing so could greatly |
local auditors and has |
so could greatly enlarge |
auditors and has two |
could greatly enlarge the |
and has two main |
has two main roles |
this information is a |
greatly enlarge the web |
information is a simple |
enlarge the web services |
is a simple xor |
the web services market |
publish n s data |
n s data exchange |
s data exchange history |
the size of the |
size of the xor |
so what s the |
of the xor is |
what s the bottom |
n s local auditor |
our benchmark is an |
s the bottom line |
the xor is equal |
s local auditor periodically |
benchmark is an nary |
xor is equal to |
local auditor periodically compiles |
are web services distributed |
is equal to the |
auditor periodically compiles and |
web services distributed objects |
equal to the mtu |
periodically compiles and distributes |
to the mtu of |
compiles and distributes the |
the mtu of the |
and distributes the history |
mtu of the data |
of course they are |
linked to the qsm |
of the data center |
distributes the history of |
to the qsm library |
the data center network |
the history of packets |
the marketing people are |
history of packets exchanged |
marketing people are listening |
of packets exchanged by |
people are listening to |
running in the same |
packets exchanged by n |
and to avoid fragmentation |
are listening to customers |
in the same process |
to avoid fragmentation of |
avoid fragmentation of repair |
fragmentation of repair packets |
of repair packets we |
and they want distributed |
repair packets we require |
they want distributed objects |
packets we require that |
we require that the |
require that the mtu |
it queries the local |
that the mtu of |
queries the local streaming |
but vogels is right |
the local streaming application |
the mtu of the |
local streaming application running |
mtu of the long |
streaming application running on |
application running on n |
running on n for |
on n for the |
haul network be set |
n for the set |
network be set to |
for the set of |
be set to a |
the set of packets |
set to a slightly |
set of packets it |
to a slightly larger |
of packets it sent |
a slightly larger value |
packets it sent and |
it sent and received |
sent and received using |
and received using the |
received using the streaming |
this requirement is easily |
using the streaming protocol |
requirement is easily satisfied |
it s time for |
the streaming protocol in |
is easily satisfied in |
s time for the |
streaming protocol in the |
easily satisfied in practice |
time for the web |
protocol in the most |
for the web services |
in the most recent |
the web services community |
the most recent time |
since gigabit links very |
web services community to |
most recent time interval |
gigabit links very often |
services community to come |
links very often use |
community to come to |
very often use jumbo |
at the maximum possible |
to come to grips |
often use jumbo frames |
the maximum possible rate |
come to grips with |
use jumbo frames of |
to grips with the |
jumbo frames of up |
grips with the needs |
frames of up to |
the local auditor signs |
with the needs of |
local auditor signs and |
the needs of their |
auditor signs and publishes |
needs of their customer |
signs and publishes the |
of their customer base |
the majority of the |
and publishes the collected |
majority of the figures |
publishes the collected history |
of the figures include |
one can justify solutions |
the collected history to |
can justify solutions that |
collected history to an |
justify solutions that make |
history to an assigned |
to an assigned subset |
an assigned subset of |
assigned subset of its |
subset of its neighboring |
of its neighboring nodes |
from whom other auditors |
whom other auditors may |
other auditors may obtain |
auditors may obtain it |
while lan networks have |
lan networks have standard |
of the customers happy |
networks have standard mtus |
this level of indirection |
the customers happy but |
have standard mtus of |
level of indirection is |
customers happy but leave |
of indirection is used |
but these intervals are |
indirection is used to |
these intervals are sometimes |
is used to prevent |
intervals are sometimes so |
used to prevent nodes |
are sometimes so small |
to prevent nodes from |
sometimes so small that |
prevent nodes from masking |
so small that they |
nodes from masking their |
small that they may |
from masking their real |
that they may not |
at the receiving data |
masking their real upload |
they may not always |
the receiving data center |
their real upload and |
may not always be |
real upload and download |
a solution that tries |
not always be visible |
upload and download factors |
the appliance examines incoming |
solution that tries to |
and download factors by |
appliance examines incoming repair |
that tries to do |
download factors by presenting |
examines incoming repair packets |
tries to do better |
growing cost of memory |
factors by presenting different |
incoming repair packets and |
to do better will |
cost of memory allocation |
by presenting different information |
repair packets and uses |
do better will probably |
presenting different information to |
packets and uses them |
better will probably overreach |
different information to different |
and uses them to |
information to different auditors |
uses them to recover |
them to recover missing |
to recover missing data |
recover missing data packets |
but you can t |
you can t get |
audit n s neighbors |
can t get there |
n s neighbors histories |
t get there if |
get there if you |
there if you close |
the data packet is |
n s local auditor |
if you close your |
data packet is injected |
s local auditor periodically |
you close your eyes |
packet is injected transparently |
local auditor periodically audits |
close your eyes to |
is injected transparently into |
auditor periodically audits the |
your eyes to the |
injected transparently into the |
periodically audits the published |
eyes to the way |
transparently into the stream |
audits the published histories |
to the way the |
into the stream to |
the published histories of |
the way the customers |
the stream to the |
published histories of the |
way the customers are |
stream to the receiving |
histories of the nodes |
the customers are likely |
to the receiving end |
of the nodes with |
customers are likely to |
the nodes with whom |
are likely to use |
nodes with whom n |
likely to use the |
with whom n exchanges |
to use the technology |
recovered data packets will |
whom n exchanges packets |
data packets will typically |
packets will typically arrive |
will typically arrive out |
will the web services |
the web services community |
web services community have |
if node n exchanges |
services community have the |
node n exchanges packets |
order at the end |
community have the wisdom |
n exchanges packets with |
have the wisdom to |
exchanges packets with nodes |
the wisdom to tackle |
packets with nodes p |
wisdom to tackle the |
to tackle the tough |
tackle the tough issues |
and hence it is |
q and r in |
hence it is vital |
the tough issues before |
and r in the |
it is vital that |
tough issues before circumstances |
r in the livestreaming |
throughput as a function |
is vital that packets |
issues before circumstances force |
in the livestreaming protocol |
as a function of |
vital that packets be |
before circumstances force it |
a function of the |
n s local auditor |
circumstances force it upon |
that packets be recovered |
function of the number |
s local auditor compares |
force it upon them |
packets be recovered by |
of the number of |
local auditor compares these |
be recovered by the |
the number of nodes |
auditor compares these three |
recovered by the appliance |
compares these three nodes |
by the appliance extremely |
these three nodes histories |
the appliance extremely quickly |
three nodes histories with |
appliance extremely quickly to |
nodes histories with n |
extremely quickly to avoid |
histories with n s |
a fellow of the |
quickly to avoid triggering |
with n s own |
fellow of the acm |
to avoid triggering mechanisms |
n s own history |
avoid triggering mechanisms in |
triggering mechanisms in commodity |
mechanisms in commodity stacks |
in commodity stacks that |
commodity stacks that interpret |
this involves ensuring that |
stacks that interpret out |
order arrival as congestion |
arrival as congestion in |
as congestion in the |
congestion in the network |
the amount of data |
amount of data sent |
of data sent by |
data sent by these |
sent by these nodes |
by these nodes satisfies |
flow control while relaying |
these nodes satisfies the |
control while relaying tcp |
nodes satisfies the defined |
satisfies the defined minimum |
the defined minimum threshold |
defined minimum threshold for |
minimum threshold for the |
threshold for the system |
maelstrom has two flow |
has two flow control |
two flow control modes |
processor utilization as a |
utilization as a function |
and has worked on |
as a function of |
has worked on reliability |
a function of the |
the set of packets |
worked on reliability and |
set of packets they |
on reliability and scalability |
of packets they claim |
reliability and scalability issues |
packets they claim to |
function of the multicast |
illustrates these two modes |
they claim to have |
and scalability issues in |
of the multicast rate |
claim to have sent |
scalability issues in distributed |
to have sent to |
issues in distributed systems |
have sent to and |
in distributed systems since |
sent to and received |
distributed systems since starting |
to and received from |
systems since starting his |
and received from node |
since starting his research |
received from node n |
starting his research career |
from node n corresponds |
node n corresponds to |
n corresponds to the |
corresponds to the set |
to the set of |
he is the author |
the set of packets |
is the author of |
set of packets n |
the author of many |
the appliance treats tcp |
of packets n claims |
author of many articles |
packets n claims to |
of many articles on |
n claims to have |
many articles on the |
ip packets as conventional |
claims to have respectively |
articles on the subject |
packets as conventional ip |
to have respectively received |
as conventional ip packets |
have respectively received from |
conventional ip packets and |
respectively received from and |
ip packets and routes |
received from and sent |
packets and routes them |
from and sent to |
and routes them through |
and sent to them |
routes them through without |
them through without modification |
if the first check |
the first check comparison |
first check comparison fails |
control to proceed between |
to proceed between the |
proceed between the end |
the local auditor issues |
and applications will be |
memory overheads on the |
local auditor issues an |
overheads on the sender |
applications will be published |
on the sender we |
auditor issues an accusation |
the sender we begin |
will be published by |
issues an accusation against |
sender we begin by |
be published by springer |
an accusation against the |
we begin by showing |
published by springer verlag |
accusation against the node |
ip s semantics are |
begin by showing that |
by springer verlag in |
against the node to |
s semantics are not |
by showing that memory |
springer verlag in fall |
the node to a |
semantics are not modified |
showing that memory overhead |
node to a global |
that memory overhead at |
to a global auditor |
when the sending endhost |
memory overhead at the |
the sending endhost receives |
overhead at the sender |
in the second case |
at the sender is |
sending endhost receives an |
the sender is a |
endhost receives an acknowledgment |
the local auditor is |
sender is a central |
local auditor is not |
is a central to |
it can assume that |
auditor is not able |
a central to throughput |
can assume that the |
is not able to |
assume that the receiving |
not able to prove |
that the receiving end |
able to prove the |
to prove the neighbor |
prove the neighbor s |
the neighbor s misbehavior |
host successfully received the |
successfully received the message |
it instructs its local |
instructs its local streaming |
its local streaming application |
local streaming application to |
streaming application to not |
shows throughput in messages |
maelstrom functions as a |
application to not further |
functions as a passive |
web services are not |
to not further exchange |
as a passive device |
services are not distributed |
not further exchange packets |
s in experiments with |
are not distributed objects |
further exchange packets with |
snooping outgoing and incoming |
exchange packets with the |
outgoing and incoming traffic |
packets with the misbehaving |
and incoming traffic at |
with the misbehaving neighbor |
incoming traffic at the |
traffic at the data |
at the data center |
the data center s |
more complex types of |
senders multicasting to a |
data center s edge |
complex types of checks |
multicasting to a varying |
center s edge its |
types of checks may |
to a varying number |
s edge its failure |
of checks may also |
a varying number of |
edge its failure does |
checks may also be |
varying number of receivers |
its failure does not |
may also be performed |
failure does not disrupt |
also be performed to |
does not disrupt the |
be performed to address |
not disrupt the flow |
all of which belong |
performed to address other |
disrupt the flow of |
of which belong to |
to address other types |
the flow of packets |
which belong to a |
address other types of |
flow of packets between |
belong to a single |
other types of byzantine |
of packets between the |
to a single group |
types of byzantine behavior |
packets between the two |
between the two data |
the two data centers |
with a single sender |
no rate limit was |
rate limit was used |
side appliance acts as |
appliance acts as a |
acts as a tcp |
the sender has more |
sender has more work |
has more work to |
more work to do |
work to do than |
to do than the |
terminating connections and sending |
do than the receivers |
connections and sending back |
than the receivers and |
and sending back acks |
the receivers and on |
sending back acks immediately |
receivers and on our |
back acks immediately before |
and on our clusters |
acks immediately before relaying |
immediately before relaying data |
before relaying data on |
relaying data on appliance |
isn t fast enough |
t fast enough to |
fast enough to saturate |
enough to saturate the |
to saturate the network |
split mode is extremely |
mode is extremely useful |
is extremely useful when |
extremely useful when endhosts |
useful when endhosts have |
when endhosts have limited |
endhosts have limited buffering |
have limited buffering capacity |
since it allows the |
it allows the receive |
side appliance to buffer |
appliance to buffer incoming |
to buffer incoming data |
buffer incoming data over |
incoming data over the |
data over the highspeed |
over the highspeed long |
it also mitigates tcp |
we report the highest |
start effects for short |
report the highest combined |
the highest combined send |
highest combined send rate |
combined send rate that |
send rate that the |
rate that the system |
that the system could |
the system could sustain |
system could sustain without |
maelstrom has to operate |
could sustain without developing |
has to operate as |
sustain without developing backlogs |
to operate as an |
without developing backlogs at |
operate as an active |
developing backlogs at the |
as an active device |
backlogs at the senders |
inserted into the critical |
into the critical communication |
the critical communication path |
why does performance decrease |
critical communication path its |
does performance decrease with |
communication path its failure |
performance decrease with the |
path its failure disconnects |
decrease with the number |
its failure disconnects the |
with the number of |
failure disconnects the communication |
the number of receivers |
disconnects the communication path |
the communication path between |
communication path between the |
path between the two |
between the two data |
the two data centers |
let s focus on |
s focus on a |
while maelstrom respects endto |
end flow control connections |
or splits them and |
splits them and implements |
them and implements its |
and implements its own |
implements its own proxy |
proxy flow control as |
flow control as described |
control as described above |
shows that whereas receivers |
that whereas receivers are |
whereas receivers are not |
receivers are not cpu |
it is not designed |
is not designed for |
not designed for routinely |
designed for routinely congested |
for routinely congested networks |
the addition of fec |
addition of fec under |
of fec under tcp |
and loss rates in |
loss rates in this |
ip flow control allows |
rates in this experiment |
flow control allows it |
control allows it to |
allows it to steal |
it to steal bandwidth |
to steal bandwidth from |
steal bandwidth from other |
bandwidth from other competing |
from other competing flows |
other competing flows running |
competing flows running without |
flows running without fec |
running without fec in |
without fec in the |
fec in the link |
though maintaining fairness versus |
maintaining fairness versus similarly |
fairness versus similarly fec |
the sender is saturated |
and hence is the |
hence is the bottleneck |
global auditing there are |
auditing there are two |
running this test again |
there are two ways |
this test again in |
are two ways in |
test again in a |
two ways in which |
again in a profiler |
ways in which a |
in a profiler reveals |
in which a node |
friendliness with conventional tcp |
a profiler reveals that |
which a node could |
profiler reveals that the |
a node could pretend |
reveals that the percentage |
ip flows is not |
node could pretend to |
that the percentage of |
flows is not a |
could pretend to be |
the percentage of time |
is not a primary |
pretend to be sending |
percentage of time spent |
not a primary protocol |
to be sending more |
of time spent in |
a primary protocol design |
be sending more or |
time spent in qsm |
primary protocol design goal |
sending more or receiving |
spent in qsm code |
protocol design goal on |
more or receiving less |
in qsm code is |
design goal on over |
or receiving less data |
qsm code is decreasing |
receiving less data than |
less data than it |
data than it actually |
than it actually does |
aware adaptation techniques for |
adaptation techniques for mobile |
it could send different |
whereas more and more |
which are often dedicated |
could send different histories |
techniques for mobile file |
more and more time |
are often dedicated to |
send different histories to |
for mobile file systems |
and more time is |
often dedicated to specific |
different histories to each |
mobile file systems benjamin |
more time is spent |
dedicated to specific highvalue |
histories to each neighbor |
file systems benjamin atkin |
time is spent in |
to specific highvalue applications |
systems benjamin atkin kenneth |
always lying about its |
is spent in mscorwks |
benjamin atkin kenneth p |
lying about its interactions |
we see evidence for |
about its interactions with |
see evidence for this |
its interactions with other |
evidence for this assertion |
interactions with other neighbors |
birman nec laboratories america |
for this assertion in |
nec laboratories america cornell |
this assertion in the |
laboratories america cornell university |
assertion in the routine |
america cornell university atkin |
in the routine use |
n could send a |
the routine use of |
could send a history |
routine use of parallel |
send a history to |
use of parallel flows |
a history to p |
history to p pretending |
to p pretending to |
p pretending to send |
pretending to send more |
to send more data |
send more data to |
more data to q |
data to q than |
to q than it |
q than it actually |
than it actually did |
while it sends a |
and udp blast protocols |
it sends a different |
sends a different history |
a different history to |
different history to q |
history to q where |
to q where it |
edu abstract therefore react |
q where it pretends |
abstract therefore react to |
where it pretends to |
therefore react to bandwidth |
it pretends to send |
react to bandwidth variations |
pretends to send more |
to bandwidth variations in |
to send more data |
bandwidth variations in a |
send more data to |
variations in a fine |
more data to p |
data to p than |
to p than it |
p than it actually |
than it actually did |
shows that the main |
that the main culprit |
n s goal would |
the main culprit behind |
s goal would be |
main culprit behind the |
goal would be to |
culprit behind the increase |
would be to send |
life file system traffic |
behind the increase of |
be to send less |
file system traffic featuring |
the increase of overhead |
to send less data |
system traffic featuring high |
increase of overhead is |
send less data while |
traffic featuring high read |
of overhead is a |
both in commercial deployments |
less data while not |
overhead is a figure |
in commercial deployments and |
data while not being |
write wireless networks present |
commercial deployments and by |
while not being caught |
wireless networks present unusual |
deployments and by researchers |
not being caught by |
networks present unusual challenges |
and by researchers seeking |
being caught by any |
present unusual challenges for |
by researchers seeking to |
caught by any of |
unusual challenges for mobile |
researchers seeking to transfer |
by any of its |
challenges for mobile file |
the percentages of the |
any of its neighbors |
seeking to transfer large |
for mobile file contention |
percentages of the profiler |
to transfer large amounts |
of the profiler samples |
the process of publishing |
transfer large amounts of |
process of publishing a |
mafs is able to |
of publishing a node |
the profiler samples taken |
large amounts of data |
is able to achieve |
publishing a node s |
profiler samples taken from |
amounts of data over |
able to achieve improvements |
a node s history |
samples taken from qsm |
of data over high |
to achieve improvements in |
node s history to |
taken from qsm and |
achieve improvements in execusystem |
s history to a |
from qsm and clr |
improvements in execusystem clients |
history to a predefined |
qsm and clr dlls |
to a predefined set |
a predefined set of |
layered interleaving in layered |
predefined set of neighbors |
interleaving in layered interleaving |
since they are characterised |
set of neighbors ensures |
they are characterised by |
of neighbors ensures that |
an fec protocol with |
are characterised by unpredictable |
neighbors ensures that the |
fec protocol with rate |
characterised by unpredictable tion |
ensures that the node |
by unpredictable tion time |
that the node cannot |
unpredictable tion time of |
the node cannot send |
tion time of up |
node cannot send conflicting |
time of up to |
cannot send conflicting histories |
send conflicting histories to |
memory allocation and garbage |
is produced by running |
conflicting histories to different |
allocation and garbage collection |
produced by running c |
histories to different neighbors |
and garbage collection overheads |
by running c multiple |
to different neighbors undetected |
garbage collection overheads on |
running c multiple instances |
collection overheads on the |
c multiple instances of |
overheads on the sender |
multiple instances of an |
therefore avoiding this problem |
on the sender node |
a node could also |
node could also lie |
could also lie about |
the former grows by |
also lie about the |
at both low and |
lie about the set |
both low and high |
about the set of |
low and high bandwidths |
the set of packets |
fec protocol simultaneously with |
set of packets sent |
protocol simultaneously with increasing |
of packets sent to |
simultaneously with increasing interleave |
packets sent to or |
with increasing interleave indices |
sent to or received |
increasing interleave indices i |
to or received from |
or received from a |
received from a particular |
and the latter by |
from a particular neighbor |
a particular neighbor p |
the traditional approach to |
traditional approach to adapting |
approach to adapting network |
to adapting network communication |
adapting network communication to |
network communication to these |
p will be able |
communication to these conditions |
will be able to |
to these conditions is |
be able to identify |
these conditions is to |
able to identify that |
conditions is to write |
to identify that the |
is to write back |
identify that the node |
to write back file |
that the node has |
write back file updates |
the node has lied |
back file updates asynchronously |
node has lied and |
file updates asynchronously when |
has lied and will |
updates asynchronously when bandwidth |
lied and will therefore |
asynchronously when bandwidth is |
and will therefore stop |
will therefore stop exchanging |
therefore stop exchanging packets |
stop exchanging packets with |
exchanging packets with n |
given that an opportunistic |
that an opportunistic node |
an opportunistic node s |
opportunistic node s goal |
node s goal is |
s goal is to |
goal is to maximize |
this configuration is typical |
is to maximize its |
this can lead to |
configuration is typical of |
to maximize its utility |
can lead to underutilisation |
is typical of the |
lead to underutilisation of |
typical of the host |
to underutilisation of bandwidth |
it should have no |
of the host environment |
underutilisation of bandwidth and |
should have no interest |
the host environment expected |
of bandwidth and inconsistencies |
have no interest in |
host environment expected for |
bandwidth and inconsistencies between |
no interest in losing |
environment expected for our |
and inconsistencies between clients |
interest in losing data |
expected for our target |
in losing data exchange |
for our target applications |
losing data exchange partners |
we describe a new |
describe a new mobile |
a new mobile access |
new mobile access to |
mobile access to shared |
access to shared data |
to shared data is |
opportunistic nodes have no |
shared data is complicated |
nodes have no incentive |
data is complicated by |
have no incentive to |
is complicated by an |
no incentive to publish |
complicated by an unpredictable |
incentive to publish incorrect |
by an unpredictable mobile |
to publish incorrect histories |
an unpredictable mobile file |
unpredictable mobile file system |
of the overhead is |
the overhead is the |
overhead is the allocation |
is the allocation of |
local auditing ensures that |
the allocation of byte |
auditing ensures that correct |
allocation of byte arrays |
ensures that correct information |
that supports graceful degradation |
of byte arrays to |
that correct information is |
supports graceful degradation computing |
byte arrays to send |
correct information is available |
graceful degradation computing environment |
arrays to send in |
information is available regarding |
to send in the |
is available regarding the |
send in the application |
the network or a |
available regarding the set |
network or a particular |
regarding the set of |
or a particular destination |
the set of data |
a particular destination of |
three instances of an |
set of data sent |
particular destination of file |
of data sent and |
destination of file system |
data sent and received |
of file system performance |
sent and received by |
file system performance as |
and received by any |
system performance as bandwidth |
received by any node |
performance as bandwidth is |
as bandwidth is reduced |
and allows nodes to |
allows nodes to monitor |
nodes to monitor each |
to monitor each other |
as well as may |
monitor each other s |
well as may be |
the first instance with |
each other s contribution |
as may be unavailable |
first instance with interleave |
other s contribution rates |
instance with interleave i |
or the throughput may |
the throughput may be |
throughput may be substandard |
as rapid propagation of |
rapid propagation of essential |
propagation of essential file |
of essential file updates |
the second with interleave |
second with interleave i |
mafs is able to |
is able to shown |
able to shown in |
global auditors global auditors |
to shown in figure |
auditors global auditors are |
global auditors are trusted |
auditors are trusted components |
are trusted components with |
trusted components with global |
components with global membership |
with global membership knowledge |
and the third with |
the third with interleave |
third with interleave i |
this graph shows results |
who interact with one |
graph shows results from |
interact with one another |
shows results from packet |
with one another and |
one another and with |
another and with the |
and with the local |
with the local auditors |
of time is spent |
time is spent exclusively |
is spent exclusively on |
as shown in figure |
spent exclusively on copying |
exclusively on copying memory |
on copying memory in |
copying memory in the |
memory in the clr |
global auditors execute on |
auditors execute on nodes |
execute on nodes external |
on nodes external to |
nodes external to the |
external to the system |
their main roles are |
define the minimum upload |
the minimum upload threshold |
improvements in execution time |
even though we used |
fec encoding is simply |
in execution time for |
though we used our |
encoding is simply an |
global auditors periodically sample |
execution time for real |
we used our own |
is simply an xor |
auditors periodically sample the |
used our own scatter |
simply an xor of |
periodically sample the state |
life measurements of available |
an xor of the |
sample the state of |
measurements of available bandwidth |
xor of the r |
the state of the |
of available bandwidth between |
gather serialization scheme that |
of the r data |
state of the system |
available bandwidth between a |
serialization scheme that efficiently |
the r data packets |
of the system by |
bandwidth between a mobile |
scheme that efficiently uses |
r data packets hence |
the system by querying |
between a mobile host |
that efficiently uses scatter |
system by querying local |
a mobile host on |
by querying local auditors |
in layered interleaving each |
mobile host on a |
layered interleaving each data |
host on a wireless |
interleaving each data packet |
they then cooperate to |
on a wireless network |
each data packet is |
then cooperate to analyze |
data packet is included |
cooperate to analyze the |
packet is included in |
to analyze the collected |
is included in c |
included in c xors |
analyze the collected samples |
the increase in the |
and a wired host |
increase in the memory |
a wired host near |
in the memory allocation |
each of which is |
the memory allocation overhead |
wired host near the |
memory allocation overhead and |
of which is generated |
allocation overhead and the |
host near the base |
and on this basis |
which is generated at |
overhead and the activity |
near the base station |
on this basis compute |
is generated at different |
and the activity of |
this basis compute the |
generated at different interleaves |
the activity of the |
basis compute the minimum |
file system traces featuring |
at different interleaves from |
activity of the garbage |
compute the minimum upload |
system traces featuring read |
different interleaves from the |
of the garbage collector |
the minimum upload contribution |
interleaves from the original |
the garbage collector are |
minimum upload contribution threshold |
from the original data |
garbage collector are caused |
the original data stream |
collector are caused by |
as the mobile host |
different strategies may be |
are caused by the |
the mobile host moves |
strategies may be employed |
caused by the increasing |
may be employed for |
by the increasing memory |
as we shall describe |
be employed for choosing |
factors such as the |
we shall describe shortly |
such as the distance |
employed for choosing the |
the increasing memory usage |
as the distance to |
ensures that the c |
the distance to the |
for choosing the best |
that the c xors |
distance to the base |
choosing the best possible |
the c xors containing |
to the base station |
the best possible threshold |
c xors containing a |
the base station and |
xors containing a data |
base station and local |
containing a data packet |
station and local interference |
a data packet do |
and local interference cause |
data packet do not |
once thresholds are varied |
local interference cause the |
packet do not have |
interference cause the host |
reflectsan increase of the |
they are gossiped to |
increase of the average |
do not have any |
cause the host s |
are gossiped to all |
of the average number |
not have any other |
the host s network |
gossiped to all local |
to all local auditors |
have any other data |
host s network card |
the average number of |
any other data packet |
s network card to |
average number of multicasts |
who then enforce the |
other data packet in |
data packet in common |
number of multicasts pending |
then enforce the determined |
network card to switch |
of multicasts pending completion |
enforce the determined threshold |
card to switch to |
the resulting protocol effectively |
to switch to higher |
resulting protocol effectively has |
expurge nodes from the |
protocol effectively has a |
nodes from the system |
effectively has a rate |
has a rate of |
global auditors are also |
auditors are also responsible |
are also responsible for |
also responsible for verifying |
responsible for verifying accusations |
for verifying accusations issued |
verifying accusations issued by |
accusations issued by local |
issued by local auditors |
by local auditors against |
local auditors against particular |
auditors against particular nodes |
with each xor generated |
each xor generated from |
xor generated from r |
such switching causes available |
and after validating the |
switching causes available bandwidth |
generated from r data |
after validating the accusation |
causes available bandwidth to |
from r data packets |
available bandwidth to oscillate |
r data packets and |
bandwidth to oscillate distributed |
expurging misbehaving nodes from |
data packets and each |
to oscillate distributed file |
misbehaving nodes from the |
packets and each data |
oscillate distributed file systems |
nodes from the system |
and each data packet |
distributed file systems are |
a copy is kept |
each data packet included |
file systems are a |
copy is kept by |
data packet included in |
validation involves verifying that |
systems are a common |
is kept by the |
packet included in c |
involves verifying that the |
are a common feature |
kept by the sender |
included in c xors |
verifying that the accused |
a common feature of |
by the sender for |
that the accused node |
common feature of large |
the sender for possible |
the accused node s |
feature of large com |
sender for possible loss |
illustrates layered interleaving for |
accused node s history |
for possible loss recovery |
layered interleaving for a |
node s history indeed |
s history indeed indicates |
history indeed indicates that |
even when the mobile |
indeed indicates that the |
when the mobile host |
indicates that the node |
notice that memory consumption |
the mobile host is |
that the node is |
that memory consumption grows |
mobile host is stationary |
the node is sending |
memory consumption grows nearly |
node is sending less |
is sending less data |
sending less data than |
if it is to |
less data than the |
it is to enputing |
data than the current |
is to enputing environments |
than the current threshold |
times faster than the |
faster than the number |
than the number of |
since they simplify sharing |
expurging a node involves |
they simplify sharing data |
a node involves informing |
simplify sharing data between |
node involves informing the |
the number of messages |
sharing data between sure |
involves informing the nodes |
number of messages pending |
data between sure that |
informing the nodes immediate |
of messages pending acknowledgement |
between sure that clients |
the nodes immediate neighbors |
sure that clients file |
nodes immediate neighbors of |
that clients file operations |
immediate neighbors of its |
clients file operations are |
neighbors of its status |
file operations are executed |
if we freeze the |
of its status and |
we freeze the sender |
operations are executed in |
its status and forcing |
freeze the sender process |
are executed in a |
status and forcing the |
the sender process and |
executed in a timely |
and forcing the removal |
sender process and inspect |
in a timely way |
forcing the removal of |
process and inspect the |
the removal of the |
standard fec schemes can |
and inspect the contents |
removal of the node |
fec schemes can be |
inspect the contents of |
of the node from |
schemes can be made |
and can provide scalable |
the contents of the |
the node from the |
can be made resistant |
can provide scalable and |
contents of the managed |
node from the overlay |
be made resistant to |
provide scalable and highly |
of the managed heap |
from the overlay mesh |
made resistant to a |
scalable and highly available |
resistant to a certain |
and highly available file |
to a certain loss |
highly available file ac |
the number of global |
a certain loss burst |
we find that the |
number of global auditors |
certain loss burst length |
find that the number |
of global auditors may |
that the number of |
loss burst length at |
the number of objects |
global auditors may vary |
number of objects in |
burst length at the |
of objects in memory |
auditors may vary according |
objects in memory is |
length at the cost |
in memory is more |
may vary according to |
file system must adapt |
at the cost of |
memory is more than |
vary according to different |
system must adapt to |
the cost of increased |
is more than twice |
according to different parameters |
must adapt to this |
cost of increased recovery |
more than twice the |
adapt to this variation |
of increased recovery latency |
than twice the number |
such as the size |
increased recovery latency for |
twice the number of |
as the size of |
recovery latency for all |
the number of multicasts |
the size of the |
latency for all lost |
for all lost packets |
size of the system |
number of multicasts pending |
of multicasts pending acknowledgement |
including smaller bursts and |
the use of more |
smaller bursts and singleton |
use of more global |
bursts and singleton drops |
of more global auditors |
more global auditors distributes |
although some of these |
global auditors distributes the |
some of these have |
supporting mobile clients requires |
auditors distributes the load |
of these have already |
mobile clients requires coping |
layered interleaving provides graceful |
distributes the load of |
these have already been |
clients requires coping existing |
interleaving provides graceful degradation |
the load of sampling |
have already been acknowledged |
requires coping existing systems |
provides graceful degradation in |
load of sampling and |
coping existing systems tailored |
graceful degradation in the |
of sampling and improves |
existing systems tailored to |
degradation in the face |
they haven t yet |
systems tailored to low |
sampling and improves efficiency |
in the face of |
haven t yet been |
and improves efficiency in |
the face of bursty |
bandwidth clients differenwith the |
improves efficiency in reacting |
t yet been garbage |
face of bursty loss |
clients differenwith the atypical |
efficiency in reacting to |
yet been garbage collected |
of bursty loss for |
differenwith the atypical patterns |
in reacting to accusations |
bursty loss for constant |
the atypical patterns of |
reacting to accusations against |
loss for constant encoding |
atypical patterns of connectivity |
to accusations against nodes |
the growing amount of |
patterns of connectivity that |
growing amount of unacknowledged |
for constant encoding overhead |
amount of unacknowledged data |
of connectivity that characterise |
of unacknowledged data is |
constant encoding overhead singleton |
unacknowledged data is caused |
connectivity that characterise them |
global auditors are also |
encoding overhead singleton random |
data is caused by |
auditors are also perfect |
is caused by the |
overhead singleton random losses |
caused by the increase |
are also perfect candidates |
tiate between types of |
singleton random losses are |
by the increase of |
also perfect candidates to |
between types of file |
random losses are recovered |
the increase of the |
perfect candidates to perform |
types of file system |
losses are recovered as |
increase of the average |
candidates to perform membership |
of file system communication |
are recovered as quickly |
of the average time |
to perform membership tasks |
recovered as quickly as |
as quickly as possible |
perform membership tasks such |
the average time to |
so that bandwhile a |
membership tasks such as |
average time to acknowledge |
that bandwhile a desktop |
tasks such as acting |
by xors generated with |
time to acknowledge a |
bandwhile a desktop client |
such as acting as |
xors generated with an |
to acknowledge a message |
a desktop client is |
as acting as entry |
generated with an interleave |
desktop client is well |
acting as entry points |
with an interleave of |
as entry points to |
entry points to the |
points to the p |
connected to a file |
to a file server |
a file server un |
and each successive layer |
each successive layer of |
width can be devoted |
successive layer of xors |
since they are required |
can be devoted to |
layer of xors generated |
they are required to |
be devoted to important |
of xors generated at |
are required to have |
xors generated at a |
required to have full |
generated at a higher |
to have full membership |
at a higher interleave |
have full membership knowledge |
a higher interleave catches |
full membership knowledge of |
higher interleave catches larger |
membership knowledge of the |
interleave catches larger bursts |
knowledge of the system |
catches larger bursts missed |
of the system for |
this grows because of |
larger bursts missed by |
the system for performing |
bursts missed by the |
grows because of the |
system for performing their |
missed by the previous |
by the previous layer |
for performing their auditing |
a mobile client frequently |
because of the increasing |
performing their auditing roles |
the implementation of this |
of the increasing time |
mobile client frequently lacks |
the increasing time to |
implementation of this algorithm |
increasing time to circulate |
client frequently lacks the |
of this algorithm is |
global auditing monitors the |
time to circulate a |
this algorithm is simple |
to circulate a token |
auditing monitors the global |
algorithm is simple and |
monitors the global health |
circulate a token around |
is simple and shown |
the global health of |
a token around the |
simple and shown in |
global health of the |
token around the region |
and shown in figure |
health of the system |
around the region for |
of the system to |
the region for purposes |
the system to identify |
region for purposes of |
system to identify the |
for purposes of state |
to identify the best |
purposes of state aggregation |
a set of repair |
identify the best value |
set of repair bins |
the best value for |
of repair bins is |
best value for the |
repair bins is maintained |
value for the minimum |
bins is maintained for |
for the minimum upload |
is maintained for each |
writes back changes to |
the minimum upload threshold |
maintained for each layer |
back changes to files |
minimum upload threshold at |
changes to files asynbandwidth |
upload threshold at any |
with i bins for |
to files asynbandwidth to |
threshold at any time |
i bins for a |
files asynbandwidth to perform |
at any time during |
bins for a layer |
the time to acknowledge |
asynbandwidth to perform all |
any time during a |
for a layer with |
a layer with interleave |
to perform all its |
time during a streaming |
during a streaming session |
layer with interleave i |
perform all its file |
time to acknowledge is |
all its file operations |
and makes final decisions |
to acknowledge is only |
its file operations in |
a repair bin consists |
makes final decisions regarding |
acknowledge is only slightly |
file operations in a |
repair bin consists of |
final decisions regarding punishment |
is only slightly higher |
operations in a timely |
bin consists of a |
decisions regarding punishment of |
regarding punishment of nodes |
in a timely fashion |
consists of a partially |
only slightly higher than |
of a partially constructed |
slightly higher than the |
a partially constructed repair |
higher than the expected |
partially constructed repair packet |
an xor and the |
xor and the recipe |
adaptive threshold strategies choosing |
and the recipe list |
threshold strategies choosing an |
the recipe list of |
strategies choosing an upload |
recipe list of identifiers |
choosing an upload threshold |
list of identifiers of |
assigns lower priorities to |
an upload threshold requires |
of identifiers of data |
lower priorities to asynmobile |
upload threshold requires care |
identifiers of data packets |
priorities to asynmobile file |
of data packets that |
s to wait until |
a low threshold may |
data packets that compose |
to asynmobile file systems |
to wait until the |
low threshold may not |
packets that compose the |
that compose the xor |
wait until the next |
threshold may not be |
asynmobile file systems typically |
until the next token |
may not be sufficient |
file systems typically assume |
each intercepted data packet |
the next token round |
not be sufficient to |
systems typically assume that |
intercepted data packet is |
be sufficient to identify |
typically assume that a |
data packet is added |
sufficient to identify opportunistic |
assume that a client |
packet is added to |
to identify opportunistic nodes |
plus the roundtrip time |
that a client is |
is added to each |
a client is strongly |
while high thresholds may |
added to each layer |
high thresholds may incorrectly |
to each layer where |
thresholds may incorrectly punish |
as we scale up |
each layer where adding |
may incorrectly punish correct |
chronous operations at the |
layer where adding to |
incorrectly punish correct nodes |
operations at the ip |
where adding to a |
at the ip level |
adding to a layer |
the ip level to |
to a layer simply |
we considered different strategies |
ip level to reduce |
a layer simply means |
considered different strategies for |
roundtrip time becomes dominant |
layer simply means choosing |
level to reduce interference |
different strategies for the |
simply means choosing a |
to reduce interference with |
strategies for the choice |
means choosing a repair |
reduce interference with connected |
for the choice of |
choosing a repair bin |
interference with connected like |
these experiments show that |
the choice of the |
a repair bin from |
with connected like a |
experiments show that the |
choice of the minimum |
repair bin from the |
connected like a desktop |
show that the critical |
of the minimum contribution |
that the critical factor |
like a desktop host |
the critical factor determining |
the minimum contribution t |
bin from the layer |
from the layer s |
the layer s set |
performance of mfs priorities |
critical factor determining performance |
minimum contribution t hreshold |
of mfs priorities and |
factor determining performance is |
connected and should foreground |
contribution t hreshold used |
incrementally updating the xor |
mfs priorities and writeback |
determining performance is the |
and should foreground operations |
t hreshold used for |
updating the xor with |
priorities and writeback schemes |
performance is the time |
hreshold used for identifying |
the xor with the |
limit its bandwidth consumption |
is the time needed |
each test consists of |
xor with the new |
its bandwidth consumption to |
used for identifying misbehaving |
the time needed for |
test consists of two |
with the new data |
bandwidth consumption to a |
for identifying misbehaving nodes |
time needed for the |
consists of two concurrent |
the new data packet |
consumption to a minimum |
needed for the system |
of two concurrent processes |
the simplest strategy sets |
and adding the data |
two concurrent processes executing |
for the system to |
simplest strategy sets a |
adding the data packet |
concurrent processes executing different |
the system to aggregate |
strategy sets a fixed |
the data packet s |
processes executing different workloads |
system to aggregate state |
sets a fixed threshold |
data packet s header |
to aggregate state over |
packet s header to |
aggregate state over regions |
s header to the |
header to the recipe |
to the recipe list |
mean times to completion |
times to completion are |
to completion are shown |
a counter is incremented |
completion are shown with |
adaptation by deferred transmission |
counter is incremented as |
are shown with standard |
by deferred transmission of |
is incremented as each |
shown with standard deviations |
they shed light on |
deferred transmission of file |
incremented as each data |
shed light on a |
transmission of file upwidth |
as each data packet |
light on a mechanism |
of file upwidth lies |
each data packet arrives |
on a mechanism that |
file upwidth lies between |
data packet arrives at |
three different policies for |
a mechanism that links |
upwidth lies between these |
packet arrives at the |
different policies for writing |
mechanism that links latency |
independent of the current |
lies between these extremes |
arrives at the appliance |
policies for writing back |
that links latency to |
of the current state |
for writing back files |
links latency to throughput |
the current state of |
writing back files are |
assuming weak connectivity dates |
and choosing the repair |
current state of the |
back files are listed |
weak connectivity dates has |
via increased memory consumption |
choosing the repair bin |
increased memory consumption and |
connectivity dates has the |
state of the system |
the repair bin from |
memory consumption and the |
under uniform or differentiated |
dates has the disadvantage |
repair bin from the |
consumption and the resulting |
uniform or differentiated priorities |
has the disadvantage of |
bin from the layer |
and the resulting increase |
any node contributing at |
the disadvantage of increasing |
from the layer s |
the resulting increase in |
node contributing at a |
reads take precedence over |
disadvantage of increasing the |
the layer s set |
resulting increase in allocation |
contributing at a rate |
take precedence over writes |
of increasing the delay |
layer s set is |
increase in allocation and |
at a rate of |
increasing the delay before |
s set is done |
in allocation and garbage |
a rate of less |
the delay before upcan |
set is done by |
allocation and garbage collection |
rate of less than |
values in bold are |
delay before upcan be |
is done by taking |
and garbage collection overheads |
in bold are of |
before upcan be too |
done by taking the |
bold are of particular |
upcan be too conservative |
by taking the modulo |
are of particular significance |
of the stream rate |
taking the modulo of |
the stream rate would |
since it delays sending |
the modulo of the |
stream rate would be |
it delays sending updates |
modulo of the counter |
rate would be removed |
delays sending updates to |
of the counter with |
note that elapsed times |
sending updates to the |
one downside of using |
that elapsed times for |
ms increase in latency |
elapsed times for write |
downside of using a |
times for write workloads |
updates to the dates |
for write workloads give |
of using a fixed |
write workloads give the |
to the dates are |
the counter with the |
using a fixed threshold |
workloads give the time |
the dates are applied |
counter with the number |
a fixed threshold is |
give the time until |
mb increase in memory |
with the number of |
fixed threshold is that |
dates are applied at |
the time until the |
increase in memory consumption |
the number of bins |
threshold is that opportunistic |
are applied at the |
time until the process |
can inflate overheads by |
is that opportunistic nodes |
applied at the file |
number of bins in |
of bins in each |
bins in each layer |
at the file server |
until the process running |
that opportunistic nodes that |
the process running the |
opportunistic nodes that learn |
for a layer with |
process running the workload |
and therefore reduces the |
nodes that learn the |
a layer with interleave |
running the workload finishes |
therefore reduces the deserver |
that learn the threshold |
reduces the deserver in |
learn the threshold can |
the deserver in order |
the threshold can simply |
deserver in order to |
threshold can simply contribute |
not when the log |
the xth intercepted packet |
can simply contribute at |
in order to aggregate |
when the log is |
xth intercepted packet is |
simply contribute at the |
order to aggregate modifications |
the log is flushed |
intercepted packet is added |
packet is added to |
contribute at the lowest |
and degrade the throughput |
is added to the |
this is shown in |
gree of consistency between |
degrade the throughput by |
at the lowest possible |
is shown in figure |
of consistency between clients |
the lowest possible upload |
consistency between clients cached |
lowest possible upload factor |
between clients cached copies |
for its own this |
its own this paper |
own this paper examines |
this paper examines the |
when a repair bin |
from the graphs in |
paper examines the effectiveness |
a repair bin fills |
the graphs in section |
one way to alleviate |
repair bin fills up |
examines the effectiveness of |
way to alleviate the |
bin fills up its |
modified and then deleted |
the effectiveness of mafs |
to alleviate the problem |
fills up its recipe |
it is clear that |
alleviate the problem we |
up its recipe list |
is clear that such |
which requires the file |
its recipe list contains |
clear that such a |
requires the file update |
recipe list contains r |
that such a stretagy |
ve identified could be |
the file update rpc |
list contains r data |
such a stretagy may |
bandwidth client may decide |
identified could be to |
client may decide to |
contains r data packets |
may decide to delay |
file update rpc to |
could be to reduce |
a stretagy may disrupt |
r data packets it |
decide to delay sending |
update rpc to be |
be to reduce the |
stretagy may disrupt the |
data packets it fires |
to delay sending a |
rpc to be cancelled |
to reduce the latency |
may disrupt the streaming |
delay sending a file |
to be cancelled if |
reduce the latency of |
disrupt the streaming session |
a repair packet is |
sending a file system |
be cancelled if it |
the latency of state |
repair packet is generated |
a file system that |
cancelled if it is |
latency of state aggregation |
packet is generated consisting |
file system that propagates |
if it is still |
choosing a high threshold |
is generated consisting of |
system that propagates file |
it is still in |
a high threshold is |
so that it grows |
that it grows sub |
that propagates file modifications |
is still in transmission |
high threshold is not |
generated consisting of the |
propagates file modifications asynchronously |
still in transmission when |
threshold is not a |
consisting of the xor |
file modifications asynchronously file |
in transmission when the |
is not a practical |
of the xor and |
modifications asynchronously file s |
transmission when the remove |
not a practical option |
the xor and the |
this might be achieved |
asynchronously file s update |
when the remove rpc |
xor and the recipe |
might be achieved by |
file s update to |
the remove rpc is |
since correct nodes would |
and the recipe list |
be achieved by using |
s update to the |
remove rpc is initiated |
correct nodes would get |
the recipe list and |
achieved by using a |
update to the file |
nodes would get unfairly |
recipe list and is |
by using a deeper |
to the file server |
would get unfairly punished |
an update to a |
list and is scheduled |
using a deeper hierarchy |
update to a file |
to avoid this problem |
but this decision may |
and is scheduled for |
to a file will |
a deeper hierarchy of |
we have explored adaptive |
is scheduled for sending |
a file will supersede |
this decision may also |
deeper hierarchy of rings |
have explored adaptive strategies |
file will supersede any |
decision may also affect |
while the repair bin |
and by letting tokens |
may also affect at |
one simple strategy starts |
the repair bin is |
will supersede any previous |
by letting tokens in |
also affect at all |
simple strategy starts with |
repair bin is re |
supersede any previous queued |
letting tokens in each |
affect at all bandwidth |
strategy starts with a |
initialized with an empty |
tokens in each of |
at all bandwidth levels |
any previous queued updates |
starts with a minimum |
with an empty recipe |
in each of these |
with a minimum threshold |
an empty recipe list |
rather than delaying writes |
each of these rings |
empty recipe list and |
of these rings circulate |
recipe list and blank |
list and blank xor |
compiles the entire mfs |
these rings circulate independently |
mafs other clients that |
the entire mfs file |
other clients that would |
entire mfs file system |
clients that would like |
mfs file system and |
that would like to |
this would create a |
file system and its |
incoming repair packets are |
would like to read |
would create a more |
system and its rpc |
repair packets are processed |
like to read the |
create a more complex |
and its rpc library |
packets are processed as |
to read the file |
a more complex structure |
are processed as follows |
optimistic concuruses rpc priorities |
increasing it only if |
concuruses rpc priorities to |
it only if the |
but aggregation latency would |
rpc priorities to reduce |
if all the data |
priorities to reduce interference |
all the data packets |
to reduce interference between |
aggregation latency would grow |
only if the system |
the data packets contained |
reduce interference between read |
latency would grow logarithmically |
if the system is |
files and directories comprising |
data packets contained in |
interference between read and |
would grow logarithmically rather |
the system is compromised |
packets contained in the |
between read and rency |
grow logarithmically rather than |
contained in the repair |
global auditors sample the |
logarithmically rather than linearly |
read and rency control |
in the repair s |
auditors sample the system |
and rency control and |
the repair s recipe |
sample the system to |
rency control and reconciliation |
repair s recipe list |
is reducing state aggregation |
control and reconciliation of |
the system to identify |
s recipe list have |
reducing state aggregation latency |
and reconciliation of conflicting |
system to identify the |
recipe list have been |
state aggregation latency the |
reconciliation of conflicting updates |
to identify the average |
list have been received |
none of the files |
aggregation latency the only |
of conflicting updates are |
identify the average download |
have been received successfully |
of the files are |
latency the only option |
conflicting updates are typwrite |
the average download factor |
the repair packet is |
repair packet is discarded |
the files are initially |
updates are typwrite traffic |
we evaluated two alternative |
files are initially in |
and if this factor |
are typwrite traffic at |
evaluated two alternative approaches |
if the repair s |
are initially in the |
if this factor is |
typwrite traffic at low |
the repair s recipe |
initially in the cache |
this factor is lower |
traffic at low bandwidth |
but found that neither |
repair s recipe list |
found that neither can |
factor is lower than |
that neither can substitute |
to ensure that file |
this workload performs an |
s recipe list contains |
neither can substitute for |
ensure that file modifications |
workload performs an intensive |
recipe list contains a |
can substitute for lowering |
that file modifications ically |
performs an intensive pattern |
list contains a single |
substitute for lowering the |
file modifications ically used |
an intensive pattern of |
contains a single missing |
for lowering the latency |
modifications ically used to |
intensive pattern of reads |
a single missing data |
lowering the latency of |
ically used to resolve |
pattern of reads and |
single missing data packet |
the latency of the |
used to resolve inconsistencies |
once the download factor |
of reads and writes |
latency of the recovery |
the download factor reaches |
recovery can occur immediately |
reads and writes files |
of the recovery state |
download factor reaches a |
can occur immediately by |
and writes files without |
the recovery state aggregation |
factor reaches a satisfactory |
writes files without raising |
reaches a satisfactory level |
files without raising the |
a satisfactory level again |
without raising the issue |
our first approach varies |
raising the issue of |
first approach varies the |
the issue of concurrent |
the threshold may be |
approach varies the rate |
issue of concurrent accesses |
threshold may be reduced |
varies the rate of |
may be reduced back |
the rate of aggregation |
when bandwidth are rapidly |
be reduced back to |
rate of aggregation by |
bandwidth are rapidly propagated |
a topic we tackle |
reduced back to its |
of aggregation by increasing |
are rapidly propagated to |
topic we tackle in |
back to its initial |
aggregation by increasing the |
rapidly propagated to the |
we tackle in section |
to its initial value |
by increasing the rate |
propagated to the clients |
increasing the rate at |
to the clients that |
the rate at which |
the clients that need |
this stepwise approach allows |
rate at which tokens |
clients that need them |
stepwise approach allows the |
at which tokens are |
approach allows the system |
which tokens are released |
mafs is very low |
allows the system to |
the system to catch |
system to catch opportunistic |
to catch opportunistic nodes |
catch opportunistic nodes in |
this can be an |
opportunistic nodes in case |
can be an acceptable |
nodes in case their |
be an acceptable price |
in case their presence |
an acceptable price to |
case their presence starts |
acceptable price to pay |
their presence starts affecting |
price to pay for |
presence starts affecting the |
to pay for the |
starts affecting the performance |
performance evaluation of these |
pay for the abilalso |
affecting the performance of |
evaluation of these workloads |
for the abilalso incorporates |
the performance of the |
the abilalso incorporates a |
performance of the system |
abilalso incorporates a new |
incorporates a new invalidation |
this helps only up |
while avoiding incorrect accusations |
we classified grep and |
helps only up to |
avoiding incorrect accusations of |
based update propagation ity |
classified grep and read |
only up to a |
incorrect accusations of correct |
update propagation ity to |
grep and read as |
up to a point |
accusations of correct nodes |
propagation ity to continue |
and read as foreground |
ity to continue accessing |
read as foreground workloads |
to continue accessing a |
we also considered a |
continue accessing a file |
also considered a second |
accessing a file server |
considered a second adaptive |
a second adaptive strategy |
and compile and write |
compile and write as |
but if bandwidth is |
and write as background |
if bandwidth is less |
write as background workloads |
bandwidth is less scheme |
for computing the threshold |
computing the threshold based |
unlike previous mobile file |
the threshold based on |
previous mobile file systems |
layer with interleave of |
threshold based on periodically |
four combined workloads were |
based on periodically sampled |
combined workloads were then |
on periodically sampled download |
workloads were then generated |
periodically sampled download and |
were then generated by |
sampled download and upload |
more than one aggregation |
then generated by running |
download and upload factors |
than one aggregation is |
generated by running a |
client consistency is achievable |
one aggregation is underway |
by running a foreground |
aggregation is underway at |
the average download factors |
running a foreground and |
is underway at a |
underway at a time |
average download factors once |
a foreground and a |
codaniques that are oblivious |
download factors once again |
foreground and a background |
that are oblivious to |
factors once again are |
and successive tokens perform |
and a background workload |
are oblivious to the |
once again are used |
successive tokens perform redundant |
a background workload concurrently |
oblivious to the exact |
again are used for |
tokens perform redundant work |
to the exact bandwidth |
are used for detecting |
the exact bandwidth level |
we denote these as |
used for detecting whether |
denote these as gc |
for detecting whether the |
and can like file |
detecting whether the threshold |
can like file systems |
whether the threshold should |
like file systems therefore |
processing all these tokens |
the threshold should be |
file systems therefore switch |
all these tokens is |
threshold should be varied |
systems therefore switch between |
these tokens is costly |
should be varied or |
therefore switch between a |
be varied or not |
switch between a low |
our initial threshold is |
initial threshold is set |
threshold is set to |
is set to null |
writes mode and a |
mode and a synchronous |
and the threshold is |
the threshold is chosen |
threshold is chosen from |
is chosen from sampled |
chosen from sampled upload |
from sampled upload factors |
acthe authors were supported |
authors were supported in |
were supported in part |
supported in part by |
if the system seems |
in part by darpa |
the system seems to |
part by darpa under |
system seems to be |
by darpa under afrl |
s decreases the amount |
seems to be in |
darpa under afrl grant |
decreases the amount of |
to be in a |
under afrl grant radc |
the amount of unacknowledged |
be in a compromised |
afrl grant radc cording |
amount of unacknowledged data |
in a compromised state |
grant radc cording to |
of unacknowledged data by |
radc cording to the |
cording to the available |
to the available bandwidth |
the collected upload factors |
collected upload factors are |
upload factors are ordered |
factors are ordered and |
are ordered and the |
ordered and the value |
and the value dividing |
the value dividing the |
value dividing the lowest |
in a wireless f |
percent is used as |
is used as the |
used as the new |
as the new threshold |
this approach relies on |
approach relies on efficiently |
three types of rpcs |
relies on efficiently sampling |
but increases throughput by |
types of rpcs predominate |
on efficiently sampling the |
efficiently sampling the system |
increases throughput by less |
throughput by less than |
and on fact that |
on fact that if |
fact that if the |
that if the system |
combining the xor in |
if the system s |
the xor in the |
the system s performance |
xor in the repair |
fetches of file data |
system s performance is |
in the repair with |
s performance is not |
the repair with the |
performance is not satisfactory |
repair with the other |
with the other successfully |
the other successfully received |
other successfully received data |
and store operations for |
successfully received data packets |
store operations for files |
if the repair contains |
the repair contains multiple |
repair contains multiple missing |
percent of the nodes |
contains multiple missing data |
of the nodes are |
multiple missing data packets |
in descending order of |
the nodes are opportunistic |
descending order of priority |
it cannot be used |
time spent allocating byte |
and by afosr under |
cannot be used immediately |
spent allocating byte arrays |
by afosr under muri |
be used immediately for |
the aim of the |
evaluation in this section |
afosr under muri grant |
used immediately for recovery |
allocating byte arrays in |
aim of the experiments |
under muri grant f |
immediately for recovery it |
byte arrays in the |
of the experiments was |
we evaluate the performance |
for recovery it is |
arrays in the application |
the experiments was to |
evaluate the performance of |
experiments was to demonstrate |
recovery it is instead |
was to demonstrate that |
the performance of our |
it is instead stored |
to demonstrate that priorities |
performance of our proposed |
is instead stored in |
demonstrate that priorities improve |
of our proposed auditing |
instead stored in a |
that priorities improve the |
our proposed auditing strategy |
stored in a table |
priorities improve the performance |
proposed auditing strategy over |
in a table that |
improve the performance of |
auditing strategy over the |
a table that maps |
the performance of the |
memory used on sender |
table that maps missing |
strategy over the original |
performance of the foreground |
used on sender and |
that maps missing data |
over the original streaming |
of the foreground workloads |
on sender and the |
maps missing data packets |
the original streaming protocol |
sender and the number |
missing data packets to |
and the number of |
the four combined workloads |
data packets to repair |
the number of multicast |
we built an event |
four combined workloads were |
packets to repair packets |
number of multicast requests |
combined workloads were executed |
of multicast requests in |
driven simulator and used |
whenever a data packet |
multicast requests in progress |
workloads were executed on |
simulator and used it |
a data packet is |
were executed on top |
and used it to |
data packet is subsequently |
executed on top of |
used it to simulate |
packet is subsequently received |
variations in bandwidth can |
on top of mfs |
it to simulate streaming |
is subsequently received or |
in bandwidth can occur |
top of mfs configured |
to simulate streaming sessions |
subsequently received or recovered |
bandwidth can occur without |
of mfs configured with |
simulate streaming sessions on |
token roundtrip time and |
can occur without the |
mfs configured with either |
streaming sessions on networks |
roundtrip time and an |
this table is checked |
occur without the user |
table is checked to |
without the user s |
is checked to see |
configured with either synchronous |
checked to see if |
time and an average |
to see if any |
sessions on networks with |
with either synchronous writes |
the user s with |
and an average time |
see if any xors |
user s with additional |
an average time to |
update logging or asynchronous |
s with additional support |
if any xors now |
average time to acknowledge |
logging or asynchronous writeback |
with additional support from |
any xors now have |
time to acknowledge a |
nodes and an average |
xors now have singleton |
the update logging mechanism |
to acknowledge a message |
and an average of |
additional support from microsoft |
now have singleton losses |
update logging mechanism was |
support from microsoft research |
have singleton losses due |
logging mechanism was configured |
from microsoft research and |
singleton losses due to |
mechanism was configured to |
microsoft research and from |
losses due to the |
was configured to delay |
research and from the |
due to the presence |
the target streaming rate |
and from the intel |
varying token circulation rate |
to the presence of |
target streaming rate in |
configured to delay flushing |
from the intel corporation |
the presence of the |
streaming rate in the |
to delay flushing an |
presence of the new |
rate in the experiments |
delay flushing an update |
of the new packet |
in the experiments was |
flushing an update for |
the new packet and |
the experiments was fixed |
so that changing modes |
an update for at |
new packet and can |
experiments was fixed to |
that changing modes creates |
our second approach increased |
update for at least |
packet and can be |
changing modes creates unexpected |
second approach increased the |
for at least a |
and can be used |
modes creates unexpected incon |
approach increased the amount |
at least a second |
can be used for |
increased the amount of |
be used for recovering |
several clients concurrently modify |
the amount of feedback |
used for recovering other |
clients concurrently modify a |
and all our experiments |
amount of feedback to |
for recovering other missing |
every experiment was repeated |
concurrently modify a file |
all our experiments were |
of feedback to the |
recovering other missing packets |
experiment was repeated ten |
our experiments were repeated |
feedback to the sender |
was repeated ten times |
the final contents depend |
repeated ten times at |
final contents depend on |
ten times at each |
in our base implementation |
contents depend on the |
xors received from different |
times at each of |
depend on the client |
received from different layers |
confidence intervals were small |
at each of five |
on the client that |
each aggregate ack contains |
from different layers interact |
each of five possible |
the client that closed |
aggregate ack contains a |
different layers interact to |
and for simplicity are |
of five possible bandwidth |
client that closed it |
ack contains a single |
layers interact to recover |
for simplicity are omitted |
five possible bandwidth values |
that closed it last |
contains a single value |
interact to recover missing |
simplicity are omitted from |
a single value maxcontiguous |
a client can lock |
are omitted from the |
to recover missing data |
client can lock a |
omitted from the graphs |
recover missing data packets |
can lock a file |
lock a file to |
shows the time taken |
a file to synchronise |
representing the maximum number |
the time taken for |
file to synchronise accesses |
the source of the |
the maximum number such |
time taken for each |
since an xor received |
source of the stream |
maximum number such that |
taken for each workload |
the server grants the |
an xor received at |
of the stream has |
number such that messages |
for each workload at |
server grants the client |
xor received at a |
the stream has an |
such that messages with |
each workload at a |
grants the client a |
received at a higher |
stream has an upload |
that messages with this |
workload at a bandwidth |
the client a lease |
at a higher interleave |
has an upload capacity |
messages with this and |
at a bandwidth of |
a higher interleave can |
an upload capacity of |
with this and all |
higher interleave can recover |
upload capacity of four |
this and all lower |
interleave can recover a |
capacity of four times |
and all lower numbers |
can recover a packet |
that is renewed each |
of four times the |
all lower numbers are |
recover a packet that |
is renewed each time |
four times the stream |
times the stream rate |
a packet that makes |
renewed each time the |
lower numbers are stable |
packet that makes an |
each time the client |
numbers are stable in |
that makes an earlier |
time the client communicates |
are stable in the |
makes an earlier xor |
the client communicates with |
stable in the region |
an earlier xor at |
and is connected to |
client communicates with the |
shows overall results for |
to increase the amount |
communicates with the file |
earlier xor at a |
overall results for selected |
increase the amount of |
with the file server |
xor at a lower |
results for selected configurations |
the amount of feedback |
other nodes have enough |
at a lower interleave |
nodes have enough download |
a lower interleave usable |
have enough download capacity |
lower interleave usable hence |
enough download capacity to |
the results in table |
download capacity to receive |
we permit ack to |
though layered interleaving is |
capacity to receive the |
to receive the stream |
layered interleaving is equivalent |
permit ack to contain |
demonstrate the benefit of |
interleaving is equivalent to |
ack to contain up |
the benefit of priorities |
is equivalent to c |
and upload factor of |
to contain up to |
benefit of priorities when |
equivalent to c different |
contain up to k |
of priorities when there |
up to k numeric |
priorities when there is |
to k numeric ranges |
when there is high |
there is high contention |
is high contention between |
high contention between high |
we defined an availability |
defined an availability window |
an availability window of |
instances in terms of |
in terms of overhead |
terms of overhead and |
of overhead and design |
priority rpcs and writes |
its recovery power is |
seconds and an interest |
recovery power is much |
and an interest window |
power is much higher |
an interest window of |
in both the i |
is much higher and |
much higher and comes |
higher and comes close |
and comes close to |
comes close to standard |
to evaluate the quality |
evaluate the quality of |
the quality of each |
quality of each auditing |
of each auditing strategy |
bound gw and rw |
we evaluate the average |
gw and rw workloads |
evaluate the average download |
the average download factors |
average download factors of |
download factors of correct |
factors of correct nodes |
of correct nodes during |
optimizations staggered start for |
correct nodes during a |
staggered start for rate |
adding priorities decreases the |
priorities decreases the time |
decreases the time required |
limiting in the naive |
the time required for |
in the naive implementation |
time required for the |
the naive implementation of |
required for the foreground |
naive implementation of the |
second time interval after |
for the foreground workload |
implementation of the layered |
time interval after auditing |
the foreground workload to |
of the layered interleaving |
interval after auditing is |
foreground workload to execute |
the layered interleaving algorithm |
after auditing is first |
auditing is first applied |
is first applied to |
first applied to the |
applied to the system |
repair packets are transmitted |
packets are transmitted as |
are transmitted as soon |
transmitted as soon as |
as soon as repair |
soon as repair bins |
as repair bins fill |
repair bins fill and |
bins fill and allow |
fill and allow them |
and allow them to |
allow them to be |
them to be constructed |
we considered that global |
considered that global auditors |
that global auditors collected |
global auditors collected information |
auditors collected information from |
all the repair bins |
the repair bins in |
repair bins in a |
bins in a layer |
in a layer fill |
a layer fill in |
layer fill in quick |
fill in quick succession |
nodes between each interval |
between each interval of |
see elapsed times for |
elapsed times for rw |
the arrival of packets |
the system can now |
read with synchronous writes |
adaptive remote procedure call |
notice that the sample |
system can now cleanup |
with synchronous writes in |
remote procedure call figure |
that the sample size |
can now cleanup message |
synchronous writes in the |
the sample size does |
now cleanup message sequences |
writes in the table |
sample size does not |
cleanup message sequences that |
time series of wireless |
size does not increase |
message sequences that have |
series of wireless bandwidth |
does not increase with |
sequences that have as |
not increase with the |
that have as gaps |
increase with the size |
this is particularly true |
mafs uses adaptive remote |
with the size of |
is particularly true in |
uses adaptive remote procedure |
the size of the |
will successively fill the |
particularly true in the |
adaptive remote procedure call |
messages that are still |
size of the system |
successively fill the four |
true in the rw |
in the rw test |
that are still unstable |
fill the four repair |
remote procedure call for |
which is a positive |
the four repair bins |
procedure call for client |
is a positive aspect |
four repair bins in |
a positive aspect of |
where the foreground workload |
repair bins in layer |
the foreground workload generates |
positive aspect of the |
foreground workload generates heavy |
aspect of the auditing |
in the experiment shown |
workload generates heavy contention |
of the auditing approach |
the experiment shown in |
generates heavy contention by |
this behavior leads to |
experiment shown in figures |
heavy contention by fetching |
behavior leads to a |
contention by fetching a |
leads to a large |
adaptation based on low |
by fetching a large |
to a large number |
fetching a large volume |
a large number of |
a large volume of |
large number of repair |
we discuss the costs |
large volume of data |
number of repair packets |
discuss the costs involved |
of repair packets being |
the costs involved in |
adaptive rpc is based |
repair packets being generated |
costs involved in collecting |
rpc is based on |
packets being generated and |
involved in collecting these |
the greatest benefits are |
is based on our |
we set k to |
in collecting these samples |
greatest benefits are observable |
being generated and sent |
based on our earlier |
set k to the |
benefits are observable for |
generated and sent within |
on our earlier work |
k to the number |
are observable for the |
and sent within a |
our earlier work in |
to the number of |
observable for the combination |
sent within a short |
earlier work in modes |
the number of partitions |
for the combination of |
within a short period |
work in modes can |
the combination of asynchronous |
a short period of |
in modes can be |
modes can be ill |
short period of time |
combination of asynchronous writes |
of asynchronous writes with |
while the amount of |
suited to situations where |
which results in undesirable |
the amount of acknowledged |
asynchronous writes with priorities |
to situations where bandwidth |
results in undesirable overhead |
amount of acknowledged data |
situations where bandwidth is |
in undesirable overhead and |
of acknowledged data is |
where bandwidth is not |
undesirable overhead and traffic |
since here the performance |
acknowledged data is reduced |
here the performance of |
overhead and traffic spikes |
bandwidth is not network |
data is reduced by |
the performance of the |
performance of the background |
of the background workload |
we would like to |
the background workload can |
would like to rate |
background workload can also |
workload can also improve |
can also improve by |
limit transmissions of repair |
also improve by not |
transmissions of repair packets |
improve by not having |
of repair packets to |
and differs from severely |
by not having to |
repair packets to one |
number of false positives |
not having to wait |
differs from severely constrained |
packets to one for |
of false positives download |
having to wait for |
to one for every |
false positives download factor |
but insufficient for a |
one for every r |
and the overall throughput |
to wait for its |
insufficient for a client |
for every r data |
the overall throughput is |
wait for its writes |
for a client to |
every r data packets |
overall throughput is actually |
for its writes to |
a client to ignore |
throughput is actually lower |
its writes to be |
client to ignore it |
this problem is fixed |
is actually lower because |
writes to be committed |
to ignore it a |
problem is fixed by |
actually lower because token |
to be committed at |
ignore it a typical |
is fixed by staggering |
lower because token processing |
be committed at the |
it a typical rpc |
fixed by staggering the |
because token processing becomes |
committed at the server |
a typical rpc system |
by staggering the starting |
token processing becomes more |
typical rpc system in |
staggering the starting sizes |
processing becomes more costly |
in the gc and |
rpc system in allowing |
the starting sizes of |
the gc and rc |
system in allowing applications |
starting sizes of the |
sizes of the bins |
in allowing applications to |
gc and rc tests |
the system becomes unstable |
allowing applications to control |
analogous to the starting |
applications to control how |
to the starting positions |
to control how concurrent |
the starting positions of |
control how concurrent rpcs |
starting positions of runners |
where there is lighter |
notice the large variances |
how concurrent rpcs are |
positions of runners in |
there is lighter contention |
the large variances in |
concurrent rpcs are transmitted |
of runners in a |
runners in a sprint |
large variances in figure |
the impact of priorities |
impact of priorities is |
and special handling for |
of priorities is negligible |
the very first time |
special handling for failwhen |
very first time bin |
handling for failwhen deciding |
first time bin number |
for failwhen deciding what |
time bin number x |
and in some cases |
failwhen deciding what to |
bin number x in |
in some cases results |
deciding what to send |
number x in a |
some cases results in |
what to send over |
x in a layer |
cases results in a |
to send over the |
because our flow control |
in a layer of |
results in a slight |
send over the network |
our flow control scheme |
a layer of interleave |
in a slight overhead |
layer of interleave i |
of interleave i fires |
ures due to insufficient |
due to insufficient bandwidth |
based on limiting the |
it does so at |
on limiting the amount |
does so at size |
but this is chiefly |
adaptive rpc requests and |
limiting the amount of |
so at size x |
this is chiefly after |
rpc requests and replies |
the amount of unacknowledged |
at size x mod |
size x mod r |
requests and replies can |
amount of unacknowledged data |
is chiefly after adding |
and replies can contain |
chiefly after adding priorities |
replies can contain an |
after adding priorities to |
can contain an arbitrary |
adding priorities to rpcs |
contain an arbitrary amount |
an arbitrary amount of |
arbitrary amount of data |
the first repair bin |
while the sender can |
first repair bin in |
it is natural to |
the sender can cleanup |
a sender also attaches |
repair bin in the |
is natural to ask |
sender can cleanup any |
sender also attaches a |
bin in the second |
natural to ask when |
can cleanup any portion |
also attaches a priority |
in the second layer |
to ask when they |
cleanup any portion of |
attaches a priority and |
the second layer with |
ask when they are |
any portion of the |
a priority and timeout |
second layer with interleave |
when they are beneficial |
portion of the message |
priority and timeout to |
of the message sequence |
would fire at size |
and timeout to the |
timeout to the send |
to the send operation |
and to what degree |
receivers have to deliver |
the second would fire |
have to deliver in |
second would fire at |
file system overview rover |
to deliver in fifo |
in addition to comparing |
would fire at size |
system overview rover queued |
deliver in fifo order |
addition to comparing mfs |
overview rover queued rpc |
to comparing mfs with |
comparing mfs with and |
mfs with and without |
with and without prioritised |
the amount of data |
and without prioritised rpcs |
amount of data they |
of data they cache |
data they cache is |
they cache is larger |
an adaptive rpc can |
we also investigate the |
adaptive rpc can be |
and this reduces their |
also investigate the performance |
rpc can be asynchronous |
this reduces their ability |
investigate the performance impact |
reduces their ability to |
the performance impact of |
their ability to accept |
performance impact of replacing |
ability to accept incoming |
impact of replacing synchronous |
to accept incoming traffic |
adaptive mobile file system |
of replacing synchronous rpcs |
replacing synchronous rpcs for |
synchronous rpcs for file |
is a distributed file |
a distributed file sys |
notice the linkage to |
rpcs for file updates |
the linkage to memory |
for file updates with |
file updates with asynchronous |
so that an application |
updates with asynchronous writeback |
that an application need |
an application need not |
application need not block |
need not block waiting |
not block waiting for |
block waiting for the |
waiting for the result |
the performance of these |
the growth in memory |
intem designed to support |
performance of these alternatives |
growth in memory occurs |
designed to support efficient |
of these alternatives is |
in memory occurs on |
to support efficient access |
these alternatives is compared |
memory occurs on the |
support efficient access to |
alternatives is compared in |
occurs on the receivers |
efficient access to a |
is compared in a |
access to a remote |
compared in a set |
to a remote file |
in a set of |
a remote file server |
but the pattern is |
a set of microbenchmarks |
remote file server stead |
the pattern is similar |
pattern is similar to |
is similar to what |
similar to what we |
the library makes an |
to what we saw |
and with workloads gathered |
library makes an upcall |
what we saw earlier |
with workloads gathered from |
makes an upcall when |
workloads gathered from windows |
an upcall when the |
gathered from windows nt |
upcall when the reply |
from windows nt file |
merely having more cached |
when the reply arrives |
windows nt file system |
having more cached data |
nt file system traces |
more cached data is |
cached data is enough |
since an application can |
data is enough to |
an application can perform |
is enough to slow |
application can perform multiple |
our experimental setup consists |
enough to slow them |
can perform multiple rpcs |
experimental setup consists of |
to slow them down |
perform multiple rpcs concurby |
setup consists of two |
multiple rpcs concurby mobile |
rpcs concurby mobile clients |
quality of streaming when |
concurby mobile clients that |
of streaming when applying |
mobile clients that must |
streaming when applying the |
clients that must cope |
when applying the fixed |
ghz pentium iii desktop |
that must cope with |
applying the fixed threshold |
token roundtrip time increases |
pentium iii desktop machines |
must cope with variations |
the fixed threshold strategy |
iii desktop machines running |
cope with variations in |
desktop machines running the |
with variations in available |
threshold is varied from |
machines running the freebsd |
variations in available bandwidth |
the mafs design and |
this delays state aggregation |
mafs design and terminology |
design and terminology are |
and terminology are similar |
terminology are similar to |
are similar to rently |
increases pending messages and |
adaptive rpc schedules their |
pending messages and reduces |
rpc schedules their transmission |
messages and reduces throughput |
this corresponds to allocating |
corresponds to allocating bandwidth |
to allocating bandwidth among |
allocating bandwidth among the |
and the contribution rate |
one of which acts |
bandwidth among the competing |
the contribution rate of |
of which acts as |
among the competing rpcs |
contribution rate of opportunistic |
which acts as an |
rate of opportunistic nodes |
acts as an mfs |
of opportunistic nodes is |
the andrew file system |
as an mfs server |
opportunistic nodes is varied |
nodes is varied from |
and the other as |
the other as an |
other as an mfs |
as an mfs client |
the client machine makes |
client machine makes use |
machine makes use of |
attaching priorities to rpcs |
more aggressive cleanup with |
makes use of the |
priorities to rpcs allows |
presents the average download |
aggressive cleanup with o |
use of the dummynet |
to rpcs allows applications |
of the dummynet trafficshaping |
the average download factors |
the dummynet trafficshaping module |
rpcs allows applications to |
average download factors across |
dummynet trafficshaping module in |
allows applications to control |
download factors across all |
trafficshaping module in freebsd |
applications to control this |
factors across all correct |
feedback in the token |
module in freebsd to |
to control this scheduling |
across all correct nodes |
in the token and |
in freebsd to limit |
control this scheduling policy |
the token and in |
freebsd to limit its |
token and in acks |
to limit its incoming |
a programmer divides rpcs |
limit its incoming and |
programmer divides rpcs into |
its incoming and outgoing |
divides rpcs into classes |
second set of rsized |
presents the number of |
incoming and outgoing bandwidth |
set of rsized xors |
the number of correct |
of rsized xors staggered |
number of correct nodes |
rsized xors staggered start |
of correct nodes incorrectly |
xors staggered start xors |
correct nodes incorrectly punished |
the experiments we conduct |
file access model based |
experiments we conduct in |
access model based on |
we conduct in this |
more work with o |
model based on the |
conduct in this section |
based on the importance |
in this section have |
on the importance of |
this section have a |
the importance of their |
section have a constant |
importance of their results |
have a constant bandwidth |
of their results to |
we consider the use |
a constant bandwidth over |
their results to the |
consider the use of |
constant bandwidth over the |
and lower rates despite |
results to the user |
the use of fixed |
bandwidth over the duration |
lower rates despite saving |
use of fixed thresholds |
over the duration of |
rates despite saving on |
and then mafs clients |
the duration of the |
we studied the effects |
then mafs clients use |
despite saving on memory |
duration of the experiment |
studied the effects of |
mafs clients use whole |
the effects of using |
effects of using different |
of using different values |
using different values for |
different values for t |
but we analyse the |
when a file is |
we analyse the performance |
a file is accessed |
analyse the performance of |
file is accessed assigns |
the performance of mfs |
is accessed assigns priorities |
performance of mfs when |
accessed assigns priorities to |
of mfs when the |
assigns priorities to the |
and increasing it until |
mfs when the bandwidth |
priorities to the classes |
memory overheads on the |
when the bandwidth varies |
overheads on the receiver |
the bandwidth varies over |
on the receiver the |
the library schedules rpcs |
bandwidth varies over the |
the receiver the reader |
library schedules rpcs for |
varies over the course |
receiver the reader may |
schedules rpcs for the |
over the course of |
the reader may doubt |
rpcs for the first |
the course of an |
reader may doubt that |
for the first time |
course of an experiment |
may doubt that memory |
of an experiment in |
doubt that memory overhead |
an experiment in section |
of the stream rate |
that memory overhead on |
a client fetches the |
memory overhead on receivers |
client fetches the entire |
overhead on receivers is |
fetches the entire file |
on receivers is the |
the entire file from |
and present a detailed |
receivers is the real |
entire file from the |
present a detailed set |
is the real issue |
file from the file |
a detailed set of |
from the file based |
detailed set of results |
the file based on |
set of results on |
file based on priorities |
of results on applying |
considering that their cpus |
based on priorities whenever |
results on applying different |
that their cpus are |
on priorities whenever there |
on applying different thresholds |
their cpus are half |
priorities whenever there is |
applying different thresholds to |
whenever there is insufficient |
different thresholds to different |
there is insufficient bandwidth |
thresholds to different scenarios |
is insufficient bandwidth to |
insufficient bandwidth to server |
bandwidth to server and |
microbenchmarks the first set |
to server and caches |
the first set of |
server and caches it |
first set of experiments |
the ratio of opportunistic |
set of experiments compares |
ratio of opportunistic nodes |
of experiments compares different |
mafs only sends the |
of opportunistic nodes is |
experiments compares different mfs |
only sends the server |
opportunistic nodes is fixed |
nodes is fixed to |
sends the server the |
compares different mfs configurations |
the server the contents |
different mfs configurations for |
server the contents transmit |
mfs configurations for specific |
the contents transmit competing |
configurations for specific types |
contents transmit competing rpcs |
for specific types of |
can increasing memory consumption |
transmit competing rpcs without |
specific types of contention |
but their contribution factor |
competing rpcs without a |
increasing memory consumption affect |
rpcs without a noticeable |
memory consumption affect a |
without a noticeable delay |
four workloads were used |
consumption affect a half |
rpcs of a modified |
of a modified file |
a modified file when |
modified file when it |
file when it is |
when it is closed |
it is closed by |
is closed by an |
closed by an application |
executes the grep utility |
this is from higher |
the grep utility several |
grep utility several times |
utility several times on |
several times on each |
we performed an experiment |
priority classes are performed |
times on each of |
performed an experiment with |
classes are performed first |
and rpcs of referred |
rpcs of referred to |
of referred to as |
referred to as writeback |
directory operations cache equal |
operations cache equal priority |
cache equal priority are |
equal priority are performed |
priority are performed in |
are performed in parallel |
this ensures that the |
ensures that the directory |
that the directory contents |
the directory contents and |
directory contents and apply |
nodes follow the protocol |
contents and apply changes |
and apply changes locally |
the files are present |
in which we vary |
with a maximum contribution |
which we vary the |
files are present in |
a maximum contribution rate |
as well as mak |
we vary the number |
are present in the |
maximum contribution rate set |
vary the number of |
application adapts itself to |
contribution rate set to |
present in the cache |
the number of receivers |
adapts itself to the |
number of receivers that |
itself to the available |
of receivers that cache |
to the available bandwidth |
receivers that cache a |
but must be validated |
the available bandwidth gracefully |
that cache a copy |
must be validated before |
cache a copy of |
be validated before they |
a copy of each |
validated before they are |
before they are used |
copy of each message |
ing an rpc to |
an rpc to apply |
rpc to apply the |
to apply the changes |
apply the changes to |
the changes to the |
changes to the server |
to the server s |
the server s copy |
replication factor in figure |
whole since lower bandwidth |
since lower bandwidth translates |
lower bandwidth translates into |
bandwidth translates into longer |
we present the average |
translates into longer delays |
present the average download |
into longer delays for |
the average download rates |
longer delays for lowerfile |
delays for lowerfile caching |
for lowerfile caching is |
lowerfile caching is effective |
caching is effective if |
is effective if a |
effective if a client |
if a client s |
and the number of |
a client s connectivity |
the number of correct |
client s connectivity is |
number of correct nodes |
s connectivity is uncertain |
of correct nodes mistakenly |
increasing this value results |
correct nodes mistakenly removed |
this value results in |
mb files in sequence |
value results in a |
nodes mistakenly removed from |
results in a linear |
mistakenly removed from the |
in a linear increase |
rpc timeouts allow the |
removed from the system |
a linear increase of |
writing the contents of |
timeouts allow the application |
linear increase of memory |
the contents of each |
allow the application to |
increase of memory usage |
contents of each file |
the application to prevent |
of memory usage on |
of each file to |
application to prevent since |
memory usage on receivers |
to prevent since the |
prevent since the client |
for each of these |
since the client can |
each of these configurations |
the client can always |
client can always use |
can always use cached |
always use cached copies |
if memory overheads were |
use cached copies of |
the threshold applied is |
memory overheads were not |
cached copies of files |
threshold applied is presented |
the files are not |
copies of files instead |
overheads were not a |
applied is presented on |
files are not initially |
of files instead low |
were not a significant |
is presented on the |
are not initially present |
not a significant issue |
priority rpcs being silently |
not initially present in |
presented on the x |
a significant issue on |
rpcs being silently starved |
initially present in the |
significant issue on half |
present in the cache |
using priorities alof incrementally |
in the left graph |
priorities alof incrementally fetching |
alof incrementally fetching them |
incrementally fetching them from |
fetching them from the |
them from the server |
as the threshold increases |
higher download averages are |
download averages are observed |
we would expect performance |
would expect performance to |
lows a programmer to |
expect performance to remain |
since more opportunistic nodes |
a programmer to write |
performance to remain unchanged |
more opportunistic nodes are |
programmer to write an |
opportunistic nodes are detected |
to write an adaptive |
nodes are detected and |
write an adaptive application |
are detected and punished |
an adaptive application without |
adaptive application without ports |
application without ports this |
without ports this type |
ports this type of |
this type of disconnected |
type of disconnected operation |
we see a dramatic |
mb files from the |
the number of nodes |
files from the local |
number of nodes incorrectly |
from the local file |
of nodes incorrectly accused |
the local file system |
nodes incorrectly accused also |
local file system into |
incorrectly accused also increases |
file system into the |
accused also increases with |
but not to the |
not to the ex |
system into the mfs |
also increases with higher |
linear increase of the |
into the mfs file |
increases with higher thresholds |
increase of the token |
the mfs file system |
having to take account |
of the token roundtrip |
to take account of |
as observed in the |
the token roundtrip time |
take account of the |
observed in the right |
account of the actual |
in the right graph |
of the actual bandwidth |
the actual bandwidth or |
actual bandwidth or current |
bandwidth or current mix |
scenarios where opportunistic nodes |
or current mix tent |
a slow increase of |
where opportunistic nodes contribute |
current mix tent of |
slow increase of the |
opportunistic nodes contribute at |
mix tent of automatic |
increase of the number |
nodes contribute at higher |
tent of automatic reconciliation |
of the number of |
contribute at higher rates |
of automatic reconciliation of |
the number of messages |
automatic reconciliation of update |
number of messages pending |
reconciliation of update conflicts |
of messages pending ack |
messages pending ack on |
pending ack on the |
ack on the sender |
are less disruptive to |
less disruptive to the |
disruptive to the system |
and a sharp decrease |
a sharp decrease in |
sharp decrease in throughput |
but they also require |
they also require higher |
also require higher thresholds |
on of rpcs at |
require higher thresholds to |
of rpcs at runtime |
higher thresholds to be |
thresholds to be applied |
and avoid having to |
different thresholds yield best |
avoid having to specify |
thresholds yield best results |
having to specify thresholds |
yield best results under |
to specify thresholds at |
best results under different |
specify thresholds at the |
results under different scenarios |
thresholds at the other |
at the other hand |
from the results presented |
the results presented in |
results presented in figure |
level caching reduces the |
caching reduces the delay |
reduces the delay incurred |
the delay incurred which |
delay incurred which it |
incurred which it should |
which it should switch |
it should switch communication |
we concluded that the |
should switch communication modes |
the underlying mechanism is |
concluded that the best |
underlying mechanism is as |
that the best fixed |
an rpc whose results |
mechanism is as follows |
the best fixed threshold |
rpc whose results are |
best fixed threshold is |
whose results are urgently |
fixed threshold is t |
results are urgently required |
are urgently required should |
urgently required should be |
required should be aswhen |
should be aswhen an |
the increased activity of |
be aswhen an application |
aswhen an application opens |
increased activity of the |
an application opens a |
application opens a file |
activity of the garbage |
of the garbage collector |
the garbage collector and |
providing the best compromise |
the best compromise in |
garbage collector and allocation |
best compromise in terms |
as has been shown |
collector and allocation overheads |
compromise in terms of |
has been shown in |
been shown in the |
in terms of performance |
and allocation overheads slow |
shown in the low |
terms of performance and |
allocation overheads slow the |
of performance and false |
overheads slow the system |
performance and false positives |
slow the system down |
and false positives across |
the system down and |
false positives across all |
positives across all scenarios |
system down and processing |
down and processing of |
and processing of the |
processing of the incoming |
of the incoming packets |
the incoming packets and |
incoming packets and tokens |
we compare all three |
packets and tokens takes |
it is possible to |
compare all three strategies |
all three strategies proposed |
is possible to use |
and tokens takes more |
three strategies proposed in |
possible to use a |
tokens takes more time |
strategies proposed in subsection |
to use a signed |
use a signed the |
a signed the highest |
signed the highest priority |
particularly if the rpc |
although the effect is |
if the rpc contains |
the effect is not |
against each other and |
the rpc contains outcontent |
effect is not significant |
each other and against |
is not significant when |
other and against a |
based division of files |
not significant when considering |
and against a configuration |
division of files into |
significant when considering a |
against a configuration with |
of files into blocks |
when considering a single |
a configuration with no |
files into blocks as |
considering a single node |
configuration with no auditing |
into blocks as the |
a single node in |
blocks as the basis |
single node in isolation |
as the basis for |
the basis for re |
a token must visit |
token must visit all |
must visit all nodes |
visit all nodes in |
all nodes in a |
for the fixed threshold |
nodes in a region |
but still important rpcs |
the fixed threshold strategy |
in a region to |
still important rpcs can |
fixed threshold strategy and |
a region to aggregate |
important rpcs can ducing |
threshold strategy and as |
region to aggregate the |
rpcs can ducing client |
strategy and as the |
to aggregate the recovery |
and as the initial |
aggregate the recovery state |
as the initial threshold |
the initial threshold in |
initial threshold in the |
threshold in the stepwise |
in the stepwise adaptive |
the stepwise adaptive strategy |
comparison of packet recovery |
of packet recovery probability |
and delays are cumulative |
we summarize the three |
summarize the three strategies |
the three strategies in |
three strategies in table |
while the lowest levels |
the lowest levels are |
lowest levels are useful |
levels are useful for |
we simulated sessions where |
are useful for server |
useful for server traffic |
for server traffic does |
server traffic does not |
qsm is configured so |
traffic does not eliminate |
is configured so that |
does not eliminate the |
configured so that five |
not eliminate the fundamental |
of the nodes were |
so that five nodes |
eliminate the fundamental problem |
the nodes were opportunistic |
that five nodes in |
the fundamental problem of |
nodes were opportunistic and |
staggered start first i |
five nodes in each |
fundamental problem of rpcs |
were opportunistic and with |
start first i data |
nodes in each region |
problem of rpcs that |
opportunistic and with varying |
first i data packets |
in each region cache |
of rpcs that can |
and with varying ratios |
i data packets added |
each region cache each |
rpcs that can be |
with varying ratios of |
data packets added to |
region cache each packet |
that can be arbitrarily |
varying ratios of contribution |
packets added to a |
can be arbitrarily delayed |
added to a layer |
to a layer with |
a layer with interleave |
layer with interleave i |
if half the nodes |
such as speculative activities |
half the nodes in |
as speculative activities like |
the nodes in a |
the contribution rate of |
speculative activities like prefetching |
r fire immediately with |
contribution rate of opportunistic |
activities like prefetching and |
fire immediately with just |
rate of opportunistic nodes |
like prefetching and transferring |
immediately with just one |
of opportunistic nodes is |
prefetching and transferring archival |
with just one packet |
opportunistic nodes is varied |
and transferring archival data |
just one packet in |
nodes is varied from |
one packet in them |
if the inicontention for |
the inicontention for insufficient |
inicontention for insufficient bandwidth |
for the next i |
the next i data |
next i data packets |
i data packets added |
tial assumption regarding the |
assumption regarding the correct |
regarding the correct priority |
the correct priority level |
node region cache each |
correct priority level for |
region cache each figure |
r fire immediately with |
priority level for an |
all other nodes are |
other nodes are correct |
level for an rpc |
fire immediately with two |
for an rpc proves |
immediately with two data |
contributing at a maximum |
an rpc proves incorrect |
with two data packets |
at a maximum rate |
two data packets in |
a maximum rate of |
data packets in them |
a call to the |
call to the library |
to the library can |
the library can be |
library can be made |
can be made to |
and so on until |
be made to assign |
so on until r |
made to assign a |
on until r i |
until r i data |
r i data packets |
i data packets have |
we present both the |
data packets have been |
present both the average |
packets have been added |
both the average and |
have been added to |
the average and the |
been added to the |
average and the minimum |
added to the layer |
and the minimum download |
to the layer and |
the minimum download factors |
client cache consistency new |
the layer and all |
minimum download factors across |
cache consistency new priority |
layer and all bins |
download factors across all |
and all bins have |
factors across all correct |
all bins have fired |
across all correct nodes |
when a client fetches |
bins have fired exactly |
all correct nodes in |
a client fetches a |
have fired exactly once |
correct nodes in the |
nodes in the system |
client fetches a file |
as the contribution rate |
the contribution rate of |
the file server grants |
contribution rate of opportunistic |
all bins fire at |
file server grants it |
rate of opportunistic nodes |
of opportunistic nodes increases |
server grants it permission |
bins fire at size |
fire at size r |
grants it permission to |
varying the number of |
the download factors are |
it permission to cache |
the number of caching |
download factors are expected |
now that they have |
number of caching replicas |
permission to cache the |
factors are expected to |
that they have been |
of caching replicas per |
to cache the file |
are expected to increase |
they have been staggered |
caching replicas per message |
cache the file for |
have been staggered at |
replicas per message in |
the file for a |
which is clear from |
been staggered at the |
staggered at the start |
file for a limited |
is clear from the |
per message in a |
for a limited period |
clear from the curves |
from the curves presented |
r fire for any |
fire for any i |
for any i data |
and adds it to |
any i data packets |
strategy no auditing fixed |
adds it to a |
no auditing fixed threshold |
it to a list |
the outlined scheme works |
auditing fixed threshold stepwise |
outlined scheme works when |
fixed threshold stepwise adaptive |
scheme works when i |
threshold stepwise adaptive percentile |
works when i is |
when i is greater |
i is greater than |
is greater than or |
greater than or equal |
than or equal to |
or equal to r |
based adaptive description fixed |
implementation of clients that |
adaptive description fixed t |
of clients that cache |
as is usually the |
is usually the case |
clients that cache the |
that cache the file |
if i is smaller |
i is smaller than |
is smaller than r |
if the client modifies |
the client modifies and |
client modifies and then |
modifies and then closes |
the bin with index |
as the replication factor |
and then closes the |
bin with index x |
with index x fires |
then closes the file |
the replication factor increasess |
index x fires at |
it transmits the new |
transmits the new contents |
the new contents to |
new contents to the |
contents to the server |
the sender s flow |
sender s flow control |
s flow control policy |
which mafs is implemented |
flow control policy kicks |
mafs is implemented in |
control policy kicks in |
is implemented in c |
implemented in c on |
in c on freebsd |
if avg sampled download |
avg sampled download factor |
the client is a |
and the system goes |
client is a usermakes |
gc test rw test |
the system goes into |
is a usermakes a |
a usermakes a callback |
usermakes a callback rpc |
a callback rpc to |
callback rpc to any |
rpc to any other |
to any other clients |
any other clients on |
other clients on the |
clients on the list |
the initial firing sizes |
initial firing sizes would |
firing sizes would be |
a client level process |
client level process that |
for the first bin |
the first bin and |
level process that stores |
process that stores cached |
that stores cached files |
stores cached files in |
cached files in a |
for the second bin |
files in a local |
in a local filesystem |
a form of the |
form of the oscillating |
if r and i |
decrease t back to |
of the oscillating state |
r and i are |
the that receives a |
the oscillating state we |
and i are not |
that receives a callback |
oscillating state we encountered |
i are not integral |
receives a callback rpc |
state we encountered in |
are not integral multiples |
when avg download is |
a callback rpc discards |
we encountered in figure |
not integral multiples of |
avg download is satisfactory |
callback rpc discards its |
integral multiples of each |
download is satisfactory again |
rpc discards its cached |
multiples of each other |
discards its cached copy |
its cached copy of |
cached copy of the |
copy of the file |
server also stores its |
limiting still works but |
also stores its copies |
still works but is |
stores its copies of |
works but is slightly |
its copies of files |
but is slightly less |
copies of files in |
is slightly less effective |
the amount of memory |
of files in a |
slightly less effective due |
if avg sampled download |
files in a local |
in a local filesystem |
less effective due to |
avg sampled download factor |
amount of memory in |
effective due to rounding |
due to rounding errors |
of memory in use |
memory in use at |
if an application has |
in use at the |
delaying xors in the |
use at the sender |
an application has the |
at the sender ceases |
xors in the straightforward |
the sender ceases to |
application has the file |
sender ceases to be |
in the straightforward implementation |
ceases to be a |
has the file open |
to be a good |
t is chosen based |
be a good predictor |
repair packets are transmitted |
the file open when |
is chosen based on |
a good predictor of |
packets are transmitted as |
file open when its |
chosen based on sampled |
good predictor of the |
are transmitted as soon |
open when its client |
based on sampled upload |
on sampled upload factors |
transmitted as soon as |
when its client re |
predictor of the amount |
as soon as they |
of the amount of |
soon as they are |
as they are generated |
system operations from applications |
the amount of memory |
operations from applications are |
amount of memory in |
from applications are redirected |
this results in the |
of memory in use |
applications are redirected to |
results in the repair |
memory in use at |
are redirected to user |
in the repair packet |
in use at receivers |
redirected to user level |
the repair packet leaving |
to user level ceives |
repair packet leaving immediately |
user level ceives the |
packet leaving immediately after |
level ceives the callback |
leaving immediately after the |
strategies used for defining |
immediately after the last |
violating what turns out |
used for defining the |
after the last data |
the file is discarded |
what turns out to |
for defining the minimum |
the last data packet |
file is discarded once |
turns out to be |
defining the minimum upload |
last data packet that |
is discarded once it |
out to be an |
the minimum upload threshold |
data packet that was |
discarded once it is |
once it is closed |
minimum upload threshold t |
packet that was added |
that was added to |
was added to it |
to be an implicit |
gw test rc test |
upload threshold t figure |
when through a kernel |
which lowers burst tolerance |
be an implicit requirement |
through a kernel module |
lowers burst tolerance if |
an implicit requirement of |
shows that all strategies |
a kernel module at |
burst tolerance if the |
implicit requirement of our |
that all strategies yield |
kernel module at the |
tolerance if the repair |
requirement of our flow |
all strategies yield significantly |
module at the client |
if the repair packet |
strategies yield significantly better |
the repair packet was |
yield significantly better results |
repair packet was generated |
significantly better results compared |
packet was generated at |
better results compared to |
was generated at interleave |
results compared to an |
generated at interleave i |
compared to an approach |
to an approach with |
an approach with no |
approach with no auditing |
the resulting protocol can |
fetch prefetch metadata store |
resulting protocol can tolerate |
prefetch metadata store fetch |
while both adaptive strategies |
protocol can tolerate a |
metadata store fetch file |
both adaptive strategies yield |
can tolerate a burst |
store fetch file attributes |
adaptive strategies yield excellent |
tolerate a burst of |
strategies yield excellent download |
a burst of i |
yield excellent download rates |
burst of i lost |
excellent download rates to |
of i lost data |
download rates to correct |
i lost data packets |
pull file update fetch |
rates to correct nodes |
lost data packets excluding |
file update fetch file |
data packets excluding the |
packets excluding the repair |
update fetch file data |
overheads in a perturbed |
the fixed threshold strategy |
fixed threshold strategy s |
but the burst could |
in a perturbed system |
threshold strategy s performance |
the burst could swallow |
a perturbed system the |
strategy s performance is |
burst could swallow both |
s performance is not |
perturbed system the reader |
prefetch file data lock |
could swallow both the |
performance is not as |
file data lock a |
data lock a file |
swallow both the repair |
is not as good |
system the reader might |
both the repair and |
not as good when |
the reader might wonder |
the repair and the |
as good when opportunistic |
reader might wonder whether |
repair and the last |
most metadata rpcs store |
good when opportunistic nodes |
might wonder whether our |
and the last data |
metadata rpcs store file |
when opportunistic nodes are |
wonder whether our results |
the last data packet |
rpcs store file data |
opportunistic nodes are contributing |
nodes are contributing with |
last data packet in |
whether our results would |
unlink file such as |
data packet in it |
file such as deleting |
our results would be |
packet in it as |
results would be different |
such as deleting a |
would be different if |
or slightly more kbps |
in it as they |
as deleting a modified |
deleting a modified file |
be different if the |
it as they are |
relative speedup relative speedup |
different if the system |
as they are not |
speedup relative speedup relative |
such optimisations can be |
if the system experienced |
they are not separated |
relative speedup relative speedup |
optimisations can be effective |
the system experienced high |
are not separated by |
can be effective at |
system experienced high loss |
not separated by the |
be effective at low |
at those rates opportunistic |
experienced high loss rates |
separated by the requisite |
effective at low bandwidth |
those rates opportunistic nodes |
high loss rates or |
by the requisite interleave |
rates opportunistic nodes are |
loss rates or was |
when there is a |
opportunistic nodes are harmful |
nodes are harmful to |
there is a natural |
is a natural delay |
the solution to this |
are harmful to the |
solution to this is |
harmful to the system |
rates or was otherwise |
to this is simple |
but at high bandwidth |
or was otherwise perturbed |
this is simple delay |
yet the fixed threshold |
is simple delay sending |
the fixed threshold of |
an artificial delay in |
simple delay sending the |
uniform priorities async relative |
artificial delay in writing |
delay sending the repair |
priorities async relative speedup |
delay in writing back |
sending the repair packet |
async relative speedup gc |
is not able to |
not able to detect |
the repair packet generated |
in writing back updates |
relative speedup gc test |
we performed an experiment |
able to detect them |
performed an experiment in |
writing back updates introduces |
repair packet generated by |
an experiment in which |
back updates introduces inconsistencies |
packet generated by a |
experiment in which one |
updates introduces inconsistencies between |
generated by a repair |
in which one of |
introduces inconsistencies between the |
by a repair bin |
which one of the |
inconsistencies between the client |
a repair bin until |
we consider a scenario |
between the client and |
one of the receiver |
repair bin until the |
consider a scenario where |
the client and the |
of the receiver nodes |
bin until the next |
a scenario where opportunistic |
client and the file |
the receiver nodes experiences |
until the next time |
scenario where opportunistic nodes |
and the file server |
receiver nodes experiences a |
the next time a |
where opportunistic nodes contribute |
nodes experiences a periodic |
next time a data |
opportunistic nodes contribute with |
this can be acceptable |
time a data packet |
nodes contribute with different |
can be acceptable at |
a data packet is |
contribute with different rates |
be acceptable at low |
data packet is added |
acceptable at low bandwidths |
packet is added to |
is added to the |
we varied the percentage |
added to the now |
varied the percentage of |
to the now empty |
the now empty bin |
the percentage of opportunistic |
when the user may |
percentage of opportunistic nodes |
the user may table |
of opportunistic nodes in |
opportunistic nodes in the |
which happens i packets |
nodes in the system |
in the system from |
happens i packets later |
s the node sleeps |
i packets later and |
the node sleeps for |
priorities for mafs remote |
packets later and introduces |
for mafs remote procedure |
later and introduces the |
mafs remote procedure calls |
and introduces the required |
introduces the required interleave |
the required interleave between |
required interleave between the |
interleave between the repair |
be grateful to be |
between the repair packet |
grateful to be able |
and evenly assigned them |
the repair packet and |
to be able to |
evenly assigned them different |
repair packet and the |
be able to use |
assigned them different contribution |
packet and the last |
able to use the |
them different contribution rates |
and the last data |
to use the file |
the last data packet |
use the file system |
last data packet included |
the file system at |
the graphs present the |
data packet included in |
file system at all |
this simulates the effect |
graphs present the average |
packet included in it |
simulates the effect of |
present the average and |
but should be avoided |
the effect of disruptive |
the average and minimum |
should be avoided when |
notice that although transmitting |
average and minimum download |
be avoided when bandwidth |
that although transmitting the |
and minimum download rates |
avoided when bandwidth is |
although transmitting the xor |
minimum download rates for |
when bandwidth is unconstrained |
transmitting the xor immediately |
download rates for these |
the xor immediately results |
rates for these scenarios |
xor immediately results in |
in the loss scenario |
immediately results in faster |
results in faster recovery |
no auditing performs significantly |
doing so also reduces |
auditing performs significantly worse |
so also reduces the |
performs significantly worse than |
also reduces the probability |
significantly worse than any |
mafs avoids the need |
reduces the probability of |
avoids the need for |
worse than any of |
the need for modes |
s the node drops |
the probability of a |
than any of the |
need for modes by |
the node drops all |
probability of a lost |
any of the proposed |
for modes by using |
node drops all incoming |
of a lost packet |
of the proposed strategies |
modes by using asynchronous |
drops all incoming packets |
a lost packet being |
by using asynchronous remote |
all incoming packets for |
lost packet being recovered |
using asynchronous remote procedure |
the stepwise adaptive approach |
asynchronous remote procedure calls |
stepwise adaptive approach yields |
remote procedure calls between |
adaptive approach yields the |
procedure calls between a |
approach yields the best |
off results in a |
calls between a client |
yields the best results |
results in a minor |
between a client and |
the best results when |
in a minor control |
a client and the |
best results when large |
a minor control knob |
client and the file |
results when large percentages |
minor control knob permitting |
and the file server |
when large percentages of |
control knob permitting us |
the file server writeback |
large percentages of opportunistic |
knob permitting us to |
file server writeback at |
percentages of opportunistic nodes |
permitting us to balance |
server writeback at all |
of opportunistic nodes are |
us to balance speed |
writeback at all bandwidth |
opportunistic nodes are present |
to balance speed against |
at all bandwidth levels |
nodes are present in |
balance speed against burst |
are present in the |
speed against burst tolerance |
present in the system |
and incorporates a new |
incorporates a new upare |
a new upare divided |
our default configuration is |
new upare divided into |
it is also simpler |
default configuration is to |
upare divided into several |
is also simpler than |
configuration is to transmit |
divided into several types |
also simpler than the |
simpler than the percentile |
is to transmit the |
into several types depending |
the loss rate is |
to transmit the xor |
several types depending on |
loss rate is higher |
transmit the xor immediately |
types depending on their |
since it is based |
depending on their function |
it is based only |
is based only on |
based only on samples |
only on samples of |
on samples of the |
samples of the download |
rpcs date propagation algorithm |
of the download rates |
date propagation algorithm to |
the download rates of |
propagation algorithm to reduce |
download rates of nodes |
algorithm to reduce the |
to reduce the possibility |
reduce the possibility of |
the possibility of inconsisto |
in both sets of |
envelope analysis to start |
possibility of inconsisto fetch |
both sets of experiments |
analysis to start with |
of inconsisto fetch and |
inconsisto fetch and store |
fetch and store data |
and store data are |
the number of false |
we note that no |
store data are self |
number of false positives |
note that no two |
of false positives was |
that no two repair |
false positives was practically |
no two repair packets |
positives was practically null |
two repair packets generated |
was practically null under |
repair packets generated at |
practically null under all |
packets generated at different |
null under all three |
generated at different interleaves |
under all three strategies |
at different interleaves i |
because recovery traffic interferes |
all three strategies considered |
recovery traffic interferes with |
traffic interferes with regular |
interferes with regular multicast |
at most one in |
most one in some |
one in some cases |
as new operations are |
new operations are added |
operations are added to |
are added to the |
added to the tail |
to the tail tions |
the tail tions include |
tail tions include fetching |
tions include fetching and |
include fetching and setting |
fetching and setting file |
and setting file attributes |
and directory of the |
directory of the log |
will have more than |
the client flushes operations |
have more than one |
cpu utilization at the |
auditing costs the overheads |
utilization at the receivers |
client flushes operations serially |
more than one data |
costs the overheads imposed |
at the receivers is |
flushes operations serially from |
than one data packet |
the overheads imposed by |
the receivers is in |
operations serially from the |
relative speedup relative speedup |
overheads imposed by auditing |
receivers is in the |
one data packet in |
serially from the head |
speedup relative speedup relative |
imposed by auditing are |
data packet in common |
from the head of |
relative speedup relative speedup |
by auditing are an |
packet in common as |
the head of operations |
auditing are an important |
in common as long |
head of operations such |
are an important consideration |
common as long as |
of operations such as |
as long as the |
operations such as creating |
which we address in |
long as the least |
such as creating and |
we address in this |
as the least common |
as creating and unlinking |
address in this subsection |
the least common multiple |
creating and unlinking files |
most of the work |
of the work of |
the work of auditing |
control rpcs the log |
work of auditing is |
of auditing is performed |
of the interleaves is |
auditing is performed by |
the interleaves is greater |
is performed by local |
interleaves is greater than |
performed by local auditors |
is greater than r |
greater than r i |
server traffic consists of |
range and doesn t |
which are executed on |
traffic consists of a |
and doesn t grow |
are executed on the |
consists of a variety |
doesn t grow with |
executed on the user |
pairings of repair bins |
of a variety of |
t grow with system |
on the user nodes |
of repair bins in |
a variety of foreground |
grow with system size |
repair bins in two |
variety of foreground include |
the overhead is constant |
bins in two different |
of foreground include locking |
in two different layers |
foreground include locking files |
independent of the size |
include locking files and |
two different layers with |
of the size of |
locking files and the |
different layers with interleaves |
the size of the |
files and the server |
layers with interleaves i |
size of the system |
and the server s |
the server s callback |
server s callback to |
s callback to invalidate |
and is not significant |
callback to invalidate a |
to invalidate a rpcs |
invalidate a rpcs for |
a rpcs for control |
rpcs for control operations |
since nodes only exchange |
for control operations and |
nodes only exchange a |
control operations and fetching |
only exchange a small |
operations and fetching file |
exchange a small amount |
and fetching file data |
a small amount of |
small amount of accounting |
amount of accounting data |
of accounting data at |
accounting data at pre |
and a stream client |
a stream client s |
stream client s cached |
client s cached copy |
s cached copy of |
defined intervals of time |
cached copy of a |
copy of a file |
in the sleep scenario |
a good rule of |
of background rpcs for |
good rule of thumb |
background rpcs for logged |
rule of thumb is |
rpcs for logged operations |
of thumb is to |
thumb is to select |
the decrease starts at |
is to select interleaves |
when bandwidth is high |
decrease starts at about |
to select interleaves that |
select interleaves that are |
interleaves that are relatively |
if we consider a |
replayed logged operations complete |
that are relatively prime |
we consider a packet |
logged operations complete quickly |
are relatively prime to |
consider a packet rate |
relatively prime to maximize |
a packet rate of |
prime to maximize their |
with little extra delay |
to maximize their lcm |
nodes and proceeds steadily |
and proceeds steadily thereafter |
when bandwidth is low |
and also ensure that |
also ensure that the |
ensure that the larger |
that the larger interleave |
logged operations are de |
the larger interleave is |
larger interleave is greater |
interleave is greater than |
is greater than r |
it doesn t appear |
communication adaptation layed in |
doesn t appear to |
adaptation layed in proportion |
let us assume that |
t appear to be |
layed in proportion to |
seconds the maximum number |
us assume that packets |
appear to be correlated |
in proportion to the |
the maximum number of |
assume that packets are |
that packets are dropped |
proportion to the foreground |
maximum number of packets |
to be correlated to |
packets are dropped with |
to the foreground rpc |
number of packets received |
be correlated to the |
are dropped with uniform |
the foreground rpc traffic |
of packets received and |
correlated to the amount |
foreground rpc traffic and |
packets received and sent |
to the amount of |
rpc traffic and the |
received and sent by |
given a lost data |
a lost data packet |
traffic and the availto |
and sent by each |
the amount of loss |
and the availto reduce |
sent by each node |
what is the probability |
the availto reduce its |
by each node is |
is the probability that |
availto reduce its network |
the probability that we |
reduce its network communication |
which oscillates at the |
probability that we can |
its network communication when |
oscillates at the level |
that we can recover |
network communication when bandwidth |
at the level of |
we can recover it |
communication when bandwidth is |
when bandwidth is low |
for each packet sent |
we can recover a |
each packet sent or |
can recover a data |
packet sent or received |
recover a data packet |
a data packet if |
a mobile file system |
data packet if at |
the history needs to |
mobile file system client |
packet if at least |
history needs to indicate |
file system client can |
if at least one |
needs to indicate which |
system client can automatically |
at least one of |
to indicate which neighbor |
client can automatically adapt |
least one of the |
indicate which neighbor sent |
can automatically adapt its |
one of the c |
which neighbor sent or |
automatically adapt its communication |
of the c xors |
neighbor sent or received |
adapt its communication strategy |
the c xors containing |
sent or received the |
its communication strategy to |
c xors containing it |
or received the packet |
communication strategy to the |
xors containing it is |
strategy to the available |
containing it is received |
to the available bandwidth |
it is received correctly |
is received correctly and |
received correctly and usable |
bits to identify each |
to identify each neighbor |
the history s size |
history s size adds |
s size adds up |
size adds up to |
all the other data |
the other data packets |
rpc priorities cations transfer |
other data packets in |
priorities cations transfer a |
data packets in it |
cations transfer a large |
packets in it have |
transfer a large volume |
in it have also |
a large volume of |
it have also been |
large volume of data |
have also been received |
also been received correctly |
volume of data that |
in the controlled loss |
of data that the |
the controlled loss scenario |
data that the user |
the probability of which |
that the user is |
probability of which is |
of which is simply |
the user is unlikely |
user is unlikely to |
is unlikely to require |
unlikely to require immediately |
throughput remains fairly constant |
this is not significant |
is not significant compared |
not significant compared to |
significant compared to the |
consuming bandwidth that can |
compared to the amount |
bandwidth that can be |
to the amount of |
that can be used |
the amount of regular |
until it falls sharply |
can be used mafs |
amount of regular data |
the probability of a |
be used mafs uses |
it falls sharply beyond |
of regular data exchanged |
probability of a received |
used mafs uses priorities |
regular data exchanged in |
of a received xor |
mafs uses priorities to |
data exchanged in a |
a received xor being |
uses priorities to reduce |
exchanged in a streaming |
in a streaming session |
priorities to reduce contention |
to reduce contention between |
reduce contention between foreground |
contention between foreground for |
we also analyzed the |
between foreground for important |
also analyzed the costs |
foreground for important tasks |
analyzed the costs of |
received xor being unusable |
the costs of the |
xor being unusable is |
costs of the global |
being unusable is the |
of the global auditors |
unusable is the complement |
consider an application that |
an application that activities |
application that activities and |
that activities and deferrable |
since they are dedicated |
activities and deferrable background |
they are dedicated and |
and deferrable background activities |
are dedicated and external |
dedicated and external to |
and external to the |
external to the system |
performance does not appear |
adaptive rpc fetches images |
does not appear to |
rpc fetches images from |
the overhead imposed by |
not appear to be |
fetches images from a |
overhead imposed by them |
appear to be directly |
images from a file |
imposed by them is |
by them is of |
from a file server |
to be directly correlated |
them is of higher |
is of higher concern |
be directly correlated to |
processes each in turn |
directly correlated to the |
global auditors main tasks |
correlated to the observed |
auditors main tasks consist |
to the observed packet |
main tasks consist of |
the observed packet loss |
tasks consist of sampling |
preferentially allocates bandwidth to |
the probability x of |
consist of sampling the |
probability x of a |
allocates bandwidth to foreground |
x of a sent |
of sampling the system |
of a sent xor |
bandwidth to foreground rpcs |
sampling the system to |
a sent xor being |
the system to collect |
sent xor being dropped |
system to collect download |
unlike plays the resulting |
xor being dropped or |
to collect download and |
throughput is uncorrelated with |
being dropped or unusable |
plays the resulting image |
collect download and upload |
is uncorrelated with memory |
dropped or unusable is |
download and upload rates |
and upload rates of |
upload rates of nodes |
or unusable is the |
uncorrelated with memory use |
and writes it to |
unusable is the sum |
and of occasionally disseminating |
is the sum of |
with memory use both |
writes it to the |
it to the server |
the sum of the |
memory use both on |
of occasionally disseminating updates |
sum of the probability |
use both on the |
occasionally disseminating updates to |
if the user little |
of the probability that |
both on the perturbed |
disseminating updates to the |
the user little work |
the probability that it |
on the perturbed receiver |
updates to the threshold |
probability that it was |
to the threshold value |
that it was dropped |
it was dropped and |
was dropped and the |
dropped and the probability |
and the probability that |
the probability that it |
probability that it was |
that it was received |
it was received and |
was received and unusable |
the sample size remains |
sample size remains fixed |
which assigns a lower |
size remains fixed independent |
assigns a lower priority |
remains fixed independent of |
a lower priority to |
fixed independent of the |
lower priority to writeback |
independent of the size |
priority to writeback in |
of the size of |
to writeback in wants |
the size of the |
writeback in wants to |
size of the population |
in wants to see |
wants to see the |
to see the processed |
see the processed images |
we ran simulations to |
ran simulations to estimate |
simulations to estimate the |
to estimate the worst |
case standard deviation of |
one else wants to |
else wants to im |
standard deviation of the |
deviation of the download |
of the download rates |
the download rates across |
download rates across all |
rates across all nodes |
mafs has a finer |
we estimate that a |
estimate that a sample |
that a sample size |
a sample size of |
grained differentiation mediately read |
differentiation mediately read them |
performance of prioritised rpc |
at scales of up |
of prioritised rpc with |
writing the output back |
scales of up to |
prioritised rpc with respect |
the output back will |
rpc with respect to |
nodes is sufficient to |
output back will interfere |
with respect to bandwidth |
is sufficient to provide |
back will interfere with |
will interfere with between |
interfere with between rpcs |
respect to bandwidth variation |
and uses priorities at |
uses priorities at all |
priorities at all bandwidths |
since it is easy |
memory usage actually decreases |
each pair of graphs |
it is easy to |
independent of the population |
of the population size |
this alfetching the next |
is easy to ensure |
a consequence of the |
alfetching the next image |
pair of graphs in |
easy to ensure that |
consequence of the cooperative |
of graphs in shows |
to ensure that no |
such as the ones |
of the cooperative caching |
and slow down the |
slow down the application |
ensure that no two |
as the ones simulated |
the cooperative caching policy |
graphs in shows the |
that no two xors |
the ones simulated in |
cooperative caching policy described |
in shows the speedup |
lows control over bandwidth |
no two xors share |
ones simulated in this |
caching policy described in |
shows the speedup of |
control over bandwidth allocation |
two xors share more |
simulated in this work |
policy described in section |
the speedup of one |
over bandwidth allocation at |
xors share more than |
even a smaller number |
bandwidth allocation at the |
speedup of one of |
share more than one |
a smaller number of |
allocation at the level |
of one of three |
more than one data |
smaller number of samples |
at the level of |
the shape of the |
one of three cache |
than one data packet |
number of samples was |
the level of individinterference |
shape of the performance |
of three cache manager |
the usability probabilities of |
level of individinterference due |
of the performance curve |
of samples was found |
three cache manager configurations |
usability probabilities of different |
of individinterference due to |
the performance curve does |
samples was found to |
probabilities of different xors |
individinterference due to write |
was found to be |
of different xors are |
relative to the time |
due to write traffic |
found to be sufficient |
different xors are independent |
to the time taken |
to write traffic is |
to be sufficient to |
correlate closely with the |
the time taken by |
write traffic is often |
be sufficient to yield |
the probability of all |
closely with the number |
time taken by uniform |
traffic is often solved |
sufficient to yield satisfactory |
probability of all the |
with the number of |
taken by uniform priorities |
is often solved by |
to yield satisfactory results |
of all the c |
the number of unacknowledged |
by uniform priorities with |
often solved by writing |
all the c xors |
number of unacknowledged requests |
uniform priorities with synchronous |
solved by writing ual |
the c xors being |
centralized costs are fixed |
priorities with synchronous rpcs |
by writing ual rpcs |
c xors being dropped |
with synchronous rpcs at |
xors being dropped or |
and provide a clear |
being dropped or unusable |
provide a clear advantage |
dropped or unusable is |
without requiring that an |
a clear advantage for |
or unusable is xc |
requiring that an mafs |
clear advantage for using |
that an mafs client |
advantage for using auditing |
an mafs client is |
for using auditing against |
mafs client is aware |
using auditing against tit |
client is aware of |
the probability of correctly |
is aware of back |
probability of correctly receiving |
aware of back updates |
of correctly receiving at |
of back updates asynchronously |
we conclude that the |
tat approaches in large |
correctly receiving at least |
conclude that the drop |
receiving at least one |
that the drop in |
the application in our |
at least one usable |
as well as uniform |
the drop in performance |
application in our example |
least one usable xor |
well as uniform priorities |
drop in performance in |
in our example the |
one usable xor is |
as uniform priorities and |
in performance in these |
heterogenous systems so far |
uniform priorities and synchronous |
our example the precise |
example the precise bandwidth |
systems so far we |
priorities and synchronous rpcs |
performance in these scenarios |
so far we considered |
in these scenarios can |
can start reading another |
the probability of recovering |
far we considered the |
these scenarios can t |
start reading another image |
probability of recovering the |
we considered the use |
scenarios can t be |
reading another image without |
of recovering the lost |
considered the use of |
can t be explained |
the graphs also show |
recovering the lost data |
the use of auditing |
another image without waiting |
t be explained by |
graphs also show curves |
the lost data packet |
use of auditing to |
image without waiting for |
be explained by correlation |
also show curves for |
lost data packet is |
of auditing to enforce |
without waiting for the |
explained by correlation with |
show curves for differentiated |
auditing to enforce node |
waiting for the previwhen |
by correlation with cpu |
curves for differentiated priorities |
to enforce node contribution |
for the previwhen choosing |
correlation with cpu activity |
for differentiated priorities and |
enforce node contribution in |
the previwhen choosing priorities |
differentiated priorities and synchronous |
node contribution in systems |
priorities and synchronous rpcs |
contribution in systems where |
automatic assignment and fine |
in systems where all |
assignment and fine ous |
systems where all nodes |
and fine ous output |
where all nodes are |
or loss rates at |
fine ous output to |
all nodes are assumed |
loss rates at the |
ous output to be |
nodes are assumed to |
rates at the receivers |
output to be sent |
are assumed to have |
and differentiated priorities and |
to be sent to |
assumed to have homogeneous |
differentiated priorities and asynchronous |
form formula only gives |
be sent to the |
but that it does |
to have homogeneous bandwidth |
priorities and asynchronous rpcs |
formula only gives us |
sent to the file |
to the file server |
have homogeneous bandwidth resources |
only gives us a |
that it does appear |
gives us a lower |
it does appear correlated |
asynchronous writeback granularity are |
us a lower bound |
does appear correlated to |
enough to upload and |
writeback granularity are preferable |
a lower bound on |
appear correlated to slower |
to upload and download |
lower bound on the |
to avoid the need |
correlated to slower cleanup |
upload and download at |
the values plotted for |
bound on the recovery |
avoid the need for |
to slower cleanup and |
and download at a |
values plotted for bandwidth |
on the recovery probability |
the need for user |
slower cleanup and the |
download at a rate |
plotted for bandwidth of |
need for user intervenallows |
for user intervenallows i |
at a rate close |
since the xor usability |
cleanup and the resulting |
a rate close to |
the xor usability formula |
and the resulting memory |
o and cpu processing |
rate close to the |
xor usability formula does |
and cpu processing to |
close to the stream |
usability formula does not |
cpu processing to be |
to the stream rate |
formula does not factor |
processing to be overlapped |
related overheads at the |
does not factor in |
pullbased streaming may be |
not factor in the |
overheads at the sender |
streaming may be extended |
factor in the probability |
may be extended to |
in the probability of |
tion and provide the |
be extended to heterogenous |
the probability of the |
s are the same |
the effect is much |
and provide the maximum |
probability of the other |
extended to heterogenous systems |
are the same as |
effect is much stronger |
provide the maximum degree |
of the other data |
to heterogenous systems by |
the same as shown |
is much stronger than |
the maximum degree of |
the other data packets |
heterogenous systems by organizing |
same as shown in |
much stronger than in |
maximum degree of differentiation |
other data packets in |
systems by organizing nodes |
as shown in table |
stronger than in the |
degree of differentiation among |
data packets in the |
by organizing nodes into |
than in the undisturbed |
of differentiation among ecution |
packets in the xor |
organizing nodes into multiple |
nodes into multiple groups |
due to the overhead |
in the xor being |
in the undisturbed experiments |
differentiation among ecution time |
to the overhead of |
the xor being dropped |
among ecution time and |
the overhead of priorities |
xor being dropped and |
ecution time and utilising |
the number of pending |
overhead of priorities for |
being dropped and recovered |
time and utilising bandwidth |
number of pending messages |
of priorities for small |
and utilising bandwidth more |
of pending messages starts |
priorities for small rpcs |
utilising bandwidth more efficiently |
pending messages starts at |
for small rpcs mentioned |
we extend the analysis |
messages starts at a |
small rpcs mentioned in |
extend the analysis to |
starts at a higher |
rpcs mentioned in section |
the analysis to bursty |
analysis to bursty losses |
at a higher level |
scheduling rpcs based on |
rpcs based on priorities |
based on priorities is |
on priorities is only |
if the lost data |
priorities is only ever |
no auditing fixed threshold |
the lost data packet |
auditing fixed threshold stepwise |
lost data packet was |
if bandwidth is low |
fixed threshold stepwise percentile |
data packet was part |
packet was part of |
was part of a |
part of a loss |
of a loss burst |
contention arises when files |
a loss burst of |
arises when files are |
loss burst of size |
burst of size b |
when files are being |
files are being effective |
are being effective if |
being effective if concurrent |
repair packets generated at |
effective if concurrent rpcs |
packets generated at interleaves |
if concurrent rpcs usually |
comparing the execution time |
generated at interleaves less |
concurrent rpcs usually end |
avg download factor min |
the execution time of |
at interleaves less than |
rpcs usually end up |
token roundtrip time increases |
execution time of the |
interleaves less than b |
download factor min download |
usually end up with |
time of the foreground |
less than b are |
factor min download factor |
end up with different |
of the foreground workloads |
than b are dropped |
up with different prifetched |
the foreground workloads with |
b are dropped or |
with different prifetched at |
foreground workloads with synchronous |
are dropped or useless |
and if a failure |
different prifetched at the |
workloads with synchronous writes |
dropped or useless with |
or useless with high |
prifetched at the same |
if a failure occurs |
useless with high probability |
at the same time |
the same time as |
same time as updates |
update logging and asynchronous |
and we can discount |
we can discount them |
logging and asynchronous writeback |
time as updates are |
as updates are written |
updates are written back |
and asynchronous writeback reveals |
asynchronous writeback reveals that |
of recovering the data |
writeback reveals that the |
token rounds before repair |
no auditing fixed threshold |
recovering the data packet |
reveals that the latter |
rounds before repair occurs |
auditing fixed threshold stepwise |
the data packet is |
but processes are too |
processes are too coarse |
fixed threshold stepwise percentile |
data packet is then |
that the latter two |
and then another round |
the latter two options |
grained for this purpose |
then another round before |
latter two options generally |
another round before cleanup |
two options generally perform |
round before cleanup takes |
tention can be mitigated |
options generally perform comparably |
before cleanup takes place |
is the number of |
can be mitigated by |
generally perform comparably to |
the number of xors |
be mitigated by prioritising |
perform comparably to or |
number of xors generated |
mitigated by prioritising file |
comparably to or better |
of xors generated at |
by prioritising file fetch |
to or better than |
xors generated at interleaves |
prioritising file fetch rpcs |
or better than synchronous |
generated at interleaves greater |
file fetch rpcs above |
fetch rpcs above file |
at interleaves greater than |
interleaves greater than b |
better than synchronous writes |
based priorities provide some |
priorities provide some more |
provide some more detail |
the formulae derived for |
formulae derived for xor |
derived for xor usability |
for xor usability still |
but the imporwriteback rpcs |
xor usability still hold |
the imporwriteback rpcs to |
logging and asynchronous writeback |
imporwriteback rpcs to ensure |
and asynchronous writeback greatly |
rpcs to ensure that |
since packet losses with |
asynchronous writeback greatly improve |
to ensure that they |
packet losses with more |
writeback greatly improve the |
ensure that they will |
losses with more than |
greatly improve the performance |
that they will be |
these account for the |
improve the performance of |
with more than b |
they will be preferentially |
will be preferentially allo |
the performance of the |
more than b intervening |
account for the rapid |
performance of the background |
tance of a file |
for the rapid increase |
than b intervening packets |
of the background workloads |
of a file can |
the rapid increase in |
b intervening packets between |
a file can be |
rapid increase in acknowledgement |
intervening packets between them |
file can be hard |
as has been noted |
increase in acknowledgement latency |
packets between them have |
can be hard to |
has been noted previously |
between them have independent |
be hard to determine |
them have independent probability |
hard to determine automatically |
there is only correlation |
is only correlation within |
only correlation within the |
correlation within the bursts |
upload rate of opportunistic |
rate of opportunistic nodes |
how does this compare |
does this compare to |
this compare to traditional |
files can be too |
can be too numerous |
be too numerous for |
as the number of |
too numerous for the |
the number of caching |
numerous for the user |
number of caching replicas |
for the user to |
codes such as reed |
of caching replicas increases |
the user to manually |
user to manually assign |
to manually assign priin |
manually assign priin this |
assign priin this section |
we assess the effectiveness |
assess the effectiveness of |
the effectiveness of asynchronous |
effectiveness of asynchronous orities |
rpcs are more numerous |
we focus on mfs |
focus on mfs with |
but priorities can be |
on mfs with asynchronous |
priorities can be autowriteback |
mfs with asynchronous writeback |
can be autowriteback and |
c repair packets are |
be autowriteback and rpc |
with asynchronous writeback in |
repair packets are generated |
asynchronous writeback in the |
autowriteback and rpc priorities |
writeback in the rest |
packets are generated and |
in the rest of |
and rpc priorities in |
the rest of this |
rpc priorities in mafs |
rest of this paper |
priorities in mafs under |
of this paper because |
in mafs under different |
are generated and sent |
throughput in the experiments |
this paper because it |
mafs under different levels |
generated and sent for |
in the experiments with |
paper because it provides |
under different levels matically |
and sent for every |
the experiments with a |
because it provides comparable |
different levels matically assigned |
sent for every r |
experiments with a perturbed |
it provides comparable performance |
levels matically assigned to |
for every r data |
every r data packets |
provides comparable performance to |
matically assigned to them |
with a perturbed node |
comparable performance to logged |
assigned to them according |
and the correct delivery |
performance to logged updates |
to them according to |
the correct delivery of |
them according to the |
correct delivery of any |
according to the operation |
delivery of any r |
to the operation the |
of any r of |
the operation the rpc |
upload rate of opportunistic |
any r of the |
r of the r |
operation the rpc of |
rate of opportunistic nodes |
allows straightforward modeless adaptation |
c packets transmitted is |
the rpc of bandwidth |
rpc of bandwidth availability |
packets transmitted is sufficient |
straightforward modeless adaptation to |
transmitted is sufficient to |
modeless adaptation to bandwidth |
is sufficient to reconstruct |
adaptation to bandwidth variation |
sufficient to reconstruct the |
we examine the degree |
to reconstruct the original |
minimum and average download |
examine the degree corresponds |
reconstruct the original r |
and average download factors |
the degree corresponds to |
the original r data |
and is easily extensible |
average download factors across |
original r data packets |
is easily extensible to |
download factors across all |
as shown in table |
easily extensible to more |
factors across all correct |
extensible to more than |
across all correct nodes |
given a lost data |
a lost data packet |
all correct nodes when |
to more than one |
average packet loss observed |
correct nodes when using |
we can recover it |
packet loss observed at |
more than one level |
nodes when using different |
or rpcs to which |
can recover it if |
loss observed at the |
than one level of |
when using different strategies |
rpcs to which a |
recover it if at |
observed at the perturbed |
one level of priority |
using different strategies for |
to which a file |
it if at least |
at the perturbed node |
different strategies for choosing |
which a file system |
if at least r |
which is required for |
strategies for choosing the |
a file system client |
at least r packets |
is required for our |
for choosing the threshold |
file system client that |
least r packets are |
required for our cache |
system client that avoids |
the upload contribution rate |
for our cache consistency |
r packets are received |
client that avoids switching |
upload contribution rate of |
memory usage at the |
packets are received correctly |
that avoids switching modes |
our cache consistency algorithm |
contribution rate of opportunistic |
usage at the perturbed |
are received correctly in |
avoids switching modes in |
switching modes in re |
at the perturbed node |
received correctly in the |
rate of opportunistic nodes |
since reducing available bandwidth |
correctly in the encoding |
reducing available bandwidth increases |
that the user has |
of opportunistic nodes is |
in the encoding set |
the encoding set of |
available bandwidth increases the |
the user has to |
opportunistic nodes is varied |
at unperturbed nodes it |
encoding set of r |
bandwidth increases the contention |
user has to wait |
nodes is varied in |
unperturbed nodes it is |
increases the contention between |
has to wait for |
is varied in the |
varied in the x |
nodes it is similar |
the contention between rpcs |
c data and repair |
contention between rpcs of |
data and repair packets |
between rpcs of different |
and the number of |
and repair packets that |
rpcs of different types |
the number of opportunistic |
repair packets that the |
or sponse to bandwidth |
number of opportunistic nodes |
packets that the lost |
sponse to bandwidth changes |
of opportunistic nodes is |
that the lost packet |
to bandwidth changes is |
opportunistic nodes is fixed |
the lost packet belongs |
lost packet belongs to |
bandwidth changes is able |
nodes is fixed at |
the benefits of rpc |
changes is able to |
benefits of rpc priorities |
is able to adapt |
of rpc priorities should |
able to adapt to |
the probability of recovering |
rpc priorities should be |
to adapt to both |
although it would be |
probability of recovering a |
priorities should be more |
adapt to both insufficient |
it would be hard |
of recovering a lost |
should be more apparent |
avg download factor min |
would be hard to |
recovering a lost packet |
to both insufficient rpcs |
be more apparent at |
download factor min download |
be hard to precisely |
a lost packet is |
both insufficient rpcs whose |
more apparent at lower |
factor min download factor |
hard to precisely measure |
lost packet is equivalent |
insufficient rpcs whose results |
apparent at lower priorities |
to precisely measure these |
packet is equivalent to |
rpcs whose results can |
whose results can be |
is equivalent to the |
precisely measure these delays |
results can be delayed |
equivalent to the probability |
to the probability of |
the probability of losing |
probability of losing c |
such as writing back |
as writing back data |
writing back data bandwidth |
shows the experiments of |
or less packets from |
measuring alarm delays sheds |
and conditions under which |
less packets from the |
the experiments of table |
alarm delays sheds light |
conditions under which bandwidth |
packets from the total |
from the total r |
under which bandwidth is |
which bandwidth is plentiful |
extended to a wider |
delays sheds light on |
to a wider range |
no auditing fixed threshold |
sheds light on the |
a wider range of |
auditing fixed threshold stepwise |
light on the magnitude |
since the number of |
wider range of bandwidth |
fixed threshold stepwise percentile |
on the magnitude of |
the number of other |
range of bandwidth values |
prefetching is an example |
the magnitude of the |
number of other lost |
is an example of |
magnitude of the problem |
of other lost packets |
an example of speculative |
in these and later |
other lost packets in |
example of speculative communication |
recall that our timesharing |
lost packets in the |
these and later experiments |
that our timesharing policy |
packets in the xor |
our timesharing policy assigns |
in the xor is |
timesharing policy assigns quanta |
priority rpc whose results |
we evaluate mfs performance |
policy assigns quanta to |
the xor is a |
rpc whose results can |
evaluate mfs performance with |
assigns quanta to different |
xor is a random |
whose results can improve |
mfs performance with bandwidths |
quanta to different types |
is a random variable |
results can improve performance |
performance with bandwidths from |
to different types of |
a random variable y |
can improve performance if |
different types of events |
random variable y and |
improve performance if bandwidth |
variable y and has |
no auditing fixed threshold |
performance if bandwidth is |
y and has a |
auditing fixed threshold stepwise |
if bandwidth is high |
and has a binomial |
fixed threshold stepwise percentile |
high volumes of i |
has a binomial distribution |
a binomial distribution with |
binomial distribution with parameters |
asynchronous writeback but can |
writeback but can be |
but can be safely |
can be safely omitted |
be safely omitted if |
safely omitted if bandwidth |
such as caused by |
omitted if bandwidth is |
if bandwidth is low |
as caused by the |
caused by the increased |
by the increased forwarding |
the increased forwarding traffic |
mafs asynchronous writeback is |
asynchronous writeback is based |
writeback is based on |
is based on similar |
based on similar mechanisms |
on similar mechanisms the |
similar mechanisms the initial |
will cause qsm to |
mechanisms the initial priority |
is the summation p |
cause qsm to use |
the initial priority is |
the summation p z |
summation p z c |
initial priority is never |
priority is never modified |
qsm to use a |
to use a larger |
use a larger fraction |
a larger fraction of |
but the file server |
larger fraction of its |
the file server somefound |
fraction of its i |
file server somefound in |
server somefound in many |
somefound in many mobile |
in many mobile file |
many mobile file systems |
s is not low |
is not low in |
o quantum to process |
not low in the |
quantum to process i |
low in the sense |
in the sense of |
the sense of prior |
sense of prior work |
we plot the recovery |
plot the recovery probability |
the recovery probability curves |
recovery probability curves for |
probability curves for layered |
curves for layered interleaving |
for layered interleaving and |
layered interleaving and reed |
it is low enough |
solomon against uniformly random |
is low enough to |
with the consequence that |
rather than making times |
against uniformly random loss |
than making times requests |
the consequence that timers |
uniformly random loss rate |
low enough to cause |
making times requests an |
consequence that timers will |
enough to cause significant |
times requests an increase |
that timers will fire |
to cause significant contention |
requests an increase in |
timers will fire late |
cause significant contention for |
an increase in the |
significant contention for the |
increase in the priority |
contention for the workloads |
in the priority of |
for the workloads we |
the priority of an |
this effect is magnified |
the workloads we have |
priority of an rpc |
effect is magnified each |
workloads we have considered |
of an rpc to |
is magnified each time |
an rpc to transmit |
magnified each time qsm |
note that the curves |
each time qsm is |
rpc to transmit an |
time qsm is preempted |
that the curves are |
to transmit an rpc |
and we believe that |
qsm is preempted by |
the curves are very |
transmit an rpc when |
we believe that our |
is preempted by other |
curves are very close |
an rpc when an |
believe that our results |
preempted by other processes |
are very close to |
rpc when an application |
that our results will |
by other processes or |
very close to each |
when an application performs |
our results will hold |
other processes or by |
close to each other |
an application performs a |
results will hold if |
processes or by its |
application performs a metadata |
especially in the loss |
or by its own |
will hold if available |
performs a metadata update |
in the loss range |
by its own garbage |
hold if available bandwidth |
a metadata update or |
the loss range of |
its own garbage collector |
if available bandwidth and |
metadata update or file |
loss range of interest |
range of interest between |
update or file data |
available bandwidth and grep |
such delays are typically |
bandwidth and grep write |
delays are typically shorter |
are typically shorter than |
typically shorter than the |
shorter than the i |
the operation is logged |
operation is logged and |
is logged and replayed |
logged and replayed to |
and replayed to the |
replayed to the file |
to the file server |
the file server after |
file server after a |
server after a delay |
this scheme reduces bandwidth |
scheme reduces bandwidth utilisation |
reduces bandwidth utilisation because |
bandwidth utilisation because some |
yet longer than the |
local recovery for receiver |
utilisation because some logged |
longer than the alarm |
recovery for receiver loss |
because some logged operations |
than the alarm quantum |
for receiver loss in |
some logged operations may |
receiver loss in the |
ratio of freeloaders figure |
logged operations may be |
loss in the absence |
operations may be superceded |
in the absence of |
thus causing the alarm |
may be superceded by |
the absence of intelligent |
be superceded by later |
absence of intelligent flow |
afs mfs afs mfs |
superceded by later ones |
of intelligent flow control |
minimum and average download |
mfs afs mfs elapsed |
but not the i |
intelligent flow control mechanisms |
and average download factors |
afs mfs elapsed time |
flow control mechanisms like |
average download factors across |
control mechanisms like tcp |
download factors across all |
factors across all correct |
across all correct nodes |
all correct nodes when |
correct nodes when using |
nodes when using different |
when using different strategies |
using different strategies for |
different strategies for choosing |
strategies for choosing the |
for choosing the threshold |
inexpensive data center end |
each session has mixed |
session has mixed set |
has mixed set of |
the maximum alarm firing |
mixed set of opportunistic |
hosts can be easily |
maximum alarm firing delays |
set of opportunistic nodes |
can be easily overwhelmed |
alarm firing delays taken |
be easily overwhelmed and |
firing delays taken from |
contributing at different rates |
easily overwhelmed and drop |
delays taken from samples |
overwhelmed and drop packets |
taken from samples in |
and percentage of opportunistic |
and drop packets during |
percentage of opportunistic nodes |
drop packets during traffic |
of opportunistic nodes is |
packets during traffic spikes |
opportunistic nodes is varied |
during traffic spikes or |
s intervals are indeed |
nodes is varied on |
traffic spikes or cpu |
intervals are indeed much |
is varied on the |
varied on the x |
are indeed much larger |
intensive maintenance tasks like |
indeed much larger in |
maintenance tasks like garbage |
tasks like garbage collection |
much larger in the |
to their upload bandwidths |
larger in the perturbed |
in the perturbed experiments |
nodes able to upload |
able to upload at |
level protocols layered over |
to upload at a |
protocols layered over udp |
upload at a rate |
layered over udp for |
at a rate higher |
over udp for reliable |
both on the sender |
a rate higher than |
udp for reliable multicast |
on the sender and |
rate higher than the |
the sender and on |
higher than the stream |
sender and on the |
than the stream rate |
the stream rate are |
stream rate are placed |
and on the receiver |
rate are placed in |
on the receiver side |
are placed in higher |
or high speed data |
high speed data transfer |
which are closer to |
are closer to the |
closer to the source |
the source sends data |
source sends data to |
sends data to the |
data to the highest |
to the highest level |
the highest level group |
highest level group only |
for example would ordinarily |
example would ordinarily go |
would ordinarily go back |
who uses the basic |
ordinarily go back to |
uses the basic protocol |
go back to the |
the basic protocol to |
back to the sender |
basic protocol to disseminate |
to the sender to |
protocol to disseminate data |
the sender to retrieve |
to disseminate data among |
sender to retrieve the |
disseminate data among each |
to retrieve the lost |
data among each other |
retrieve the lost packet |
nodes in lower levels |
in lower levels may |
even though it was |
lower levels may receive |
though it was dropped |
levels may receive data |
it was dropped at |
may receive data at |
was dropped at the |
receive data at smaller |
dropped at the receiver |
data at smaller rates |
at the receiver after |
large delays are also |
the receiver after covering |
delays are also more |
receiver after covering the |
after some filtering is |
are also more frequent |
after covering the entire |
some filtering is applied |
covering the entire geographical |
the entire geographical distance |
the maelstrom proxy acts |
maelstrom proxy acts as |
level nodes may be |
proxy acts as a |
nodes may be used |
acts as a local |
may be used to |
as a local packet |
be used to act |
a local packet cache |
used to act as |
to act as sources |
act as sources to |
as sources to the |
sources to the lower |
storing incoming packets for |
incoming packets for a |
the maximum delay measured |
packets for a short |
maximum delay measured on |
alleviating the burden at |
for a short period |
delay measured on receivers |
the burden at the |
burden at the source |
measured on receivers in |
a short period of |
on receivers in the |
auditing can be used |
short period of time |
receivers in the perturbed |
period of time and |
can be used to |
of time and providing |
in the perturbed runs |
be used to avoid |
time and providing hooks |
the perturbed runs is |
used to avoid the |
and providing hooks that |
to avoid the presence |
providing hooks that allow |
avoid the presence of |
hooks that allow protocols |
the presence of opportunistic |
that allow protocols to |
presence of opportunistic and |
allow protocols to first |
of opportunistic and lower |
protocols to first query |
opportunistic and lower bandwidth |
to first query the |
and lower bandwidth nodes |
first query the cache |
lower bandwidth nodes in |
query the cache to |
bandwidth nodes in the |
the cache to locate |
nodes in the higher |
cache to locate missing |
to locate missing packets |
locate missing packets before |
missing packets before sending |
packets before sending retransmission |
before sending retransmission requests |
it can ensure that |
sending retransmission requests back |
can ensure that the |
retransmission requests back to |
ensure that the hierarchy |
requests back to the |
that the hierarchy of |
back to the sender |
the hierarchy of nodes |
hierarchy of nodes is |
of nodes is obeyed |
nodes is obeyed by |
is obeyed by all |
obeyed by all nodes |
future versions of maelstrom |
versions of maelstrom could |
of maelstrom could potentially |
maelstrom could potentially use |
while allowing the system |
could potentially use knowledge |
allowing the system to |
potentially use knowledge of |
writes execution time speedup |
the system to leverage |
use knowledge of protocol |
system to leverage additional |
knowledge of protocol internals |
to leverage additional resources |
of protocol internals to |
leverage additional resources from |
protocol internals to transparently |
additional resources from privileged |
internals to transparently intervene |
resources from privileged altruistic |
from privileged altruistic nodes |
privileged altruistic nodes to |
altruistic nodes to forward |
nodes to forward data |
to forward data to |
forward data to lower |
data to lower level |
to lower level groups |
by intercepting and satisfying |
intercepting and satisfying retransmission |
and satisfying retransmission requests |
satisfying retransmission requests sent |
we intend to explore |
retransmission requests sent by |
intend to explore this |
requests sent by the |
to explore this further |
sent by the receiver |
by the receiver in |
the receiver in a |
receiver in a nak |
execution time speedup execution |
explore this further in |
time speedup execution time |
this further in future |
speedup execution time speedup |
further in future work |
execution time speedup execution |
time speedup execution time |
speedup execution time speedup |
or by resending packets |
execution time speedup no |
by resending packets when |
ms in the unperturbed |
time speedup no priorities |
resending packets when acknowledgments |
in the unperturbed experiments |
related work several p |
packets when acknowledgments are |
when acknowledgments are not |
acknowledgments are not observed |
are not observed within |
not observed within a |
observed within a certain |
within a certain time |
a certain time period |
certain time period in |
streaming protocols have been |
time period in an |
period in an ack |
protocols have been previously |
have been previously proposed |
the value grows from |
the first generation of |
first generation of systems |
implementation details we initially |
details we initially implemented |
we initially implemented and |
initially implemented and evaluated |
implemented and evaluated maelstrom |
and evaluated maelstrom as |
evaluated maelstrom as a |
maelstrom as a user |
performance turned out to |
turned out to be |
relied on approaches based |
out to be limited |
on approaches based on |
to be limited by |
approaches based on pushing |
be limited by copying |
based on pushing data |
limited by copying and |
by copying and context |
on pushing data through |
pushing data through a |
data through a single |
through a single dissemination |
a single dissemination tree |
and we subsequently reimplemented |
later approaches focused on |
comparison of mfs and |
we subsequently reimplemented the |
approaches focused on improving |
of mfs and afs |
subsequently reimplemented the system |
focused on improving fairness |
mfs and afs performance |
reimplemented the system as |
on improving fairness among |
the system as a |
improving fairness among peers |
system as a module |
fairness among peers and |
as a module that |
among peers and resilience |
a module that runs |
mfs with synchronous rpcs |
the problem could be |
peers and resilience to |
module that runs within |
with synchronous rpcs and |
problem could be alleviated |
and resilience to churn |
that runs within the |
synchronous rpcs and priorities |
could be alleviated by |
resilience to churn by |
runs within the linux |
rpcs and priorities is |
be alleviated by making |
to churn by breaking |
and priorities is compared |
alleviated by making our |
churn by breaking data |
priorities is compared to |
by making our priority |
by breaking data into |
is compared to a |
making our priority scheduling |
breaking data into multiple |
compared to a version |
our priority scheduling more |
data into multiple substreams |
to a version of |
priority scheduling more fine |
into multiple substreams and |
a version of the |
multiple substreams and sending |
version of the andrew |
at an encoding rate |
substreams and sending them |
of the andrew file |
an encoding rate of |
and sending them along |
the andrew file system |
sending them along disjoing |
them along disjoing paths |
speedups for the two |
for the two workloads |
the two workloads of |
two workloads of the |
varying priorities for control |
workloads of the gw |
priorities for control packets |
of the gw test |
the experimental prototype of |
the gw test are |
experimental prototype of the |
gw test are shown |
prototype of the kernel |
or by assigning priorities |
more recent systems like |
of the kernel version |
by assigning priorities to |
recent systems like coolstreaming |
relative to the performance |
assigning priorities to feeds |
the kernel version reaches |
to the performance of |
priorities to feeds in |
kernel version reaches output |
the performance of afs |
to feeds in the |
version reaches output speeds |
performance of afs at |
feeds in the sending |
reaches output speeds close |
based style of data |
style of data dissemination |
output speeds close to |
in the sending stack |
coolstreaming breaks the data |
breaks the data into |
the data into packets |
gigabit per second of |
per second of combined |
second of combined data |
of combined data and |
combined data and fec |
data and fec traffic |
and peers organized into |
peers organized into a |
organized into a mesh |
into a mesh request |
a mesh request packets |
limited only by the |
mesh request packets from |
only by the capacity |
request packets from their |
by the capacity of |
packets from their neighbors |
the capacity of the |
from their neighbors using |
capacity of the outbound |
their neighbors using a |
traffic are scaled down |
of the outbound network |
neighbors using a scheduling |
using a scheduling algorithm |
the outbound network card |
are scaled down further |
number of messages awaiting |
scaled down further in |
as we saw earlier |
of messages awaiting acknowledgement |
down further in parallel |
messages awaiting acknowledgement in |
chainsaw uses a simpler |
lambda networks are already |
awaiting acknowledgement in experiments |
uses a simpler policy |
networks are already reaching |
acknowledgement in experiments with |
a simpler policy for |
the graphs in figure |
are already reaching speeds |
already reaching speeds of |
simpler policy for requesting |
policy for requesting packets |
in experiments with perturbances |
validate the incorporation of |
the incorporation of rpc |
randomly fetching them while |
incorporation of rpc priorities |
fetching them while respecting |
them while respecting a |
while respecting a maximum |
respecting a maximum limit |
a maximum limit on |
maximum limit on the |
limit on the number |
on the number of |
the number of outstanding |
number of outstanding requests |
of outstanding requests to |
since all the foreground |
outstanding requests to each |
all the foreground workloads |
requests to each neighbor |
the foreground workloads improve |
and higher speeds are |
foreground workloads improve their |
higher speeds are a |
chainsaw presents smaller delays |
workloads improve their performance |
speeds are a certainty |
presents smaller delays for |
improve their performance substantially |
token roundtrip time and |
are a certainty down |
smaller delays for the |
their performance substantially at |
roundtrip time and the |
a certainty down the |
delays for the receipt |
performance substantially at lower |
time and the time |
certainty down the road |
for the receipt of |
substantially at lower bandwidths |
and the time to |
the receipt of packets |
the time to recover |
receipt of packets compared |
time to recover in |
of packets compared to |
to recover in the |
packets compared to the |
compared to the coolstreaming |
we envision maelstrom as |
relative to mfs with |
to the coolstreaming protocol |
envision maelstrom as a |
to mfs with no |
maelstrom as a small |
mfs with no priorities |
in a more recent |
a more recent work |
as a small rack |
style cluster of servers |
each acting as an |
acting as an individual |
as an individual proxy |
based approaches are shown |
the decrease in throughput |
approaches are shown to |
traffic would be distributed |
decrease in throughput for |
are shown to present |
would be distributed over |
in throughput for the |
shown to present better |
be distributed over such |
to present better performance |
distributed over such a |
workloads with contention between |
token roundtrip time and |
parameter trace mostly writes |
over such a rack |
with contention between priority |
present better performance over |
better performance over tree |
such a rack by |
contention between priority levels |
roundtrip time and the |
a rack by partitioning |
time and the time |
rack by partitioning the |
the grep workload consists |
previous papers have considered |
by partitioning the address |
and the time to |
grep workload consists of |
papers have considered a |
partitioning the address space |
the time to recover |
workload consists of validating |
have considered a variety |
the address space of |
time to recover in |
consists of validating cached |
considered a variety of |
address space of the |
to recover in the |
of validating cached files |
a variety of possible |
space of the remote |
variety of possible mechanisms |
of the remote data |
elapsed time to compile |
of possible mechanisms to |
the remote data center |
time to compile mafs |
possible mechanisms to encourage |
remote data center and |
mechanisms to encourage node |
data center and routing |
to encourage node contribution |
center and routing different |
and routing different segments |
routing different segments of |
different segments of the |
segments of the space |
of the space through |
the space through distinct |
space through distinct maelstrom |
through distinct maelstrom appliance |
distinct maelstrom appliance pairs |
it is worth noting |
is a framework proposed |
is worth noting that |
a framework proposed to |
worth noting that the |
we plan to experiment |
framework proposed to enforce |
noting that the doubled |
plan to experiment with |
proposed to enforce download |
that the doubled token |
to experiment with such |
to enforce download rate |
the doubled token roundtrip |
experiment with such configurations |
enforce download rate limitations |
download rate limitations on |
rate limitations on p |
writes execution time speedup |
which would also permit |
doubled token roundtrip time |
would also permit us |
p media streaming systems |
also permit us to |
permit us to explore |
us to explore fault |
the protocol relies on |
protocol relies on a |
as compared to unperturbed |
relies on a set |
compared to unperturbed experiments |
on a set of |
if a maelstrom blade |
a set of trusted |
a maelstrom blade fails |
set of trusted nodes |
of trusted nodes that |
trusted nodes that store |
nodes that store information |
that store information on |
store information on the |
can t be accounted |
information on the data |
t be accounted for |
distinct processes distinct files |
on the data downloaded |
and to support load |
processes distinct files total |
be accounted for by |
the data downloaded by |
accounted for by the |
balancing schemes that might |
data downloaded by each |
distinct files total of |
files total of file |
total of file sizes |
downloaded by each node |
for by the increase |
schemes that might vary |
by the increase in |
by each node receiving |
that might vary the |
the increase in memory |
each node receiving data |
might vary the ip |
increase in memory overhead |
vary the ip address |
in memory overhead or |
the ip address space |
nodes only send an |
memory overhead or cpu |
ip address space partitioning |
only send an object |
overhead or cpu activity |
address space partitioning dynamically |
send an object after |
or cpu activity on |
space partitioning dynamically to |
an object after consulting |
cpu activity on the |
partitioning dynamically to spread |
object after consulting the |
activity on the receivers |
dynamically to spread the |
after consulting the trusted |
to spread the encoding |
consulting the trusted nodes |
spread the encoding load |
the trusted nodes to |
the encoding load over |
trusted nodes to verify |
encoding load over multiple |
nodes to verify if |
load over multiple machines |
as was the case |
to verify if the |
was the case in |
verify if the nodes |
the case in experiments |
if the nodes requesting |
case in experiments where |
the nodes requesting the |
in experiments where we |
we present the implementation |
nodes requesting the stream |
experiments where we varied |
present the implementation and |
requesting the stream are |
where we varied the |
the implementation and performance |
the stream are not |
we varied the replication |
implementation and performance of |
stream are not overrequesting |
are not overrequesting data |
and performance of a |
performance of a single |
varied the replication factor |
it is targeted to |
is targeted to systems |
targeted to systems where |
to systems where nodes |
systems where nodes upload |
where nodes upload full |
nodes upload full media |
the kernel implementation is |
upload full media objects |
the problem can be |
kernel implementation is a |
full media objects from |
media objects from each |
objects from each other |
problem can be traced |
implementation is a module |
can be traced to |
is a module for |
and not for live |
be traced to a |
a module for linux |
traced to a priority |
to a priority inversion |
streaming systems where all |
systems where all nodes |
where all nodes are |
all nodes are interested |
nodes are interested in |
are interested in receiving |
interested in receiving the |
in receiving the exact |
receiving the exact same |
because of repeated losses |
the exact same data |
exact same data in |
same data in close |
data in close to |
in close to real |
close to real time |
the system maintains a |
with hooks into the |
system maintains a high |
hooks into the kernel |
maintains a high volume |
into the kernel packet |
the kernel packet filter |
a high volume of |
high volume of forwarding |
consider fairness issues in |
volume of forwarding traffic |
fairness issues in the |
issues in the context |
in the context of |
the context of tree |
the forwarded messages tend |
forwarded messages tend to |
messages tend to get |
maelstrom proxies work in |
proxies work in pairs |
tend to get ahead |
to get ahead of |
the authors present mechanisms |
one on each side |
authors present mechanisms that |
get ahead of the |
on each side of |
present mechanisms that rank |
ahead of the token |
each side of the |
mechanisms that rank peers |
side of the long |
that rank peers according |
of the long haul |
rank peers according to |
the long haul link |
peers according to their |
according to their level |
both on the send |
to their level of |
on the send path |
their level of cooperation |
each proxy acts both |
level of cooperation with |
proxy acts both as |
of cooperation with the |
acts both as an |
cooperation with the system |
both as an ingress |
where in the sinks |
as an ingress and |
an ingress and egress |
one of their techniques |
ingress and egress router |
of their techniques involves |
and egress router at |
their techniques involves the |
egress router at the |
we use a simple |
techniques involves the reconstruction |
router at the same |
these traces are representative |
use a simple round |
involves the reconstruction of |
at the same time |
traces are representative periods |
the reconstruction of trees |
the same time since |
robin policy of multiplexing |
reconstruction of trees as |
are representative periods of |
same time since they |
policy of multiplexing between |
of trees as a |
representative periods of mixed |
time since they handle |
of multiplexing between data |
trees as a way |
periods of mixed read |
since they handle duplex |
multiplexing between data feeds |
as a way of |
of mixed read and |
they handle duplex traffic |
a way of punishing |
mixed read and write |
handle duplex traffic in |
way of punishing opportunistic |
and on the receive |
read and write activity |
duplex traffic in the |
of punishing opportunistic nodes |
on the receive path |
traffic in the following |
in the following manner |
most of their mechanisms |
the durations are from |
of their mechanisms require |
durations are from the |
the egress router captures |
their mechanisms require peers |
are from the original |
where forwarded packets are |
egress router captures ip |
mechanisms require peers to |
from the original ntfs |
forwarded packets are treated |
router captures ip packets |
require peers to keep |
the original ntfs traces |
packets are treated as |
captures ip packets and |
peers to keep track |
are treated as control |
ip packets and creates |
to keep track of |
note that the total |
packets and creates redundant |
treated as control traffic |
keep track of their |
that the total file |
and creates redundant fec |
the total file sizes |
track of their parents |
total file sizes represent |
creates redundant fec packets |
and while they re |
file sizes represent the |
of their parents and |
while they re prioritized |
sizes represent the amount |
their parents and children |
they re prioritized over |
the original ip packets |
represent the amount fetched |
parents and children s |
re prioritized over data |
original ip packets are |
the amount fetched by |
and children s behavior |
ip packets are routed |
amount fetched by mfs |
they are treated as |
packets are routed through |
fetched by mfs during |
are treated as equally |
are routed through unaltered |
studied the effect of |
treated as equally important |
by mfs during the |
routed through unaltered as |
the effect of different |
as equally important as |
mfs during the trace |
through unaltered as they |
effect of different types |
equally important as tokens |
unaltered as they would |
of different types of |
as they would have |
different types of incentives |
they would have been |
would have been originally |
types of incentives on |
they also increase the |
where this is exceed |
of incentives on the |
the redundant packets are |
this is exceed by |
also increase the overall |
incentives on the chainsaw |
redundant packets are then |
is exceed by the |
increase the overall volume |
on the chainsaw protocol |
packets are then forwarded |
exceed by the write |
the overall volume of |
are then forwarded to |
by the write traffic |
overall volume of i |
then forwarded to the |
forwarded to the remote |
to the remote ingress |
the remote ingress router |
remote ingress router via |
ingress router via a |
router via a udp |
via a udp channel |
the additional traffic is |
o that the nodes |
additional traffic is due |
the ingress router captures |
tat and some variations |
that the nodes process |
traffic is due to |
ingress router captures and |
is due to new |
router captures and stores |
the authors propose an |
due to new files |
captures and stores ip |
authors propose an algorithm |
to new files being |
and stores ip packets |
propose an algorithm that |
tokens are processed with |
stores ip packets coming |
new files being created |
an algorithm that sets |
are processed with higher |
ip packets coming from |
files being created or |
algorithm that sets up |
processed with higher latency |
packets coming from the |
being created or existing |
that sets up local |
coming from the direction |
created or existing ones |
sets up local markets |
from the direction of |
or existing ones extended |
up local markets at |
the direction of the |
local markets at every |
direction of the egress |
markets at every node |
of the egress router |
where neighbors compete for |
neighbors compete for the |
time spent on rpcs |
upon receipt of a |
compete for the node |
receipt of a redundant |
for the node s |
of a redundant packet |
the node s upload |
node s upload capacity |
an ip packet is |
ip packet is recovered |
nodes favor neighbors who |
packet is recovered if |
favor neighbors who contribute |
is recovered if there |
neighbors who contribute more |
recovered if there is |
if there is an |
there is an opportunity |
is an opportunity to |
an opportunity to do |
opportunity to do so |
with nodes classified as |
nodes classified as fast |
redundant packets that can |
classified as fast or |
packets that can be |
as fast or slow |
that can be used |
fast or slow nodes |
can be used at |
be used at a |
used at a later |
at a later time |
a later time are |
later time are stored |
histogram of maximum alarm |
the results indicate that |
of maximum alarm delays |
if the redundant packet |
results indicate that the |
maximum alarm delays in |
the redundant packet is |
indicate that the proposed |
traffic numbers are for |
redundant packet is useless |
that the proposed algorithm |
numbers are for synchronous |
packet is useless it |
the proposed algorithm improves |
are for synchronous writeback |
is useless it is |
proposed algorithm improves the |
useless it is immediately |
algorithm improves the performance |
it is immediately discarded |
improves the performance of |
the performance of the |
performance of the system |
of the system when |
upon recovery the ip |
the system when the |
recovery the ip packet |
system when the total |
the ip packet is |
when the total upload |
ip packet is sent |
the total upload capacity |
packet is sent through |
total upload capacity is |
is sent through a |
upload capacity is not |
sent through a raw |
capacity is not enough |
through a raw socket |
is not enough to |
a raw socket to |
not enough to supply |
raw socket to its |
enough to supply all |
socket to its intended |
to supply all the |
to its intended destination |
supply all the nodes |
compiling mafs on top |
mafs on top of |
on top of mafs |
using fec requires that |
histogram of maximum alarm |
fec requires that each |
of maximum alarm delays |
requires that each data |
maximum alarm delays in |
that each data packet |
each data packet have |
data packet have a |
streaming system where nodes |
packet have a unique |
system where nodes choose |
have a unique identifier |
where nodes choose their |
a unique identifier that |
nodes choose their neighbors |
unique identifier that the |
choose their neighbors based |
bandwidth is high enough |
identifier that the receiver |
their neighbors based on |
is high enough to |
that the receiver can |
neighbors based on their |
high enough to eliminate |
the receiver can use |
based on their history |
enough to eliminate differences |
receiver can use to |
on their history of |
to eliminate differences between |
can use to keep |
their history of interaction |
eliminate differences between writeback |
use to keep track |
differences between writeback schemes |
to keep track of |
nodes are placed in |
keep track of received |
are placed in the |
track of received data |
placed in the system |
of received data packets |
in the system according |
asynchronous writeback is clearly |
received data packets and |
the system according to |
writeback is clearly beneficial |
data packets and to |
system according to their |
packets and to identify |
according to their current |
and to identify missing |
and priortwo questions are |
to their current trading |
overheads in a lightly |
to identify missing data |
priortwo questions are of |
their current trading performances |
identify missing data packets |
questions are of particular |
missing data packets in |
are of particular interest |
data packets in a |
of particular interest in |
encouraging nodes to contribute |
packets in a repair |
particular interest in evaluating |
interest in evaluating the |
nodes to contribute more |
in a repair packet |
loaded system so far |
in evaluating the perfor |
system so far the |
if we had access |
to contribute more and |
we had access to |
had access to end |
contribute more and therefore |
so far the evaluation |
ities are advantageous in |
far the evaluation has |
more and therefore be |
the evaluation has focused |
are advantageous in reducing |
and therefore be closer |
we could have added |
evaluation has focused on |
advantageous in reducing contention |
therefore be closer to |
could have added a |
has focused on scenarios |
in reducing contention between |
be closer to the |
have added a header |
focused on scenarios where |
reducing contention between reading |
closer to the source |
added a header to |
on scenarios where the |
contention between reading mance |
a header to each |
scenarios where the system |
between reading mance of |
header to each packet |
where the system was |
is a more recent |
reading mance of mafs |
to each packet with |
the system was heavily |
a more recent live |
mance of mafs communication |
each packet with a |
system was heavily loaded |
of mafs communication adaptation |
packet with a unique |
streaming approach that tolerates |
ntfs workloads in addition |
with a unique sequence |
workloads in addition to |
approach that tolerates the |
a unique sequence number |
which is not possible |
that tolerates the existence |
with unbounded multicast rates |
in addition to measuring |
is not possible when |
tolerates the existence of |
unbounded multicast rates and |
addition to measuring the |
not possible when synchronous |
the existence of opportunistic |
multicast rates and occasional |
to measuring the performance |
possible when synchronous writeback |
existence of opportunistic and |
rates and occasional perturbations |
measuring the performance of |
when synchronous writeback is |
of opportunistic and malicious |
the performance of mfs |
synchronous writeback is used |
opportunistic and malicious nodes |
performance of mfs with |
we intercept traffic transparently |
of mfs with synthetic |
time is divided into |
intercept traffic transparently and |
mfs with synthetic workloads |
is divided into rounds |
we traced degraded performance |
traffic transparently and need |
do priorities improve performance |
traced degraded performance or |
transparently and need to |
priorities improve performance by |
in which each peer |
degraded performance or scheduling |
we have also conducted |
improve performance by reducing |
which each peer communicates |
performance or scheduling delays |
route it without modification |
have also conducted experiments |
performance by reducing rpc |
each peer communicates with |
or scheduling delays to |
it without modification or |
also conducted experiments with |
by reducing rpc conthe |
peer communicates with another |
scheduling delays to memory |
without modification or addition |
conducted experiments with traces |
reducing rpc conthe second |
communicates with another peer |
experiments with traces gathered |
rpc conthe second microbenchmark |
with another peer selected |
with traces gathered from |
conthe second microbenchmark evaluates |
another peer selected using |
traces gathered from the |
second microbenchmark evaluates a |
peer selected using a |
selected using a pseudo |
we identify ip packets |
gathered from the windows |
microbenchmark evaluates a workload |
but how does the |
identify ip packets by |
from the windows nt |
evaluates a workload that |
a workload that contention |
ip packets by a |
the windows nt file |
how does the system |
peers exchange their current |
windows nt file system |
packets by a tuple |
does the system behave |
tains explicit contention between |
exchange their current history |
by a tuple consisting |
the system behave when |
explicit contention between different |
their current history containing |
a tuple consisting of |
system behave when lightly |
contention between different types |
current history containing the |
tuple consisting of the |
behave when lightly loaded |
between different types of |
history containing the identifiers |
consisting of the source |
different types of rpc |
containing the identifiers of |
of the source and |
types of rpc traf |
the identifiers of all |
do similar phenomena occur |
the source and destination |
identifiers of all the |
source and destination ip |
of all the current |
and destination ip address |
all the current data |
is it possible to |
the current data they |
it possible to combine |
here we ll see |
current data they hold |
possible to combine the |
we ll see that |
to combine the benefit |
size of the ip |
ll see that load |
combine the benefit of |
as basis for the |
of the ip header |
although mfs is implemented |
see that load has |
the benefit of asynchronous |
basis for the next |
the ip header plus |
ip header plus data |
that load has a |
benefit of asynchronous write |
for the next exchanges |
mfs is implemented on |
and a checksum over |
load has a super |
is implemented on a |
a checksum over the |
nodes also perform a |
implemented on a variant |
checksum over the ip |
linear impact on performance |
on a variant of |
also perform a phase |
over the ip data |
the ip data payload |
perform a phase of |
a variant of unix |
a phase of optimistic |
phase of optimistic push |
the checksum over the |
checksum over the payload |
over the payload is |
one process performs a |
forwarding useful updates to |
the payload is necessary |
useful updates to pseudo |
process performs a grep |
payload is necessary since |
the growth in memory |
and ntfs has a |
growth in memory consumption |
is necessary since the |
randomly picked peers with |
performs a grep on |
ntfs has a somewhat |
in memory consumption causes |
necessary since the ip |
picked peers with no |
a grep on a |
has a somewhat different |
memory consumption causes slowdowns |
since the ip identification |
peers with no guarantee |
grep on a set |
a somewhat different interface |
consumption causes slowdowns that |
the ip identification field |
with no guarantee of |
on a set of |
somewhat different interface to |
causes slowdowns that amplify |
ip identification field is |
no guarantee of useful |
a set of back |
different interface to the |
slowdowns that amplify the |
identification field is only |
guarantee of useful return |
set of back at |
interface to the file |
that amplify the increased |
of back at low |
to the file system |
amplify the increased latencies |
back at low bandwidth |
bits long and a |
the increased latencies associated |
at low bandwidth with |
conclusion we propose and |
low bandwidth with acceptable |
long and a single |
increased latencies associated with |
we propose and evaluate |
the traces were converted |
bandwidth with acceptable performance |
and a single pair |
latencies associated with the |
propose and evaluate a |
traces were converted to |
with acceptable performance at |
a single pair of |
single pair of end |
and evaluate a scalable |
were converted to run |
acceptable performance at cached |
associated with the growth |
evaluate a scalable auditing |
converted to run on |
performance at cached files |
with the growth in |
hosts communicating at high |
to run on top |
at cached files that |
the growth in traffic |
based technique for enforcing |
communicating at high speeds |
run on top of |
cached files that need |
technique for enforcing fairness |
to show this we |
on top of mfs |
files that need to |
at high speeds will |
for enforcing fairness in |
show this we designed |
top of mfs with |
that need to be |
high speeds will use |
enforcing fairness in a |
fairness in a live |
of mfs with little |
need to be validated |
speeds will use the |
this we designed experiments |
mfs with little difficulty |
to be validated before |
will use the same |
our approach employs local |
use the same identifier |
we designed experiments that |
be validated before they |
the original traces recorded |
the same identifier for |
designed experiments that vary |
approach employs local auditors |
validated before they can |
original traces recorded file |
same identifier for different |
experiments that vary the |
employs local auditors that |
before they can be |
they can be opened |
identifier for different data |
that vary the multicast |
local auditors that execute |
traces recorded file accesses |
for different data packets |
vary the multicast rate |
auditors that execute on |
recorded file accesses on |
different data packets within |
that execute on all |
file accesses on a |
data packets within a |
execute on all nodes |
accesses on a set |
packets within a fairly |
on all nodes in |
on a set of |
another process either writes |
all nodes in a |
within a fairly short |
a set of machines |
showed that the load |
nodes in a streaming |
a fairly short interval |
process either writes higher |
set of machines in |
that the load on |
in a streaming session |
fairly short interval unless |
either writes higher bandwidths |
of machines in a |
the load on receivers |
they are responsible for |
machines in a lan |
short interval unless the |
load on receivers grows |
are responsible for collecting |
data to files rapidly |
interval unless the checksum |
a majority of the |
responsible for collecting auditable |
on receivers grows roughly |
unless the checksum is |
majority of the accesses |
for collecting auditable information |
receivers grows roughly linearly |
the checksum is added |
of the accesses were |
collecting auditable information about |
checksum is added to |
the accesses were local |
auditable information about other |
is added to differentiate |
as expected given the |
accesses were local but |
information about other neighbors |
added to differentiate between |
expected given the linearly |
grepwe compare mafs to |
were local but some |
about other neighbors data |
to differentiate between them |
given the linearly increasing |
compare mafs to alternative |
local but some were |
other neighbors data exchanges |
the linearly increasing load |
mafs to alternative approaches |
but some were to |
to alternative approaches in |
some were to remote |
and for verifying that |
unique identifiers result in |
alternative approaches in two |
were to remote machines |
for verifying that neighbors |
negligible loss rates and |
identifiers result in garbled |
approaches in two sets |
verifying that neighbors upload |
loss rates and the |
we extracted subintervals from |
in two sets of |
that neighbors upload more |
result in garbled recovery |
rates and the nearly |
extracted subintervals from the |
two sets of compile |
neighbors upload more data |
in garbled recovery by |
and the nearly flat |
subintervals from the traces |
upload more data than |
garbled recovery by maelstrom |
the nearly flat curve |
from the traces which |
more data than a |
nearly flat curve of |
the traces which featured |
data than a specified |
flat curve of memory |
an event which will |
traces which featured interesting |
event which will be |
which featured interesting file |
which will be caught |
featured interesting file system |
will be caught by |
curve of memory consumption |
than a specified threshold |
interesting file system behaviour |
be caught by higher |
file system behaviour and |
one process reads files |
caught by higher level |
this threshold is defined |
by higher level checksums |
process reads files at |
higher level checksums designed |
threshold is defined by |
system behaviour and processed |
reads files at the |
behaviour and processed them |
is defined by dedicated |
level checksums designed to |
files at the same |
at the same experiments |
defined by dedicated global |
checksums designed to deal |
and processed them to |
by dedicated global auditors |
designed to deal with |
processed them to remove |
microbenchmarks to measure execution |
the latter reflecting our |
which periodically sample the |
to deal with tranmission |
to measure execution time |
them to remove accesses |
latter reflecting our cooperative |
periodically sample the state |
deal with tranmission errors |
measure execution time time |
to remove accesses to |
reflecting our cooperative caching |
sample the state of |
with tranmission errors on |
execution time time as |
remove accesses to files |
our cooperative caching policy |
the state of the |
tranmission errors on commodity |
time time as another |
accesses to files over |
state of the system |
errors on commodity networks |
time as another is |
as another is writing |
another is writing files |
on commodity networks and |
load on the sender |
of the system to |
commodity networks and hence |
the system to determine |
networks and hence does |
system to determine if |
and hence does not |
this preprocessing was necessary |
to determine if the |
preprocessing was necessary to |
hence does not have |
was necessary to eliminate |
shows that priorispeedup for |
necessary to eliminate the |
determine if the overall |
does not have significant |
that priorispeedup for simple |
priorispeedup for simple workloads |
if the overall download |
not have significant consequences |
to eliminate the influence |
the overall download rate |
because the linear growth |
have significant consequences unless |
eliminate the influence of |
and traces of actual |
overall download rate is |
the linear growth of |
significant consequences unless it |
the influence of extremely |
traces of actual windows |
download rate is compromised |
linear growth of traffic |
consequences unless it occurs |
unless it occurs frequently |
of actual windows ties |
rate is compromised by |
influence of extremely large |
actual windows ties are |
is compromised by the |
combined with our fixed |
of extremely large nt |
windows ties are beneficial |
the kernel version of |
compromised by the presence |
with our fixed rate |
extremely large nt system |
ties are beneficial for |
kernel version of maelstrom |
by the presence of |
our fixed rate of |
large nt system files |
are beneficial for the |
version of maelstrom can |
the presence of opportunistic |
presence of opportunistic nodes |
beneficial for the small |
of maelstrom can generate |
fixed rate of state |
for the small validation |
maelstrom can generate up |
rate of state aggregation |
global auditing determines the |
the small validation rpcs |
can generate up to |
auditing determines the minimum |
small validation rpcs when |
generate up to a |
determines the minimum threshold |
validation rpcs when the |
up to a gigabit |
increases the amount of |
rpcs when the backnt |
the minimum threshold for |
to a gigabit per |
the amount of unacknowledged |
when the backnt file |
minimum threshold for uploads |
of the file system |
a gigabit per second |
amount of unacknowledged data |
the backnt file system |
and works with local |
gigabit per second of |
the file system traffic |
works with local auditing |
per second of data |
file system traffic in |
with local auditing to |
second of data and |
system traffic in some |
local auditing to punish |
of data and fec |
data and fec traffic |
the ntfs traces were |
auditing to punish nodes |
traffic in some portions |
ntfs traces were gathered |
with the input data |
in some portions of |
to punish nodes that |
traces were gathered ground |
the input data rate |
some portions of the |
punish nodes that do |
were gathered ground traffic |
input data rate depending |
portions of the original |
nodes that do not |
gathered ground traffic is |
data rate depending on |
of the original traces |
this triggers higher overheads |
that do not upload |
ground traffic is heavy |
rate depending on the |
do not upload enough |
depending on the encoding |
not upload enough data |
on the encoding rate |
with the sporadic background |
given that mfs retrieves |
the sporadic background traffic |
we study the efficiency |
that mfs retrieves and |
sporadic background traffic in |
the time spent in |
mfs retrieves and writes |
study the efficiency of |
background traffic in the |
we were able to |
time spent in the |
retrieves and writes back |
the efficiency of our |
traffic in the cornell |
were able to saturate |
spent in the garbage |
and writes back whole |
efficiency of our auditing |
in the cornell university |
able to saturate the |
in the garbage collector |
writes back whole files |
of our auditing approach |
the cornell university computer |
to saturate the outgoing |
the garbage collector grows |
our auditing approach through |
including these system files |
saturate the outgoing card |
garbage collector grows from |
cornell university computer science |
auditing approach through simulation |
these system files would |
the outgoing card at |
university computer science department |
system files would have |
outgoing card at rates |
and show that it |
files would have distorted |
card at rates as |
and of compiling mafs |
show that it is |
would have distorted the |
at rates as high |
that it is able |
improvements are confined to |
rates as high as |
have distorted the experiments |
it is able to |
are confined to low |
distorted the experiments at |
is able to maintain |
confined to low bandcontain |
the experiments at low |
able to maintain the |
to low bandcontain access |
experiments at low bandwidths |
to maintain the throughput |
low bandcontain access to |
maintain the throughput of |
bandcontain access to local |
the throughput of the |
access to local and |
throughput of the streaming |
to local and remote |
of the streaming system |
local and remote file |
with cpu overload occurring |
the streaming system even |
and remote file systems |
cpu overload occurring at |
streaming system even in |
remote file systems by |
gives statistics for the |
system even in the |
file systems by clients |
statistics for the three |
even in the presence |
systems by clients in |
combined with a linear |
for the three traces |
in the presence of |
by clients in a |
clients in a width |
the presence of a |
with a linear growth |
in a width levels |
where each incoming data |
a trace in which |
a linear growth of |
presence of a large |
each incoming data packet |
trace in which reads |
linear growth of cpu |
of a large number |
incoming data packet had |
in which reads predominate |
growth of cpu usage |
a large number of |
of cpu usage due |
data packet had to |
large number of opportunistic |
demonstrates that priorities can |
a trace in which |
packet had to be |
number of opportunistic nodes |
that priorities can imlocal |
cpu usage due to |
trace in which writes |
had to be xored |
usage due to the |
in which writes predominate |
due to the increasing |
to the increasing volume |
the increasing volume of |
increasing volume of traffic |
and one containing exceptionally |
buffering requirements at the |
requirements at the receive |
one containing exceptionally heavy |
these overheads cause the |
containing exceptionally heavy file |
overheads cause the super |
exceptionally heavy file system |
heavy file system traffic |
priority read performance with |
incoming data packets are |
read performance with only |
data packets are buffered |
performance with only a |
packets are buffered so |
linear growth of cpu |
with only a small |
are buffered so that |
each trace was run |
only a small overhead |
a small overhead for |
buffered so that they |
trace was run over |
growth of cpu overhead |
was run over mfs |
so that they can |
a case for end |
small overhead for writes |
of cpu overhead shown |
run over mfs with |
that they can be |
case for end system |
cpu overhead shown on |
over mfs with the |
they can be used |
for end system multicast |
overhead shown on figure |
mfs with the combinations |
can be used in |
with the combinations of |
these microbenchmarks show that |
the combinations of synchronous |
be used in conjunction |
combinations of synchronous and |
microbenchmarks show that asynmicrobenchmarks |
of synchronous and asynchronous |
used in conjunction with |
synchronous and asynchronous writes |
show that asynmicrobenchmarks chronous |
in conjunction with xors |
and asynchronous writes and |
that asynmicrobenchmarks chronous writeback |
conjunction with xors to |
asynchronous writes and differentiated |
asynmicrobenchmarks chronous writeback improves |
with xors to recover |
the increasing number of |
writes and differentiated and |
increasing number of unacknowledged |
xors to recover missing |
chronous writeback improves performance |
and differentiated and uniform |
number of unacknowledged requests |
to recover missing data |
writeback improves performance even |
differentiated and uniform priorities |
of unacknowledged requests and |
recover missing data packets |
improves performance even at |
and uniform priorities in |
unacknowledged requests and the |
performance even at comparaour |
uniform priorities in previous |
requests and the resulting |
even at comparaour first |
priorities in previous experiments |
and the resulting overheads |
at comparaour first microbenchmark |
the resulting overheads rise |
any received xor that |
resulting overheads rise sharply |
comparaour first microbenchmark compiles |
overheads rise sharply at |
received xor that is |
first microbenchmark compiles mafs |
and the results are |
rise sharply at the |
xor that is missing |
microbenchmark compiles mafs from |
the results are given |
sharply at the highest |
that is missing more |
results are given in |
at the highest rates |
is missing more than |
are given in figure |
the highest rates because |
missing more than one |
highest rates because of |
more than one data |
rates because of the |
than one data packet |
because of the increasing |
mb of tively high |
one data packet is |
of the increasing token |
of tively high bandwidths |
to interpret these graphs |
the increasing token roundtrip |
data packet is stored |
packet is stored temporarily |
and priorities are effective |
increasing token roundtrip time |
priorities are effective in |
are effective in mitigating |
look for instance at |
effective in mitigating source |
in case all but |
for instance at the |
in mitigating source code |
case all but one |
instance at the heavy |
the issue here is |
mitigating source code stored |
all but one of |
at the heavy load |
issue here is that |
source code stored in |
reliable multicasting with an |
but one of the |
the heavy load bar |
here is that the |
code stored in an |
multicasting with an overlay |
one of the missing |
heavy load bar mostly |
is that the amount |
stored in an mafs |
with an overlay network |
of the missing packets |
load bar mostly reads |
that the amount of |
in an mafs filesystem |
the missing packets are |
the amount of i |
missing packets are received |
packets are received later |
are received later or |
received later or recovered |
later or recovered through |
th symposium on operating |
or recovered through other |
symposium on operating systems |
o to be processed |
recovered through other xors |
on operating systems design |
to be processed increases |
operating systems design and |
systems design and implementation |
allowing the recovery of |
mb contention between different |
the recovery of the |
contention between different classes |
recovery of the remaining |
between different classes of |
of the remaining missing |
much as in some |
different classes of rpcs |
the remaining missing packet |
as in some of |
remaining missing packet from |
in some of the |
missing packet from this |
of output in the |
some of the earlier |
packet from this xor |
output in the same |
in the same filesystem |
of the earlier scenarios |
in practice we stored |
practice we stored data |
we stored data and |
stored data and xor |
data and xor packets |
and xor packets in |
compares the execution time |
xor packets in double |
the execution time speedup |
packets in double buffered |
this delays tokens as |
execution time speedup for |
in double buffered red |
delays tokens as a |
time speedup for the |
double buffered red black |
tokens as a function |
speedup for the benchmark |
buffered red black trees |
red black trees for |
for the benchmark under |
as a function of |
the benchmark under differing |
a function of the |
benchmark under differing asynchronous |
function of the growing |
under differing asynchronous writeback |
of the growing volume |
differing asynchronous writeback and |
the growing volume of |
asynchronous writeback and priority |
growing volume of multicast |
writeback and priority schemes |
volume of multicast traffic |
as bandwidth is var |
we confirm the hypothesis |
entries this occupies around |
confirm the hypothesis by |
we evaluated mafs at |
the hypothesis by looking |
evaluated mafs at a |
hypothesis by looking at |
mafs at a larger |
by looking at the |
at a larger scale |
looking at the end |
a larger scale using |
larger scale using the |
scale using the ntfs |
the repair bins in |
repair bins in the |
derived the dominant feature |
bins in the layered |
the dominant feature of |
in the layered interleaving |
dominant feature of figure |
the layered interleaving scheme |
layered interleaving scheme store |
interleaving scheme store incrementally |
scheme store incrementally computed |
is that asynchronous write |
highbandwidth content distribution in |
store incrementally computed xors |
content distribution in cooperative |
incrementally computed xors and |
traces summarised in table |
distribution in cooperative environments |
computed xors and lists |
xors and lists of |
and lists of data |
lists of data packet |
of data packet headers |
although the original execution |
the original execution back |
without the data packet |
original execution back is |
the data packet payloads |
execution back is beneficial |
back is beneficial at |
is beneficial at all |
beneficial at all bandwidths |
at all bandwidths until |
th acm symposium on |
resulting in low storage |
acm symposium on operating |
in low storage overheads |
symposium on operating systems |
low storage overheads for |
on operating systems principles |
storage overheads for each |
overheads for each layer |
for each layer that |
each layer that rise |
layer that rise linearly |
that rise linearly with |
there is less times |
rise linearly with the |
is less times of |
linearly with the value |
less times of these |
with the value of |
times of these traces |
the value of the |
of these traces were |
value of the interleave |
these traces were short |
traces were short on |
were short on windows |
short on windows nt |
the memory footprint for |
we would expect latency |
they execute improvement at |
memory footprint for a |
would expect latency to |
applications unique files total |
footprint for a longrunning |
expect latency to decrease |
unique files total file |
for a longrunning proxy |
latency to decrease as |
files total file sizes |
a longrunning proxy was |
to decrease as the |
longrunning proxy was around |
decrease as the sending |
where throughput is so |
throughput is so low |
is so low that |
so low that con |
as the sending rate |
the sending rate increases |
mb in our experiments |
slowly on mafs due |
sending rate increases because |
on mafs due to |
rate increases because the |
mafs due to high |
increases because the system |
due to high bandwidth |
because the system operates |
to high bandwidth requirements |
other performance enhancing roles |
the system operates more |
performance enhancing roles maelstrom |
system operates more smoothly |
trol traffic and the |
enhancing roles maelstrom appliances |
traffic and the delay |
roles maelstrom appliances can |
and the delay in |
maelstrom appliances can optionally |
the delay in fetching |
appliances can optionally aggregate |
delay in fetching files |
can optionally aggregate small |
avoiding context switching overheads |
in fetching files become |
optionally aggregate small subkilobyte |
fetching files become dominating |
files become dominating figure |
aggregate small subkilobyte packets |
context switching overheads and |
small subkilobyte packets from |
switching overheads and the |
subkilobyte packets from different |
overheads and the extra |
shows execution times under |
and the extra latencies |
packets from different flows |
the extra latencies caused |
grep in the gw |
extra latencies caused by |
in the gw workload |
execution times under four |
from different flows into |
eliminating trees from overlay |
trees from overlay multicast |
the gw workload even |
times under four combinations |
different flows into larger |
latencies caused by the |
gw workload even is |
under four combinations of |
th international workshop on |
international workshop on peer |
workload even is less |
flows into larger ones |
four combinations of writeback |
caused by the small |
even is less than |
into larger ones for |
combinations of writeback scheme |
by the small amount |
is less than would |
larger ones for better |
of writeback scheme and |
the small amount of |
less than would be |
ones for better communication |
writeback scheme and priorities |
small amount of buffering |
than would be expected |
for better communication efficiency |
amount of buffering in |
would be expected with |
better communication efficiency over |
of buffering in our |
be expected with reduced |
communication efficiency over the |
buffering in our protocol |
expected with reduced bandwidth |
efficiency over the long |
in our protocol stack |
here uniform priorities result |
uniform priorities result in |
priorities result in throughput |
with larger packets once |
result in throughput linear |
in split flow control |
larger packets once the |
in throughput linear in |
split flow control mode |
packets once the rate |
throughput linear in the |
flow control mode they |
once the rate exceeds |
linear in the bandwidth |
control mode they can |
mode they can perform |
they can perform send |
side buffering of in |
while differentiated priorities are |
flight data for multi |
differentiated priorities are less |
priorities are less sensitive |
gigabyte flows that exceed |
flows that exceed the |
that exceed the sending |
exceed the sending end |
host s buffering capacity |
the rc and gc |
rc and gc tests |
and gc tests show |
gc tests show the |
tests show the benefit |
maelstrom appliances can act |
show the benefit of |
appliances can act as |
the benefit of asynchronous |
can act as multicast |
benefit of asynchronous writeback |
act as multicast forwarding |
as multicast forwarding nodes |
appliances send multicast packets |
the latency starts increasing |
send multicast packets to |
since the updates from |
latency starts increasing again |
multicast packets to each |
driven overlay network for |
the updates from the |
packets to each other |
updates from the compile |
overlay network for efficient |
to each other across |
due to the longer |
from the compile workload |
network for efficient live |
each other across the |
other across the long |
the compile workload are |
for efficient live media |
to the longer pipeline |
compile workload are committed |
efficient live media streaming |
the longer pipeline at |
workload are committed sooner |
longer pipeline at the |
and use ip multicast |
are committed sooner to |
pipeline at the receive |
committed sooner to the |
at the receive side |
sooner to the server |
the receive side and |
to the server than |
receive side and other |
the server than with |
th conference on computer |
side and other phenomena |
server than with synchronous |
conference on computer communications |
and other phenomena just |
to spread them within |
than with synchronous writes |
on computer communications and |
other phenomena just mentioned |
spread them within their |
computer communications and networking |
them within their data |
within their data centers |
due to the overlap |
to the overlap of |
this is not the |
the overlap of think |
is not the case |
appliances can take on |
overlap of think time |
not the case for |
can take on other |
of think time with |
the case for small |
take on other existing |
think time with asynchronous |
case for small packets |
on other existing roles |
time with asynchronous writes |
other existing roles in |
existing roles in the |
roles in the data |
in the data center |
acting as security and |
as security and vpn |
security and vpn gateways |
and vpn gateways and |
vpn gateways and as |
gateways and as conventional |
and as conventional performance |
as conventional performance enhancing |
conventional performance enhancing proxies |
though uniform priorities provide |
uniform priorities provide better |
priorities provide better performance |
provide better performance for |
better performance for the |
performance for the write |
for the write component |
the write component of |
write component of the |
component of the rw |
of the rw test |
here the load on |
the rw test at |
defense against intrusion in |
the load on the |
against intrusion in a |
load on the system |
intrusion in a live |
e valuation we evaluated |
on the system is |
in a live streaming |
valuation we evaluated maelstrom |
the system is much |
a live streaming multicast |
we evaluated maelstrom on |
system is much smaller |
live streaming multicast system |
evaluated maelstrom on the |
maelstrom on the emulab |
on the emulab testbed |
the emulab testbed at |
emulab testbed at utah |
th ieee international conference |
ieee international conference on |
international conference on peer |
the above observations are |
above observations are consistent |
observations are consistent with |
for all the experiments |
are consistent with the |
as is to be |
consistent with the sharp |
is to be expected |
we used a dumbbell |
with the sharp rise |
used a dumbbell topology |
the sharp rise of |
a dumbbell topology of |
sharp rise of the |
dumbbell topology of two |
since we are prioritising |
rise of the average |
topology of two clusters |
we are prioritising reads |
of the average delay |
of two clusters of |
the average delay for |
two clusters of nodes |
average delay for timer |
clusters of nodes connected |
delay for timer events |
of nodes connected via |
nodes connected via routing |
connected via routing nodes |
via routing nodes with |
routing nodes with a |
nodes with a high |
this benefit largely vanishes |
benefit largely vanishes at |
latency link in between |
link in between them |
largely vanishes at lower |
vanishes at lower bandwidths |
designed to emulate the |
to emulate the setup |
emulate the setup in |
the setup in figure |
though we have concentrated |
and ran the proxy |
ran the proxy code |
we have concentrated on |
the proxy code on |
proxy code on the |
code on the routers |
have concentrated on determining |
concentrated on determining the |
on determining the benefit |
determining the benefit of |
as the rate changes |
the benefit of rpc |
the rate changes from |
benefit of rpc priorities |
of rpc priorities by |
rpc priorities by a |
priorities by a comparison |
show the performance of |
by a comparison of |
the performance of the |
a comparison of different |
performance of the kernel |
comparison of different configurations |
of the kernel version |
of different configurations of |
the kernel version at |
different configurations of mfs |
kernel version at gigabit |
configurations of mfs to |
version at gigabit speeds |
th conference on computer |
of mfs to one |
conference on computer communications |
mfs to one another |
the remainder of the |
remainder of the graphs |
of the graphs show |
the graphs show the |
graphs show the performance |
show the performance of |
the performance of the |
performance of the user |
we have also performed |
space version at slower |
version at slower speeds |
have also performed a |
also performed a few |
to emulate the mtu |
emulate the mtu difference |
the mtu difference between |
mtu difference between the |
difference between the long |
performed a few experiments |
a few experiments to |
haul link and the |
few experiments to compare |
link and the data |
experiments to compare the |
and the data center |
to compare the performance |
timer delays at the |
the data center network |
compare the performance of |
delays at the receiver |
the performance of mfs |
at the receiver increase |
performance of mfs to |
the receiver increase from |
of mfs to a |
mfs to a standard |
to a standard distributed |
we set an mtu |
set an mtu of |
a standard distributed file |
standard distributed file system |
bytes on the network |
on the network connecting |
the network connecting the |
network connecting the end |
illustrates the result of |
the result of running |
hosts to the proxy |
result of running the |
elapsed time for all |
to the proxy and |
of running the gw |
time for all fetch |
running the gw test |
the proxy and an |
for all fetch rpcs |
and on the sender |
the gw test over |
proxy and an mtu |
and an mtu of |
gw test over mfs |
test over mfs and |
over mfs and an |
mfs and an andrew |
and an andrew file |
an andrew file system |
mostly writes mostly reads |
bytes on the long |
writes mostly reads trace |
mostly reads trace mixed |
haul link between proxies |
mostly writes mostly reads |
writes mostly reads trace |
mostly reads trace mixed |
the only exception is |
only exception is figure |
mostly writes mostly reads |
writes mostly reads trace |
mostly reads trace mixed |
reads trace mixed figure |
we used the arla |
used the arla implementation |
the arla implementation of |
arla implementation of the |
implementation of the afs |
where we maintained equal |
we maintained equal mtus |
of the afs cache |
maintained equal mtus of |
the afs cache manager |
trace duration for asynchronous |
duration for asynchronous writes |
for asynchronous writes is |
asynchronous writes is until |
writes is until completion |
is until completion of |
until completion of the |
completion of the last |
of the last read |
number of unacknowledged messages |
th symposium on operating |
server is beneficial in |
of unacknowledged messages and |
symposium on operating systems |
unacknowledged messages and average |
bytes on both links |
messages and average token |
on operating systems design |
is beneficial in the |
and average token roundtrip |
operating systems design and |
beneficial in the mostly |
average token roundtrip time |
systems design and implementation |
and the openafs server |
all the experiments are |
token roundtrip time as |
in the mostly writes |
the mostly writes trace |
roundtrip time as a |
afs uses a udp |
the experiments are done |
time as a function |
experiments are done with |
which has high readwrite |
has high readwrite contention |
based rpc library without |
as a function of |
are done with maelstrom |
rpc library without priorities |
a function of the |
done with maelstrom using |
function of the sending |
with maelstrom using end |
of the sending rate |
the results largely correspond |
results largely correspond to |
largely correspond to those |
correspond to those in |
to those in figure |
it is less effective |
is less effective than |
less effective than synchronous |
effective than synchronous writeback |
due to increased contention |
but this effect is |
this effect is mitigated |
effect is mitigated by |
is mitigated by using |
mitigated by using priorities |
mfs significantly outperforms afs |
significantly outperforms afs for |
outperforms afs for the |
this is clearer in |
linearly growing memory use |
afs for the foreground |
growing memory use on |
which illustrates the performance |
is clearer in the |
for the foreground grep |
memory use on sender |
illustrates the performance of |
clearer in the graph |
the foreground grep workload |
use on sender and |
the performance of split |
on sender and the |
in the graph for |
sender and the nearly |
performance of split mode |
and the nearly flat |
the graph for time |
of split mode flow |
since afs effectively uses |
the nearly flat usage |
graph for time spent |
split mode flow control |
afs effectively uses synchronous |
nearly flat usage on |
for time spent on |
high bandwidth data dissemination |
effectively uses synchronous rpcs |
flat usage on the |
time spent on fetch |
spent on fetch rpcs |
uses synchronous rpcs with |
usage on the receiver |
bandwidth data dissemination using |
synchronous rpcs with uniform |
on the receiver as |
data dissemination using an |
rpcs with uniform priorities |
at the timescales in |
dissemination using an overlay |
using an overlay mesh |
the timescales in the |
the receiver as a |
timescales in the ntfs |
show that commodity tcp |
in the background write |
receiver as a function |
in the ntfs traces |
the background write workload |
ip throughput collapses in |
as a function of |
throughput collapses in the |
the improvements are less |
th acm symposium on |
collapses in the presence |
afs slightly outperforms mfs |
a function of the |
improvements are less dramatic |
acm symposium on operating |
in the presence of |
the presence of non |
but it is both |
symposium on operating systems |
function of the sending |
are less dramatic than |
it is both a |
on operating systems principles |
of the sending rate |
less dramatic than in |
is both a more |
and that maelstrom successfully |
dramatic than in the |
than in the microbenchmarks |
that maelstrom successfully masks |
both a more mature |
maelstrom successfully masks loss |
a more mature system |
successfully masks loss and |
but they demonstrate that |
masks loss and prevents |
they demonstrate that mafs |
loss and prevents this |
demonstrate that mafs can |
and prevents this collapse |
that mafs can improve |
prevents this collapse from |
and more optimised than |
mafs can improve the |
this collapse from occurring |
more optimised than mfs |
can improve the performance |
optimised than mfs for |
improve the performance of |
the performance of large |
than mfs for this |
mfs for this sort |
for this sort of |
this sort of communication |
shows the performance of |
the performance of the |
performance of the userspace |
store rpc begins to |
of the userspace version |
receive latency for varying |
rpc begins to arrive |
since the results of |
latency for varying rate |
the userspace version on |
begins to arrive store |
the results of running |
userspace version on a |
to arrive store rpc |
results of running the |
with various message sizes |
arrive store rpc received |
of running the other |
store rpc received dat |
running the other tests |
rpc received dat ar |
the other tests are |
received dat ar re |
other tests are similar |
mbps link and figure |
dat ar re sto |
ar re sto reply |
re sto reply ata |
sto reply ata e |
reply ata e d |
ata e d stor |
e d stor pc |
d stor pc time |
stor pc time open |
we omit them for |
shows the kernel version |
pc time open file |
omit them for brevity |
a comparative study of |
time open file for |
the kernel version on |
alarm firing delays on |
open file for writing |
comparative study of live |
study of live p |
firing delays on sender |
file for writing close |
for writing close file |
delays on sender and |
mostly reads mostly writes |
kernel version on a |
on sender and receiver |
replay log log update |
reads mostly writes heavy |
sender and receiver as |
mostly writes heavy load |
log log update store |
and receiver as a |
writes heavy load store |
log update store rpc |
receiver as a function |
the experiment in each |
heavy load store overhead |
th conference on computer |
update store rpc complete |
as a function of |
experiment in each case |
load store overhead priorities |
conference on computer communications |
store rpc complete writeback |
a function of sending |
in each case involves |
store overhead priorities uniform |
rpc complete writeback window |
function of sending rate |
each case involves running |
overhead priorities uniform priorities |
complete writeback window analysis |
case involves running iperf |
priorities uniform priorities uniform |
writeback window analysis client |
uniform priorities uniform synchronous |
window analysis client both |
priorities uniform synchronous asynchronous |
analysis client both experiments |
uniform synchronous asynchronous time |
client both experiments confirm |
synchronous asynchronous time spent |
both experiments confirm the |
asynchronous time spent on |
experiments confirm the benefits |
flows from one node |
time spent on rpcs |
confirm the benefits of |
from one node to |
the benefits of asynchronous |
one node to another |
benefits of asynchronous writeback |
node to another across |
to another across the |
another across the long |
even at bandwidths where |
at bandwidths where a |
bandwidths where a typical |
where a typical mobile |
distance link with and |
a typical mobile file |
link with and without |
typical mobile file system |
with and without intermediary |
mobile file system performs |
and without intermediary maelstrom |
file system performs all |
without intermediary maelstrom proxies |
system performs all rpcs |
intermediary maelstrom proxies and |
performs all rpcs synchronously |
maelstrom proxies and measuring |
proxies and measuring obtained |
and measuring obtained throughput |
measuring obtained throughput while |
obtained throughput while varying |
asynchronous writeback avoids the |
throughput while varying loss |
writeback avoids the need |
while varying loss rate |
avoids the need to |
the need to switch |
need to switch operation |
to switch operation into |
switch operation into a |
operation into a distinct |
left graph on each |
into a distinct low |
graph on each figure |
group memory consumption in |
preventing dos attacks in |
dos attacks in peer |
memory consumption in a |
consumption in a final |
in a final set |
and choosing a bandwidth |
a final set of |
choosing a bandwidth threshold |
final set of experiments |
peer media streaming systems |
a bandwidth threshold at |
bandwidth threshold at which |
threshold at which to |
at which to switch |
when used by themselves |
we focus on scalability |
the error bars on |
focus on scalability with |
error bars on the |
priorities do not always |
on scalability with the |
bars on the graphs |
do not always result |
th annual multimedia computing |
scalability with the number |
on the graphs to |
not always result in |
annual multimedia computing and |
with the number of |
the graphs to the |
always result in improved |
multimedia computing and networking |
the number of groups |
graphs to the left |
result in improved performance |
computing and networking conference |
to the left are |
the left are standard |
left are standard errors |
are standard errors of |
since they are only |
standard errors of the |
they are only effective |
a single sender multicasts |
errors of the throughput |
single sender multicasts to |
are only effective if |
sender multicasts to a |
of the throughput over |
only effective if concurrent |
multicasts to a varying |
the throughput over ten |
effective if concurrent rpcs |
to a varying number |
throughput over ten runs |
if concurrent rpcs have |
a varying number of |
concurrent rpcs have different |
rpcs have different priorities |
varying number of groups |
number of groups in |
of groups in a |
groups in a roundrobin |
in a roundrobin fashion |
they reduce uservisible delay |
ip s cache of |
reduce uservisible delay and |
s cache of tuning |
uservisible delay and contention |
cache of tuning parameters |
delay and contention that |
all receivers join all |
of tuning parameters to |
and contention that is |
receivers join all groups |
tuning parameters to allow |
contention that is introduced |
parameters to allow for |
that is introduced by |
to allow for repeatable |
is introduced by asynchronous |
allow for repeatable results |
introduced by asynchronous writeback |
and since the groups |
since the groups are |
the groups are perfectly |
the clients in the |
groups are perfectly overlapped |
clients in the experiment |
in the experiment are |
update propagation using asynchronous |
the experiment are running |
propagation using asynchronous writeback |
experiment are running tcp |
using asynchronous writeback at |
asynchronous writeback at all |
writeback at all bandwidths |
the system contains a |
ip reno on a |
at all bandwidths delays |
system contains a single |
reno on a linux |
all bandwidths delays sending |
contains a single region |
bandwidths delays sending updates |
delays sending updates to |
sending updates to the |
updates to the file |
to the file server |
qsm s regional recovery |
s regional recovery protocol |
we evaluate the effectiveness |
regional recovery protocol is |
evaluate the effectiveness of |
recovery protocol is oblivious |
the effectiveness of an |
protocol is oblivious to |
effectiveness of an update |
nd workshop on the |
is oblivious to the |
of an update propagation |
workshop on the economics |
on the economics of |
an update propagation scheme |
the maelstrom parameters used |
oblivious to the groups |
the economics of peer |
update propagation scheme to |
maelstrom parameters used are |
propagation scheme to reduce |
parameters used are r |
scheme to reduce this |
to reduce this delay |
hence the receivers behave |
mafs allows a client |
the receivers behave identically |
allows a client to |
receivers behave identically no |
a client to delay |
behave identically no matter |
client to delay transmitting |
to delay transmitting updates |
identically no matter how |
no matter how many |
matter how many groups |
how many groups we |
but the file server |
many groups we use |
the file server forces |
file server forces file |
server forces file updates |
forces file updates to |
file updates to be |
updates to be written |
to be written back |
be written back when |
written back when another |
back when another client |
on the other hand |
when another client must |
another client must read |
client must read an |
must read an up |
the sender maintains a |
sender maintains a number |
date copy of the |
copy of the file |
maintains a number of |
a number of per |
timeline of a file |
of a file update |
improving robustness of peer |
time advances from left |
advances from left to |
from left to right |
this affects the sender |
affects the sender s |
client will access stale |
will access stale data |
space version involved running |
the sender s memory |
peer streaming with incentives |
due to network latency |
sender s memory footprint |
version involved running a |
involved running a single |
the writeback window can |
writeback window can never |
window can never be |
can never be eliminated |
st workshop on the |
so changes to throughput |
second iperf flow from |
but adding an additional |
iperf flow from one |
changes to throughput or |
workshop on the economics |
to throughput or protocol |
flow from one node |
throughput or protocol behavior |
from one node to |
adding an additional delay |
one node to another |
or protocol behavior must |
on the economics of |
an additional delay before |
node to another with |
protocol behavior must be |
the economics of networked |
additional delay before writing |
to another with and |
behavior must be directly |
economics of networked systems |
delay before writing back |
another with and without |
must be directly or |
before writing back the |
with and without maelstrom |
be directly or indirectly |
writing back the update |
and without maelstrom running |
directly or indirectly linked |
back the update increases |
without maelstrom running on |
or indirectly linked to |
the update increases the |
maelstrom running on the |
indirectly linked to memory |
update increases the scope |
running on the routers |
linked to memory usage |
increases the scope for |
on the routers and |
the scope for inconsistency |
the routers and measuring |
routers and measuring throughput |
and measuring throughput while |
measuring throughput while varying |
throughput while varying the |
while varying the random |
varying the random loss |
we do not expect |
the random loss rate |
random loss rate on |
do not expect the |
loss rate on the |
not expect the token |
rate on the link |
illustrates how this inconsistency |
on the link and |
expect the token roundtrip |
how this inconsistency can |
the link and the |
this inconsistency can arise |
the token roundtrip time |
link and the oneway |
and the oneway latency |
token roundtrip time or |
roundtrip time or the |
like file system such |
file system such as |
system such as mafs |
to test the kernel |
time or the amount |
test the kernel version |
or the amount of |
the kernel version at |
the amount of messages |
a different type of |
amount of messages pending |
kernel version at gigabit |
different type of inconsistency |
of messages pending acknowledgement |
version at gigabit speeds |
type of inconsistency is |
messages pending acknowledgement to |
of inconsistency is introduced |
pending acknowledgement to vary |
inconsistency is introduced between |
we ran eight parallel |
acknowledgement to vary with |
is introduced between a |
ran eight parallel iperf |
p live streaming system |
to vary with the |
introduced between a client |
eight parallel iperf flows |
vary with the number |
between a client and |
parallel iperf flows from |
with the number of |
a client and the |
iperf flows from one |
of the ninth ieee |
the number of groups |
client and the server |
flows from one node |
the ninth ieee global |
and the server when |
from one node to |
ninth ieee global internet |
the server when a |
one node to another |
ieee global internet workshop |
server when a file |
node to another for |
when a file is |
a file is modified |
since the change is |
the change is hidden |
change is hidden from |
is hidden from the |
hidden from the server |
from the server until |
the server until the |
server until the file |
until the file is |
the file is closed |
the curves obtained from |
curves obtained from the |
for the purposes of |
obtained from the two |
the purposes of this |
from the two versions |
purposes of this investigation |
the two versions are |
of this investigation we |
two versions are almost |
this investigation we assume |
versions are almost identical |
investigation we assume that |
we assume that the |
assume that the open |
groups this is the |
we present both to |
this is the case |
present both to show |
close interval for a |
both to show that |
interval for a file |
to show that the |
for a file is |
show that the kernel |
a file is small |
that the kernel version |
file is small relative |
the kernel version successfully |
is small relative to |
kernel version successfully scales |
small relative to the |
version successfully scales up |
relative to the network |
successfully scales up the |
to the network latency |
scales up the performance |
the network latency and |
up the performance of |
network latency and writeback |
the performance of the |
latency and writeback delay |
performance of the userspace |
of the userspace version |
the userspace version to |
userspace version to hundreds |
version to hundreds of |
the update propagation techniques |
to hundreds of megabits |
update propagation techniques we |
hundreds of megabits of |
propagation techniques we describe |
of megabits of traffic |
techniques we describe can |
megabits of traffic per |
we describe can be |
of traffic per second |
describe can be applied |
can be applied equally |
be applied equally well |
applied equally well to |
equally well to individual |
well to individual file |
to individual file writes |
individual file writes as |
file writes as to |
writes as to writeback |
in this range memory |
this range memory consumption |
range memory consumption on |
memory consumption on the |
consumption on the sender |
on the sender grows |
techniques for update propagation |
for update propagation although |
update propagation although coda |
like file systems can |
file systems can generate |
systems can generate inconsistencies |
can generate inconsistencies between |
generate inconsistencies between clients |
we show how tcp |
they were designed to |
ip performance degrades on |
were designed to permit |
performance degrades on a |
designed to permit a |
to permit a client |
permit a client to |
a client to function |
client to function at |
to function at low |
function at low bandwidth |
and so does the |
ms link as the |
rather than for rapid |
than for rapid update |
for rapid update propagation |
so does the time |
link as the loss |
does the time spent |
as the loss rate |
since it is impractical |
the time spent in |
the loss rate is |
it is impractical to |
time spent in the |
loss rate is increased |
is impractical to lock |
spent in the clr |
rate is increased from |
impractical to lock files |
to lock files if |
lock files if clients |
files if clients are |
if clients are permitted |
clients are permitted to |
are permitted to modify |
permitted to modify the |
to modify the filesystem |
modify the filesystem while |
the filesystem while they |
filesystem while they are |
while they are disconnected |
coda supports stronger consistency |
supports stronger consistency through |
stronger consistency through optimistic |
consistency through optimistic replication |
maelstrom masks loss up |
masks loss up to |
an alternative approach is |
alternative approach is to |
approach is to allow |
is to allow a |
to allow a client |
allow a client to |
a client to use |
client to use asynchronous |
to use asynchronous writeback |
without significant throughput degradation |
but require that it |
require that it alerts |
with the kernel version |
that it alerts the |
the kernel version achieving |
it alerts the file |
kernel version achieving two |
alerts the file server |
version achieving two orders |
the file server when |
achieving two orders of |
file server when a |
two orders of magnitude |
server when a file |
orders of magnitude higher |
when a file is |
of magnitude higher throughput |
a file is modified |
magnitude higher throughput that |
higher throughput that conventional |
throughput that conventional tcp |
by sending an invalidation |
sending an invalidation rpc |
this informs the server |
informs the server that |
inspection of the managed |
the server that the |
of the managed heap |
server that the update |
that the update exists |
the managed heap in |
the graphs on the |
the update exists before |
graphs on the right |
managed heap in a |
update exists before the |
heap in a debugger |
on the right side |
in a debugger shows |
exists before the new |
a debugger shows that |
before the new file |
debugger shows that the |
the new file contents |
new file contents ar |
shows that the growth |
the right side of |
right side of figures |
that the growth in |
the growth in memory |
growth in memory used |
in memory used is |
memory used is caused |
used is caused not |
is caused not by |
caused not by messages |
origin of inconsistencies since |
of inconsistencies since asynchronous |
inconsistencies since asynchronous writeback |
since asynchronous writeback decouples |
asynchronous writeback decouples modifying |
writeback decouples modifying a |
decouples modifying a file |
modifying a file from |
but by the per |
a file from notifying |
ip throughput declining on |
file from notifying the |
throughput declining on a |
from notifying the server |
declining on a link |
notifying the server that |
on a link of |
group elements of the |
the server that a |
a link of increasing |
elements of the protocol |
server that a change |
link of increasing length |
of the protocol stack |
that a change has |
of increasing length when |
a change has occurred |
increasing length when subjected |
length when subjected to |
when subjected to uniform |
subjected to uniform loss |
to uniform loss rates |
it can generate inconsistencies |
uniform loss rates of |
each maintains a queue |
can generate inconsistencies between |
generate inconsistencies between cached |
inconsistencies between cached copies |
illustrates the potential for |
the potential for inconsistency |
during the writeback window |
another client accessing a |
client accessing a cached |
accessing a cached copy |
the top line in |
small structures for profiling |
top line in the |
or fetching the file |
structures for profiling etc |
line in the graphs |
fetching the file from |
in the graphs is |
the file from the |
the graphs is the |
file from the file |
graphs is the performance |
from the file server |
is the performance of |
the performance of tcp |
with thousands of groups |
will not read up |
ip without loss and |
without loss and provides |
loss and provides an |
and provides an upper |
provides an upper bound |
an upper bound for |
upper bound for performance |
these add up to |
bound for performance on |
add up to tens |
from the server s |
for performance on the |
performance on the link |
the server s perspective |
up to tens of |
to tens of megabytes |
there is no inconsistency |
space and kernel versions |
since it is unaware |
it is unaware of |
is unaware of the |
unaware of the new |
we can confirm the |
maelstrom masks packet loss |
of the new update |
can confirm the theory |
masks packet loss and |
confirm the theory by |
packet loss and tracks |
the theory by turning |
loss and tracks the |
from a global perspective |
theory by turning on |
and tracks the lossless |
tracks the lossless line |
by turning on additional |
the lossless line closely |
turning on additional tracing |
writing client writes a |
on additional tracing in |
lagging only when the |
client writes a closes |
writes a closes a |
only when the link |
additional tracing in the |
when the link latency |
tracing in the per |
the link latency is |
link latency is low |
latency is low and |
is low and tcp |
reading client server fetch |
client server fetch a |
server fetch a fetch |
fetch a fetch reply |
ip s throughput is |
s throughput is very |
throughput is very high |
cloudifying source code repositories |
how much does it |
much does it cost |
this tracing is lightweight |
flushes update store a |
tracing is lightweight and |
update store a callback |
michael siegenthaler hakim weatherspoon |
siegenthaler hakim weatherspoon dept |
store a callback for |
is lightweight and has |
a callback for a |
lightweight and has little |
callback for a fetch |
of computer science cornell |
and has little effect |
for a fetch a |
computer science cornell university |
science cornell university msiegen |
a fetch a open |
fetch a open a |
has little effect on |
little effect on cpu |
effect on cpu consumption |
writes a closes a |
of computer science cornell |
computer science cornell university |
but it increases the |
science cornell university hweather |
it increases the memory |
increases the memory footprint |
the memory footprint by |
memory footprint by adding |
footprint by adding additional |
by adding additional data |
adding additional data structures |
flushes update open a |
edu abstract cloud computing |
additional data structures that |
abstract cloud computing provides |
data structures that are |
cloud computing provides us |
structures that are updated |
computing provides us with |
fetch reply reading client |
that are updated once |
provides us with general |
reply reading client server |
are updated once per |
us with general purpose |
reading client server invalidate |
updated once per second |
with general purpose storage |
client server invalidate a |
general purpose storage and |
purpose storage and server |
storage and server hosting |
and server hosting platforms |
server hosting platforms at |
hosting platforms at a |
platforms at a reasonable |
at a reasonable price |
which burdens the gc |
we explore the possibility |
explore the possibility of |
the possibility of tapping |
possibility of tapping these |
of tapping these resources |
tapping these resources for |
these resources for the |
resources for the purpose |
for the purpose of |
the purpose of hosting |
purpose of hosting source |
of hosting source code |
hosting source code repositories |
source code repositories for |
code repositories for individual |
repositories for individual projects |
for individual projects as |
individual projects as well |
projects as well as |
writing client pull a |
as well as entire |
well as entire open |
as entire open source |
entire open source communities |
an analysis of storage |
analysis of storage costs |
of storage costs is |
storage costs is presented |
callback for a fetch |
and a complete hosting |
for a fetch a |
a complete hosting solution |
ip no loss maelstrom |
complete hosting solution is |
no loss maelstrom no |
hosting solution is built |
loss maelstrom no loss |
solution is built and |
maelstrom no loss maelstrom |
is built and evaluated |
built and evaluated as |
and evaluated as a |
evaluated as a proof |
store a fetch reply |
it is worth noting |
is worth noting that |
worth noting that the |
noting that the memory |
i ntroduction the advent |
that the memory usages |
ntroduction the advent of |
the memory usages reported |
the advent of cloud |
memory usages reported here |
advent of cloud computing |
usages reported here are |
of cloud computing has |
reported here are averages |
cloud computing has brought |
computing has brought us |
has brought us a |
brought us a dazzling |
us a dazzling array |
a dazzling array of |
asynchronous writeback with invalidations |
dazzling array of public |
writeback with invalidations figure |
array of public computing |
of public computing services |
public computing services that |
computing services that can |
services that can be |
that can be instantly |
can be instantly tapped |
be instantly tapped by |
instantly tapped by anyone |
tapped by anyone with |
by anyone with a |
anyone with a credit |
with a credit card |
a credit card number |
a client s update |
client s update is |
s update is logged |
and the peak values |
update is logged when |
the peak values are |
users are spared from |
is logged when the |
peak values are typically |
are spared from having |
logged when the file |
spared from having to |
when the file is |
from having to invest |
the file is closed |
having to invest in |
to invest in expensive |
invest in expensive infrastructure |
in expensive infrastructure such |
expensive infrastructure such as |
while it is in |
infrastructure such as servers |
it is in the |
is in the log |
other clients see the |
clients see the server |
see the server s |
the server s stale |
and cooling equipment because |
server s stale version |
cooling equipment because the |
equipment because the service |
because the service provider |
the service provider takes |
service provider takes care |
an invalidation rpc allows |
provider takes care of |
invalidation rpc allows the |
takes care of these |
rpc allows the server |
care of these and |
allows the server to |
of these and amortizes |
the server to invalidate |
these and amortizes the |
server to invalidate other |
and amortizes the cost |
to invalidate other clients |
amortizes the cost across |
invalidate other clients cached |
the cost across many |
other clients cached copies |
cost across many clients |
achieving efficiency through economies |
efficiency through economies of |
through economies of scale |
a client that modifies |
client that modifies a |
that modifies a file |
modifies a file could |
companies are realizing that |
a file could save |
are realizing that it |
file could save bandwidth |
realizing that it no |
the nodes on our |
could save bandwidth by |
that it no longer |
nodes on our cluster |
save bandwidth by not |
it no longer makes |
on our cluster only |
bandwidth by not sending |
no longer makes sense |
our cluster only have |
by not sending it |
longer makes sense to |
not sending it to |
makes sense to build |
sending it to the |
sense to build and |
it to the file |
to build and manage |
to the file server |
build and manage all |
the file server at |
file server at all |
and manage all of |
manage all of their |
all of their own |
of their own infrastructure |
unless the server pulls |
the server pulls it |
server pulls it to |
pulls it to supply |
and services in the |
it to supply it |
services in the cloud |
to supply it to |
in the cloud are |
supply it to another |
the cloud are quickly |
it to another client |
cloud are quickly becoming |
are quickly becoming popular |
mafs clients push updates |
clients push updates to |
push updates to the |
updates to the server |
to the server in |
the server in the |
server in the background |
that software development projects |
to reduce the delay |
software development projects will |
reduce the delay incurred |
development projects will turn |
the delay incurred when |
projects will turn to |
delay incurred when fetching |
will turn to cloud |
incurred when fetching an |
turn to cloud computing |
when fetching an invalidated |
to cloud computing to |
fetching an invalidated file |
cloud computing to store |
computing to store their |
to store their master |
store their master code |
their master code repositories |
pushing updates can result |
updates can result in |
can result in the |
result in the server |
in the server having |
either on a project |
the server having received |
server having received some |
or all of the |
all of the update |
project basis or as |
of the update by |
basis or as part |
the update by the |
or as part of |
update by the time |
as part of a |
by the time another |
part of a larger |
the time another client |
of a larger migration |
time another client accesses |
a larger migration of |
another client accesses it |
larger migration of a |
migration of a sourceforge |
memory footprint is significant |
even small code repositories |
small code repositories represent |
code repositories represent a |
repositories represent a huge |
represent a huge investment |
a huge investment of |
huge investment of developerhours |
selective invalidation with reader |
invalidation with reader pull |
with reader pull the |
reader pull the effect |
so the need to |
pull the effect of |
the need to store |
the effect of selective |
need to store this |
effect of selective invalidation |
to store this data |
of selective invalidation and |
store this data durably |
selective invalidation and reader |
this data durably and |
invalidation and reader pull |
data durably and reliably |
and reader pull is |
durably and reliably is |
reader pull is that |
and reliably is obvious |
pull is that mafs |
is that mafs incorporates |
that mafs incorporates sirp |
less obvious are the |
obvious are the shortcomings |
are the shortcomings of |
the shortcomings of traditional |
a new algorithm for |
shortcomings of traditional storage |
of traditional storage systems |
new algorithm for maintaining |
algorithm for maintaining inter |
sirp behaves similarly to |
behaves similarly to synchronous |
similarly to synchronous writeback |
to synchronous writeback if |
synchronous writeback if a |
the peak footprint approaches |
writeback if a client |
if a client client |
a client client consistency |
which combines asynchronous writeback |
combines asynchronous writeback with |
asynchronous writeback with concurrently |
writeback with concurrently fetches |
with concurrently fetches a |
protect against data loss |
concurrently fetches a file |
but they are neither |
they are neither cheap |
but behaves like asynchronous |
are neither cheap nor |
behaves like asynchronous writeinvalidations |
neither cheap nor simple |
like asynchronous writeinvalidations and |
asynchronous writeinvalidations and expedited |
writeinvalidations and expedited transmission |
and expedited transmission of |
especially when developers and |
expedited transmission of updates |
when developers and server |
transmission of updates for |
developers and server administrators |
of updates for files |
and server administrators are |
updates for files back |
and the system is |
server administrators are geographically |
for files back when |
the system is close |
administrators are geographically spread |
are geographically spread thin |
system is close to |
files back when there |
is close to swapping |
back when there are |
when there are no |
there are no concurrent |
are no concurrent fetches |
we focus on the |
focus on the costs |
on the costs of |
like synchronous that other |
the costs of moving |
synchronous that other clients |
costs of moving source |
that other clients are |
of moving source code |
other clients are attempting |
moving source code repositories |
clients are attempting to |
source code repositories to |
are attempting to read |
code repositories to the |
repositories to the cloud |
to the cloud as |
the cloud as an |
cloud as an example |
as an example of |
an example of moving |
example of moving services |
of moving services in |
moving services in general |
sirp sends an rpc |
services in general to |
sends an rpc to |
in general to the |
general to the cloud |
an rpc to the |
rpc to the server |
to the server as |
the server as soon |
server as soon as |
especially collaborative open source |
as soon as an |
collaborative open source projects |
soon as an application |
as an application closes |
an application closes a |
application closes a modified |
closes a modified file |
such an endeavor includes |
an endeavor includes many |
endeavor includes many costs |
but it can defer |
it can defer transmitting |
can defer transmitting the |
defer transmitting the selective |
transmitting the selective invalidation |
the most critical of |
most critical of which |
critical of which is |
of which is storage |
using an invalidation rpc |
which is storage since |
an invalidation rpc to |
is storage since that |
invalidation rpc to alert |
storage since that is |
rpc to alert the |
since that is the |
to alert the actual |
that is the simplest |
alert the actual contents |
is the simplest and |
the actual contents until |
the simplest and likely |
actual contents until they |
simplest and likely first |
contents until they are |
until they are needed |
and likely first component |
likely first component to |
first component to be |
component to be moved |
file server to the |
server to the existence |
groups are enough to |
we set an agenda |
to the existence of |
are enough to trigger |
set an agenda for |
the existence of a |
enough to trigger signs |
an agenda for demonstrating |
existence of a new |
to trigger signs of |
agenda for demonstrating the |
of a new update |
trigger signs of instability |
for demonstrating the financial |
a new update improves |
demonstrating the financial storage |
new update improves cache |
the financial storage and |
update improves cache consistency |
financial storage and computing |
storage and computing costs |
token roundtrip times start |
and computing costs of |
roundtrip times start to |
but consumes additional bandwidth |
computing costs of moving |
times start to grow |
costs of moving source |
of moving source code |
moving source code repositories |
if writeback traffic is |
source code repositories to |
writeback traffic is low |
code repositories to the |
traffic is low enough |
repositories to the cloud |
thus delaying message cleanup |
is low enough for |
low enough for the |
enough for the server |
for the server to |
in section ii we |
the server to start |
tcp no loss maelstrom |
section ii we explain |
server to start receiving |
no loss maelstrom no |
ii we explain what |
to start receiving an |
loss maelstrom no loss |
we explain what it |
start receiving an update |
maelstrom no loss maelstrom |
explain what it means |
what it means to |
it means to store |
means to store a |
to store a code |
store a code repository |
a code repository in |
code repository in the |
repository in the cloud |
in the cloud and |
the cloud and why |
experimental evaluation immediately after |
cloud and why there |
evaluation immediately after it |
and why there are |
immediately after it receives |
why there are cost |
after it receives the |
it receives the invalidation |
there are cost advantages |
and increasing memory overhead |
are cost advantages to |
cost advantages to doing |
the invalidation we conclude |
advantages to doing so |
invalidation we conclude this |
we conclude this section |
conclude this section with |
this section with an |
section with an experiment |
section iii is a |
with an experiment that |
iii is a case |
an experiment that compares |
is a case study |
experiment that compares the |
a case study on |
that compares the is |
case study on using |
study on using amazon |
on using amazon s |
using amazon s s |
compares the is superfluous |
to host some popular |
host some popular open |
sirp avoids this overhead |
some popular open source |
avoids this overhead by |
popular open source communities |
this overhead by performing |
overhead by performing selec |
and includes a cost |
includes a cost analysis |
effectiveness of sirp to |
of sirp to three |
sirp to three alternatives |
in section iv we |
although the process is |
section iv we present |
the process is fairly |
iv we present an |
process is fairly unpredictable |
we present an implementation |
when a client adds |
present an implementation that |
a client adds an |
an implementation that ties |
client adds an update |
implementation that ties subversion |
that ties subversion to |
ties subversion to s |
adds an update to |
we see spikes and |
an update to the |
see spikes and anomalies |
update to the writeback |
to the writeback back |
the writeback back transmits |
writeback back transmits an |
back transmits an update |
transmits an update as |
an update as soon |
end servers running on |
update as soon as |
servers running on amazon |
as soon as a |
running on amazon s |
soon as a file |
on amazon s ec |
as a file is |
a file is closed |
we can easily recognize |
and using yahoo s |
can easily recognize a |
using yahoo s zookeeper |
easily recognize a super |
yahoo s zookeeper for |
it only sends an |
s zookeeper for consistency |
only sends an invalidation |
sends an invalidation if |
an invalidation if the |
invalidation if the queue |
if the queue is |
in section v we |
the queue is not |
queue is not empty |
linear trend starting at |
section v we evaluate |
trend starting at around |
v we evaluate the |
chronous writeback puts the |
we evaluate the performance |
writeback puts the update |
evaluate the performance of |
one way link latency |
puts the update in |
the performance of this |
the update in a |
performance of this solution |
update in a queue |
in a queue and |
a queue and transmits |
queue and transmits it |
and transmits it if |
and in section vi |
transmits it if the |
in section vi we |
it if the queue |
section vi we address |
if the queue is |
vi we address related |
the queue is empty |
we address related work |
the invalidation is piggybacked |
invalidation is piggybacked onto |
is piggybacked onto the |
piggybacked onto the as |
onto the as soon |
c loudifying s ource |
the as soon as |
loudifying s ource r |
as soon as it |
s ource r epositories |
soon as it reaches |
at around this point |
ource r epositories in |
as it reaches the |
r epositories in a |
it reaches the front |
epositories in a revision |
reaches the front of |
in a revision control |
the front of the |
a revision control system |
we also start to |
front of the queue |
also start to see |
a master copy of |
start to see occasional |
we also compare update |
master copy of the |
way latency throughput as |
to see occasional bursts |
copy of the source |
latency throughput as a |
sirp against a policy |
see occasional bursts of |
of the source code |
throughput as a function |
against a policy we |
occasional bursts of packet |
as a function of |
a policy we refer |
bursts of packet losses |
a function of latency |
policy we refer to |
is stored in a |
we refer to as |
refer to as sirp |
stored in a logically |
in a logically centralized |
a logically centralized repository |
each developer checks out |
which only differs from |
developer checks out and |
only differs from sirp |
checks out and then |
differs from sirp in |
out and then keeps |
from sirp in performing |
and then keeps a |
sirp in performing compulsory |
then keeps a working |
in performing compulsory invalidations |
keeps a working copy |
often roughly correlated across |
a working copy on |
roughly correlated across receivers |
working copy on his |
copy on his machine |
on his machine that |
his machine that mirrors |
machine that mirrors the |
when the server receives |
ip to attain very |
that mirrors the repository |
the server receives an |
to attain very high |
server receives an invalidation |
such events trigger bursty |
attain very high speeds |
receives an invalidation from |
the developer edits files |
events trigger bursty recovery |
very high speeds on |
an invalidation from a |
developer edits files in |
trigger bursty recovery overloads |
high speeds on the |
invalidation from a date |
edits files in his |
speeds on the gigabit |
from a date results |
files in his working |
on the gigabit link |
a date results in |
in his working copy |
date results in an |
his working copy and |
results in an invalidation |
working copy and periodically |
we had to set |
in an invalidation rpc |
copy and periodically commits |
had to set the |
an invalidation rpc to |
and periodically commits the |
to set the mtu |
invalidation rpc to the |
periodically commits the changes |
set the mtu of |
rpc to the server |
commits the changes back |
the mtu of the |
the changes back to |
mtu of the entire |
changes back to the |
of the entire path |
back to the repository |
the entire path to |
entire path to be |
it makes callbacks to |
path to be the |
makes callbacks to all |
to be the maximum |
and updates his working |
callbacks to all the |
updates his working copy |
to all the other |
number of messages pending |
his working copy to |
of messages pending ack |
all the other clients |
messages pending ack and |
working copy to reflect |
pending ack and token |
the other clients that |
ack and token roundtrip |
copy to reflect the |
and token roundtrip time |
other clients that cache |
to reflect the changes |
token roundtrip time as |
which meant that the |
clients that cache the |
reflect the changes made |
roundtrip time as a |
meant that the long |
that cache the are |
the changes made by |
time as a function |
cache the are of |
changes made by other |
haul link had the |
as a function of |
the are of particular |
made by other developers |
link had the same |
a function of the |
are of particular interest |
had the same mtu |
function of the number |
of particular interest in |
each commit is assigned |
the same mtu as |
same mtu as the |
mtu as the inter |
commit is assigned a |
is assigned a unique |
particular interest in this |
interest in this comparison |
of the number of |
the number of groups |
this resulted in the |
resulted in the fragmentation |
the repository maintains complete |
in the fragmentation of |
repository maintains complete history |
the fragmentation of repair |
maintains complete history so |
to tell them to |
fragmentation of repair packets |
complete history so at |
tell them to discard |
of repair packets sent |
history so at any |
them to discard their |
repair packets sent over |
so at any point |
to discard their copies |
packets sent over udp |
at any point in |
sent over udp on |
any point in time |
if several clients modify |
over udp on the |
point in time it |
several clients modify are |
udp on the longhaul |
in time it is |
clients modify are the |
on the longhaul link |
time it is possible |
modify are the files |
memory usage grows with |
it is possible to |
the longhaul link into |
are the files readers |
the files readers read |
is possible to check |
longhaul link into two |
usage grows with the |
possible to check out |
link into two ip |
grows with the number |
to check out a |
into two ip packet |
two ip packet fragments |
how is the performance |
check out a working |
with the number of |
is the performance of |
since the loss of |
the number of groups |
out a working copy |
the performance of the |
the loss of a |
a working copy for |
performance of the same |
of the same file |
working copy for any |
loss of a single |
copy for any specified |
beyond a certain threshold |
of a single fragment |
for any specified version |
modifications are serialised in |
a single fragment resulted |
any specified version number |
are serialised in the |
single fragment resulted in |
serialised in the order |
fragment resulted in the |
storing a repository in |
in the order of |
the system is increasingly |
resulted in the loss |
a repository in the |
the order of their |
system is increasingly unstable |
in the loss of |
repository in the cloud |
order of their readers |
the loss of the |
loss of the repair |
of their readers and |
in the cloud eliminates |
their readers and writers |
the cloud eliminates worries |
readers and writers affected |
we observed a higher |
cloud eliminates worries of |
and writers affected by |
observed a higher loss |
eliminates worries of data |
writers affected by stronger |
a higher loss rate |
worries of data loss |
affected by stronger consistency |
higher loss rate for |
of data loss due |
loss rate for repairs |
data loss due to |
rate for repairs than |
loss due to hardware |
for repairs than for |
due to hardware failure |
repairs than for data |
than for data packets |
the client that made |
time spent in the |
but issues of access |
client that made the |
spent in the clr |
issues of access control |
that made the update |
we expect performance to |
in the clr code |
of access control and |
made the update only |
expect performance to be |
access control and consistency |
the update only transmits |
performance to be better |
control and consistency must |
update only transmits it |
to be better on |
and consistency must still |
only transmits it when |
be better on a |
consistency must still be |
transmits it when it |
better on a network |
must still be addressed |
it when it reaches |
on a network where |
when it reaches the |
a network where the |
it reaches the head |
network where the mtu |
authorized users should be |
reaches the head of |
where the mtu of |
users should be able |
the head of the |
the mtu of the |
mtu of the long |
head of the writeback |
should be able to |
of the writeback queue |
be able to commit |
throughput decreases with the |
haul link is truly |
able to commit new |
if another client attempts |
link is truly larger |
decreases with the number |
to commit new versions |
another client attempts to |
is truly larger than |
with the number of |
commit new versions of |
mostly reads mostly writes |
client attempts to fetch |
truly larger than the |
the number of groups |
new versions of files |
versions of files to |
attempts to fetch the |
larger than the mtu |
than the mtu within |
of files to the |
to fetch the file |
reads mostly writes heavy |
the mtu within each |
mostly writes heavy load |
fetch the file during |
writes heavy load store |
mtu within each cluster |
heavy load store overhead |
the file during the |
load store overhead priorities |
files to the repository |
store overhead priorities uniform |
even with zero loss |
file during the update |
overhead priorities uniform priorities |
during the update s |
but not edit existing |
not edit existing history |
the update s experimental |
priorities uniform priorities uniform |
update s experimental setup |
ip throughput in figure |
uniform priorities uniform synchronous |
s experimental setup writeback |
users expect the repository |
priorities uniform synchronous asynchronous |
experimental setup writeback window |
expect the repository to |
uniform synchronous asynchronous figure |
all groups have the |
the repository to be |
the server blocks that |
groups have the same |
repository to be consistent |
server blocks that client |
have the same subscribers |
declines with link latency |
to be consistent and |
blocks that client until |
be consistent and for |
that client until the |
consistent and for any |
graphs of ntfs traces |
client until the update |
and for any changes |
this is due to |
until the update has |
for any changes they |
is due to the |
the update has arrived |
any changes they make |
due to the cap |
each trace ran with |
changes they make not |
trace ran with synchronous |
to the cap on |
ran with synchronous or |
the server also makes |
with synchronous or asynchronous |
the cap on throughput |
they make not to |
server also makes a |
synchronous or asynchronous writes |
cap on throughput placed |
make not to be |
not to be pre |
or asynchronous writes and |
on throughput placed by |
also makes a pull |
asynchronous writes and uniform |
throughput placed by the |
makes a pull rpc |
writes and uniform or |
placed by the buffering |
a pull rpc to |
even in the face |
and uniform or differentiated |
by the buffering available |
pull rpc to the |
the key insight is |
in the face of |
uniform or differentiated priorities |
the buffering available at |
rpc to the client |
key insight is that |
the face of cloud |
insight is that all |
to the client that |
buffering available at the |
face of cloud services |
is that all these |
the client that experiments |
the total height of |
available at the receiving |
of cloud services that |
that all these effects |
client that experiments were |
total height of each |
at the receiving end |
cloud services that offer |
all these effects originate |
that experiments were conducted |
height of each bar |
services that offer lesser |
these effects originate at |
experiments were conducted in |
the preceding experiments were |
that offer lesser guarantees |
effects originate at the |
of each bar denotes |
were conducted in a |
preceding experiments were done |
originate at the sender |
each bar denotes the |
conducted in a network |
experiments were done with |
for these reasons we |
at the sender node |
bar denotes the time |
in a network of |
were done with maelstrom |
these reasons we do |
denotes the time from |
a network of five |
done with maelstrom in |
reasons we do not |
the time from the |
which is more loaded |
network of five hosts |
with maelstrom in endto |
we do not expect |
time from the first |
is more loaded and |
do not expect that |
from the first to |
more loaded and less |
one modified the file |
end flow control mode |
not expect that clients |
the first to last |
loaded and less responsive |
expect that clients will |
first to last write |
instructing it to expedite |
where it is oblivious |
that clients will be |
it to expedite sending |
it is oblivious to |
is oblivious to tcp |
to expedite sending the |
clients will be directly |
expedite sending the update |
will be directly using |
ip and does not |
and does not split |
be directly using the |
and the shaded portion |
does not split connections |
detailed analysis of the |
directly using the cloud |
the shaded portion denotes |
analysis of the captured |
one writer client that |
using the cloud storage |
shaded portion denotes the |
and is consequently sensitive |
of the captured network |
writer client that was |
the cloud storage api |
portion denotes the time |
is consequently sensitive to |
the captured network traffic |
client that was responsible |
cloud storage api anytime |
denotes the time from |
consequently sensitive to the |
captured network traffic shows |
that was responsible for |
storage api anytime soon |
the time from the |
sensitive to the size |
network traffic shows that |
was responsible for modifying |
but that they will |
to the size of |
traffic shows that the |
time from the first |
responsible for modifying when |
that they will contact |
the size of the |
shows that the multicast |
from the first to |
for modifying when it |
they will contact one |
size of the receiver |
of the receiver buffer |
the first to last |
modifying when it receives |
will contact one of |
that the multicast stream |
first to last read |
when it receives the |
contact one of a |
one of a set |
it receives the pull |
receives the pull rpc |
of a set of |
shows the performance of |
the white portions denote |
a set of front |
the multicast stream in |
the performance of split |
white portions denote the |
the client begins sending |
multicast stream in all |
performance of split mode |
portions denote the extra |
end servers that are |
client begins sending back |
stream in all cases |
of split mode flow |
denote the extra time |
servers that are responsible |
begins sending back a |
in all cases looks |
split mode flow control |
the extra time required |
that are responsible for |
sending back a collection |
all cases looks basically |
extra time required to |
are responsible for enforcing |
back a collection of |
where maelstrom breaks a |
cases looks basically identical |
time required to complete |
responsible for enforcing access |
a collection of files |
maelstrom breaks a single |
breaks a single tcp |
for enforcing access control |
required to complete all |
and hence we cannot |
and three reader clients |
to complete all writes |
ip connection into three |
hence we cannot attribute |
three reader clients that |
complete all writes after |
connection into three hops |
we cannot attribute token |
and pushing the data |
reader clients that only |
all writes after the |
cannot attribute token latency |
pushing the data into |
clients that only read |
writes after the last |
attribute token latency or |
the data into the |
data into the cloud |
after the last read |
token latency or losses |
that only read the |
the last read has |
latency or losses to |
these might consist of |
only read the the |
last read has finished |
or losses to the |
split mode flow control |
might consist of virtualized |
read the the update |
losses to the increased |
mode flow control eliminates |
consist of virtualized server |
the the update at |
to the increased volume |
flow control eliminates the |
for asynchronous writeback with |
the update at the |
the increased volume of |
of virtualized server instances |
control eliminates the requirement |
asynchronous writeback with priorities |
update at the same |
increased volume of traffic |
virtualized server instances in |
eliminates the requirement for |
writeback with priorities in |
at the same priority |
server instances in the |
the requirement for large |
with priorities in the |
the same priority as |
throughput spikes or longer |
instances in the cloud |
requirement for large buffers |
same priority as an |
spikes or longer bursts |
for large buffers at |
priority as an rpc |
or longer bursts of |
or traditional physical machines |
large buffers at the |
as an rpc to |
longer bursts of data |
traditional physical machines owned |
buffers at the receiving |
an rpc to fetch |
physical machines owned by |
at the receiving end |
rpc to fetch file |
machines owned by the |
to fetch file data |
owned by the community |
this shows that the |
shows that the total |
throughput is essentially insensitive |
is essentially insensitive to |
but in either case |
that the total duration |
the bandwidth between the |
the sender spends more |
in either case their |
essentially insensitive to one |
the total duration of |
bandwidth between the writer |
sender spends more time |
either case their local |
total duration of the |
between the writer client |
spends more time transmitting |
with a slight drop |
duration of the trace |
the writer client and |
case their local storage |
more time transmitting at |
a slight drop due |
of the trace with |
writer client and the |
their local storage systems |
time transmitting at lower |
slight drop due to |
the trace with this |
client and the server |
local storage systems are |
transmitting at lower rates |
drop due to buffering |
trace with this mfs |
and the server that |
storage systems are allowed |
due to buffering overhead |
with this mfs configuration |
the server that it |
systems are allowed to |
to buffering overhead on |
but doesn t produce |
this mfs configuration is |
server that it will |
are allowed to be |
buffering overhead on the |
overhead on the maelstrom |
that it will be |
allowed to be cheap |
doesn t produce any |
on the maelstrom boxes |
t produce any faster |
to be cheap and |
it will be preferentially |
produce any faster data |
be cheap and unresilient |
any faster data bursts |
will be preferentially allocated |
faster data bursts than |
cheap and unresilient against |
data bursts than those |
be preferentially allocated bandwidth |
and unresilient against hardware |
compares split mode to |
split mode to end |
unresilient against hardware failure |
bursts than those we |
if the update was |
the update was set |
but all the fetch |
another consideration with any |
than those we observe |
update was set to |
all the fetch traffic |
consideration with any hosting |
those we observe with |
the fetch traffic is |
with any hosting solution |
we observe with smaller |
fetch traffic is completed |
any hosting solution is |
observe with smaller numbers |
traffic is completed within |
hosting solution is resource |
solution is resource provisioning |
with smaller numbers of |
smaller numbers of groups |
open source communities with |
source communities with limited |
communities with limited budgets |
with limited budgets and |
limited budgets and private |
budgets and private enterprises |
and private enterprises that |
private enterprises that are |
enterprises that are increasingly |
that are increasingly cost |
sensitive may well prefer |
may well prefer to |
well prefer to pay |
prefer to pay just |
to pay just for |
pay just for the |
just for the resources |
for the resources they |
the resources they use |
and the reader client |
rather than trying to |
seconds of the start |
than trying to budget |
trying to budget in |
to budget in advance |
budget in advance what |
in advance what they |
server was already being |
advance what they are |
was already being written |
what they are going |
already being written back |
they are going to |
are going to need |
this is a significant |
receiver performance indicators such |
is a significant improvement |
performance indicators such as |
cloud computing makes this |
the client increases its |
client increases its priority |
indicators such as delays |
computing makes this a |
makes this a possibility |
such as delays in |
a significant improvement over |
bandwidth was always set |
was always set to |
significant improvement over the |
as delays in firing |
and increased competition among |
improvement over the alternative |
delays in firing timer |
increased competition among providers |
over the alternative configurations |
in firing timer event |
competition among providers of |
the alternative configurations measured |
firing timer event or |
among providers of commodity |
timer event or cpu |
providers of commodity services |
so that it can |
event or cpu utilization |
of commodity services will |
that it can prevent |
or cpu utilization don |
commodity services will ensure |
it can prevent inconsistencies |
cpu utilization don t |
services will ensure that |
can prevent inconsistencies by |
utilization don t show |
will ensure that prices |
prevent inconsistencies by inhibiting |
don t show any |
ensure that prices are |
inconsistencies by inhibiting access |
t show any noticeable |
that prices are reasonable |
by inhibiting access to |
seconds of the trace |
show any noticeable trend |
inhibiting access to the |
of the trace are |
access to the file |
the trace are taken |
to the file by |
c ase s tudy |
trace are taken up |
the file by other |
file by other clients |
are taken up by |
taken up by asynchronously |
up by asynchronously writing |
as shown in figure |
all roads lead back |
by asynchronously writing back |
roads lead back to |
asynchronously writing back file |
lead back to the |
by far the most |
writing back file updates |
back to the sender |
far the most popular |
the most popular general |
most popular general purpose |
popular general purpose cloud |
general purpose cloud storage |
purpose cloud storage service |
invalidations are used in |
cloud storage service today |
are used in fluid |
storage service today is |
in all cases the |
used in fluid replication |
and the main thing |
service today is amazon |
all cases the traces |
the main thing going |
today is amazon s |
is amazon s s |
main thing going on |
cases the traces take |
thing going on in |
the traces take significantly |
to allow clients to |
going on in the |
traces take significantly longer |
allow clients to avoid |
on in the sender |
we chose to use |
take significantly longer than |
mode buffering flow control |
in the sender is |
chose to use this |
clients to avoid sending |
significantly longer than they |
buffering flow control against |
flow control against one |
to use this as |
to avoid sending data |
longer than they originally |
the sender is that |
use this as a |
avoid sending data across |
than they originally did |
way link latency left |
sender is that it |
this as a basis |
sending data across a |
data across a wide |
most bar represents maelstrom |
as a basis for |
they originally did in |
is that it has |
bar represents maelstrom in |
a basis for cost |
originally did in ntfs |
that it has a |
represents maelstrom in end |
basis for cost studies |
it has a steadily |
for cost studies and |
the server only asks |
where they were mostly |
cost studies and for |
has a steadily growing |
end mode with manually |
they were mostly accessing |
studies and for the |
server only asks the |
a steadily growing memory |
mode with manually configured |
were mostly accessing the |
and for the implementation |
only asks the client |
steadily growing memory footprint |
with manually configured large |
mostly accessing the local |
for the implementation of |
asks the client for |
manually configured large buffers |
we also looked at |
the implementation of our |
the client for a |
accessing the local file |
configured large buffers at |
large buffers at end |
implementation of our system |
client for a file |
the local file system |
also looked at token |
for a file s |
local file system and |
looked at token round |
a file s data |
file system and therefore |
and the second and |
file s data if |
is an appealing choice |
system and therefore had |
the second and third |
s data if another |
an appealing choice because |
and therefore had no |
second and third bar |
data if another client |
appealing choice because amazon |
the distribution of token |
therefore had no bandwidth |
and third bar from |
if another client requests |
choice because amazon also |
distribution of token roundtrip |
had no bandwidth constraints |
third bar from left |
another client requests it |
because amazon also offers |
amazon also offers the |
also offers the ec |
of token roundtrip times |
bar from left are |
the results largely repeat |
token roundtrip times for |
from left are split |
results largely repeat those |
roundtrip times for different |
so it is possible |
times for different numbers |
left are split mode |
largely repeat those seen |
it is possible to |
for different numbers of |
are split mode and |
split mode and end |
is possible to use |
different numbers of groups |
s read staleness at |
repeat those seen in |
possible to use their |
numbers of groups shows |
those seen in the |
to use their services |
of groups shows an |
seen in the microbenchmarks |
use their services as |
groups shows an increase |
their services as a |
with standard buffers at |
standard buffers at end |
services as a complete |
shows an increase of |
as a complete hosting |
to the extent that |
an increase of the |
a complete hosting solution |
the extent that the |
increase of the token |
complete hosting solution with |
split mode performs as |
extent that the greatest |
of the token roundtrip |
hosting solution with low |
mode performs as well |
that the greatest performance |
the token roundtrip time |
solution with low latency |
performs as well with |
the greatest performance improvements |
with low latency access |
greatest performance improvements are |
as well with default |
performance improvements are seen |
caused almost entirely by |
low latency access to |
well with default sized |
improvements are seen at |
latency access to storage |
with default sized buffers |
are seen at low |
default sized buffers as |
sized buffers as end |
seen at low bandwidth |
at low bandwidth when |
low bandwidth when there |
bandwidth when there is |
when there is high |
there is high read |
end mode performs with |
of the tokens that |
mode performs with large |
performs with large end |
the tokens that are |
tokens that are delayed |
that are delayed the |
are delayed the most |
and much better than |
much better than end |
the cost analysis is |
cost analysis is based |
analysis is based on |
is based on real |
such as in the |
as in the mostly |
world traces taken from |
end mode with default |
traces taken from the |
mode with default sized |
taken from the subversion |
with default sized buffers |
from the subversion repositories |
writes trace where there |
the subversion repositories of |
trace where there is |
subversion repositories of popular |
where there is an |
repositories of popular open |
of popular open source |
popular open source projects |
synchronous writeback asynchronous writeback |
subversion represents each revision |
writeback asynchronous writeback sirp |
represents each revision in |
asynchronous writeback sirp c |
each revision in a |
writeback sirp c sirp |
revision in a repository |
which points to disruptive |
in a repository s |
a repository s history |
points to disruptive events |
to disruptive events as |
disruptive events as the |
decrease in the time |
regardless of how many |
events as the culprit |
in the time spent |
of how many changes |
the time spent to |
how many changes it |
time spent to read |
many changes it contains |
spent to read all |
rather than a uniform |
to read all the |
than a uniform increase |
read all the files |
a uniform increase of |
staleness of version retrieved |
the first for data |
uniform increase of the |
increase of the token |
of the token processing |
as a diff against |
a diff against previous |
diff against previous revisions |
the token processing overhead |
and the second for |
the second for meta |
even at the higher |
at the higher bandwidth |
the higher bandwidth of |
data such as the |
such as the author |
and other revision properties |
our cost analysis is |
cost analysis is based |
analysis is based on |
we find that these |
is based on the |
find that these tokens |
based on the sizes |
that these tokens were |
on the sizes of |
these tokens were most |
the sizes of these |
tokens were most commonly |
sizes of these files |
were most commonly delayed |
of these files and |
most commonly delayed on |
these files and the |
commonly delayed on the |
files and the time |
there is a decrease |
delayed on the sender |
and the time at |
is a decrease of |
the time at which |
time at which each |
at which each revision |
which each revision was |
each revision was committed |
with many thousands of |
synchronous writeback asynchronous writeback |
many thousands of groups |
writeback asynchronous writeback sirp |
looking up the size |
asynchronous writeback sirp c |
up the size of |
writeback sirp c sirp |
the size of these |
size of these special |
of these special files |
these special files is |
the average time to |
special files is only |
average time to travel |
files is only possible |
time to travel by |
is only possible if |
to travel by one |
only possible if one |
travel by one hop |
possible if one has |
by one hop from |
cumulative proportion of reads |
the mostlyreads trace is |
if one has filesystem |
one hop from sender |
mostlyreads trace is not |
one has filesystem level |
hop from sender to |
trace is not much |
has filesystem level access |
from sender to receiver |
is not much affected |
filesystem level access to |
sender to receiver or |
not much affected by |
level access to the |
to receiver or receiver |
much affected by changes |
access to the disk |
receiver or receiver to |
affected by changes in |
to the disk on |
or receiver to sender |
by changes in the |
the disk on which |
receiver to sender can |
changes in the configuration |
disk on which the |
to sender can grow |
on which the repository |
sender can grow to |
which the repository is |
the repository is stored |
although there is a |
can grow to nearly |
there is a slight |
is a slight decrease |
so we had to |
a slight decrease in |
we had to use |
slight decrease in both |
had to use subversion |
decrease in both read |
staleness of version retrieved |
to use subversion s |
in both read and |
of version retrieved read |
use subversion s mirroring |
both read and write |
version retrieved read staleness |
subversion s mirroring capability |
read and write times |
retrieved read staleness at |
s mirroring capability to |
and write times for |
mirroring capability to fetch |
write times for prioritised |
capability to fetch revisions |
times for prioritised asynchronous |
to fetch revisions from |
for prioritised asynchronous writeback |
fetch revisions from the |
revisions from the network |
as compared to an |
accessible repository and replay |
compared to an average |
repository and replay them |
and replay them against |
replay them against a |
them against a local |
against a local copy |
doing this also implicitly |
cumulative proportion of reads |
ms per hop from |
this also implicitly gives |
proportion of reads cumulative |
per hop from receiver |
also implicitly gives us |
of reads cumulative proportion |
hop from receiver to |
implicitly gives us the |
reads cumulative proportion of |
cumulative proportion of reads |
gives us the log |
from receiver to receiver |
us the log of |
the log of timestamps |
log of timestamps indicating |
of timestamps indicating when |
timestamps indicating when each |
indicating when each revision |
when each revision was |
each revision was committed |
thus it is possible |
it is possible to |
is possible to calculate |
possible to calculate the |
to calculate the bandwidth |
load trace performs best |
trace performs best with |
performs best with uniform |
best with uniform asynchronous |
with uniform asynchronous writeback |
transaction costs of pushing |
costs of pushing the |
of pushing the two |
pushing the two files |
the two files for |
two files for each |
we once again attribute |
files for each revision |
the overloaded sender occasionally |
once again attribute this |
for each revision into |
each revision into s |
again attribute this to |
overloaded sender occasionally releases |
attribute this to inefficiency |
sender occasionally releases the |
this to inefficiency in |
occasionally releases the tokens |
to inefficiency in the |
based on amazon s |
releases the tokens with |
inefficiency in the rpc |
on amazon s current |
the tokens with a |
in the rpc protocol |
amazon s current pricing |
s current pricing structure |
tokens with a delay |
shown in table i |
since under extremely heavy |
table i a mazon |
i a mazon s |
a mazon s s |
under extremely heavy load |
synchronous writeback asynchronous writeback |
extremely heavy load and |
writeback asynchronous writeback sirp |
heavy load and high |
asynchronous writeback sirp c |
load and high bandwidth |
writeback sirp c sirp |
and high bandwidth it |
high bandwidth it performs |
bandwidth it performs better |
it performs better when |
performs better when all |
better when all messages |
when all messages have |
all messages have the |
messages have the same |
have the same priority |
a file group is |
the value of the |
file group is implemented |
staleness of version retrieved |
value of the delay |
group is implemented as |
of the delay grows |
is implemented as a |
the delay grows with |
implemented as a special |
delay grows with the |
as a special type |
grows with the number |
a special type of |
with the number of |
special type of file |
the number of groups |
type of file within |
staleness of reader file |
of reader file accesses |
way delivery latency against |
of file within the |
delivery latency against loss |
file within the mfs |
latency against loss rate |
cumulative distributions for the |
within the mfs file |
distributions for the staleness |
the mfs file system |
for the staleness of |
the staleness of all |
staleness of all accesses |
of all accesses to |
all accesses to files |
accesses to files by |
to files by the |
files by the three |
by the three readers |
the three readers are |
three readers are shown |
with its own file |
its own file identifier |
higher curves represent less |
curves represent less staleness |
not included in the |
included in the analysis |
total writer execution time |
our old culprit is |
but not attached to |
in the analysis is |
old culprit is back |
not attached to any |
the analysis is the |
attached to any specific |
analysis is the cost |
to any specific directory |
is the cost of |
the cost of fetching |
cost of fetching data |
of fetching data out |
fetching data out of |
data out of s |
related costs at the |
to be served to |
be served to clients |
costs at the sender |
the file group a |
synchronous writeback asynchronous writeback |
file group a file |
this cost will vary |
writeback asynchronous writeback sirp |
group a file belongs |
cost will vary depending |
asynchronous writeback sirp c |
a file belongs to |
will vary depending on |
writeback sirp c sirp |
vary depending on how |
increasing the number of |
depending on how much |
the number of groups |
on how much caching |
number of groups slows |
how much caching is |
of groups slows the |
much caching is done |
groups slows the sender |
caching is done on |
is done on the |
done on the front |
is one of its |
one of its attributes |
and this cascades to |
this cascades to create |
cascades to create all |
to create all sorts |
the mfs prefetching subsystem |
create all sorts of |
mfs prefetching subsystem derives |
all sorts of downstream |
prefetching subsystem derives much |
sorts of downstream problems |
subsystem derives much of |
of downstream problems that |
synchronous writeback asynchronous writeback |
derives much of its |
downstream problems that can |
writeback asynchronous writeback sirp |
much of its effectiveness |
problems that can destabilize |
asynchronous writeback sirp c |
of its effectiveness from |
and dedicated servers potentially |
that can destabilize the |
writeback sirp c sirp |
its effectiveness from being |
dedicated servers potentially having |
can destabilize the system |
effectiveness from being combined |
servers potentially having much |
destabilize the system as |
from being combined with |
potentially having much more |
the system as a |
being combined with prioritised |
having much more due |
system as a whole |
combined with prioritised rpcs |
much more due to |
more due to inexpensive |
due to inexpensive sata |
to inexpensive sata disks |
it is not unreasonable |
is not unreasonable to |
while the prefetching algorithm |
not unreasonable to assume |
the prefetching algorithm in |
unreasonable to assume that |
prefetching algorithm in mfs |
to assume that a |
discussion the experiments just |
algorithm in mfs is |
assume that a cache |
the experiments just reported |
in mfs is straightforward |
that a cache hit |
experiments just reported make |
a cache hit rate |
just reported make it |
cache hit rate of |
reported make it clear |
hit rate of close |
it can still make |
rate of close to |
can still make bad |
make it clear that |
still make bad decisions |
it clear that the |
make bad decisions without |
clear that the performance |
bad decisions without a |
decisions without a large |
without a large overall |
a large overall performance |
limiting factor in the |
large overall performance penalty |
factor in the qsm |
overall performance penalty because |
in the qsm system |
performance penalty because the |
the qsm system is |
penalty because the interference |
qsm system is latency |
because the interference of |
the interference of prefetching |
interference of prefetching with |
of prefetching with other |
public subversion repositories of |
prefetching with other file |
subversion repositories of the |
and that in addition |
with other file system |
repositories of the debian |
that in addition to |
other file system activity |
of the debian linux |
in addition to protocol |
file system activity is |
the debian linux community |
addition to protocol factors |
system activity is minimised |
debian linux community amount |
to protocol factors such |
linux community amount to |
protocol factors such as |
community amount to a |
factors such as the |
amount to a total |
such as the length |
in the same way |
to a total of |
a total of only |
the same way that |
as the length of |
same way that some |
the length of token |
way that some local |
length of token rings |
that some local file |
some local file systems |
local file systems execute |
the only outgoing bandwidth |
file systems execute speculative |
only outgoing bandwidth costs |
latency is strongly influenced |
systems execute speculative operations |
outgoing bandwidth costs are |
is strongly influenced by |
execute speculative operations to |
bandwidth costs are then |
strongly influenced by the |
speculative operations to improve |
costs are then to |
influenced by the memory |
operations to improve performance |
are then to to |
by the memory footprint |
average reader execution time |
then to to replace |
the memory footprint of |
to to replace failed |
memory footprint of the |
to replace failed frontend |
footprint of the system |
replace failed frontend servers |
failed frontend servers or |
frontend servers or to |
servers or to synchronize |
or to synchronize replicas |
to synchronize replicas if |
synchronize replicas if more |
replicas if more than |
if more than one |
more than one is |
than one is in |
one is in use |
in the case of |
the case of ec |
when we built the |
mfs makes use of |
we built the system |
makes use of the |
built the system it |
use of the speculative |
the bandwidth costs are |
the system it was |
of the speculative communication |
bandwidth costs are actually |
system it was obvious |
the speculative communication of |
costs are actually waived |
it was obvious that |
speculative communication of prioritised |
are actually waived and |
was obvious that minimizing |
communication of prioritised rpcs |
actually waived and the |
obvious that minimizing latency |
of prioritised rpcs in |
waived and the user |
that minimizing latency would |
prioritised rpcs in the |
and the user then |
minimizing latency would be |
rpcs in the hope |
the user then pays |
latency would be important |
in the hope of |
user then pays only |
the hope of achieving |
then pays only for |
hope of achieving a |
pays only for the |
of achieving a benefit |
only for the traffic |
this motivated several of |
achieving a benefit through |
for the traffic between |
motivated several of the |
a benefit through prefetching |
the traffic between the |
traffic between the front |
benefit through prefetching files |
several of the design |
of the design decisions |
end servers and their |
servers and their clients |
the design decisions discussed |
design decisions discussed in |
decisions discussed in section |
table ii shows the |
ii shows the cost |
shows the cost of |
the cost of using |
cost of using s |
execution times for concurrent |
times for concurrent access |
for concurrent access trace |
for a number of |
a number of individual |
number of individual open |
of individual open source |
individual open source projects |
reader execution times are |
execution times are averages |
times are averages for |
are averages for the |
averages for the three |
as well as an |
for the three readers |
but the repeated linkage |
well as an aggregate |
the repeated linkage of |
mfs prefetching implementation the |
as an aggregate for |
an aggregate for the |
prefetching implementation the mfs |
repeated linkage of latency |
higher bandwidth results in |
implementation the mfs cache |
linkage of latency and |
bandwidth results in less |
the mfs cache manager |
of latency and oscillatory |
results in less staleness |
mfs cache manager incorporates |
repositories of the debian |
of the debian community |
cache manager incorporates a |
latency and oscillatory throughputs |
since writes can be |
manager incorporates a small |
also shown is an |
writes can be sent |
and oscillatory throughputs to |
incorporates a small prefetching |
shown is an estimate |
can be sent to |
oscillatory throughputs to memory |
a small prefetching module |
packet delivery latencies throughput |
be sent to the |
throughputs to memory was |
is an estimate for |
sent to the file |
to the file server |
an estimate for the |
to memory was a |
which can be optionally |
the file server faster |
estimate for the apache |
memory was a surprise |
can be optionally enabled |
for the apache software |
be optionally enabled at |
the apache software foundation |
optionally enabled at start |
we expected a much |
expected a much smaller |
apache has taken the |
a much smaller impact |
has taken the unusual |
taken the unusual approach |
the unusual approach of |
sirp is most effective |
unusual approach of using |
is most effective at |
approach of using a |
most effective at reducing |
of using a single |
when it is initialised |
we can summarize our |
effective at reducing staleness |
using a single repository |
can summarize our design |
a single repository for |
summarize our design insights |
single repository for all |
though many reads return |
many reads return out |
our design insights as |
repository for all of |
a prefetching thread starts |
design insights as follows |
for all of its |
all of its projects |
prefetching thread starts and |
date file contents when |
thread starts and initiates |
file contents when compared |
starts and initiates prefetch |
both public and restricted |
and initiates prefetch requests |
mbps flow alongside on |
contents when compared to |
initiates prefetch requests in |
flow alongside on the |
when compared to the |
due to access control |
minimize the memory footprint |
alongside on the same |
compared to the optimal |
to the optimal version |
to access control restrictions |
on the same link |
prefetch requests in parallel |
access control restrictions on |
we expected that the |
the same link to |
requests in parallel with |
control restrictions on some |
expected that the primary |
same link to simulate |
in parallel with the |
restrictions on some paths |
that the primary cost |
link to simulate a |
more sirp reads are |
sirp reads are up |
the primary cost of |
to simulate a real |
parallel with the main |
subversion s mirroring tool |
primary cost of managed |
with the main activity |
s mirroring tool was |
cost of managed memory |
time stream combined with |
the main activity of |
compared to synchronous or |
of managed memory would |
stream combined with other |
mirroring tool was unable |
main activity of the |
to synchronous or asynchronous |
managed memory would be |
combined with other inter |
tool was unable to |
activity of the cache |
synchronous or asynchronous writeback |
memory would be associated |
was unable to create |
of the cache manager |
would be associated with |
unable to create local |
to create local copy |
be associated with garbage |
allowing higher degrees of |
higher degrees of staleness |
associated with garbage collection |
the core component of |
the complete log of |
complete log of timestamps |
core component of the |
component of the cache |
shows the average delivery |
of the cache manager |
the average delivery latency |
the cache manager alerts |
more reads performed with |
cache manager alerts the |
average delivery latency of |
manager alerts the prefetching |
all costs associated with |
alerts the prefetching module |
costs associated with managed |
the prefetching module every |
associated with managed memory |
prefetching module every time |
reads performed with sirp |
how much does it |
much does it cost |
level packets in the |
module every time an |
performed with sirp are |
with managed memory rise |
description monthly storage bandwidth |
with sirp are within |
every time an application |
managed memory rise in |
monthly storage bandwidth in |
time an application reads |
memory rise in the |
storage bandwidth in bandwidth |
versions of the optimal |
an application reads or |
as loss rates go |
loss rates go up |
rise in the amount |
application reads or writes |
bandwidth in bandwidth out |
with this bandwidth level |
in the amount of |
reads or writes a |
in bandwidth out per |
the amount of allocated |
or writes a file |
synchronous and asynchronous writeback |
amount of allocated memory |
and asynchronous writeback coincide |
asynchronous writeback coincide in |
writeback coincide in performance |
by calling the file |
since they are constrained |
calling the file access |
they are constrained by |
at least in the |
the file access routine |
are constrained by the |
least in the windows |
constrained by the bandwidth |
shows the same scenario |
in the windows clr |
by the bandwidth bottleneck |
the same scenario with |
the bandwidth bottleneck and |
this routine checks whether |
same scenario with a |
bandwidth bottleneck and send |
routine checks whether the |
scenario with a constant |
bottleneck and send updates |
checks whether the file |
with a constant uniformly |
and send updates in |
whether the file belongs |
a constant uniformly random |
reads apache software foundation |
send updates in the |
updates in the same |
in the same order |
apache software foundation debian |
the file belongs to |
constant uniformly random loss |
software foundation debian linux |
file belongs to a |
uniformly random loss rate |
by suppressing unnecessary invalidations |
foundation debian linux community |
belongs to a file |
random loss rate of |
to a file group |
sirp reduces its bandwidth |
whereas traditional multicast systems |
a file group if |
reduces its bandwidth usage |
traditional multicast systems accept |
file group if not |
its bandwidth usage and |
multicast systems accept messages |
bandwidth usage and achieves |
systems accept messages whenever |
usage and achieves a |
accept messages whenever the |
the access is ignored |
and achieves a small |
messages whenever the application |
achieves a small improvement |
whenever the application layer |
a small improvement over |
maelstrom s delivery latency |
the application layer or |
small improvement over sirp |
s delivery latency is |
application layer or the |
delivery latency is almost |
prefetching it is a |
layer or the multicast |
latency is almost exactly |
it is a member |
or the multicast protocols |
is almost exactly equal |
since devoting less bandwidth |
is a member of |
the multicast protocols produce |
almost exactly equal to |
devoting less bandwidth to |
a member of a |
multicast protocols produce it |
exactly equal to the |
less bandwidth to invalidations |
member of a file |
equal to the one |
bandwidth to invalidations results |
qsm uses an upcall |
of a file group |
to invalidations results in |
way latency on the |
latency on the link |
invalidations results in data |
results in data reaching |
in data reaching the |
data reaching the server |
reaching the server faster |
the group is put |
group is put at |
ip takes more than |
is put at the |
takes more than twice |
asynchronous writeback performs as |
put at the head |
often we can delay |
more than twice as |
writeback performs as well |
at the head of |
we can delay generating |
than twice as long |
performs as well as |
as well as sirp |
can delay generating a |
twice as long once |
as long once one |
delay generating a message |
the head of the |
generating a message until |
head of the prefetch |
synchronous writeback continues to |
way latencies go past |
a message until the |
of the prefetch list |
writeback continues to underperform |
message until the last |
until the last minute |
this is because the |
is because the progress |
because the progress of |
the progress of writers |
the prefetch thread periodically |
progress of writers using |
prefetch thread periodically examines |
and we can also |
of writers using asynchronous |
thread periodically examines the |
we can also avoid |
writers using asynchronous writeback |
periodically examines the prefetching |
can also avoid situations |
using asynchronous writeback schemes |
examines the prefetching is |
also avoid situations in |
asynchronous writeback schemes is |
the prefetching is commonly |
avoid situations in which |
writeback schemes is less |
prefetching is commonly used |
situations in which data |
schemes is less constrained |
is commonly used to |
in which data piles |
is less constrained by |
commonly used to improve |
which data piles up |
less constrained by the |
used to improve the |
data piles up on |
constrained by the bandwidth |
to improve the performance |
piles up on behalf |
improve the performance of |
up on behalf of |
the performance of lo |
ip one way link |
one way link latency |
and they can overlap |
on behalf of an |
they can overlap computation |
behalf of an aggressive |
can overlap computation and |
group at the head |
of an aggressive sender |
overlap computation and fetching |
at the head of |
computation and fetching file |
the head of the |
and fetching file contents |
head of the list |
fetching file contents with |
file contents with writeback |
rather than simply being |
than simply being a |
simply being a selfinterested |
being a selfinterested optimisation |
if the group file |
a selfinterested optimisation by |
the group file for |
selfinterested optimisation by writers |
group file for the |
optimisation by writers to |
file for the group |
by writers to improve |
split with regular buffers |
limit buffering and caching |
writers to improve their |
size of repository stored |
for the group is |
to improve their own |
of repository stored in |
repository stored in s |
improve their own performance |
the group is cal |
most existing multicast protocols |
group is cal file |
existing multicast protocols buffer |
is cal file systems |
asynchronous writeback therefore benefits |
end with large buffers |
multicast protocols buffer data |
so we based our |
writeback therefore benefits both |
protocols buffer data at |
we based our analysis |
therefore benefits both writers |
benefits both writers and |
both writers and readers |
based our analysis on |
as well as distributed |
buffer data at many |
our analysis on that |
well as distributed file |
data at many layers |
the files shared between |
analysis on that along |
and outperforms it with |
as distributed file systems |
at many layers and |
files shared between the |
many layers and cache |
outperforms it with regular |
on that along with |
shared between the clients |
layers and cache data |
it with regular buffers |
that along with the |
between the clients were |
and cache data rather |
along with the assumption |
the clients were divided |
not in the cache |
cache data rather casually |
with the assumption each |
clients were divided into |
data rather casually for |
the assumption each revision |
rather casually for recovery |
latency metrics to measure |
assumption each revision data |
it retrieves it from |
casually for recovery purposes |
metrics to measure the |
each revision data file |
retrieves it from the |
to measure the latency |
revision data file would |
it from the server |
measure the latency effects |
data file would be |
file lengths were randomised |
this turns out to |
the latency effects of |
then it scans the |
with an average length |
it scans the in |
turns out to be |
latency effects of tcp |
an average length of |
scans the in a |
out to be extremely |
the in a file |
to be extremely costly |
in a file system |
be extremely costly in |
a file system with |
extremely costly in a |
kib and each revision |
file system with whole |
costly in a managed |
and each revision property |
in a managed setting |
to prevent the clients |
each revision property file |
a managed setting and |
mbps stream between two |
prevent the clients falling |
managed setting and must |
stream between two nodes |
the clients falling into |
setting and must be |
between two nodes over |
clients falling into lockstep |
a mechanism is required |
and must be avoided |
mechanism is required files |
falling into lockstep in |
two nodes over a |
must be avoided whenever |
the averages observed for |
is required files in |
into lockstep in the |
be avoided whenever possible |
averages observed for the |
required files in the |
lockstep in the course |
observed for the other |
in the course of |
files in the group |
for the other repositories |
the course of fetching |
in the group in |
the other repositories in |
course of fetching and |
other repositories in table |
repositories in table ii |
of fetching and writing |
the group in order |
fetching and writing back |
and writing back the |
table ii m ost |
writing back the files |
group in order until |
ii m ost recent |
in order until it |
m ost recent monthly |
cumulative distribution of the |
order until it finds |
ost recent monthly cost |
distribution of the multicast |
plots delivery latency against |
until it finds the |
recent monthly cost of |
of the multicast rates |
delivery latency against message |
it finds the first |
monthly cost of storing |
the multicast rates for |
latency against message identifier |
consisting of selecting a |
finds the first one |
cost of storing repositories |
the first one which |
of selecting a random |
of storing repositories in |
a key point is |
first one which is |
selecting a random file |
storing repositories in s |
key point is that |
one which is not |
a random file set |
point is that we |
which is not to |
random file set and |
is that we are |
for individual projects and |
is not to determine |
file set and performing |
that we are plotting |
individual projects and entire |
not to determine appropriate |
set and performing a |
we are plotting the |
projects and entire communities |
to determine appropriate prefetching |
and performing a sequence |
are plotting the delivery |
token roundtrip times for |
and entire communities software |
determine appropriate prefetching hints |
performing a sequence of |
plotting the delivery latency |
entire communities software project |
a sequence of reads |
the delivery latency of |
communities software project squirrelmail |
sequence of reads or |
delivery latency of all |
software project squirrelmail phpmyadmin |
of reads or writes |
latency of all packets |
earlier work in file |
project squirrelmail phpmyadmin subversion |
reads or writes on |
work in file in |
squirrelmail phpmyadmin subversion mono |
or writes on files |
not just lost ones |
in file in the |
phpmyadmin subversion mono kde |
writes on files in |
on files in it |
subversion mono kde hosting |
file in the cache |
the spikes in latency |
mono kde hosting community |
the writer performed a |
spikes in latency are |
kde hosting community debian |
writer performed a file |
in latency are triggered |
hosting community debian linux |
performed a file set |
latency are triggered by |
community debian linux community |
a file set operation |
are triggered by losses |
debian linux community apache |
file set operation of |
triggered by losses that |
linux community apache software |
by losses that lead |
community apache software foundation |
and issues a prefetch |
losses that lead to |
apache software foundation monthly |
intervals between the subsequent |
issues a prefetch request |
that lead to packets |
software foundation monthly cost |
between the subsequent tokens |
a prefetch request or |
lead to packets piling |
to packets piling up |
prefetch request or system |
packets piling up both |
piling up both at |
request or system prefetching |
up both at the |
both at the receiver |
or system prefetching has |
at the receiver and |
the receiver and the |
receiver and the sender |
system prefetching has used |
prefetching has used clustering |
has used clustering to |
with each access being |
used clustering to derive |
ip delays correctly received |
clustering to derive file |
each access being equally |
delays correctly received packets |
to derive file groups |
access being equally likely |
correctly received packets at |
derive file groups from |
being equally likely to |
received packets at the |
file groups from validation |
equally likely to open |
packets at the receiver |
groups from validation request |
likely to open a |
at the receiver while |
from validation request for |
to open a file |
the receiver while waiting |
validation request for it |
open a file for |
receiver while waiting for |
a file for reading |
while waiting for missing |
file for reading or |
waiting for missing packets |
for reading or writing |
for missing packets sequenced |
missing packets sequenced earlier |
packets sequenced earlier by |
sequenced earlier by the |
earlier by the sender |
readers performed a file |
if all the files |
performed a file set |
all the files are |
clear messages out of |
a file set operation |
it also delays packets |
the files are valid |
messages out of the |
file set operation of |
also delays packets at |
files are valid and |
out of the system |
delays packets at the |
are valid and are |
of the system quickly |
packets at the sender |
valid and are in |
at the sender when |
and are in the |
the sender when it |
are in the cache |
data paths should have |
sender when it cuts |
in the cache access |
paths should have rapid |
when it cuts down |
the cache access statistics |
should have rapid data |
it cuts down on |
have rapid data movement |
cuts down on the |
rapid data movement as |
down on the sending |
data movement as a |
on the sending window |
movement as a key |
the sending window size |
file sets were treated |
as a key goal |
sending window size in |
sets were treated as |
window size in response |
were treated as hot |
size in response to |
in response to the |
response to the loss |
to the loss events |
the delays caused by |
delays caused by these |
caused by these two |
by these two mechanisms |
these two mechanisms are |
two mechanisms are illustrated |
mechanisms are illustrated in |
even for the fairly |
are illustrated in figure |
of the file set |
for the fairly large |
the file set operations |
the fairly large apache |
predicted future file accesses |
file set operations were |
fairly large apache software |
future file accesses from |
we ve already mentioned |
large apache software foundation |
set operations were directed |
file accesses from cache |
ve already mentioned that |
where single packet losses |
operations were directed to |
the current cost of |
current cost of using |
cost of using s |
the group is moved |
already mentioned that data |
single packet losses cause |
were directed to those |
for storage is less |
mentioned that data paths |
packet losses cause spikes |
group is moved to |
directed to those file |
to those file sets |
that data paths should |
losses cause spikes in |
is moved to the |
storage is less than |
data paths should clear |
cause spikes in delivery |
moved to the end |
read staleness comparing update |
paths should clear messages |
spikes in delivery latency |
to the end of |
staleness comparing update propagation |
should clear messages quickly |
in delivery latency that |
the end of the |
comparing update propagation schemes |
delivery latency that last |
it is very unlikely |
but there are other |
update propagation schemes requires |
latency that last for |
end of the prefetch |
is very unlikely that |
there are other important |
propagation schemes requires a |
that last for hundreds |
of the prefetch list |
very unlikely that any |
are other important forms |
schemes requires a criterion |
last for hundreds of |
unlikely that any vendor |
other important forms of |
requires a criterion for |
for hundreds of packets |
that any vendor could |
important forms of delay |
a criterion for measuring |
any vendor could provide |
the maelstrom configuration used |
vendor could provide a |
criterion for measuring the |
maelstrom configuration used is |
could provide a traditional |
for measuring the staleness |
configuration used is r |
provide a traditional storage |
measuring the staleness of |
a traditional storage solution |
the staleness of file |
traditional storage solution consisting |
staleness of file reads |
storage solution consisting of |
solution consisting of scsi |
consisting of scsi disks |
most situations in which |
we identified updates to |
of scsi disks and |
situations in which qsm |
identified updates to files |
scsi disks and tape |
or allowed applications to |
in which qsm developed |
updates to files by |
disks and tape backup |
allowed applications to specify |
which qsm developed convoy |
to files by associating |
and tape backup at |
tape backup at this |
files by associating a |
applications to specify prefetch |
backup at this price |
by associating a version |
like behavior or oscillatory |
associating a version number |
a version number with |
behavior or oscillatory throughput |
the amount of s |
version number with each |
number with each file |
or oscillatory throughput can |
storage required of course |
oscillatory throughput can be |
required of course increases |
the thread rechecks the |
and incrementing it every |
of course increases each |
throughput can be traced |
thread rechecks the head |
can be traced to |
rechecks the head of |
incrementing it every time |
course increases each month |
be traced to design |
the head of the |
it every time the |
increases each month as |
traced to design decisions |
head of the list |
every time the file |
each month as the |
to design decisions that |
of the list ing |
time the file was |
month as the repository |
as the repository grows |
the list ing hints |
the file was modified |
design decisions that caused |
list ing hints explicitly |
decisions that caused scheduling |
but as shown in |
that caused scheduling jitter |
reads were labelled with |
as shown in figure |
caused scheduling jitter or |
were labelled with the |
scheduling jitter or allowed |
labelled with the version |
jitter or allowed some |
with the version number |
or allowed some form |
the increase is roughly |
the version number of |
allowed some form of |
increase is roughly linear |
version number of the |
some form of priority |
number of the file |
form of priority inversion |
of the file at |
as developer productivity remains |
of priority inversion to |
the file at the |
developer productivity remains constant |
priority inversion to occur |
file at the time |
at the time the |
the time the read |
time the read occurred |
the cost of storage |
cost of storage is |
of storage is declining |
storage is declining exponentially |
the staleness of a |
delaying a crucial message |
staleness of a particular |
a crucial message behind |
of a particular read |
crucial message behind a |
a particular read was |
message behind a less |
particular read was determined |
behind a less important |
to find the next |
read was determined according |
so if amazon s |
a less important one |
find the next file |
was determined according to |
if amazon s pricing |
the next file to |
determined according to an |
amazon s pricing stays |
next file to prefetch |
implications included the following |
according to an ideal |
s pricing stays competitive |
to an ideal version |
an ideal version number |
ideal version number derived |
version number derived from |
number derived from executing |
a new group may |
derived from executing the |
new group may now |
term trend is towards |
from executing the experiment |
group may now be |
trend is towards lower |
is towards lower costs |
may now be at |
executing the experiment with |
now be at the |
additional costs will be |
the experiment with all |
be at the inter |
costs will be incurred |
experiment with all participants |
event handlers should be |
will be incurred for |
with all participants running |
handlers should be short |
be incurred for front |
all participants running on |
file dependencies can also |
participants running on a |
running on a single |
on a single host |
dependencies can also be |
can also be used |
for the case of |
in a real execution |
the case of ec |
also be used as |
be used as a |
used as a source |
the difference between the |
as a source of |
a standard machine instance |
difference between the version |
we struggled to make |
a source of hints |
standard machine instance is |
between the version number |
struggled to make the |
machine instance is billed |
the version number a |
to make the overall |
instance is billed at |
version number a read |
head of the list |
make the overall behavior |
number a read returns |
the overall behavior of |
of the list as |
a read returns and |
overall behavior of the |
the list as a |
read returns and the |
behavior of the system |
list as a result |
returns and the optimal |
of the system as |
as a result of |
and the optimal version |
the system as predictable |
a result of further |
system as predictable as |
the optimal version number |
result of further application |
plus data transfer of |
as predictable as possible |
optimal version number determines |
of further application accesses |
predictable as possible not |
version number determines how |
further application accesses to |
as possible not a |
number determines how stale |
application accesses to files |
possible not a trivial |
determines how stale the |
how stale the read |
stale the read is |
not a trivial task |
a trivial task in |
per gib in and |
trivial task in configurations |
task in configurations where |
shows cumulative distributions for |
in configurations where hundreds |
cumulative distributions for the |
configurations where hundreds of |
it may be known |
where hundreds of processes |
distributions for the staleness |
may be known that |
hundreds of processes might |
be known that a |
of processes might be |
for the staleness of |
known that a certain |
processes might be multicasting |
the staleness of reads |
discounts are available if |
might be multicasting in |
that a certain shared |
staleness of reads at |
are available if data |
be multicasting in thousands |
a certain shared library |
of reads at different |
available if data transfer |
multicasting in thousands of |
certain shared library is |
reads at different writer |
if data transfer exceeds |
in thousands of overlapping |
shared library is reprefetch |
thousands of overlapping groups |
library is reprefetch requests |
is reprefetch requests are |
improved consistency results in |
reprefetch requests are similar |
consistency results in fewer |
results in fewer stale |
and the instance cost |
by keeping event handlers |
requests are similar to |
in fewer stale reads |
the instance cost may |
keeping event handlers short |
are similar to regular |
instance cost may be |
and this is reflected |
similar to regular fetch |
event handlers short and |
cost may be reduced |
percentage of packets recovered |
this is reflected by |
to regular fetch requests |
handlers short and predictable |
may be reduced to |
is reflected by a |
regular fetch requests for |
short and predictable and |
reflected by a curve |
fetch requests for files |
and predictable and eliminating |
by a curve that |
predictable and eliminating the |
a curve that is |
and eliminating the need |
curve that is higher |
eliminating the need for |
that is higher on |
quired to run a |
the need for locking |
is higher on the |
to run a text |
per hour by paying |
hour by paying a |
run a text editor |
higher on the left |
we obtained a more |
on the left side |
obtained a more predictable |
the left side of |
relatively prime interleaves offer |
a more predictable system |
in this case it |
prime interleaves offer better |
this case it would |
more predictable system and |
left side of the |
side of the graph |
case it would be |
predictable system and were |
interleaves offer better performance |
year reservation fee in |
consistency maintenance cost the |
system and were able |
reservation fee in advance |
it would be advantageous |
maintenance cost the overhead |
would be advantageous with |
and were able to |
be advantageous with the |
this gives an amortized |
advantageous with the exception |
were able to eliminate |
with the exception that |
gives an amortized monthly |
cost the overhead of |
able to eliminate multithreading |
the exception that they |
an amortized monthly cost |
the overhead of the |
amortized monthly cost of |
exception that they are |
overhead of the update |
that they are issued |
of the update propagation |
with the associated context |
they are issued at |
the update propagation schemes |
the associated context switching |
are issued at the |
update propagation schemes can |
associated context switching and |
issued at the lowest |
propagation schemes can be |
context switching and locking |
at the lowest level |
schemes can be compared |
switching and locking overheads |
the lowest level of |
can be compared by |
lowest level of prito |
be compared by referring |
level of prito retrieve |
compared by referring to |
of prito retrieve the |
as we show in |
prito retrieve the shared |
by referring to the |
we show in the |
show in the next |
referring to the reader |
retrieve the shared library |
in the next section |
to the reader and |
the shared library from |
the reader and writer |
shared library from the |
reader and writer execution |
one instance should be |
library from the server |
and writer execution times |
instance should be enough |
from the server as |
should be enough for |
the server as well |
be enough for almost |
here we encounter a |
server as well as |
enough for almost any |
we encounter a tension |
acknowledgements shown in figure |
as well as retriev |
for almost any individual |
encounter a tension between |
almost any individual project |
a tension between two |
any individual project or |
reader execution time is |
tension between two goals |
individual project or moderately |
execution time is the |
project or moderately sized |
time is the average |
or moderately sized community |
is the average for |
all other rpc traffic |
the average for all |
from a memory footprint |
other rpc traffic takes |
average for all three |
for all three readers |
rpc traffic takes precedence |
a memory footprint perspective |
usage patterns in addition |
traffic takes precedence over |
patterns in addition to |
takes precedence over a |
in addition to getting |
precedence over a prefetch |
addition to getting a |
the reduced staleness achievable |
over a prefetch rpc |
one might prefer not |
to getting a grasp |
might prefer not to |
reduced staleness achievable by |
getting a grasp of |
prefer not to pull |
staleness achievable by sirp |
a grasp of the |
ing the text editor |
not to pull in |
achievable by sirp has |
grasp of the costs |
the text editor executable |
to pull in a |
by sirp has little |
of the costs involved |
pull in a message |
sirp has little or |
the costs involved in |
in a message until |
has little or no |
costs involved in moving |
a message until qsm |
little or no cost |
involved in moving a |
message until qsm can |
or no cost compared |
in moving a repository |
as shown in table |
no cost compared to |
moving a repository to |
a repository to s |
cost compared to asynchronous |
until qsm can process |
compared to asynchronous writeback |
qsm can process it |
to asynchronous writeback with |
it is important to |
asynchronous writeback with no |
is important to understand |
writeback with no invalidations |
important to understand the |
to understand the usage |
understand the usage patterns |
but in a datacenter |
since the writer is |
in a datacenter or |
and only one tion |
the writer is up |
especially the rate at |
a datacenter or cluster |
only one tion such |
writer is up to |
one tion such as |
the rate at which |
rate at which commits |
at which commits take |
which commits take place |
tion such as the |
most message loss occurs |
such as the operating |
message loss occurs in |
since achieving the consistency |
slower when using sirp |
achieving the consistency properties |
as the operating system |
loss occurs in the |
the consistency properties that |
the operating system s |
occurs in the operating |
c compared to sirp |
consistency properties that developers |
operating system s database |
in the operating system |
properties that developers expect |
system s database of |
selective invalidation is clearly |
that developers expect will |
s database of installed |
not on the network |
developers expect will require |
invalidation is clearly beneficial |
database of installed software |
expect will require a |
of installed software prefetch |
will require a consistency |
hence message loss rates |
installed software prefetch is |
sirp has the highest |
message loss rates soar |
require a consistency layer |
software prefetch is made |
has the highest average |
loss rates soar if |
a consistency layer to |
prefetch is made at |
the highest average execution |
rates soar if we |
consistency layer to be |
is made at a |
highest average execution time |
soar if we leave |
layer to be built |
made at a time |
if we leave messages |
but this is because |
we leave messages on |
to be built in |
this is because it |
leave messages on input |
this is more a |
is more a matter |
more a matter of |
be built in front |
built in front of |
in front of s |
a matter of implementapackages |
messages on input sockets |
is because it provides |
on input sockets for |
because it provides the |
input sockets for long |
it is crucial that |
it provides the best |
is crucial that any |
provides the best consistency |
crucial that any such |
the best consistency of |
that any such layer |
best consistency of all |
any such layer be |
consistency of all the |
such layer be able |
specified dependency information tion |
of all the schemes |
layer be able to |
dependency information tion convenience |
be able to handle |
information tion convenience than |
able to handle the |
if a reader reads |
a reader reads more |
to handle the load |
tion convenience than a |
reader reads more up |
handle the load of |
the load of commits |
control the event processing |
convenience than a design |
the event processing order |
than a design decision |
the critical statistic to |
critical statistic to consider |
statistic to consider is |
to consider is the |
consider is the number |
is the number of |
then it transfers more |
the number of simultaneous |
it transfers more data |
other work has shown |
number of simultaneous commits |
work has shown can |
has shown can be |
shown can be used |
for centralized revision control |
centralized revision control system |
the reader execution time |
revision control system such |
reader execution time for |
control system such as |
execution time for each |
system such as subversion |
time for each case |
for each case is |
the benefits initiating multiple |
each case is proportional |
each commit is assigned |
commit is assigned a |
is assigned a unique |
benefits initiating multiple concurrent |
case is proportional to |
is proportional to the |
initiating multiple concurrent prefetches |
proportional to the amount |
to the amount of |
multiple concurrent prefetches from |
the amount of data |
concurrent prefetches from differany |
and any change to |
amount of data transferred |
and the imposition of |
prefetches from differany of |
any change to a |
of data transferred between |
the imposition of an |
from differany of these |
change to a versioned |
data transferred between the |
imposition of an internal |
differany of these techniques |
to a versioned file |
transferred between the reader |
of an internal event |
layered interleaving recovery percentage |
of these techniques could |
a versioned file is |
between the reader and |
the reader and server |
interleaving recovery percentage and |
these techniques could be |
versioned file is stored |
an internal event processing |
though lack of space |
techniques could be used |
file is stored as |
recovery percentage and latency |
internal event processing prioritization |
lack of space precludes |
could be used to |
is stored as a |
percentage and latency c |
of space precludes showing |
small delays add up |
stored as a diff |
be used to derive |
space precludes showing this |
delays add up in |
as a diff against |
used to derive hints |
layered interleaving and bursty |
precludes showing this in |
add up in large |
a diff against its |
to derive hints for |
interleaving and bursty loss |
showing this in a |
this in a graph |
diff against its previous |
derive hints for use |
and bursty loss thus |
up in large systems |
we thank robbert van |
thank robbert van renesse |
bursty loss thus far |
against its previous version |
hints for use ent |
loss thus far we |
for use ent servers |
tight control over event |
a commit must be |
emin gu n sirer |
commit must be rejected |
thus far we have |
must be rejected if |
control over event processing |
far we have shown |
over event processing largely |
be rejected if any |
event processing largely eliminated |
rimon barr and stephen |
we have shown how |
rejected if any of |
processing largely eliminated convoy |
barr and stephen rago |
have shown how maelstrom |
if any of the |
largely eliminated convoy effects |
and stephen rago for |
shown how maelstrom effectively |
any of the versioned |
eliminated convoy effects and |
stephen rago for comments |
how maelstrom effectively hides |
of the versioned files |
convoy effects and oscillatory |
rago for comments regarding |
maelstrom effectively hides loss |
mfs does not currently |
the versioned files that |
effects and oscillatory throughput |
for comments regarding this |
effectively hides loss from |
hides loss from tcp |
versioned files that it |
and oscillatory throughput problems |
comments regarding this work |
does not currently make |
files that it touches |
not currently make use |
ip for packets dropped |
that it touches have |
currently make use of |
for packets dropped with |
it touches have been |
make use of timeouts |
packets dropped with uniform |
touches have been changed |
use of timeouts by |
dropped with uniform randomness |
have been changed in |
of timeouts by the |
been changed in an |
timeouts by the mfs |
changed in an earlier |
by the mfs prefetching |
we examine the performance |
act on fresh state |
in an earlier revision |
the mfs prefetching subsystem |
examine the performance of |
evaluation of an adaptive |
an earlier revision that |
the performance of the |
of an adaptive transport |
earlier revision that the |
performance of the layered |
an adaptive transport protocol |
our evaluation uses hand |
many inefficiencies can be |
of the layered interleaving |
revision that the developer |
in proceedings of the |
proceedings of the twenty |
the layered interleaving algorithm |
that the developer performing |
inefficiencies can be traced |
the developer performing the |
second annual joint conference |
developer performing the commit |
can be traced to |
showing how different parameterizations |
annual joint conference of |
as we have noted |
performing the commit was |
be traced to situations |
how different parameterizations handle |
joint conference of the |
we have noted earlier |
the commit was unaware |
commit was unaware of |
different parameterizations handle bursty |
conference of the ieee |
traced to situations in |
but it could easily |
to situations in which |
parameterizations handle bursty loss |
of the ieee computer |
it could easily to |
this ensures that every |
situations in which one |
handle bursty loss patterns |
the ieee computer and |
could easily to exspecified |
ensures that every conflict |
in which one node |
we use a loss |
easily to exspecified dependency |
that every conflict gets |
ieee computer and communications |
which one node takes |
use a loss model |
to exspecified dependency information |
every conflict gets resolved |
computer and communications societies |
one node takes action |
a loss model where |
node takes action on |
conflict gets resolved by |
takes action on the |
loss model where packets |
which is inaccurate in |
gets resolved by a |
action on the basis |
model where packets are |
is inaccurate in some |
resolved by a human |
on the basis of |
where packets are dropped |
inaccurate in some tended |
by a human before |
the basis of stale |
packets are dropped in |
in some tended to |
a human before becoming |
basis of stale state |
are dropped in bursts |
some tended to abandon |
human before becoming part |
of stale state information |
dropped in bursts of |
tended to abandon a |
before becoming part of |
stale state information from |
in bursts of fixed |
to abandon a prefetching |
becoming part of the |
state information from some |
bursts of fixed length |
abandon a prefetching attempt |
part of the repository |
information from some other |
a prefetching attempt that |
allowing us to study |
from some other node |
of the repository s |
the repository s state |
us to study the |
prefetching attempt that does |
to study the impact |
attempt that does not |
study the impact of |
triggering redundant retransmissions or |
that does not complete |
the impact of burst |
redundant retransmissions or other |
exclusive locking is required |
does not complete cases |
impact of burst length |
of burst length on |
locking is required on |
is required on commits |
burst length on performance |
retransmissions or other overheads |
rather than reimplementing an |
than reimplementing an existing |
taking a loose definition |
the link has a |
link has a one |
a loose definition of |
reimplementing an existing hint |
loose definition of simultaneous |
the pull architecture has |
definition of simultaneous to |
of simultaneous to be |
pull architecture has the |
simultaneous to be within |
generation in a timely |
to be within one |
architecture has the secondary |
in a timely manner |
has the secondary benefit |
be within one minute |
percentage of packets recovered |
the secondary benefit of |
secondary benefit of letting |
the apache repository had |
benefit of letting us |
apache repository had a |
repository had a maximum |
had a maximum of |
of letting us delay |
letting us delay the |
we focus on the |
simultaneous commits and the |
us delay the preparation |
focus on the performance |
commits and the debian |
reed solomon layered interleaving |
on the performance of |
and the debian community |
delay the preparation of |
the performance of mfs |
the preparation of status |
performance of mfs with |
ignoring for now that |
preparation of status packets |
of mfs with prefetchthe |
for now that their |
of status packets until |
mfs with prefetchthe main |
now that their use |
status packets until they |
with prefetchthe main complexity |
that their use of |
packets until they are |
prefetchthe main complexity in |
until they are about |
main complexity in implementing |
they are about to |
the importance of translucence |
complexity in implementing the |
are about to be |
importance of translucence in |
in implementing the prefetching |
about to be transmitted |
of translucence in mobile |
separate repositories allows for |
implementing the prefetching subing |
translucence in mobile computing |
repositories allows for finergrained |
in mobile computing systems |
allows for finergrained locking |
acm transactions on computer |
an aggregate maximum of |
using a deliberately simple |
a deliberately simple hint |
conclusions the premise of |
deliberately simple hint mechanism |
the premise of our |
simple hint mechanism for |
in determining these numbers |
premise of our work |
determining these numbers we |
hint mechanism for the |
of our work is |
these numbers we filtered |
mechanism for the purposes |
numbers we filtered out |
our work is that |
we filtered out any |
for the purposes system |
work is that developers |
filtered out any sequences |
the purposes system lies |
out any sequences of |
purposes system lies in |
any sequences of multiple |
is that developers of |
system lies in handling |
that developers of services |
sequences of multiple commits |
lies in handling a |
developers of services intended |
of multiple commits by |
in handling a demand |
of services intended to |
multiple commits by the |
handling a demand fetch |
services intended to run |
commits by the same |
intended to run on |
by the same author |
to run on clustered |
the same author during |
run on clustered platforms |
a compulsory fetch to |
same author during a |
on clustered platforms desire |
compulsory fetch to of |
author during a one |
clustered platforms desire the |
fetch to of evaluation |
during a one minute |
platforms desire the productivity |
a one minute period |
desire the productivity and |
one minute period since |
the productivity and robustness |
minute period since those |
productivity and robustness benefits |
dependencies between files are |
period since those were |
solomon versus layered interleaving |
between files are conveyed |
and robustness benefits of |
since those were likely |
versus layered interleaving latency |
files are conveyed using |
robustness benefits of managed |
those were likely sequential |
layered interleaving latency of |
are conveyed using a |
benefits of managed environments |
were likely sequential rather |
conveyed using a service |
likely sequential rather than |
using a service a |
ms and a loss |
sequential rather than simultaneous |
and need replication tools |
and a loss rate |
a loss rate of |
rather than simultaneous and |
need replication tools integrated |
a service a cache |
than simultaneous and do |
tolerant mechanism for distributed |
replication tools integrated with |
service a cache miss |
simultaneous and do nor |
mechanism for distributed file |
tools integrated with those |
and do nor represent |
for distributed file cache |
for a file which |
do nor represent the |
integrated with those environments |
distributed file cache consistency |
a file which is |
nor represent the common |
represent the common case |
file which is already |
in proceedings of the |
building such tools so |
which is already being |
proceedings of the twelth |
such tools so posed |
the average rates were |
is already being prefetched |
of the twelth symposium |
where it is varied |
tools so posed challenges |
the twelth symposium on |
so posed challenges to |
twelth symposium on operating |
posed challenges to us |
symposium on operating systems |
challenges to us as |
on operating systems principles |
to us as protocol |
which is a list |
us as protocol and |
is a list of |
mbps flow of udp |
as protocol and system |
a list of file |
flow of udp packets |
protocol and system designers |
list of file identifiers |
of udp packets is |
of file identifiers for |
udp packets is sent |
file identifiers for the |
packets is sent over |
is sent over it |
identifiers for the related |
which were the primary |
for the related files |
so exclusive locking for |
were the primary focus |
exclusive locking for commits |
the primary focus of |
locking for commits should |
primary focus of our |
this conflict arises very |
for commits should not |
we show that our |
focus of our paper |
conflict arises very frequently |
commits should not pose |
show that our observation |
should not pose any |
that our observation in |
not pose any scalability |
our observation in section |
pose any scalability problems |
observation in section iv |
a central insight is |
any scalability problems in |
particularly when an appliit |
central insight is that |
scalability problems in a |
this experiment demonstrates that |
e is correct for |
when an appliit is |
insight is that high |
problems in a typical |
experiment demonstrates that sirp |
is correct for high |
an appliit is assumed |
in a typical environment |
appliit is assumed that |
correct for high loss |
demonstrates that sirp is |
performance protocols running in |
is assumed that after |
for high loss rates |
that sirp is preferable |
we did not consider |
protocols running in managed |
assumed that after one |
high loss rates if |
sirp is preferable to |
did not consider the |
running in managed settings |
that after one file |
loss rates if the |
is preferable to asynchronous |
not consider the rate |
in managed settings need |
after one file in |
rates if the interleaves |
preferable to asynchronous writeback |
consider the rate of |
managed settings need to |
one file in the |
if the interleaves are |
to asynchronous writeback at |
the rate of read |
settings need to maintain |
file in the group |
the interleaves are relatively |
asynchronous writeback at low |
rate of read operations |
need to maintain the |
in the group has |
interleaves are relatively prime |
writeback at low bandwidth |
of read operations because |
to maintain the smallest |
the group has been |
performance improves substantially when |
and adds little additional |
read operations because clients |
group has been accessed |
maintain the smallest possible |
improves substantially when loss |
adds little additional overhead |
operations because clients updating |
the smallest possible memory |
cation performs a fast |
because clients updating their |
substantially when loss rates |
smallest possible memory footprint |
performs a fast linear |
clients updating their working |
when loss rates are |
the difference between asynchronous |
a fast linear scan |
updating their working copies |
loss rates are high |
difference between asynchronous schemes |
fast linear scan of |
rates are high and |
between asynchronous schemes is |
or reading from the |
reading from the repository |
are high and losses |
asynchronous schemes is minimal |
linear scan of files |
high and losses are |
do not require a |
not require a lock |
but any scheme improves |
scan of files in |
and losses are bursty |
any scheme improves over |
of files in a |
the debian community today |
scheme improves over synchronous |
the graph plots the |
debian community today uses |
files in a file |
improves over synchronous writeback |
plication of this principle |
graph plots the percentage |
community today uses only |
in a file group |
plots the percentage of |
today uses only a |
for the same reasons |
qsm achieves scalability and |
uses only a single |
the percentage of lost |
the same reasons that |
achieves scalability and stability |
only a single subversion |
percentage of lost packets |
an it becomes advantageous |
same reasons that it |
scalability and stability even |
a single subversion server |
of lost packets successfully |
it becomes advantageous to |
reasons that it improves |
and stability even at |
and the apache foundation |
becomes advantageous to prefetch |
that it improves performance |
lost packets successfully recovered |
stability even at very |
the apache foundation has |
advantageous to prefetch the |
asynchronous writeback reduces staleness |
even at very high |
apache foundation has a |
packets successfully recovered on |
to prefetch the remainder |
at very high loads |
foundation has a master |
successfully recovered on the |
and sirp makes it |
prefetch the remainder of |
has a master server |
the remainder of the |
sirp makes it an |
recovered on the y |
a master server plus |
an unexpected side effect |
remainder of the files |
unexpected side effect of |
of the files in |
side effect of building |
the files in efficient |
effect of building qsm |
makes it an acceptable |
axis against an xaxis |
master server plus a |
files in efficient implementation |
of building qsm in |
it an acceptable choice |
against an xaxis of |
server plus a european |
plus a european mirror |
building qsm in windows |
an acceptable choice at |
an xaxis of loss |
in efficient implementation of |
qsm in windows was |
acceptable choice at low |
xaxis of loss rates |
primarily for latency reasons |
efficient implementation of prefetching |
in windows was that |
choice at low bandwidth |
of loss rates on |
implementation of prefetching requires |
windows was that by |
loss rates on a |
of prefetching requires that |
was that by integrating |
we expect that most |
rates on a log |
on a log scale |
that by integrating our |
expect that most communities |
prefetching requires that the |
by integrating our system |
that most communities will |
requires that the demand |
the maelstrom configuration used |
maelstrom configuration used is |
most communities will have |
integrating our system tightly |
configuration used is r |
communities will have at |
our system tightly with |
will have at most |
system tightly with the |
have at most a |
tightly with the platform |
at most a handful |
most a handful of |
a handful of front |
we created a new |
created a new kind |
a new kind of |
new kind of live |
kind of live distributed |
achieving consistency amazon s |
of live distributed objects |
consistency amazon s infrastructure |
amazon s infrastructure is |
s infrastructure is built |
infrastructure is built on |
is built on the |
built on the principle |
on the principle of |
the principle of eventual |
principle of eventual consistency |
abstract data types that |
data types that form |
types that form groups |
and does not directly |
does not directly support |
not directly support the |
directly support the locking |
support the locking required |
the locking required for |
locking required for revision |
required for revision control |
and that are updated |
that are updated using |
are updated using qsm |
originally developed to run |
updated using qsm multicasts |
developed to run the |
to run the company |
run the company s |
the company s own |
scale and performance in |
company s own online |
and performance in a |
s own online store |
performance in a distributed |
in a distributed file |
a distributed file system |
these look natural to |
look natural to the |
the system preferred availability |
natural to the windows |
acm transactions on computer |
system preferred availability over |
to the windows user |
transactions on computer systems |
preferred availability over consistency |
availability over consistency because |
over consistency because downtime |
consistency because downtime translated |
because downtime translated directly |
downtime translated directly into |
translated directly into lost |
directly into lost revenue |
such an object changes |
an object changes faster |
object changes faster than |
customers may opt to |
we show the ability |
changes faster than the |
may opt to shop |
show the ability of |
faster than the average |
opt to shop elsewhere |
the ability of layered |
than the average windows |
to shop elsewhere or |
ability of layered interleaving |
the average windows object |
shop elsewhere or to |
of layered interleaving to |
elsewhere or to simply |
layered interleaving to provide |
or to simply forgo |
interleaving to provide gracefully |
to simply forgo impulse |
to provide gracefully degrading |
simply forgo impulse purchases |
but the same basic |
provide gracefully degrading performance |
forgo impulse purchases that |
the same basic mechanisms |
gracefully degrading performance in |
impulse purchases that they |
same basic mechanisms can |
degrading performance in the |
purchases that they didn |
basic mechanisms can support |
performance in the face |
that they didn t |
mechanisms can support them |
in the face of |
they didn t really |
the face of bursty |
didn t really need |
face of bursty loss |
t really need anyway |
and the component integration |
an inconsistent shopping cart |
the component integration environment |
we plot the percentage |
plot the percentage of |
the percentage of lost |
percentage of lost packets |
of lost packets successfully |
could be resolved by |
lost packets successfully recovered |
be resolved by heuristics |
packets successfully recovered against |
resolved by heuristics or |
successfully recovered against the |
by heuristics or user |
recovered against the length |
against the length of |
the length of loss |
length of loss bursts |
of loss bursts for |
intervention at checkout time |
loss bursts for two |
bursts for two different |
for two different sets |
two different sets of |
different sets of interleaves |
it is well known |
is well known that |
well known that consistency |
and in the bottom |
known that consistency and |
in the bottom graph |
that consistency and availability |
the bottom graph we |
consistency and availability cannot |
extends seamlessly to encompass |
bottom graph we plot |
and availability cannot both |
seamlessly to encompass them |
graph we plot the |
availability cannot both be |
we plot the average |
cannot both be achieved |
plot the average latency |
both be achieved simultaneously |
the average latency at |
be achieved simultaneously in |
average latency at which |
although a great deal |
achieved simultaneously in any |
latency at which the |
a great deal of |
simultaneously in any real |
at which the packets |
great deal of additional |
in any real network |
which the packets were |
the packets were recovered |
any real network where |
deal of additional work |
real network where hosts |
of additional work is |
recovery latency is defined |
network where hosts or |
additional work is needed |
latency is defined as |
where hosts or entire |
is defined as the |
hosts or entire subnetworks |
defined as the difference |
or entire subnetworks are |
as the difference between |
entire subnetworks are sometimes |
the difference between the |
subnetworks are sometimes unreachable |
qsm should eventually enable |
difference between the eventual |
are sometimes unreachable due |
should eventually enable casual |
between the eventual delivery |
sometimes unreachable due to |
eventually enable casual use |
the eventual delivery time |
enable casual use of |
unreachable due to connectivity |
casual use of live |
eventual delivery time of |
use of live objects |
due to connectivity losses |
of live objects not |
delivery time of the |
live objects not just |
time of the recovered |
objects not just in |
of the recovered packet |
not just in datacenters |
the recovered packet and |
just in datacenters but |
recovered packet and the |
in datacenters but also |
packet and the oneway |
datacenters but also on |
and the oneway latency |
if a cloud service |
but also on desktops |
the oneway latency of |
a cloud service is |
also on desktops in |
oneway latency of the |
cloud service is designed |
on desktops in wan |
latency of the link |
service is designed to |
desktops in wan settings |
is designed to provide |
designed to provide high |
we confirmed that the |
to provide high availability |
confirmed that the emulab |
provide high availability but |
that the emulab link |
mobile computing with the |
high availability but an |
the emulab link had |
computing with the rover |
with the rover toolkit |
availability but an application |
emulab link had almost |
opening the door to |
but an application instead |
link had almost no |
ieee transactions on computers |
the door to a |
an application instead requires |
had almost no jitter |
door to a new |
application instead requires perfect |
almost no jitter on |
to a new style |
instead requires perfect consistency |
no jitter on correctly |
a new style of |
jitter on correctly delivered |
new style of distributed |
on correctly delivered packets |
additional software infrastructure is |
style of distributed programming |
software infrastructure is required |
infrastructure is required to |
is required to bridge |
required to bridge the |
to bridge the gap |
way latency an accurate |
latency an accurate estimate |
an accurate estimate of |
accurate estimate of expected |
for revision control it |
estimate of expected lossless |
of expected lossless delivery |
revision control it makes |
the current version of |
expected lossless delivery time |
control it makes sense |
current version of qsm |
it makes sense to |
version of qsm is |
makes sense to adopt |
of qsm is stable |
sense to adopt eventual |
qsm is stable in |
to adopt eventual consistency |
is stable in cluster |
adopt eventual consistency for |
increasing the interleaves results |
stable in cluster settings |
eventual consistency for read |
the interleaves results in |
in cluster settings and |
consistency for read operations |
interleaves results in much |
results in much higher |
in much higher recovery |
much higher recovery percentages |
higher recovery percentages at |
since at worst an |
recovery percentages at large |
at worst an earlier |
percentages at large burst |
at large burst sizes |
worst an earlier revision |
an earlier revision will |
earlier revision will fig |
but comes at the |
comes at the cost |
at the cost of |
the cost of higher |
cost of higher recovery |
of higher recovery latency |
has a growing community |
a growing community of |
growing community of users |
system architecture be returned |
if the user is |
the user is aware |
looking to the future |
through some other channel |
that a newer version |
a newer version should |
newer version should exist |
we plan to scale |
plan to scale qsm |
to scale qsm into |
scale qsm into wan |
qsm into wan settings |
he can retry and |
can retry and expect |
retry and expect that |
and expect that version |
expect that version to |
that version to be |
version to be available |
to support a wider |
set of interleaves catches |
support a wider range |
to be available within |
of interleaves catches almost |
a wider range of |
be available within a |
interleaves catches almost all |
wider range of multicast |
available within a short |
catches almost all packets |
and performance in a |
performance in a wide |
within a short timeframe |
almost all packets in |
range of multicast reliability |
all packets in an |
of multicast reliability properties |
packets in an extended |
in an extended burst |
in proceedings of the |
an extended burst of |
proceedings of the first |
of the first usenix |
the first usenix conference |
first usenix conference on |
perfect consistency is required |
usenix conference on file |
and to introduce a |
consistency is required and |
to introduce a gossip |
packets at an average |
introduce a gossip infrastructure |
is required and a |
conference on file and |
at an average latency |
a gossip infrastructure that |
required and a locking |
gossip infrastructure that would |
an average latency of |
on file and storage |
and a locking layer |
infrastructure that would support |
average latency of around |
file and storage technologies |
a locking layer must |
that would support configuration |
locking layer must be |
would support configuration discovery |
layer must be built |
support configuration discovery and |
must be built to |
configuration discovery and other |
be built to support |
while repairing all random |
discovery and other self |
built to support this |
repairing all random singleton |
all random singleton losses |
random singleton losses within |
this may result in |
may result in a |
result in a commit |
in a commit being |
a commit being rejected |
commit being rejected if |
being rejected if consensus |
rejected if consensus cannot |
if consensus cannot be |
consensus cannot be reached |
this paper has described |
paper has described mafs |
the graphs also show |
but shouldn t be |
graphs also show recovery |
shouldn t be a |
also show recovery latency |
a new file system |
live objects pose a |
show recovery latency rising |
t be a problem |
new file system for |
objects pose a protocol |
recovery latency rising gracefully |
be a problem because |
file system for mobile |
pose a protocol design |
latency rising gracefully with |
a problem because code |
system for mobile clients |
a protocol design challenge |
rising gracefully with the |
problem because code changes |
for mobile clients that |
gracefully with the increase |
because code changes are |
mobile clients that is |
with the increase in |
code changes are usually |
clients that is tailored |
the increase in loss |
changes are usually not |
they give rise to |
that is tailored for |
increase in loss burst |
in loss burst length |
give rise to irregular |
is tailored for wireless |
are usually not impulse |
rise to irregular patterns |
tailored for wireless networks |
usually not impulse decisions |
the longer the burst |
to irregular patterns of |
for wireless networks by |
not impulse decisions and |
the longer it takes |
wireless networks by incorporating |
irregular patterns of overlapping |
impulse decisions and the |
longer it takes to |
networks by incorporating automatic |
patterns of overlapping multicast |
decisions and the commit |
it takes to recover |
by incorporating automatic adaptation |
of overlapping multicast groups |
and the commit can |
takes to recover the |
incorporating automatic adaptation to |
the commit can be |
to recover the lost |
automatic adaptation to the |
commit can be retried |
recover the lost packets |
adaptation to the available |
can be retried later |
to the available bandwidth |
the maelstrom configuration used |
maelstrom configuration used is |
configuration used is r |
mafs differs from previous |
differs from previous designs |
oriented state aggregation mechanisms |
from previous designs in |
d esign as a |
esign as a proof |
previous designs in making |
state aggregation mechanisms will |
designs in making use |
aggregation mechanisms will need |
in making use of |
mechanisms will need to |
making use of asynchronous |
will need to be |
use of asynchronous writeback |
need to be redesigned |
of asynchronous writeback at |
asynchronous writeback at all |
writeback at all bandwidth |
at all bandwidth levels |
a tool for integrating |
tool for integrating subversion |
rather than switching from |
we have an idea |
for integrating subversion with |
than switching from synchronous |
have an idea for |
integrating subversion with s |
switching from synchronous to |
an idea for solving |
from synchronous to asynchronous |
idea for solving this |
synchronous to asynchronous writeback |
to asynchronous writeback when |
asynchronous writeback when bandwidth |
writeback when bandwidth is |
when bandwidth is insufficient |
vn is colocated with |
is colocated with subversion |
rpc priorities and a |
colocated with subversion and |
priorities and a new |
with subversion and inserts |
and a new update |
subversion and inserts a |
a new update propagation |
and inserts a layer |
new update propagation algorithm |
inserts a layer between |
recovery would be performed |
a layer between subversion |
would be performed by |
layer between subversion and |
between subversion and s |
be performed by selecting |
reduce a client s |
performed by selecting a |
a client s contention |
by selecting a subset |
client s contention for |
as shown in figure |
s contention for wireless |
contention for wireless bandwidth |
selecting a subset of |
a subset of nodes |
subset of nodes that |
and permit a degree |
of nodes that form |
permit a degree of |
for simplicity we did |
nodes that form a |
a degree of consistency |
simplicity we did not |
that form a clean |
degree of consistency that |
we did not modify |
form a clean overlay |
of consistency that is |
did not modify the |
a clean overlay structure |
consistency that is equivalent |
not modify the subversion |
that is equivalent to |
we show histograms of |
modify the subversion server |
is equivalent to instantaneous |
show histograms of recovery |
the subversion server in |
equivalent to instantaneous propagation |
histograms of recovery latencies |
subversion server in any |
server in any way |
rather than just treating |
of recovery latencies for |
than just treating every |
to instantaneous propagation of |
recovery latencies for the |
just treating every single |
instantaneous propagation of updates |
latencies for the two |
vn is responsible for |
treating every single receiver |
for the two interleave |
is responsible for receiving |
every single receiver as |
experiments demonstrate that these |
the two interleave configurations |
responsible for receiving event |
single receiver as a |
demonstrate that these techniques |
two interleave configurations under |
for receiving event notifications |
receiver as a member |
that these techniques allow |
interleave configurations under different |
receiving event notifications from |
as a member of |
these techniques allow mafs |
configurations under different burst |
event notifications from subversion |
a member of a |
techniques allow mafs to |
under different burst lengths |
notifications from subversion and |
member of a recovery |
allow mafs to achieve |
from subversion and transferring |
of a recovery region |
mafs to achieve performance |
the histograms confirm the |
subversion and transferring data |
to achieve performance that |
histograms confirm the trends |
and transferring data between |
achieve performance that is |
confirm the trends described |
transferring data between the |
performance that is at |
the trends described above |
data between the local |
that is at least |
between the local disk |
is at least equal |
at least equal to |
packet recoveries take longer |
the local disk on |
local disk on the |
disk on the ec |
whether this can really |
recoveries take longer from |
and in most cases |
this can really scale |
take longer from left |
in most cases superior |
can really scale remains |
longer from left to |
most cases superior to |
really scale remains to |
from left to right |
cases superior to that |
scale remains to be |
vn at the start |
superior to that achievable |
left to right as |
remains to be seen |
at the start and |
to that achievable by |
to right as we |
the start and end |
that achievable by conventional |
right as we increase |
start and end of |
achievable by conventional file |
as we increase loss |
and end of each |
by conventional file system |
we increase loss burst |
end of each commit |
conventional file system designs |
increase loss burst length |
file system designs that |
system designs that switch |
designs that switch between |
that switch between lowand |
switch between lowand high |
and from top to |
from top to bottom |
top to bottom as |
vn acquires and releases |
bandwidth modes according to |
to bottom as we |
acquires and releases locks |
modes according to thresholds |
bottom as we increase |
and releases locks using |
as we increase the |
releases locks using yahoo |
we increase the interleave |
locks using yahoo s |
mafs is therefore able |
increase the interleave values |
using yahoo s open |
is therefore able to |
yahoo s open source |
therefore able to make |
s open source zookeeper |
able to make efficient |
open source zookeeper lock |
to make efficient use |
source zookeeper lock service |
make efficient use of |
efficient use of the |
illustrates the difference between |
use of the network |
the difference between a |
of the network and |
the difficulty achieving consistency |
difference between a traditional |
the network and provide |
difficulty achieving consistency with |
between a traditional fec |
network and provide predictable |
achieving consistency with a |
a traditional fec code |
and provide predictable file |
consistency with a service |
traditional fec code and |
provide predictable file system |
with a service such |
fec code and layered |
predictable file system semantics |
a service such as |
code and layered interleaving |
service such as amazon |
and layered interleaving by |
regardless of the available |
such as amazon s |
as amazon s s |
of the available bandwidth |
layered interleaving by plotting |
interleaving by plotting a |
stems from the fact |
from the fact that |
the fact that files |
fact that files pushed |
that files pushed into |
files pushed into the |
pushed into the storage |
into the storage cloud |
the storage cloud do |
prefetch no prefetch prefetch |
storage cloud do not |
no prefetch prefetch no |
cloud do not simultaneously |
prefetch prefetch no prefetch |
do not simultaneously become |
not simultaneously become available |
simultaneously become available on |
become available on all |
available on all service |
on all service endpoints |
if a file is |
a file is overwritten |
automated hoarding for mobile |
hoarding for mobile computers |
different clients may read |
clients may read back |
may read back different |
read back different versions |
in proceedings of the |
proceedings of the sixteenth |
of the sixteenth acm |
the sixteenth acm symposium |
sixteenth acm symposium on |
acm symposium on operating |
and even the same |
symposium on operating systems |
even the same client |
on operating systems principles |
the same client may |
same client may see |
client may see the |
may see the old |
see the old version |
the old version if |
prefetch no prefetch relative |
old version if it |
no prefetch relative speedup |
version if it suddenly |
prefetch relative speedup relative |
if it suddenly switches |
relative speedup relative speedup |
it suddenly switches to |
suddenly switches to speaking |
switches to speaking with |
to speaking with a |
speaking with a different |
with a different s |
the file will always |
file will always be |
will always be internally |
always be internally consistent |
since put and get |
put and get operations |
and get operations are |
get operations are atomic |
but its contents may |
its contents may not |
contents may not reflect |
may not reflect expectations |
not reflect expectations that |
reflect expectations that the |
expectations that the client |
that the client formed |
the client formed based |
client formed based on |
formed based on other |
based on other files |
on other files and |
other files and out |
files and out of |
and out of band |
out of band communication |
vn works around the |
works around the consistency |
around the consistency problem |
the consistency problem by |
consistency problem by storing |
problem by storing the |
by storing the number |
storing the number of |
the number of the |
number of the latest |
of the latest revision |
the latest revision into |
latest revision into zookeeper |
even if multiple files |
if multiple files were |
multiple files were changed |
files were changed by |
were changed by the |
changed by the client |
design and implementation of |
exploiting weak connectivity for |
is represented by subversion |
and implementation of a |
weak connectivity for mobile |
represented by subversion has |
implementation of a reliable |
connectivity for mobile file |
by subversion has a |
of a reliable group |
for mobile file access |
subversion has a single |
a reliable group communication |
has a single file |
reliable group communication toolkit |
a single file containing |
in proceedings of the |
group communication toolkit for |
single file containing binary |
proceedings of the fifteenth |
communication toolkit for java |
file containing binary diffs |
of the fifteenth acm |
containing binary diffs against |
the fifteenth acm symposium |
binary diffs against earlier |
fifteenth acm symposium on |
diffs against earlier revisions |
acm symposium on operating |
symposium on operating systems |
on operating systems principles |
a revision is never |
revision is never changed |
is never changed after |
never changed after the |
changed after the fact |
prefetch no prefetch relative |
no prefetch relative speedup |
prefetch relative speedup relative |
end server attempting to |
relative speedup relative speedup |
server attempting to fetch |
attempting to fetch a |
to fetch a revision |
fetch a revision i |
a revision i from |
revision i from s |
will receive either the |
receive either the one |
either the one true |
or a missing file |
a missing file error |
missing file error if |
file error if i |
error if i was |
if i was posted |
i was posted so |
was posted so recently |
prefetch no prefetch relative |
posted so recently that |
no prefetch relative speedup |
so recently that it |
recently that it has |
that it has not |
prefetch no prefetch relative |
no prefetch relative speedup |
prefetch relative speedup bad |
relative speedup bad groups |
bandwidth network file system |
in proceedings of the |
proceedings of the eighteenth |
of the eighteenth acm |
the eighteenth acm symposium |
eighteenth acm symposium on |
acm symposium on operating |
symposium on operating systems |
on operating systems principles |
design and evaluation of |
and evaluation of a |
evaluation of a wide |
area event notification service |
acm transactions on computer |
transactions on computer systems |
end servers are equivalent |
servers are equivalent and |
are equivalent and clients |
equivalent and clients may |
and clients may interact |
clients may interact with |
may interact with any |
interact with any of |
with any of them |
any of them fig |
e valuation we observe |
valuation we observe that |
we observe that running |
observe that running multiple |
that running multiple front |
which cloud computing makes |
cloud computing makes easy |
computing makes easy to |
makes easy to do |
increases the throughput of |
the throughput of read |
throughput of read operations |
vn by running a |
by running a fixed |
managing update conflicts in |
update conflicts in bayou |
running a fixed number |
a fixed number of |
fixed number of clients |
a weakly connected replicated |
weakly connected replicated storage |
connected replicated storage system |
each repeatedly checking out |
repeatedly checking out about |
in proceedings of the |
proceedings of the fifteenth |
of the fifteenth acm |
the fifteenth acm symposium |
fifteenth acm symposium on |
acm symposium on operating |
symposium on operating systems |
on operating systems principles |
latency histograms for i |
yet propagated through s |
in the latter case |
the server retries indefinitely |
server retries indefinitely until |
retries indefinitely until the |
indefinitely until the file |
until the file is |
the file is available |
zookeeper ensures that the |
ensures that the latest |
that the latest revision |
the latest revision number |
latest revision number is |
revision number is incremented |
number is incremented atomically |
zookeeper maintains a simple |
maintains a simple filesystem |
a simple filesystem like |
file system usage in |
simple filesystem like tree |
system usage in windows |
filesystem like tree of |
usage in windows nt |
like tree of nodes |
nodes may store a |
may store a small |
store a small amount |
a small amount of |
small amount of data |
amount of data and |
of data and can |
data and can have |
and can have children |
in proceedings of the |
proceedings of the seventeenth |
of the seventeenth acm |
the seventeenth acm symposium |
seventeenth acm symposium on |
acm symposium on operating |
symposium on operating systems |
vn stores the latest |
on operating systems principles |
stores the latest revision |
the latest revision number |
latest revision number in |
supporting multiple named repositories |
multiple named repositories in |
named repositories in a |
repositories in a single |
in a single zookeeper |
a single zookeeper tree |
before pushing a new |
pushing a new revision |
end server must acquire |
server must acquire a |
must acquire a lock |
acquire a lock by |
a lock by creating |
lock by creating a |
weight process groups in |
by creating a sequence |
creating a sequence node |
process groups in the |
groups in the isis |
in the isis system |
to which zookeeper will |
which zookeeper will append |
zookeeper will append a |
will append a unique |
monotonically increasing sequence number |
end server then lists |
server then lists the |
then lists the children |
lists the children of |
if its own lock |
its own lock node |
own lock node has |
lock node has the |
node has the lowest |
has the lowest number |
it may proceed with |
may proceed with the |
proceed with the commit |
otherwise it watches the |
it watches the node |
watches the node with |
the node with the |
node with the next |
with the next lower |
the next lower number |
next lower number in |
lower number in order |
latency histograms for i |
number in order to |
in order to be |
order to be notified |
to be notified when |
be notified when that |
notified when that node |
when that node and |
that node and its |
node and its associated |
and its associated lock |
its associated lock go |
associated lock go away |
after comitting the revision |
comitting the revision to |
the revision to s |
it releases its lock |
releases its lock by |
its lock by deleting |
lock by deleting the |
by deleting the lock |
deleting the lock node |
lock nodes are marked |
nodes are marked with |
are marked with zookeeper |
marked with zookeeper s |
with zookeeper s ephemeral |
zookeeper s ephemeral flag |
s ephemeral flag to |
ephemeral flag to ensure |
flag to ensure that |
to ensure that the |
ensure that the lock |
that the lock is |
the lock is forcibly |
lock is forcibly released |
is forcibly released if |
forcibly released if the |
released if the front |
constructing reliable distributed communication |
reliable distributed communication systems |
distributed communication systems with |
zookeeper runs as a |
runs as a replicated |
as a replicated service |
moving average of recovery |
communication systems with corba |
average of recovery latencies |
of recovery latencies for |
so it remains available |
recovery latencies for both |
it remains available as |
latencies for both codes |
remains available as long |
available as long as |
as long as a |
long as a majority |
ieee communications magazine feature |
as a majority of |
the channel is configured |
communications magazine feature topic |
a majority of the |
channel is configured to |
magazine feature topic issue |
majority of the hosts |
is configured to lose |
feature topic issue on |
of the hosts are |
configured to lose singleton |
topic issue on distributed |
the hosts are up |
to lose singleton packets |
issue on distributed object |
hosts are up and |
lose singleton packets randomly |
on distributed object computing |
are up and reachable |
singleton packets randomly at |
packets randomly at a |
randomly at a loss |
at a loss rate |
a loss rate of |
relative speedup of workloads |
a client only speaks |
speedup of workloads with |
client only speaks to |
of workloads with prefetching |
only speaks to one |
speaks to one zookeeper |
to one zookeeper server |
one zookeeper server at |
zookeeper server at a |
server at a time |
and additionally lose long |
though it may fail |
these graphs show the |
additionally lose long bursts |
graphs show the speedup |
lose long bursts of |
show the speedup gained |
over to another server |
the speedup gained by |
to another server if |
speedup gained by adding |
another server if necessary |
gained by adding prefetching |
packets at occasional intervals |
by adding prefetching for |
adding prefetching for a |
both codes are configured |
prefetching for a range |
codes are configured with |
but the server ensures |
for a range of |
are configured with r |
the server ensures that |
a range of bandwidth |
server ensures that the |
range of bandwidth values |
ensures that the relevant |
that the relevant state |
the relevant state has |
relevant state has been |
state has been replicated |
has been replicated before |
been replicated before responding |
replicated before responding to |
before responding to a |
responding to a client |
to a client s |
a client s request |
relative to the time |
to the time taken |
and recover all lost |
in general multiple front |
the time taken with |
recover all lost packets |
time taken with a |
end servers may be |
servers may be run |
taken with a bandwidth |
all lost packets reedsolomon |
with a bandwidth of |
each on its own |
on its own ec |
lost packets reedsolomon uses |
packets reedsolomon uses an |
reedsolomon uses an interleave |
uses an interleave of |
the system is organized |
system is organized as |
is organized as in |
and layered interleaving uses |
organized as in figure |
layered interleaving uses interleaves |
interleaving uses interleaves of |
unlike the traditional replicated |
the traditional replicated subversion |
s and no prefetching |
traditional replicated subversion setups |
replicated subversion setups that |
subversion setups that are |
setups that are used |
that are used today |
no single server acts |
single server acts as |
where a test comprises |
server acts as a |
a test comprises two |
acts as a master |
and consequently both have |
test comprises two separate |
consequently both have a |
comprises two separate processes |
both have a maximum |
have a maximum tolerable |
a maximum tolerable burst |
vn all are equivalent |
maximum tolerable burst length |
tolerable burst length of |
performance of simultaneous checkouts |
only the speedup for |
the speedup for the |
speedup for the foreground |
for the foreground process |
the foreground process is |
we use a publicly |
foreground process is shown |
use a publicly available |
a publicly available implementation |
publicly available implementation of |
available implementation of a |
implementation of a reed |
solomon code based on |
code based on vandermonde |
fetch wait for the |
building collaboration applications that |
based on vandermonde matrices |
wait for the prefetch |
collaboration applications that mix |
for the prefetch to |
applications that mix web |
the prefetch to complete |
that mix web services |
mix web services hosted |
web services hosted content |
services hosted content with |
hosted content with p |
or that the prefetch |
that the prefetch be |
hierarchical clustering of message |
the prefetch be aborted |
clustering of message flows |
performance of simultaneous commits |
the code is plugged |
of message flows in |
of simultaneous commits source |
code is plugged into |
issuing a fetch rpc |
simultaneous commits source code |
message flows in a |
is plugged into maelstrom |
a fetch rpc at |
commits source code from |
flows in a multicast |
plugged into maelstrom instead |
fetch rpc at the |
source code from an |
code from an ec |
into maelstrom instead of |
rpc at the same |
in a multicast data |
maelstrom instead of layered |
krzysztof ostrowski cornell university |
at the same time |
a multicast data dissemination |
instead of layered interleaving |
dept of computer science |
multicast data dissemination system |
the same time as |
same time as a |
showing that we can |
time as a prefetch |
that we can use |
and varying the number |
as a prefetch is |
we can use new |
varying the number of |
a prefetch is in |
can use new encodings |
the number of servers |
prefetch is in progress |
use new encodings within |
number of servers over |
is in progress needlessly |
new encodings within the |
of servers over which |
in progress needlessly wastes |
encodings within the same |
servers over which the |
progress needlessly wastes bandwidth |
within the same framework |
over which the load |
the same framework seamlessly |
which the load was |
the load was distributed |
since it retrieves the |
it retrieves the same |
solomon code recovers all |
retrieves the same file |
code recovers all lost |
the same file from |
recovers all lost packets |
write performance was measured |
all lost packets with |
performance was measured by |
same file from the |
lost packets with roughly |
was measured by observing |
packets with roughly the |
file from the server |
measured by observing the |
with roughly the same |
from the server twice |
by observing the latency |
roughly the same latency |
observing the latency of |
the same latency whereas |
the latency of simultaneous |
same latency whereas layered |
latency of simultaneous commits |
latency whereas layered interleaving |
of simultaneous commits from |
the same could be |
whereas layered interleaving recovers |
simultaneous commits from different |
commits from different clients |
layered interleaving recovers singleton |
same could be true |
interleaving recovers singleton losses |
could be true if |
recovers singleton losses almost |
since simultaneous commits to |
be true if we |
singleton losses almost immediately |
simultaneous commits to a |
edu abstract the most |
true if we opt |
losses almost immediately and |
optimizing buffer management for |
commits to a single |
abstract the most commonly |
if we opt for |
almost immediately and exhibits |
buffer management for reliable |
to a single repository |
the most commonly deployed |
we opt for aborting |
immediately and exhibits latency |
management for reliable multicast |
a single repository would |
most commonly deployed web |
opt for aborting prefetches |
and exhibits latency spikes |
single repository would not |
commonly deployed web service |
exhibits latency spikes whenever |
repository would not be |
proceedings of the international |
since an aborted prefetch |
latency spikes whenever the |
would not be a |
deployed web service applications |
of the international conference |
an aborted prefetch could |
spikes whenever the longer |
not be a typical |
be a typical case |
the international conference on |
aborted prefetch could be |
whenever the longer loss |
web service applications employ |
international conference on dependable |
prefetch could be very |
the longer loss burst |
service applications employ client |
conference on dependable systems |
could be very close |
longer loss burst occurs |
on dependable systems and |
be very close to |
dependable systems and networks |
very close to completion |
with clients running remotely |
clients running remotely and |
running remotely and services |
remotely and services hosted |
and services hosted in |
vn repositories were used |
services hosted in data |
r elated w ork |
hosted in data centers |
elated w ork maelstrom |
all sharing the same |
w ork maelstrom lies |
mfs therefore makes the |
sharing the same set |
ork maelstrom lies in |
therefore makes the demand |
the same set of |
maelstrom lies in the |
we make the case |
make the case for |
the case for service |
lies in the intersection |
makes the demand fetch |
same set of front |
in the intersection of |
the demand fetch wait |
the intersection of two |
demand fetch wait for |
intersection of two research |
end servers and same |
applications that combine service |
of two research areas |
fetch wait for the |
servers and same set |
two research areas that |
wait for the prefetch |
and same set of |
hosted data with collaboration |
research areas that have |
same set of three |
data with collaboration features |
areas that have seen |
set of three zookeeper |
with collaboration features implemented |
that have seen major |
of three zookeeper servers |
collaboration features implemented using |
but also raises the |
have seen major innovations |
also raises the priority |
features implemented using peerto |
raises the priority of |
each client checked out |
the priority of the |
seen major innovations in |
client checked out a |
priority of the prefetch |
major innovations in the |
collaboration features are awkward |
of the prefetch rpc |
checked out a random |
innovations in the last |
features are awkward to |
the prefetch rpc to |
out a random repository |
in the last decade |
are awkward to support |
prefetch rpc to that |
a random repository from |
the last decade high |
awkward to support solely |
rpc to that of |
random repository from a |
to support solely based |
to that of a |
repository from a random |
support solely based on |
that of a regular |
from a random front |
haul communication and forward |
solely based on the |
of a regular fetch |
communication and forward error |
based on the existing |
a regular fetch operation |
and forward error correction |
on the existing web |
and then repeatedly committed |
the existing web services |
then repeatedly committed small |
existing web services technologies |
repeatedly committed small amounts |
committed small amounts of |
small amounts of data |
to prevent a priority |
ip variants such as |
prevent a priority inversion |
indirection through the data |
variants such as compound |
changes were propgated in |
through the data center |
such as compound tcp |
were propgated in the |
the data center introduces |
propgated in the background |
data center introduces high |
in the background to |
this requires an additional |
center introduces high latencies |
the background to the |
requires an additional raise |
introduces high latencies and |
background to the other |
to the other front |
high latencies and limits |
latencies and limits scalability |
a group membership service |
group membership service for |
membership service for wans |
priority rpc to the |
and precludes collaboration between |
rpc to the server |
precludes collaboration between clients |
collaboration between clients connected |
between clients connected to |
clients connected to one |
acm transactions on computer |
use transmission delay to |
transactions on computer systems |
transmission delay to detect |
another but lacking connectivity |
shows that adding front |
which results in more |
delay to detect backed |
but lacking connectivity to |
end servers can indeed |
to detect backed up |
detect backed up routers |
lacking connectivity to the |
servers can indeed alleviate |
results in more overhead |
connectivity to the data |
in more overhead than |
replacing or supplementing packet |
more overhead than the |
to the data center |
overhead than the case |
or supplementing packet loss |
can indeed alleviate latency |
than the case where |
cornell s live distributed |
the case where a |
supplementing packet loss as |
indeed alleviate latency problems |
s live distributed objects |
case where a demand |
packet loss as a |
alleviate latency problems caused |
live distributed objects platform |
where a demand fetch |
loss as a signal |
latency problems caused by |
distributed objects platform combines |
a demand fetch occurs |
as a signal of |
a signal of congestion |
objects platform combines web |
demand fetch occurs without |
problems caused by high |
platform combines web services |
fetch occurs without a |
caused by high load |
while such protocols solve |
combines web services with |
occurs without a fetch |
such protocols solve the |
web services with direct |
and that the overhead |
protocols solve the congestion |
services with direct peerto |
that the overhead of |
solve the congestion collapse |
the overhead of propagating |
the congestion collapse experienced |
peer communication to eliminate |
overhead of propagating data |
congestion collapse experienced by |
communication to eliminate these |
of propagating data in |
collapse experienced by conventional |
on the other hand |
propagating data in the |
to eliminate these issues |
experienced by conventional tcp |
data in the backgound |
in the backgound is |
the backgound is not |
backgound is not significant |
is not significant enough |
not significant enough to |
significant enough to negatively |
enough to negatively affect |
the fetch can frequently |
to negatively affect performance |
fetch can frequently make |
introduction there is a |
there is a growing |
can frequently make use |
they cannot mitigate the |
is a growing opportunity |
r elated w orks |
frequently make use of |
cannot mitigate the longer |
a growing opportunity to |
elated w orks moving |
make use of the |
mitigate the longer packet |
growing opportunity to use |
w orks moving services |
use of the data |
the longer packet delivery |
of the data already |
orks moving services to |
the data already transferred |
longer packet delivery latencies |
data already transferred and |
moving services to the |
already transferred and so |
packet delivery latencies caused |
opportunity to use service |
services to the cloud |
transferred and so still |
delivery latencies caused by |
to the cloud has |
and so still results |
latencies caused by packet |
the cloud has been |
so still results in |
caused by packet loss |
cloud has been published |
applications in ways that |
still results in a |
has been published on |
in ways that can |
results in a faster |
been published on in |
and they do not |
ways that can slash |
that can slash health |
published on in other |
they do not eliminate |
in a faster response |
on in other contexts |
do not eliminate the |
a faster response to |
not eliminate the need |
faster response to the |
eliminate the need for |
response to the application |
the need for larger |
permit more effective search |
need for larger buffers |
more effective search and |
for larger buffers at |
effective search and rescue |
larger buffers at end |
search and rescue after |
is a backup application |
as we have explained |
and rescue after a |
a backup application that |
rescue after a disaster |
backup application that implements |
application that implements a |
fec has seen major |
that implements a custom |
has seen major innovations |
implements a custom block |
enable a more nimble |
seen major innovations in |
the implementation of the |
a more nimble information |
major innovations in the |
based file system to |
implementation of the prefetching |
innovations in the last |
file system to store |
of the prefetching subsystem |
in the last fifteen |
system to store multiple |
the prefetching subsystem is |
or make possible a |
the last fifteen years |
to store multiple versions |
prefetching subsystem is not |
make possible a world |
store multiple versions of |
subsystem is not sophisticated |
possible a world of |
multiple versions of backup |
a world of professional |
level fec was first |
versions of backup data |
world of professional dialog |
fec was first described |
of backup data on |
backup data on s |
was first described for |
of professional dialog and |
first described for high |
while it will reach |
professional dialog and collaboration |
dialog and collaboration without |
it will reach an |
and collaboration without travel |
speed wan networks as |
the authors make the |
will reach an equilibrium |
wan networks as early |
authors make the distinction |
soc applications will need |
networks as early as |
reach an equilibrium if |
make the distinction between |
applications will need to |
the distinction between thin |
an equilibrium if the |
will need to combine |
need to combine two |
to combine two types |
clouds that provide a |
that provide a low |
combine two types of |
two types of content |
equilibrium if the total |
level api and thick |
if the total size |
traditional web service hosted |
web service hosted content |
the total size of |
clouds that are designed |
total size of the |
that are designed for |
size of the file |
such as data from |
of the file groups |
are designed for a |
the file groups in |
as data from databases |
designed for a specific |
for a specific application |
file groups in the |
groups in the prefetch |
in the prefetch list |
thick clouds for a |
it was applied by |
the prefetch list is |
clouds for a variety |
and weather prediction systems |
prefetch list is less |
was applied by researchers |
for a variety of |
a variety of purposes |
applied by researchers in |
list is less than |
with a variety of |
including backup and source |
is less than the |
by researchers in the |
a variety of collaboration |
backup and source code |
less than the cache |
researchers in the context |
variety of collaboration features |
and source code repository |
source code repository hosting |
in the context of |
than the cache size |
the context of atm |
such as chat windows |
context of atm networks |
with sourceforge and google |
sourceforge and google code |
and google code being |
google code being examples |
code being examples of |
being examples of the |
examples of the latter |
there is no mechanism |
is no mechanism to |
peer video and other |
the authors of cumulus |
video and other media |
and other media streams |
authors of cumulus and |
no mechanism to prevent |
of cumulus and we |
cumulus and we show |
and we show that |
we show that thin |
mechanism to prevent the |
level fec for ip |
fec for ip networks |
to prevent the prefetching |
for ip networks was |
cloud solutions can be |
existing web service technologies |
prevent the prefetching subsystem |
ip networks was revived |
solutions can be a |
can be a cost |
the prefetching subsystem running |
networks was revived in |
prefetching subsystem running ahead |
web service technologies make |
subsystem running ahead of |
service technologies make it |
running ahead of actual |
another example of moving |
ahead of actual file |
technologies make it easy |
example of moving a |
of actual file accesses |
make it easy to |
of moving a service |
actual file accesses and |
it easy to build |
moving a service to |
file accesses and evicting |
easy to build applications |
a service to the |
accesses and evicting useful |
to build applications in |
service to the cloud |
and evicting useful files |
build applications in which |
to the cloud is |
in the context of |
applications in which all |
the cloud is metacdn |
evicting useful files from |
the context of both |
in which all data |
useful files from the |
context of both reliable |
which all data travels |
files from the cache |
of both reliable multicast |
all data travels through |
both reliable multicast and |
data travels through a |
reliable multicast and long |
travels through a data |
a content distribution network |
through a data center |
or evicting files which |
evicting files which it |
implementing collaboration features using |
rizzo subsequently provided a |
the work evaluates the |
files which it has |
collaboration features using these |
which it has prefetched |
work evaluates the latency |
subsequently provided a working |
evaluates the latency of |
it has prefetched but |
features using these technologies |
provided a working implementation |
the latency of various |
has prefetched but have |
using these technologies is |
a working implementation of |
latency of various cloud |
prefetched but have not |
these technologies is problematic |
working implementation of a |
of various cloud storage |
but have not yet |
technologies is problematic because |
implementation of a software |
various cloud storage services |
have not yet been |
is problematic because collaborative |
of a software packet |
cloud storage services from |
not yet been referenced |
problematic because collaborative applications |
storage services from several |
yet been referenced by |
because collaborative applications can |
services from several locations |
been referenced by the |
collaborative applications can generate |
from several locations and |
referenced by the user |
applications can generate high |
several locations and provides |
locations and provides an |
and provides an abstraction |
provides an abstraction to |
an abstraction to integrate |
bursty update rates and |
abstraction to integrate the |
update rates and yet |
to integrate the different |
techniques for preventing this |
rates and yet often |
integrate the different offerings |
for preventing this behaviour |
and yet often require |
the different offerings into |
maelstrom represents a natural |
preventing this behaviour have |
yet often require low |
different offerings into a |
represents a natural evolution |
this behaviour have been |
often require low latencies |
offerings into a single |
a natural evolution of |
behaviour have been discussed |
require low latencies and |
into a single system |
natural evolution of these |
evolution of these ideas |
low latencies and tight |
have been discussed elsewhere |
latencies and tight synchronization |
and tight synchronization between |
tight synchronization between collaborating |
the emphasis on applying |
synchronization between collaborating users |
emphasis on applying error |
on applying error correcting |
applying error correcting codes |
error correcting codes at |
one can often achieve |
correcting codes at higher |
can often achieve better |
codes at higher levels |
often achieve better performance |
like transactional data store |
at higher levels of |
achieve better performance using |
transactional data store backed |
higher levels of the |
better performance using direct |
data store backed by |
levels of the software |
performance using direct client |
store backed by s |
of the software stack |
the software stack has |
software stack has been |
stack has been accompanied |
has been accompanied by |
been accompanied by advances |
accompanied by advances in |
by advances in the |
advances in the codes |
and faced similar issues |
in the codes themselves |
faced similar issues as |
similar issues as s |
prior to the mid |
vn due to its |
due to its need |
to its need for |
its need for high |
need for high consistency |
elastras assigns update priviledges |
in order to characterise |
assigns update priviledges for |
order to characterise the |
update priviledges for different |
but in today s |
the standard encoding used |
to characterise the effect |
priviledges for different areas |
in today s soa |
standard encoding used was |
encoding used was reed |
for different areas of |
today s soa plat |
characterise the effect of |
different areas of the |
the effect of adding |
areas of the data |
effect of adding prefetching |
of the data store |
the data store to |
data store to individual |
an erasure code that |
store to individual front |
erasure code that performs |
band communication is hard |
code that performs excellently |
communication is hard to |
that performs excellently at |
we ran a set |
is hard to integrate |
performs excellently at small |
ran a set of |
using the lock service |
hard to integrate with |
excellently at small scale |
a set of eight |
the lock service to |
to integrate with hosted |
at small scale but |
set of eight microbenchmarks |
lock service to elect |
integrate with hosted content |
small scale but does |
service to elect an |
scale but does not |
to elect an owner |
but does not scale |
this problem is reflected |
elect an owner for |
does not scale to |
problem is reflected by |
an owner for each |
owner for each partition |
not scale to large |
is reflected by a |
the experimental setup was |
scale to large sets |
reflected by a growing |
much in the style |
experimental setup was the |
to large sets of |
by a growing number |
in the style described |
setup was the same |
large sets of data |
a growing number of |
the style described by |
was the same as |
sets of data and |
growing number of publications |
style described by google |
the same as in |
of data and error |
number of publications on |
described by google s |
by google s chubby |
data and error correcting |
of publications on the |
same as in the |
and error correcting symbols |
publications on the integration |
as in the priority |
on the integration of |
in the priority tests |
this scalability barrier resulted |
the integration of web |
scalability barrier resulted in |
integration of web services |
barrier resulted in the |
of web services with |
resulted in the development |
web services with peer |
in the development of |
a lock service based |
the development of new |
though this time mfs |
lock service based on |
development of new variants |
this time mfs was |
service based on paxos |
of new variants of |
time mfs was configured |
new variants of low |
mfs was configured to |
variants of low density |
was configured to run |
of low density parity |
low density parity check |
configured to run with |
to run with asynchronous |
run with asynchronous writeback |
defers finegrained locking to |
finegrained locking to the |
locking to the application |
to the application in |
the application in order |
application in order not |
in order not to |
order not to burden |
not to burden the |
and rpc with priorities |
to burden the global |
burden the global lock |
the global lock service |
global lock service with |
lock service with high |
service with high traffic |
and only prefetching was |
only prefetching was either |
vn we opted to |
prefetching was either enabled |
we opted to use |
was either enabled or |
opted to use the |
to use the lock |
either enabled or disabled |
use the lock service |
the tests were run |
tests were run at |
grained locking instead of |
were run at a |
locking instead of just |
run at a range |
instead of just leader |
at a range of |
of just leader election |
a range of bandwidth |
range of bandwidth values |
since the latter would |
the latter would have |
latter would have required |
would have required duplicating |
have required duplicating much |
required duplicating much of |
duplicating much of zookeeper |
as in the previous |
much of zookeeper s |
in the previous section |
which are orders of |
of zookeeper s functionality |
are orders of magnitude |
zookeeper s functionality to |
orders of magnitude faster |
s functionality to replicate |
of magnitude faster than |
functionality to replicate the |
magnitude faster than reed |
each microbenchmark consists of |
to replicate the leader |
microbenchmark consists of one |
solomon and much more |
replicate the leader s |
consists of one or |
and much more scalable |
the leader s state |
of one or two |
much more scalable in |
more scalable in input |
scalable in input size |
scalability is not an |
one or two processes |
is not an obstacle |
or two processes accessing |
not an obstacle because |
but require slightly more |
two processes accessing files |
an obstacle because there |
require slightly more data |
obstacle because there is |
slightly more data to |
because there is no |
more data to be |
there is no need |
data to be received |
is no need for |
to be received at |
no need for global |
with some or all |
be received at the |
need for global locking |
some or all of |
received at the decoder |
for global locking across |
global locking across multiple |
locking across multiple repositories |
or all of the |
while the layered interleaving |
all of the files |
the layered interleaving code |
of the files forming |
the load can be |
layered interleaving code used |
the files forming file |
load can be partitioned |
interleaving code used by |
files forming file groups |
can be partitioned across |
code used by maelstrom |
be partitioned across as |
used by maelstrom is |
partitioned across as many |
by maelstrom is similar |
across as many zookeeper |
maelstrom is similar to |
as many zookeeper instances |
is similar to the |
many zookeeper instances as |
similar to the tornado |
zookeeper instances as necessary |
lt and raptor codes |
replication is not without |
write test is the |
and raptor codes in |
is not without its |
not without its dangers |
raptor codes in its |
test is the same |
codes in its use |
is the same as |
in its use of |
the same as in |
its use of simple |
use of simple xor |
of simple xor operations |
yet the issue remains |
same as in section |
the issue remains unresolved |
and it has been |
it differs from them |
it has been shown |
differs from them in |
has been shown that |
from them in one |
been shown that replicating |
them in one very |
shown that replicating too |
in one very important |
that replicating too eagerly |
one very important aspect |
replicating too eagerly leads |
very important aspect it |
cornell s live distributed |
too eagerly leads quickly |
with a file group |
important aspect it seeks |
s live distributed objects |
eagerly leads quickly to |
a file group added |
aspect it seeks to |
live distributed objects platform |
leads quickly to degraded |
quickly to degraded performance |
it seeks to minimize |
file group added for |
seeks to minimize the |
group added for the |
to minimize the latency |
the solution proposed is |
added for the read |
minimize the latency between |
solution proposed is to |
for the read data |
the latency between the |
proposed is to use |
latency between the arrival |
is to use master |
between the arrival of |
to use master copy |
live objects for short |
the arrival of a |
use master copy replication |
the compile mfs test |
arrival of a packet |
compile mfs test has |
allow even a non |
mfs test has six |
where a transaction does |
of a packet at |
a packet at the |
packet at the send |
test has six file |
programmer to construct content |
a transaction does not |
has six file groups |
transaction does not immediately |
side proxy and its |
rich solutions that blend |
does not immediately update |
six file groups for |
proxy and its successful |
solutions that blend traditional |
not immediately update all |
file groups for the |
and its successful reception |
that blend traditional web |
immediately update all replicas |
groups for the main |
its successful reception at |
blend traditional web services |
optimizing power consumption in |
for the main directories |
successful reception at the |
traditional web services and |
web services and peer |
the main directories of |
reception at the receive |
power consumption in large |
main directories of the |
as the master copy |
consumption in large scale |
directories of the system |
in large scale storage |
large scale storage systems |
and only the lock |
and to share them |
only the lock service |
codes such as tornado |
to share them with |
share them with others |
such as tornado encode |
scale storage systems lakshmi |
as tornado encode over |
which deals with simple |
storage systems lakshmi ganesh |
mb of data in |
tornado encode over a |
this is like creating |
encode over a fixed |
is like creating a |
over a fixed set |
bandwidth operations that may |
like creating a slide |
a fixed set of |
operations that may be |
creating a slide show |
fixed set of input |
set of input symbols |
that may be concentrated |
may be concentrated on |
be concentrated on a |
concentrated on a small |
on a small number |
a small number of |
small number of servers |
without treating symbols differently |
treating symbols differently based |
symbols differently based on |
must be eagerly replicated |
differently based on their |
after which the solution |
forming a single file |
ken birman computer science |
based on their sequence |
which the solution can |
a single file group |
also relevant is sundr |
birman computer science department |
on their sequence in |
the solution can be |
the secure untrusted data |
their sequence in the |
solution can be shared |
secure untrusted data repository |
sequence in the data |
can be shared in |
in the data stream |
be shared in a |
shared in a file |
in a file or |
a file or via |
file or via email |
mb of small files |
or via email and |
via email and opened |
email and opened on |
as mentioned in section |
and opened on other |
mentioned in section iv |
opened on other machines |
this file system allows |
the users are immersed |
file system allows clients |
users are immersed in |
layered interleaving is unique |
system allows clients to |
are immersed in the |
interleaving is unique in |
allows clients to detect |
immersed in the resulting |
is unique in allowing |
clients to detect against |
in the resulting collaborative |
unique in allowing the |
to detect against malicious |
the resulting collaborative application |
in allowing the recovery |
detect against malicious or |
allowing the recovery latency |
against malicious or compromised |
the recovery latency of |
malicious or compromised storage |
they can interact with |
recovery latency of lost |
or compromised storage servers |
can interact with the |
latency of lost packets |
compromised storage servers or |
interact with the application |
of lost packets to |
storage servers or hosting |
with the application and |
all the files are |
lost packets to depend |
servers or hosting platforms |
the application and peers |
the files are in |
packets to depend on |
or hosting platforms by |
edu abstract data centers |
files are in a |
to depend on the |
application and peers see |
hosting platforms by providing |
abstract data centers are |
are in a single |
depend on the actual |
and peers see the |
platforms by providing fork |
data centers are the |
in a single file |
on the actual burst |
peers see the results |
by providing fork consistency |
centers are the backend |
a single file group |
the actual burst size |
see the results instantly |
are the backend for |
actual burst size experienced |
the backend for a |
a property which ensures |
backend for a large |
updates are applied to |
for a large number |
property which ensures that |
as opposed to the |
are applied to all |
a large number of |
which ensures that clients |
opposed to the maximum |
fetch runs as two |
applied to all replicas |
large number of services |
ensures that clients can |
to the maximum tolerable |
runs as two process |
to all replicas in |
number of services that |
that clients can detect |
the maximum tolerable burst |
all replicas in a |
replicas in a consistent |
clients can detect integrity |
maximum tolerable burst size |
of services that we |
in a consistent manner |
can detect integrity failures |
tolerable burst size as |
services that we take |
detect integrity failures as |
burst size as with |
that we take for |
integrity failures as long |
in contrast to today |
we take for granted |
size as with other |
failures as long as |
contrast to today s |
take for granted today |
as with other encoding |
as long as they |
to today s web |
with other encoding schemes |
long as they see |
today s web service |
as they see each |
which form a file |
s web service platforms |
a significant fraction of |
form a file group |
they see each other |
significant fraction of the |
c onclusion modern distributed |
see each other s |
each other s file |
onclusion modern distributed systems |
fraction of the total |
other s file modifications |
the other does the |
other does the same |
modern distributed systems are |
of the total cost |
similar techniques could be |
distributed systems are compelled |
p communication can coexist |
the total cost of |
techniques could be used |
systems are compelled by |
but without a file |
communication can coexist with |
total cost of ownership |
could be used to |
are compelled by real |
without a file group |
can coexist with more |
cost of ownership of |
be used to recover |
coexist with more standard |
of ownership of these |
simultaneous writeback executes in |
used to recover data |
with more standard solutions |
world imperatives to coordinate |
ownership of these large |
writeback executes in the |
to recover data from |
more standard solutions that |
imperatives to coordinate across |
executes in the same |
recover data from client |
standard solutions that reach |
to coordinate across data |
scale storage systems is |
in the same way |
data from client working |
solutions that reach back |
coordinate across data centers |
storage systems is the |
from client working copies |
systems is the cost |
across data centers separated |
that reach back to |
client working copies in |
but the second process |
is the cost of |
data centers separated by |
reach back to the |
working copies in the |
the second process writes |
the cost of keeping |
centers separated by thousands |
back to the hosted |
copies in the event |
second process writes the |
cost of keeping hundreds |
separated by thousands of |
to the hosted content |
in the event of |
process writes the files |
of keeping hundreds of |
by thousands of miles |
the hosted content and |
the event of a |
writes the files to |
keeping hundreds of thousands |
hosted content and trigger |
event of a catastrophic |
the files to the |
packet loss cripples the |
hundreds of thousands of |
content and trigger updates |
of a catastrophic cloud |
files to the server |
loss cripples the performance |
of thousands of disks |
and trigger updates at |
a catastrophic cloud failure |
to the server instead |
cripples the performance of |
thousands of disks spinning |
trigger updates at the |
the server instead of |
the performance of such |
updates at the associated |
once code repositories are |
server instead of reading |
performance of such systems |
at the associated data |
code repositories are stored |
we present a simple |
instead of reading them |
present a simple idea |
repositories are stored in |
a simple idea that |
the associated data centers |
simple idea that allows |
are stored in the |
idea that allows the |
and reliability and flow |
stored in the cloud |
the remaining tests investigate |
that allows the storage |
remaining tests investigate the |
when an application needs |
control protocols designed for |
allows the storage system |
one might imagine enabling |
tests investigate the overhead |
an application needs high |
protocols designed for lans |
designed for lans and |
might imagine enabling mashups |
investigate the overhead paid |
application needs high data |
needs high data rates |
imagine enabling mashups in |
the overhead paid for |
the storage system to |
or the commodity internet |
enabling mashups in ways |
overhead paid for weaknesses |
storage system to turn |
the commodity internet fail |
mashups in ways not |
paid for weaknesses in |
system to turn off |
it can use protocols |
in ways not previously |
ways not previously possible |
commodity internet fail to |
to turn off a |
can use protocols that |
for weaknesses in the |
internet fail to achieve |
turn off a large |
use protocols that bypass |
weaknesses in the prefetching |
off a large fraction |
web based code viewers |
protocols that bypass the |
in the prefetching algorithm |
optimal performance on the |
that bypass the data |
a large fraction of |
performance on the high |
bypass the data center |
and cross reference viewers |
large fraction of its |
the data center to |
cross reference viewers might |
fraction of its disks |
data center to achieve |
reference viewers might be |
haul lambda networks linking |
center to achieve the |
viewers might be built |
lambda networks linking data |
to achieve the full |
might be built by |
be built by third |
achieve the full performance |
networks linking data centers |
without incurring unacceptable performance |
the full performance of |
full performance of the |
performance of the network |
deploying new protocols is |
incurring unacceptable performance penalties |
pulling data from the |
new protocols is not |
data from the repositories |
this paper makes the |
protocols is not an |
from the repositories of |
paper makes the following |
is not an option |
the repositories of several |
makes the following contributions |
not an option for |
repositories of several distinct |
of several distinct communities |
an option for commodity |
of particular appeal is |
we describe a new |
option for commodity clusters |
particular appeal is the |
describe a new class |
for commodity clusters where |
appeal is the fact |
a new class of |
commodity clusters where standardization |
is the fact that |
new class of service |
clusters where standardization is |
the fact that our |
where standardization is critical |
kb files and forming |
fact that our solution |
seeks to enable such |
standardization is critical for |
files and forming its |
that our solution is |
to enable such applications |
is critical for cost |
and forming its own |
our solution is not |
applications that integrate service |
enable such applications by |
critical for cost mitigation |
forming its own file |
solution is not application |
that integrate service hosted |
such applications by granting |
its own file group |
integrate service hosted content |
maelstrom is an edge |
applications by granting direct |
service hosted content with |
is an edge appliance |
by granting direct access |
hosted content with peer |
an edge appliance that |
granting direct access of |
on its first iteration |
edge appliance that uses |
direct access of cloud |
appliance that uses forward |
access of cloud storage |
that uses forward error |
of cloud storage to |
savings for a very |
uses forward error correction |
the workload accesses the |
we analyze two important |
for a very generic |
cloud storage to third |
forward error correction to |
workload accesses the first |
analyze two important examples |
a very generic data |
storage to third parties |
error correction to mask |
accesses the first file |
two important examples of |
very generic data center |
subject to the data |
the first file in |
important examples of soc |
correction to mask packet |
generic data center model |
to the data owner |
first file in each |
examples of soc applications |
to mask packet loss |
the data owner s |
file in each directory |
mask packet loss from |
data owner s security |
search and rescue mission |
packet loss from endto |
owner s security requirements |
and rescue mission and |
rescue mission and virtual |
mission and virtual worlds |
we describe our solution |
a question that may |
question that may naturally |
that may naturally arise |
may naturally arise is |
ip throughput and latency |
throughput and latency by |
why not use a |
and latency by orders |
identify the parameters that |
not use a general |
latency by orders of |
we list the key |
the parameters that determine |
use a general purpose |
by orders of magnitude |
to provoke a large |
parameters that determine its |
a general purpose file |
list the key challenges |
orders of magnitude when |
provoke a large amount |
that determine its cost |
general purpose file system |
the key challenges that |
of magnitude when loss |
magnitude when loss occurs |
purpose file system interface |
key challenges that soc |
a large amount of |
maelstrom is easy to |
challenges that soc applications |
and present a simulator |
large amount of useless |
is easy to install |
file system interface to |
that soc applications place |
present a simulator that |
amount of useless prefetches |
easy to install and |
system interface to s |
soc applications place on |
a simulator that allows |
to install and deploy |
applications place on their |
simulator that allows us |
good order and bad |
place on their runtime |
that allows us to |
order and bad order |
on their runtime environments |
and is completely transparent |
allows us to explore |
and bad order investigate |
is completely transparent to |
us to explore this |
and store a repository |
bad order investigate the |
we describe a new |
completely transparent to applications |
to explore this parameter |
store a repository on |
a repository on that |
describe a new class |
transparent to applications and |
explore this parameter space |
order investigate the effect |
a new class of |
investigate the effect of |
this is indeed possible |
to applications and protocols |
new class of multi |
the effect of the |
is indeed possible to |
indeed possible to do |
we also present some |
layered mashups and contrast |
applications and protocols literally |
effect of the ordered |
also present some initial |
mashups and contrast them |
and protocols literally providing |
but would entail pushing |
of the ordered list |
present some initial simulation |
and contrast them with |
protocols literally providing reliability |
would entail pushing temporary |
the ordered list of |
some initial simulation results |
contrast them with more |
literally providing reliability in |
entail pushing temporary files |
ordered list of files |
initial simulation results that |
them with more traditional |
providing reliability in an |
pushing temporary files such |
list of files in |
simulation results that add |
reliability in an inexpensive |
temporary files such as |
of files in a |
results that add weight |
in an inexpensive box |
files such as transactions |
files in a file |
based approach to building |
approach to building mashups |
in a file group |
that add weight to |
add weight to our |
characteristic of today s |
weight to our claim |
and incurring additional monetary |
to our claim that |
of today s web |
our claim that our |
incurring additional monetary costs |
claim that our solution |
today s web development |
additional monetary costs due |
that our solution represents |
monetary costs due to |
our solution represents a |
costs due to the |
we discuss the relative |
solution represents a new |
due to the increased |
discuss the relative advantages |
represents a new powersaving |
to the increased number |
the relative advantages of |
a new powersaving opportunity |
the increased number of |
increased number of s |
relative advantages of these |
new powersaving opportunity for |
prefetching evaluation having added |
advantages of these two |
powersaving opportunity for large |
evaluation having added prefetching |
optical domain performance monitoring |
of these two approaches |
having added prefetching to |
there would also likely |
these two approaches for |
added prefetching to mfs |
would also likely be |
two approaches for building |
also likely be performance |
approaches for building soc |
likely be performance problems |
for building soc applications |
we evaluated whether such |
we discuss the advantages |
since file append and |
evaluated whether such a |
discuss the advantages of |
file append and rename |
introduction the declining costs |
whether such a straightforward |
the optical fiber communication |
the advantages of decoupling |
append and rename operations |
the declining costs of |
such a straightforward algorithm |
optical fiber communication conference |
advantages of decoupling transport |
and rename operations do |
declining costs of commodity |
a straightforward algorithm can |
of decoupling transport and |
rename operations do not |
costs of commodity disk |
straightforward algorithm can have |
decoupling transport and information |
operations do not map |
of commodity disk drives |
algorithm can have a |
transport and information layers |
do not map efficiently |
commodity disk drives has |
can have a benefit |
and information layers as |
not map efficiently to |
map efficiently to s |
have a benefit for |
information layers as a |
disk drives has made |
a benefit for some |
layers as a means |
drives has made online |
benefit for some repre |
as a means of |
has made online data |
a means of achieving |
means of achieving reusability |
made online data storage |
online data storage a |
fs that is aware |
order accesses the files |
data storage a way |
that is aware of |
accesses the files in |
storage a way of |
ability to rapidly deploy |
is aware of subversion |
the files in the |
a way of life |
to rapidly deploy soc |
aware of subversion s |
files in the group |
rapidly deploy soc applications |
of subversion s file |
in the group in |
deploy soc applications in |
subversion s file naming |
so much so that |
the group in the |
soc applications in new |
s file naming and |
much so that companies |
group in the same |
applications in new environments |
file naming and use |
so that companies like |
in the same order |
isn t quite enough |
in new environments and |
naming and use scenario |
that companies like google |
the same order as |
new environments and adapt |
and use scenario could |
companies like google and |
same order as the |
environments and adapt them |
use scenario could of |
like google and yahoo |
order as the list |
and adapt them dynamically |
scenario could of course |
google and yahoo host |
adapt them dynamically this |
could of course overcome |
and yahoo host hundreds |
them dynamically this work |
bad order accesses them |
of course overcome these |
yahoo host hundreds of |
dynamically this work was |
order accesses them in |
course overcome these limitations |
host hundreds of thousands |
this work was supported |
accesses them in reverse |
overcome these limitations by |
hundreds of thousands of |
them in reverse order |
these limitations by pushing |
of thousands of servers |
limitations by pushing only |
thousands of servers for |
by pushing only what |
of servers for storage |
pushing only what is |
only what is actually |
what is actually required |
is actually required into |
actually required into s |
qi huang is a |
huang is a visiting |
is a visiting scientist |
a visiting scientist from |
visiting scientist from the |
scientist from the school |
from the school of |
the school of computer |
school of computer sci |
but we believe that |
we believe that such |
believe that such specialized |
there is a catch |
that such specialized tools |
such specialized tools are |
specialized tools are better |
huazhong university of sci |
tools are better built |
are better built on |
better built on top |
built on top of |
on top of a |
a hundred thousand servers |
top of a file |
supported by the chinese |
by the chinese nsfc |
of a file system |
hundred thousand servers consume |
a file system abstraction |
thousand servers consume a |
file system abstraction than |
servers consume a lot |
where did my performance |
system abstraction than pushed |
consume a lot of |
a lot of power |
analysis of prefetching the |
abstraction than pushed underneath |
did my performance go |
of prefetching the graphs |
than pushed underneath it |
prefetching the graphs in |
not only does this |
the graphs in figure |
only does this translate |
rate limiting rears its |
does this translate to |
limiting rears its ugly |
this translate to many |
rears its ugly head |
c onclusion we have |
translate to many millions |
show the results of |
onclusion we have shown |
to many millions of |
the results of the |
we have shown that |
many millions of dollars |
results of the experiments |
have shown that the |
millions of dollars annually |
shown that the cost |
of dollars annually on |
that the cost of |
dollars annually on electricity |
the cost of using |
where a test such |
annually on electricity bills |
cost of using a |
a test such as |
of using a cloud |
test such as simultaneous |
using a cloud computing |
such as simultaneous demand |
the heat produced by |
a cloud computing storage |
heat produced by so |
cloud computing storage service |
produced by so much |
computing storage service for |
by so much computing |
storage service for source |
fetch incorporates more than |
so much computing power |
layered mashup to the |
incorporates more than one |
service for source code |
much computing power can |
mashup to the changing |
to the changing needs |
for source code repository |
computing power can be |
more than one workload |
source code repository hosting |
we discuss the resulting |
power can be searing |
code repository hosting is |
discuss the resulting objectoriented |
repository hosting is low |
the resulting objectoriented perspective |
only the elapsed time |
the elapsed time for |
elapsed time for the |
both for individual projects |
an article in the |
time for the foreground |
in which instances of |
for individual projects and |
article in the new |
for the foreground workload |
which instances of distributed |
individual projects and moderately |
in the new york |
instances of distributed communication |
projects and moderately sized |
the one accessing a |
of distributed communication protocols |
the new york times |
and moderately sized communities |
one accessing a file |
distributed communication protocols are |
new york times describes |
accessing a file group |
considering the costs of |
york times describes one |
communication protocols are modeled |
the costs of a |
times describes one of |
protocols are modeled uniformly |
costs of a resilient |
a cross layer study |
describes one of google |
in most of the |
of a resilient local |
cross layer study of |
are modeled uniformly as |
one of google s |
most of the microbenchmarks |
a resilient local storage |
layer study of packet |
modeled uniformly as objects |
of google s data |
resilient local storage system |
adding prefetching from the |
uniformly as objects similar |
google s data centers |
study of packet loss |
local storage system of |
prefetching from the file |
as objects similar to |
from the file groups |
storage system of scsi |
the file groups specified |
objects similar to those |
file groups specified has |
system of scsi disks |
groups specified has a |
similar to those in |
specified has a substantial |
of scsi disks and |
of packet loss in |
packet loss in all |
has a substantial improvement |
scsi disks and tape |
to those in java |
a computing center as |
a substantial improvement on |
disks and tape backup |
computing center as big |
substantial improvement on the |
center as big as |
improvement on the performance |
as big as two |
cloud computing is a |
on the performance of |
big as two football |
computing is a very |
the performance of the |
as two football fields |
is a very attractive |
performance of the workload |
the embedded script is |
a very attractive solution |
embedded script is often |
very attractive solution for |
script is often tightly |
attractive solution for this |
solution for this application |
is often tightly integrated |
with twin cooling plants |
varying with how amenable |
often tightly integrated with |
twin cooling plants protruding |
with how amenable it |
our implementation of s |
tightly integrated with backend |
cooling plants protruding four |
how amenable it is |
integrated with backend services |
vn brings this concept |
amenable it is to |
plants protruding four stories |
with backend services in |
brings this concept a |
it is to prefetching |
protruding four stories into |
backend services in the |
this concept a step |
four stories into the |
services in the data |
concept a step closer |
stories into the sky |
in the data center |
a step closer to |
step closer to becoming |
closer to becoming reality |
more surplus bandwidth and |
making it awkward to |
and provides evidence that |
surplus bandwidth and more |
it awkward to access |
provides evidence that performance |
bandwidth and more think |
awkward to access the |
evidence that performance will |
and more think time |
to access the underlying |
that performance will be |
more think time result |
access the underlying services |
performance will be acceptable |
think time result in |
the underlying services directly |
will be acceptable for |
time result in improved |
underlying services directly from |
be acceptable for typical |
result in improved performance |
services directly from a |
power conservation is an |
acceptable for typical use |
directly from a different |
conservation is an important |
for typical use scenarios |
from a different script |
this naturally means that |
is an important concern |
a different script or |
naturally means that the |
an important concern for |
different script or a |
means that the greatest |
important concern for big |
script or a standalone |
or a standalone client |
concern for big server |
that the greatest improvements |
for big server clusters |
the greatest improvements from |
greatest improvements from prefetching |
improvements from prefetching are |
the only way such |
journal of lightwave technology |
from prefetching are evident |
since disks account for |
only way such services |
prefetching are evident at |
disks account for a |
way such services can |
are evident at higher |
account for a significant |
such services can be |
technological impact of magnetic |
evident at higher bandwidths |
for a significant fraction |
services can be mashed |
impact of magnetic hard |
a significant fraction of |
can be mashed up |
of magnetic hard disk |
six out of eight |
be mashed up with |
significant fraction of the |
magnetic hard disk drives |
out of eight microbenchmarks |
mashed up with other |
fraction of the energy |
hard disk drives on |
of eight microbenchmarks run |
up with other web |
of the energy consumed |
disk drives on storage |
drives on storage systems |
with other web content |
eight microbenchmarks run at |
other web content is |
microbenchmarks run at least |
web content is by |
content is by either |
is by either having |
by either having the |
either having the data |
having the data center |
the data center compute |
data center compute the |
center compute the mashup |
so that it can |
that it can be |
it can be accessed |
can be accessed via |
be accessed via the |
accessed via the minibrowser |
several approaches for disk |
approaches for disk power |
faster when bandwidth is |
or by embedding the |
for disk power management |
by embedding the entire |
disk power management have |
embedding the entire minibrowser |
power management have been |
the entire minibrowser window |
management have been proposed |
entire minibrowser window in |
have been proposed and |
minibrowser window in a |
window in a web |
in a web page |
been proposed and studied |
but an embedded minibrowser |
an embedded minibrowser can |
embedded minibrowser can t |
minibrowser can t seamlessly |
can t seamlessly blend |
t seamlessly blend with |
seamlessly blend with the |
blend with the surrounding |
we will examine some |
with the surrounding content |
will examine some of |
examine some of these |
some of these here |
it is like a |
is like a standalone |
like a standalone browser |
a standalone browser within |
standalone browser within its |
browser within its own |
within its own frame |
but first let us |
and runs independent of |
first let us lay |
runs independent of the |
let us lay out |
independent of the rest |
us lay out some |
of the rest of |
the effects of systemic |
lay out some of |
the rest of the |
rest of the page |
out some of the |
effects of systemic packet |
at low bandwidth most |
some of the groundwork |
of systemic packet loss |
low bandwidth most workloads |
to illustrate this point |
systemic packet loss on |
bandwidth most workloads see |
packet loss on aggregate |
most workloads see no |
any disk power management |
loss on aggregate tcp |
workloads see no benefit |
disk power management scheme |
on aggregate tcp flows |
power management scheme essentially |
management scheme essentially attempts |
scheme essentially attempts to |
the figures are screenshots |
essentially attempts to exploit |
since all the bandwidth |
figures are screenshots of |
attempts to exploit one |
all the bandwidth is |
are screenshots of web |
to exploit one fact |
the bandwidth is dedicated |
screenshots of web applications |
bandwidth is dedicated to |
is dedicated to higher |
ieee conference on supercomputing |
with content from multiple |
content from multiple sources |
from multiple sources mashed |
disks can be run |
can be run in |
be run in highpower |
run in highpower mode |
was constructed using a |
constructed using a standard |
only two tests perform |
using a standard web |
a standard web services |
two tests perform worse |
standard web services approach |
tests perform worse with |
perform worse with prefetching |
worse with prefetching than |
pulling content from the |
content from the yahoo |
with prefetching than without |
with a corresponding performance |
maps and weather web |
a corresponding performance tradeoff |
and weather web services |
weather web services and |
web services and assembling |
services and assembling it |
and assembling it into |
assembling it into a |
it into a web |
into a web page |
a web page as |
web page as a |
page as a set |
as a set of |
a set of tiled |
set of tiled frames |
write test performs slightly |
test performs slightly worse |
performs slightly worse due |
each frame is a |
slightly worse due to |
a disk can be |
frame is a minibrowser |
worse due to its |
disk can be shut |
is a minibrowser with |
due to its already |
can be shut off |
a minibrowser with its |
to its already heavy |
be shut off so |
minibrowser with its own |
its already heavy network |
shut off so that |
with its own interactive |
already heavy network contention |
off so that it |
its own interactive controls |
so that it consumes |
that it consumes no |
it consumes no power |
and comes from a |
comes from a single |
the bad groups test |
end performance effects of |
from a single content |
performance effects of parallel |
a single content source |
effects of parallel tcp |
given a large cluster |
of parallel tcp sockets |
which exploits poor prefetching |
a large cluster of |
parallel tcp sockets on |
to illustrate one of |
exploits poor prefetching hints |
large cluster of disks |
tcp sockets on a |
illustrate one of the |
sockets on a lossy |
one of the many |
on a lossy wide |
of the many restrictions |
brewer s conjecture and |
s conjecture and the |
conjecture and the feasibility |
only a fraction of |
if the user pans |
a fraction of them |
and the feasibility of |
the user pans or |
fraction of them is |
performs when prefetching is |
the feasibility of consistent |
user pans or zooms |
of them is accessed |
when prefetching is used |
feasibility of consistent available |
pans or zooms in |
or zooms in the |
of consistent available partition |
them is accessed at |
zooms in the map |
in the map frame |
this effect is due |
is accessed at any |
effect is due to |
international parallel and distributed |
accessed at any time |
is due to the |
the associated map will |
in in acm sigact |
in acm sigact news |
associated map will shift |
map will shift or |
so that the rest |
due to the useless |
parallel and distributed processing |
will shift or zoom |
that the rest could |
to the useless prefetching |
and distributed processing symposium |
the rest could potentially |
but the other frames |
the useless prefetching rpcs |
rest could potentially be |
the other frames remain |
useless prefetching rpcs flooding |
could potentially be switched |
other frames remain as |
prefetching rpcs flooding the |
potentially be switched to |
frames remain as they |
rpcs flooding the outgoing |
be switched to a |
remain as they were |
flooding the outgoing link |
switched to a low |
as they were the |
the outgoing link and |
they were the frames |
outgoing link and imposing |
were the frames are |
the frames are not |
frames are not synchronized |
link and imposing minor |
and imposing minor delays |
imposing minor delays on |
minor delays on each |
delays on each demand |
on each demand fetch |
here we see a |
we see a similar |
see a similar application |
a similar application constructed |
since mode transitions consume |
similar application constructed using |
cumulatively these slow down |
mode transitions consume time |
application constructed using live |
constructed using live objects |
transitions consume time and |
the performance of tcp |
these slow down the |
consume time and power |
slow down the overall |
down the overall performance |
ip for networks with |
for networks with high |
content from different sources |
networks with high bandwidth |
from different sources is |
different sources is overlaid |
sources is overlaid in |
is overlaid in the |
disk management schemes have |
overlaid in the same |
delay products and random |
products and random loss |
management schemes have to |
in the same window |
an usual phenomenon is |
schemes have to walk |
the same window and |
usual phenomenon is that |
have to walk the |
filesystem backup to the |
phenomenon is that the |
acm transactions on networking |
same window and synchronized |
to walk the tightrope |
backup to the cloud |
is that the bad |
walk the tightrope of |
we used white backgrounds |
the tightrope of finding |
that the bad order |
used white backgrounds to |
tightrope of finding the |
the bad order test |
white backgrounds to highlight |
of finding the right |
bad order test consistently |
backgrounds to highlight the |
finding the right balance |
order test consistently outperforms |
to highlight the contributions |
the right balance between |
test consistently outperforms good |
highlight the contributions of |
right balance between power |
consistently outperforms good order |
the contributions of different |
contributions of different sources |
balance between power consumption |
between power consumption and |
power consumption and performance |
but there are no |
there are no frame |
are no frame boundaries |
even though the latter |
though the latter triggers |
the latter triggers prefetches |
the solution space explored |
elements of this mashup |
latter triggers prefetches in |
solution space explored thus |
triggers prefetches in the |
space explored thus far |
which can include map |
can include map layers |
explored thus far in |
prefetches in the correct |
thus far in the |
in the correct order |
far in the literature |
tables showing buildings or |
in the literature can |
showing buildings or points |
the literature can be |
buildings or points of |
or points of interest |
literature can be divided |
the explanation is that |
can be divided as |
be divided as follows |
icons representing severe weather |
representing severe weather reports |
the good order test |
good order test suffers |
order test suffers from |
exist layers within which |
test suffers from the |
layers within which the |
suffers from the fast |
within which the end |
from the fast linear |
which the end user |
the fast linear scan |
the end user can |
end user can easily |
user can easily navigate |
fast linear scan phenomenon |
linear scan phenomenon described |
scan phenomenon described in |
harnessing storage clouds for |
data can come from |
phenomenon described in section |
storage clouds for high |
can come from many |
clouds for high performance |
come from many kinds |
for high performance content |
from many kinds of |
high performance content delivery |
many kinds of we |
a simple model and |
kinds of we discuss |
simple model and its |
of we discuss our |
model and its empirical |
we discuss our live |
and its empirical validation |
discuss our live distributed |
our live distributed objects |
live distributed objects platform |
distributed objects platform as |
acm sigcomm computer communication |
objects platform as an |
sigcomm computer communication review |
platform as an example |
as an example of |
all prefetches in this |
an example of a |
prefetches in this test |
th international conference on |
example of a technology |
in this test conflict |
international conference on service |
of a technology that |
this test conflict with |
a technology that fits |
test conflict with demand |
technology that fits well |
conflict with demand fetches |
that fits well with |
fits well with the |
well with the layered |
componentized model we derived |
model we derived through |
we derived through our |
derived through our analysis |
at the start of |
the start of the |
start of the bad |
of the bad order |
the bad order test |
we compare performance of |
compare performance of hosted |
performance of hosted enterprise |
of hosted enterprise service |
hosted enterprise service bus |
the prefetching subsystem is |
prefetching subsystem is able |
subsystem is able to |
is able to prefetch |
able to prefetch some |
to prefetch some files |
prefetch some files accessed |
some files accessed at |
files accessed at the |
accessed at the end |
at the end of |
the end of the |
end of the test |
without conflicting with a |
peer communication protocols as |
conflicting with a demand |
communication protocols as an |
with a demand fetch |
protocols as an underlying |
as an underlying communication |
an underlying communication substrate |
underlying communication substrate for |
it can therefore achieve |
communication substrate for soc |
can therefore achieve a |
substrate for soc applications |
therefore achieve a greater |
achieve a greater speedup |
the relative strengths of |
relative strengths of each |
strengths of each of |
of each of the |
each of the solutions |
of the solutions tested |
the solutions tested and |
solutions tested and the |
tested and the lack |
and the lack of |
the lack of a |
lack of a clear |
of a clear winner |
a clear winner serve |
clear winner serve as |
winner serve as a |
serve as a further |
as a further justification |
a further justification for |
further justification for the |
justification for the decoupling |
for the decoupling of |
the decoupling of information |
decoupling of information and |
of information and transport |
information and transport layers |
and transport layers advocated |
transport layers advocated above |
each of these solutions |
of these solutions proposes |
these solutions proposes a |
limitations of the existing |
congestion control for high |
control for high bandwidth |
of the existing model |
the existing model there |
existing model there are |
model there are two |
there are two important |
an elastic transactional data |
are two important reasons |
two important reasons why |
elastic transactional data store |
transactional data store in |
important reasons why integrating |
reasons why integrating peerto |
data store in the |
store in the cloud |
solutions proposes a new |
peer collaboration with server |
proposes a new system |
a new system of |
hosted content is difficult |
new system of some |
system of some kind |
the first is not |
first is not strictly |
is not strictly limited |
not strictly limited to |
strictly limited to collaboration |
limited to collaboration and |
to collaboration and peer |
it is a general |
is a general weakness |
a general weakness of |
effective erasure codes for |
general weakness of the |
erasure codes for reliable |
weakness of the current |
codes for reliable computer |
the chubby lock service |
of the current web |
based solutions propose novel |
chubby lock service for |
for reliable computer communication |
the current web mashup |
lock service for loosely |
solutions propose novel storage |
reliable computer communication protocols |
current web mashup technologies |
propose novel storage hierarchies |
acm sigcomm computer communication |
web mashup technologies that |
novel storage hierarchies to |
sigcomm computer communication review |
mashup technologies that makes |
technologies that makes it |
storage hierarchies to strike |
that makes it hard |
makes it hard to |
hierarchies to strike the |
it hard to seamlessly |
th conference on usenix |
conference on usenix symposium |
hard to seamlessly integrate |
to seamlessly integrate data |
on usenix symposium on |
usenix symposium on operating |
seamlessly integrate data from |
integrate data from several |
symposium on operating systems |
on operating systems design |
data from several different |
from several different sources |
operating systems design and |
systems design and implementation |
to strike the right |
the web developers community |
strike the right balance |
web developers community has |
developers community has slowly |
the right balance between |
community has slowly converged |
right balance between performance |
has slowly converged towards |
balance between performance and |
slowly converged towards service |
between performance and power |
converged towards service platforms |
performance and power consumption |
towards service platforms that |
service platforms that export |
platforms that export autonomous |
that export autonomous interactive |
export autonomous interactive components |
autonomous interactive components to |
interactive components to their |
components to their clients |
in the form of |
the form of what |
form of what we |
of what we ll |
what we ll call |
we ll call minibrowser |
ll call minibrowser interfaces |
number of rpcs by |
of rpcs by type |
disk management solutions interject |
rpcs by type in |
a minibrowser is an |
by type in bandwidth |
type in bandwidth variability |
minibrowser is an interactive |
is an interactive web |
in bandwidth variability test |
management solutions interject a |
on the feasibility of |
the entries under p |
an interactive web page |
the feasibility of software |
feasibility of software fec |
entries under p denote |
interactive web page with |
web page with embedded |
under p denote periods |
universita di pisa deit |
page with embedded script |
p denote periods in |
denote periods in the |
periods in the test |
solutions interject a new |
di pisa deit technical |
pisa deit technical report |
deit technical report lr |
interject a new disk |
gives the abbreviations for |
the abbreviations for rpc |
abbreviations for rpc types |
a new disk management |
optimized for displaying a |
are likely to be |
likely to be beneficial |
for displaying a single |
displaying a single type |
a single type of |
single type of content |
the first would reduce |
first would reduce the |
would reduce the aggressiveness |
reduce the aggressiveness of |
for example interactive maps |
the aggressiveness of prefetching |
new disk management layer |
example interactive maps from |
disk management layer on |
interactive maps from google |
management layer on top |
setting a byte threshold |
layer on top of |
maps from google earth |
from a file group |
from google earth or |
a file group if |
on top of the |
google earth or virtual |
file group if it |
top of the file |
earth or virtual earth |
group if it appeared |
of the file system |
if it appeared that |
it appeared that a |
appeared that a process |
that a process was |
a process was not |
our example actually overlays |
process was not using |
example actually overlays weather |
was not using the |
actually overlays weather from |
not using the files |
overlays weather from google |
using the files prefetched |
the case for packet |
weather from google on |
the files prefetched based |
case for packet level |
which controls disk configuration |
files prefetched based on |
controls disk configuration and |
for packet level fec |
disk configuration and data |
prefetched based on its |
configuration and data layout |
from google on terrain |
based on its prior |
in fifth international workshop |
fifth international workshop on |
google on terrain maps |
on its prior accesses |
and data layout to |
international workshop on protocols |
on terrain maps from |
data layout to achieve |
workshop on protocols for |
this would reduce the |
terrain maps from microsoft |
layout to achieve power |
on protocols for high |
would reduce the overhead |
maps from microsoft s |
reduce the overhead in |
from microsoft s virtual |
the overhead in the |
microsoft s virtual earth |
overhead in the bad |
s virtual earth platform |
in the bad groups |
virtual earth platform and |
the bad groups case |
earth platform and extracts |
platform and extracts census |
and extracts census data |
optimal disk access patterns |
extracts census data from |
the second would explicitly |
census data from the |
second would explicitly detect |
data from the us |
would explicitly detect a |
from the us census |
explicitly detect a fast |
the us census bureau |
detect a fast linear |
a fast linear scan |
fast linear scan by |
linear scan by a |
scan by a process |
the lion coexists with |
lion coexists with the |
coexists with the lamb |
caching solutions devise new |
by counting the instances |
the second problem is |
second problem is that |
counting the instances of |
solutions devise new power |
problem is that with |
the instances of prefetch |
is that with the |
instances of prefetch and |
that with the traditional |
of prefetch and demand |
with the traditional style |
prefetch and demand fetch |
the traditional style of |
and demand fetch conflict |
traditional style of web |
demand fetch conflict for |
style of web development |
fetch conflict for a |
conflict for a file |
for a file group |
aware caching algorithms that |
content is assumed to |
and then disable prefetching |
is assumed to be |
then disable prefetching from |
assumed to be fetched |
to be fetched from |
disable prefetching from the |
prefetching from the group |
be fetched from a |
fetched from a server |
caching algorithms that allow |
either directly over http |
algorithms that allow large |
lateral error correction for |
error correction for time |
that allow large fractions |
or by interacting with |
the dangers of replication |
prefetching and bandwidth variability |
by interacting with a |
interacting with a web |
with a web service |
and bandwidth variability so |
bandwidth variability so far |
dangers of replication and |
of replication and a |
replication and a solution |
web pages downloaded by |
pages downloaded by clients |
our experimental results have |
downloaded by clients browsers |
experimental results have demonstrated |
by clients browsers contain |
clients browsers contain embedded |
results have demonstrated the |
allow large fractions of |
browsers contain embedded addresses |
have demonstrated the benefits |
fourth usenix symposium on |
contain embedded addresses of |
embedded addresses of specific |
demonstrated the benefits of |
usenix symposium on networked |
symposium on networked systems |
addresses of specific servers |
the benefits of mfs |
large fractions of the |
on networked systems design |
benefits of mfs adaptation |
technologies such as ajax |
networked systems design and |
systems design and implementation |
of mfs adaptation mechanisms |
acm sigmod international conference |
such as ajax allow |
as ajax allow for |
mfs adaptation mechanisms at |
sigmod international conference on |
international conference on management |
ajax allow for asynchronous |
adaptation mechanisms at various |
mechanisms at various levels |
conference on management of |
on management of data |
at various levels of |
various levels of bandwidth |
levels of bandwidth availability |
fractions of the storage |
of the storage system |
but not when the |
the storage system to |
not when the bandwidth |
storage system to remain |
but traffic is still |
system to remain idle |
when the bandwidth is |
traffic is still always |
the bandwidth is changing |
bandwidth is changing over |
is still always routed |
still always routed through |
is changing over the |
to remain idle for |
always routed through a |
remain idle for longer |
changing over the duration |
routed through a data |
through a data center |
over the duration of |
the duration of the |
duration of the test |
idle for longer periods |
the clients don t |
clients don t talk |
don t talk to |
to conclude this section |
for longer periods of |
t talk to one |
conclude this section we |
this section we will |
section we will describe |
longer periods of time |
talk to one another |
we will describe an |
will describe an example |
describe an example of |
an example of mfs |
example of mfs traffic |
of mfs traffic under |
mfs traffic under the |
traffic under the execution |
live objects allow visual |
under the execution of |
objects allow visual content |
the execution of the |
allow visual content and |
execution of the simultaneous |
visual content and update |
of the simultaneous writeback |
content and update events |
and update events to |
the simultaneous writeback test |
simultaneous writeback test described |
update events to be |
events to be communicated |
writeback test described in |
test described in section |
to be communicated using |
be communicated using any |
communicated using any sort |
using any sort of |
any sort of protocol |
allowing them to be |
an integrated experimental environment |
integrated experimental environment for |
experimental environment for distributed |
environment for distributed systems |
for distributed systems and |
this test involves two |
distributed systems and networks |
but also overlay multicast |
test involves two simultaneous |
involves two simultaneous workloads |
them to be switched |
to be switched to |
be switched to lower |
switched to lower power |
kb to the server |
to the server and |
even a custom protocol |
a custom protocol designed |
the server and the |
server and the other |
custom protocol designed by |
protocol designed by the |
and the other reads |
fifth usenix symposium on |
usenix symposium on operating |
designed by the content |
by the content provider |
symposium on operating systems |
on operating systems design |
operating systems design and |
kb files from the |
systems design and implementation |
to lower power modes |
this makes it possible |
files from the server |
makes it possible to |
it possible to achieve |
possible to achieve extremely |
to achieve extremely high |
achieve extremely high levels |
but is slightly modified |
extremely high levels of |
is slightly modified from |
high levels of throughput |
slightly modified from original |
levels of throughput and |
modified from original version |
of throughput and latency |
from original version to |
original version to use |
version to use a |
to use a longer |
use a longer think |
it also enhances security |
a longer think time |
longer think time of |
the principal contribution of |
the data center server |
data center server can |
principal contribution of this |
center server can t |
server can t see |
contribution of this paper |
can t see data |
t see data exchanged |
of this paper is |
seconds when accessing each |
see data exchanged directly |
when accessing each file |
this paper is to |
improving the potential for |
data exchanged directly between |
the potential for rpcs |
potential for rpcs to |
for rpcs to overlap |
paper is to argue |
exchanged directly between peers |
is to argue that |
to argue that there |
we enabled asynchronous writeback |
argue that there is |
the above discussion motivates |
that there is a |
physical layer impact upon |
there is a fourth |
above discussion motivates our |
enabled asynchronous writeback and |
layer impact upon packet |
impact upon packet errors |
discussion motivates our problem |
asynchronous writeback and ran |
writeback and ran the |
motivates our problem statement |
is a fourth niche |
and ran the test |
allow web applications to |
a fourth niche as |
ran the test with |
web applications to overlay |
fourth niche as yet |
the test with the |
applications to overlay content |
to overlay content from |
passive and active measurement |
and active measurement workshop |
test with the synthetic |
overlay content from multiple |
content from multiple sources |
secure untrusted data repository |
with the synthetic bandwidth |
the synthetic bandwidth trace |
from multiple sources in |
multiple sources in a |
synthetic bandwidth trace shown |
bandwidth trace shown in |
sources in a layered |
in a layered fashion |
trace shown in figure |
niche as yet unexplored |
such that the distinct |
that the distinct content |
the distinct content layers |
distinct content layers share |
content layers share a |
layers share a single |
share a single view |
a single view and |
single view and remain |
view and remain well |
and remain well synchronized |
which changes the bandwidth |
th conference on symposium |
changes the bandwidth once |
conference on symposium on |
the bandwidth once per |
on symposium on opearting |
bandwidth once per second |
symposium on opearting systems |
on opearting systems design |
or panning should cause |
panning should cause all |
should cause all layers |
this has three sections |
cause all layers to |
all layers to respond |
layers to respond simultaneously |
a brief period when |
brief period when the |
period when the bandwidth |
when the bandwidth is |
and an update in |
the bandwidth is at |
an update in any |
update in any of |
in any of the |
any of the layers |
of the layers should |
the layers should be |
layers should be reflected |
should be reflected in |
be reflected in all |
reflected in all other |
in all other layers |
allow updates to be |
updates to be carried |
to be carried by |
be carried by the |
carried by the protocol |
a gradual decrease to |
by the protocol best |
the protocol best matched |
protocol best matched to |
best matched to the |
matched to the setting |
to the setting in |
the setting in which |
setting in which the |
in which the application |
which the application is |
the application is used |
s over the course |
over the course of |
the course of ten |
course of ten seconds |
the solutions discussed here |
we do not present |
solutions discussed here are |
and then the maintenance |
then the maintenance of |
discussed here are based |
here are based on |
the maintenance of the |
do not present a |
are based on live |
based on live objects |
not present a new |
present a new system |
s rate until the |
rate until the end |
until the end of |
the end of the |
end of the test |
new types of components |
types of components must |
of components must be |
components must be created |
must be created for |
be created for each |
created for each type |
for each type of |
each type of content |
summary of results the |
of results the test |
results the test was |
the test was executed |
test was executed once |
but the existing collection |
was executed once with |
communal data sharing in |
the existing collection of |
executed once with prefetching |
data sharing in public |
existing collection of components |
once with prefetching enabled |
sharing in public clouds |
we take an idea |
and despite the simplicity |
collection of components provides |
despite the simplicity of |
take an idea that |
of components provides access |
the university of illinois |
the simplicity of the |
simplicity of the mfs |
components provides access to |
university of illinois national |
of illinois national center |
of the mfs prefetching |
provides access to several |
an idea that has |
illinois national center for |
the mfs prefetching implementation |
access to several different |
to several different types |
national center for supercomputing |
center for supercomputing applications |
several different types of |
different types of web |
once with no prefetching |
idea that has been |
types of web services |
of web services hosted |
and the rpcs were |
that has been around |
web services hosted content |
the rpcs were then |
rpcs were then divided |
has been around for |
were then divided acwe |
been around for well |
including all the examples |
around for well over |
then divided acwe have |
all the examples given |
the examples given above |
divided acwe have shown |
acwe have shown that |
have shown that workloads |
shown that workloads which |
that workloads which are |
workloads which are amenable |
which are amenable to |
are amenable to file |
for well over a |
the resulting live application |
level cording to which |
resulting live application is |
live application is stored |
cording to which period |
to which period of |
application is stored as |
global crossing current network |
crossing current network performance |
which period of the |
is stored as an |
stored as an xml |
as an xml file |
well over a decade |
period of the trace |
the file can be |
over a decade now |
of the trace they |
file can be moved |
the trace they terminated |
can be moved about |
trace they terminated in |
be moved about and |
moved about and even |
about and even embedded |
and even embedded in |
even embedded in email |
for each prefetching can |
each prefetching can achieve |
prefetching can achieve speedups |
can achieve speedups of |
users that open it |
that open it find |
open it find themselves |
it find themselves immersed |
find themselves immersed into |
themselves immersed into the |
immersed into the application |
several transport protocols optimized |
transport protocols optimized for |
protocols optimized for various |
optimized for various settings |
for various settings are |
various settings are or |
settings are or will |
are or will be |
or will be available |
four quantities are calculated |
will be available in |
be available in a |
available in a near |
in a near future |
the time spent queued |
time spent queued for |
spent queued for as |
queued for as much |
for as much as |
including support for wan |
support for wan networks |
for wan networks with |
wan networks with nats |
networks with nats and |
with nats and firewalls |
at bandwidths as low |
bandwidths as low as |
qwest ip network statistics |
prefetching both the rpc |
both the rpc request |
the rpc request and |
rpc request and reply |
and the time taken |
the time taken for |
time taken for each |
taken for each to |
for each to be |
each to be carries |
to be carries a |
be carries a small |
carries a small performance |
a small performance overhead |
even when performed at |
when performed at received |
high throughput and very |
throughput and very large |
and very large numbers |
very large numbers of |
from the first to |
large numbers of nodes |
the first to the |
first to the last |
to the last packet |
this ignores the time |
ignores the time the |
the time the lowest |
time the lowest priority |
which can reduce its |
can reduce its effectiveness |
reduce its effectiveness for |
its effectiveness for fast |
effectiveness for fast lin |
and argue that technological |
spent at the server |
at the server servicing |
argue that technological evolution |
the server servicing the |
large numbers of irregularly |
server servicing the rpc |
that technological evolution has |
numbers of irregularly overlapping |
vice president of research |
of irregularly overlapping multicast |
irregularly overlapping multicast groups |
trip time ear scan |
time ear scan workloads |
president of research and |
of research and t |
technological evolution has given |
it is possible to |
is possible to construct |
evolution has given it |
possible to construct combination |
has given it a |
to construct combination of |
given it a new |
construct combination of file |
it a new relevance |
combination of file between |
a new relevance today |
and strong reliability properties |
new relevance today as |
of file between the |
file between the client |
between the client and |
the client and the |
software defined networks and |
client and the server |
relevance today as a |
defined networks and gossip |
today as a natural |
but these quantities are |
networks and gossip protocols |
as a natural power |
these quantities are small |
and gossip protocols robert |
quantities are small groups |
gossip protocols robert soule |
are small groups and |
protocols robert soule ken |
small groups and a |
robert soule ken birman |
groups and a workload |
soule ken birman nate |
and a workload for |
ken birman nate foster |
a workload for which |
birman nate foster university |
workload for which prefetching |
nate foster university of |
for which prefetching can |
before saying more about |
foster university of lugano |
which prefetching can significantly |
prefetching can significantly compared |
saying more about our |
university of lugano cornell |
saving opportunity for large |
can significantly compared to |
more about our approach |
of lugano cornell university |
significantly compared to the |
lugano cornell university cornell |
compared to the other |
to the other costs |
cornell university cornell university |
we analyze a concrete |
university cornell university the |
analyze a concrete example |
cornell university the performance |
university the performance of |
a concrete example of |
these values are added |
the performance of data |
concrete example of a |
values are added up |
example of a soc |
are added up for |
of a soc application |
added up for each |
center applications are critically |
a soc application more |
up for each degrade |
applications are critically dependent |
soc application more carefully |
acm transactions on networking |
are critically dependent on |
for each degrade performance |
application more carefully to |
more carefully to expose |
critically dependent on the |
dependent on the underlying |
of the rpcs within |
the rpcs within a |
carefully to expose the |
on the underlying network |
the key insight is |
rpcs within a particular |
key insight is that |
to expose the full |
within a particular period |
given the complexities associated |
expose the full range |
the complexities associated with |
the full range of |
complexities associated with management |
and the results are |
full range of needs |
the results are shown |
range of needs and |
networks today typically provide |
results are shown within |
of needs and issues |
today typically provide little |
are shown within the |
needs and issues that |
and issues that arise |
typically provide little more |
shown within the constraints |
within the constraints imposed |
provide little more than |
little more than best |
the constraints imposed by |
consider a rescue mission |
a rescue mission coordinator |
constraints imposed by our |
effort packet delivery between |
imposed by our file |
by our file group |
packet delivery between hosts |
where other solutions attempt |
our file group representa |
a police or fire |
the emergence of software |
other solutions attempt to |
police or fire chief |
or fire chief coordinating |
solutions attempt to predict |
fire chief coordinating teams |
has created an opportunity |
attempt to predict disk |
chief coordinating teams who |
created an opportunity to |
to predict disk access |
coordinating teams who will |
predict disk access to |
an opportunity to build |
teams who will enter |
opportunity to build more |
disk access to determine |
who will enter a |
access to determine which |
to build more dynamic |
to determine which disks |
will enter a disaster |
build more dynamic networks |
determine which disks to |
enter a disaster zone |
more dynamic networks that |
which disks to power |
a disaster zone in |
dynamic networks that can |
the main conclusion we |
disks to power down |
disaster zone in the |
networks that can be |
main conclusion we draw |
zone in the wake |
conclusion we draw from |
that can be tailored |
we draw from the |
in the wake of |
can be tailored precisely |
draw from the test |
the wake of a |
be tailored precisely to |
from the test cases |
wake of a catastrophe |
tailored precisely to the |
the test cases exhibitthe |
of a catastrophe to |
precisely to the needs |
a method for improving |
test cases exhibitthe graphs |
a catastrophe to help |
to the needs of |
the needs of applications |
method for improving tcp |
cases exhibitthe graphs show |
catastrophe to help survivors |
the lfs automatically provides |
for improving tcp performance |
lfs automatically provides a |
exhibitthe graphs show how |
automatically provides a perfect |
existing solutions for monitoring |
improving tcp performance over |
graphs show how priorities |
provides a perfect prediction |
solutions for monitoring within |
tcp performance over wireless |
show how priorities affect |
a perfect prediction mechanism |
for monitoring within sdns |
performance over wireless links |
how priorities affect rpcs |
monitoring within sdns suffer |
priorities affect rpcs and |
within sdns suffer from |
affect rpcs and how |
and move supplies as |
sdns suffer from several |
rpcs and how prefetching |
move supplies as needed |
suffer from several short |
and how prefetching a |
how prefetching a prefetch |
prefetching a prefetch penalty |
a prefetch penalty is |
simply by virtue of |
prefetch penalty is that |
by virtue of the |
either they are inaccurate |
penalty is that the |
is that the implementation |
that the implementation could |
would arrive on the |
due to eventual consistency |
virtue of the fact |
the implementation could be |
arrive on the scene |
nd ieee wireless communications |
to eventual consistency of |
eventual consistency of architecture |
implementation could be im |
build a new collaboration |
a new collaboration tool |
ieee wireless communications and |
wireless communications and networking |
communications and networking conference |
ing changes mfs behaviour |
and distribute it to |
of the fact that |
distribute it to his |
in all three time |
all three time periods |
the fact that all |
fact that all write |
each team member would |
more time proved to |
team member would carry |
time proved to incorporate |
member would carry a |
due to limitations of |
proved to incorporate a |
would carry a tablet |
to limitations of current |
to incorporate a mechanism |
limitations of current hardware |
incorporate a mechanism to |
a mechanism to inhibit |
style device with wireless |
mechanism to inhibit prefetching |
device with wireless communication |
with wireless communication capabilities |
accesses go to the |
go to the log |
the is spent on |
the application built by |
to the log head |
is spent on rpcs |
application built by the |
spent on rpcs to |
or too costly to |
built by the coordinator |
on rpcs to fetch |
too costly to be |
by the coordinator would |
rpcs to fetch file |
costly to be practical |
the coordinator would be |
to fetch file attributes |
to be practical at |
coordinator would be installed |
fetch file attributes with |
be practical at scale |
would be installed on |
an adaptive forward error |
file attributes with prefetching |
be installed on each |
adaptive forward error correction |
due to reliance on |
attributes with prefetching enabled |
explains and expands on |
forward error correction protocol |
to reliance on switch |
installed on each team |
with prefetching enabled current |
and expands on this |
error correction protocol for |
reliance on switch forwarding |
on each team member |
prefetching enabled current prefetching |
expands on this idea |
correction protocol for end |
on switch forwarding rules |
each team member s |
enabled current prefetching algorithm |
switch forwarding rules and |
team member s mobile |
current prefetching algorithm does |
forwarding rules and centralization |
end transport of real |
prefetching algorithm does not |
member s mobile device |
algorithm does not correlate |
does not correlate file |
not correlate file accesses |
correlate file accesses with |
file accesses with than |
accesses with than without |
and in the offices |
in the offices in |
the offices in mission |
offices in mission headquarters |
since the time to |
the time to receive |
time to receive a |
to receive a fetch |
the coordinator would then |
coordinator would then deploy |
would then deploy teams |
then deploy teams in |
we argue that gossip |
attributes request the processes |
deploy teams in the |
argue that gossip protocols |
request the processes which |
teams in the field |
that gossip protocols offer |
the processes which make |
gossip protocols offer an |
processes which make them |
protocols offer an ideal |
our rescue workers now |
offer an ideal alternative |
th international conference on |
rescue workers now use |
an ideal alternative for |
but if this were |
if this were done |
workers now use the |
ideal alternative for sdn |
international conference on computer |
conference on computer communications |
two changes or reply |
alternative for sdn monitoring |
idea overview to see |
now use the solution |
on computer communications and |
changes or reply is |
or reply is negligible |
overview to see why |
due to their scalability |
computer communications and networks |
use the solution to |
to their scalability and |
their scalability and resiliency |
the increased time is |
the solution to coordinate |
solution to coordinate and |
increased time is due |
ignored the crucial monitoring |
to coordinate and prioritize |
time is due to |
is due to a |
the crucial monitoring component |
coordinate and prioritize actions |
to see why lfs |
due to a greater |
crucial monitoring component that |
inform each other of |
each other of the |
to a greater queue |
monitoring component that aggregates |
component that aggregates network |
other of the evolving |
of the evolving situation |
that aggregates network and |
aggregates network and application |
network and application state |
see why lfs is |
rpc times at intermediate |
steer clear of hazards |
why lfs is a |
times at intermediate bandwidth |
lfs is a natural |
and sends the events |
is a natural solution |
sends the events to |
as new events occur |
a natural solution to |
the events to the |
natural solution to the |
the situational status would |
events to the controller |
solution to the problem |
situational status would evolve |
to the problem of |
a complete system would |
complete system would have |
and the team member |
system would have a |
would have a closed |
the team member who |
the problem of disk |
have a closed loop |
team member who causes |
based loss recovery for |
loss recovery for reliable |
member who causes or |
continuously monitoring applications and |
recovery for reliable multicast |
for reliable multicast transmission |
who causes or observes |
monitoring applications and the |
applications and the network |
causes or observes these |
problem of disk power |
or observes these status |
of disk power management |
observes these status changes |
then adjusting sdn policies |
these status changes would |
adjusting sdn policies to |
status changes would need |
sdn policies to optimize |
changes would need to |
policies to optimize the |
would need to report |
to optimize the use |
need to report them |
optimize the use of |
to report them to |
the use of resources |
consider some of the |
report them to the |
them to the others |
some of the challenges |
of the challenges involved |
gossip protocols are an |
protocols are an ideal |
are an ideal choice |
an ideal choice for |
removing debris blocking access |
ideal choice for implementing |
debris blocking access to |
choice for implementing a |
blocking access to a |
for implementing a wide |
access to a building |
implementing a wide range |
to a building may |
a wide range monitoring |
a building may enable |
wide range monitoring tasks |
building may enable the |
may enable the team |
enable the team to |
the team to check |
team to check it |
with a gossip protocol |
to check it for |
check it for victims |
server systems typically are |
each node exchanges information |
and fire that breaks |
node exchanges information with |
systems typically are not |
end performance evaluation of |
exchanges information with a |
fire that breaks out |
typically are not idle |
information with a randomly |
are not idle long |
that breaks out in |
not idle long enough |
with a randomly selected |
breaks out in a |
idle long enough to |
a randomly selected peer |
long enough to make |
out in a chemical |
randomly selected peer at |
selected peer at periodic |
in a chemical storage |
enough to make it |
peer at periodic intervals |
a chemical storage warehouse |
to make it worthwhile |
chemical storage warehouse may |
make it worthwhile to |
because it is based |
storage warehouse may force |
warehouse may force diversion |
it is based on |
is based on periodic |
may force diversion of |
force diversion of resources |
based on periodic peer |
it worthwhile to incur |
th symposium on high |
as rescue workers capture |
symposium on high performance |
on high performance interconnects |
rescue workers capture information |
worthwhile to incur the |
to incur the time |
gossip s network load |
their mobile devices send |
s network load tends |
mobile devices send updates |
network load tends to |
devices send updates that |
load tends to be |
send updates that must |
tends to be well |
updates that must be |
that must be propagated |
must be propagated in |
be propagated in real |
power expense of switching |
expense of switching the |
scaling linearly with system |
of switching the disk |
linearly with system size |
having defined the scenario |
switching the disk to |
with system size and |
system size and not |
now let s analyze |
size and not prone |
and not prone to |
let s analyze in |
the disk to a |
not prone to reactive |
s analyze in more |
disk to a lowpower |
prone to reactive feedback |
analyze in more detail |
to a lowpower mode |
in more detail the |
more detail the requirements |
u trsu t u |
detail the requirements it |
trsu t u trsu |
because peers are selected |
the requirements it places |
t u trsu t |
peers are selected randomly |
requirements it places on |
u trsu t utrsut |
it places on our |
and switching it back |
places on our collaboration |
end forward error correction |
no single node is |
single node is indispensable |
on our collaboration tool |
request queued request send |
switching it back when |
so tools built on |
queued request send reply |
it back when it |
tools built on gossip |
request send reply queued |
back when it is |
the collaboration tool pulls |
send reply queued reply |
built on gossip are |
reply queued reply send |
collaboration tool pulls data |
tool pulls data from |
international zurich seminar on |
zurich seminar on communications |
on gossip are extremely |
pulls data from many |
data from many kinds |
gossip are extremely tolerant |
when it is accessed |
from many kinds of |
are extremely tolerant to |
many kinds of sources |
extremely tolerant to disruptions |
tolerant to disruptions and |
to disruptions and able |
disruptions and able to |
and able to rapidly |
it makes far more |
able to rapidly recover |
makes far more sense |
to rapidly recover from |
far more sense to |
this is a notable |
rapidly recover from failures |
more sense to imagine |
sense to imagine that |
to imagine that weather |
imagine that weather information |
although individual gossip protocols |
individual gossip protocols are |
gossip protocols are typically |
protocols are typically very |
are typically very simple |
is a notable point |
a notable point of |
composing multiple protocols can |
notable point of difference |
multiple protocols can lead |
point of difference between |
protocols can lead to |
can lead to complex |
of difference between server |
lead to complex interactions |
difference between server systems |
to complex interactions with |
messages and alerts come |
between server systems and |
complex interactions with unpredictable |
and alerts come from |
server systems and typical |
interactions with unpredictable behavior |
alerts come from a |
systems and typical mobile |
come from a dozen |
we designed the mica |
from a dozen providers |
and typical mobile device |
a dozen providers than |
the case for application |
typical mobile device scenarios |
dozen providers than to |
providers than to assume |
level network striping for |
than to assume that |
framework to address this |
network striping for data |
to assume that one |
to address this problem |
striping for data intensive |
assume that one organization |
for data intensive applications |
that one organization would |
data intensive applications using |
mica allows programmers to |
one organization would be |
intensive applications using high |
allows programmers to describe |
organization would be hosting |
applications using high speed |
programmers to describe gossip |
would be hosting services |
using high speed wide |
to describe gossip protocols |
be hosting services with |
high speed wide area |
describe gossip protocols with |
hosting services with everything |
speed wide area networks |
gossip protocols with a |
services with everything we |
protocols with a small |
with everything we need |
everything we need in |
we need in one |
which makes it hard |
need in one place |
makes it hard to |
and compose the protocols |
it hard to translate |
compose the protocols with |
hard to translate the |
data from distinct sources |
to translate the solutions |
ieee conference on supercomputing |
translate the solutions devised |
from distinct sources could |
the solutions devised for |
the protocols with a |
distinct sources could have |
solutions devised for mobile |
protocols with a rich |
rpc times at high |
sources could have different |
devised for mobile devices |
with a rich collection |
times at high bandwidth |
could have different format |
for mobile devices to |
a rich collection of |
have different format and |
mobile devices to server |
rich collection of operators |
different format and one |
devices to server systems |
collection of operators to |
format and one will |
of operators to create |
and one will often |
operators to create sophisticated |
one will often need |
to create sophisticated protocols |
will often need to |
create sophisticated protocols in |
often need to interface |
sophisticated protocols in a |
as we shall see |
need to interface to |
protocols in a modular |
to interface to each |
in a modular style |
interface to each using |
to each using its |
each using its own |
using its own protocols |
tsunami file transfer protocol |
its own protocols and |
mica ensures that the |
own protocols and interfaces |
ensures that the composed |
that the composed protocols |
the composed protocols maintain |
composed protocols maintain strong |
as conditions evolve the |
access to a small |
conditions evolve the team |
robustness and convergence guarantees |
to a small subset |
evolve the team might |
the team might need |
team might need to |
first international workshop on |
in our evaluation of |
our evaluation of mica |
might need to be |
international workshop on protocols |
workshop on protocols for |
need to be modify |
we have built monitoring |
on protocols for fast |
to be modify the |
be modify the application |
have built monitoring tasks |
protocols for fast long |
a small subset of |
for example adding new |
small subset of disks |
built monitoring tasks that |
example adding new types |
monitoring tasks that maintain |
adding new types of |
tasks that maintain a |
new types of information |
that maintain a predictable |
maintain a predictable performance |
changing the way it |
the way it is |
way it is represented |
even when hundreds of |
when hundreds of separate |
hundreds of separate instances |
of separate instances are |
separate instances are deployed |
or even modifying the |
instances are deployed on |
even modifying the way |
are deployed on the |
modifying the way team |
deployed on the same |
the way team members |
on the same machines |
when combined with a |
way team members communicate |
combined with a cache |
with a cache that |
a cache that absorbs |
cache that absorbs read |
back network links fail |
a control program reacts |
control program reacts to |
program reacts to network |
reacts to network events |
and updates forwarding rules |
whereas a minibrowser would |
updates forwarding rules on |
a minibrowser would typically |
forwarding rules on switches |
minibrowser would typically be |
rules on switches to |
would typically be prebuilt |
on switches to manage |
typically be prebuilt with |
switches to manage packets |
be prebuilt with all |
prebuilt with all the |
with all the available |
all the available features |
building on this interface |
the available features in |
available features in place |
results in long disk |
our work on merlin |
in long disk idle |
our scenario demands a |
long disk idle periods |
scenario demands a much |
predictable high performance bulk |
demands a much more |
high performance bulk data |
a much more flexible |
performance bulk data transfer |
much more flexible kind |
is novel among network |
more flexible kind of |
novel among network programming |
flexible kind of tool |
among network programming languages |
low predictability of idle |
kind of tool that |
network programming languages in |
predictability of idle periods |
of tool that can |
programming languages in that |
tool that can be |
languages in that it |
that can be redesigned |
in that it determines |
can be redesigned while |
that it determines allocations |
ieee international conference on |
be redesigned while in |
redesigned while in use |
international conference on cluster |
it determines allocations of |
conference on cluster computing |
determines allocations of limited |
allocations of limited network |
depending on the location |
on the location and |
wide resources such as |
the location and other |
resources such as bandwidth |
location and other factors |
such as bandwidth and |
as bandwidth and paths |
the best networking protocols |
best networking protocols and |
we have used merlin |
networking protocols and connectivity |
have used merlin to |
protocols and connectivity options |
used merlin to improve |
and connectivity options may |
merlin to improve the |
connectivity options may vary |
to improve the latency |
improve the latency of |
the latency of hadoop |
latency of hadoop jobs |
of hadoop jobs running |
in our rescue scenario |
hadoop jobs running in |
jobs running in the |
running in the presence |
in the presence of |
the workers may have |
the presence of udp |
presence of udp background |
workers may have to |
may have to use |
of udp background traffic |
have shown that there |
have to use wireless |
to use wireless p |
or prioritize classes of |
prioritize classes of traffic |
classes of traffic used |
of traffic used for |
traffic used for state |
p protocols much of |
protocols much of the |
much of the time |
shown that there exists |
machine replication in fault |
reaching back to hosted |
that there exists low |
solomon codes and their |
codes and their applications |
there exists low correlation |
back to hosted services |
to hosted services only |
exists low correlation between |
hosted services only intermittently |
services only intermittently when |
low correlation between a |
only intermittently when a |
these experiments demonstrate that |
intermittently when a drone |
when a drone aircraft |
experiments demonstrate that an |
demonstrate that an sdn |
a drone aircraft passes |
drone aircraft passes within |
that an sdn framework |
correlation between a given |
aircraft passes within radio |
passes within radio range |
with the correct information |
the correct information as |
correct information as input |
between a given idle |
a given idle period |
can provide automated network |
given idle period s |
the right choice of |
idle period s duration |
provide automated network management |
period s duration and |
right choice of protocol |
s duration and the |
automated network management customized |
choice of protocol should |
of protocol should reflect |
network management customized to |
duration and the duration |
protocol should reflect the |
management customized to the |
customized to the needs |
should reflect the operating |
reflect the operating conditions |
to the needs of |
and the duration of |
the needs of resident |
and if these change |
the duration of previous |
needs of resident distributed |
nat and packet mangling |
and packet mangling for |
of resident distributed applications |
the platform should be |
duration of previous idle |
packet mangling for linux |
while the merlin compiler |
the merlin compiler generates |
platform should be capable |
of previous idle periods |
merlin compiler generates static |
should be capable of |
compiler generates static network |
be capable of swapping |
generates static network configurations |
capable of swapping in |
of swapping in a |
swapping in a different |
in a different protocol |
a different protocol without |
merlin uses a small |
different protocol without disrupting |
protocol without disrupting the |
without disrupting the end |
disrupting the end user |
this variability makes it |
runtime component to allow |
component to allow for |
this argues for a |
variability makes it difficult |
to allow for dynamic |
makes it difficult to |
argues for a decoupling |
it difficult to devise |
allow for dynamic adaptation |
difficult to devise effective |
for a decoupling of |
to devise effective predictive |
a decoupling of functionality |
devise effective predictive mechanisms |
based approach allows this |
effective predictive mechanisms for |
approach allows this adaptation |
whereas a minibrowser packages |
a minibrowser packages it |
allows this adaptation to |
this adaptation to happen |
minibrowser packages it all |
packages it all into |
adaptation to happen safely |
predictive mechanisms for disk |
it all into one |
all into one object |
by providing policy language |
mechanisms for disk idle |
providing policy language constructs |
policy language constructs that |
language constructs that can |
better is a design |
for disk idle times |
constructs that can be |
is a design in |
that can be automatically |
a design in which |
can be automatically verified |
design in which the |
in which the presentation |
multicast routing in datagram |
which the presentation object |
routing in datagram internetworks |
implicit in the design |
the presentation object is |
in datagram internetworks and |
in the design of |
the design of this |
presentation object is distinct |
datagram internetworks and extended |
internetworks and extended lans |
design of this runtime |
object is distinct from |
the lfs neatly circumvents |
of this runtime component |
is distinct from objects |
acm transactions on computers |
transactions on computers systems |
distinct from objects representing |
and sdn networks in |
sdn networks in general |
from objects representing information |
lfs neatly circumvents this |
objects representing information sources |
neatly circumvents this problem |
is the notion that |
circumvents this problem by |
representing information sources and |
this problem by predetermining |
the notion that network |
problem by predetermining which |
information sources and objects |
notion that network events |
by predetermining which disk |
sources and objects representing |
predetermining which disk is |
that network events are |
and objects representing transport |
objects representing transport protocols |
network events are generated |
which disk is written |
events are generated in |
decoupling makes it possible |
disk is written to |
are generated in response |
makes it possible to |
is written to at |
generated in response to |
it possible to dynamically |
written to at all |
in response to the |
possible to dynamically modify |
to at all times |
response to the situational |
to dynamically modify or |
to the situational status |
dynamically modify or even |
the situational status culled |
modify or even replace |
situational status culled from |
or even replace a |
status culled from a |
even replace a component |
culled from a wide |
replace a component with |
from a wide range |
a component with some |
a wide range of |
component with some other |
wide range of sources |
server systems are often |
option when changing conditions |
when changing conditions require |
changing conditions require it |
systems are often constrained |
are often constrained by |
we have posed what |
have posed what may |
often constrained by service |
posed what may sound |
constrained by service level |
what may sound like |
by service level agreements |
may sound like a |
service level agreements to |
packet and drop rates |
sound like a very |
like a very specialized |
a very specialized problem |
level agreements to guarantee |
agreements to guarantee a |
but in fact we |
to guarantee a certain |
in fact we see |
guarantee a certain level |
fact we see this |
a certain level of |
we see this as |
certain level of performance |
see this as a |
this as a good |
as a good example |
a good example of |
good example of a |
example of a more |
performance enhancing proxies intended |
of a more general |
enhancing proxies intended to |
a more general kind |
proxies intended to mitigate |
more general kind of |
so that finding a |
intended to mitigate link |
general kind of need |
that finding a solution |
kind of need that |
of need that could |
finding a solution that |
need that could arise |
that could arise in |
could arise in many |
arise in many kinds |
in many kinds of |
many kinds of settings |
a solution that provides |
solution that provides acceptable |
that provides acceptable performance |
provides acceptable performance to |
consider a physician treating |
acceptable performance to only |
a physician treating a |
performance to only a |
physician treating a patient |
user preferences for a |
preferences for a particular |
treating a patient with |
a patient with a |
for a particular network |
to only a fraction |
patient with a complex |
with a complex condition |
only a fraction of |
a fraction of the |
fraction of the incoming |
who needs collaboration help |
needs collaboration help from |
collaboration help from specialists |
of the incoming requests |
and who might even |
who might even be |
might even be working |
even be working in |
be working in a |
working in a remote |
in a remote location |
albeit a large fraction |
a remote location under |
remote location under conditions |
location under conditions demanding |
under conditions demanding urgent |
conditions demanding urgent action |
may often not be |
the mixture of patient |
mixture of patient data |
often not be sufficient |
much of this information |
of this information must |
as we shall show |
this information must be |
information must be created |
must be created and |
be created and updated |
created and updated dynamically |
may be just as |
be just as rich |
just as rich and |
the lfs provides an |
udp bandwidth measurement tool |
as rich and dynamic |
existing sdn frameworks have |
lfs provides an applicationindependent |
rich and dynamic as |
sdn frameworks have largely |
and dynamic as in |
provides an applicationindependent solution |
frameworks have largely closing |
dynamic as in our |
have largely closing the |
largely closing the loop |
as in our search |
in our search and |
our search and rescue |
search and rescue scenario |
to accommodate the ever |
an applicationindependent solution that |
applicationindependent solution that allows |
and the underlying communication |
solution that allows the |
growing demands of cloud |
that allows the system |
the underlying communication options |
demands of cloud and |
of cloud and data |
underlying communication options equally |
allows the system to |
cloud and data center |
the system to perform |
and data center application |
communication options equally heterogeneous |
options equally heterogeneous and |
equally heterogeneous and unpredictable |
system to perform consistently |
networks will need to |
will need to become |
need to become more |
to become more flexible |
become more flexible and |
more flexible and dynamic |
to perform consistently across |
designed for a wired |
perform consistently across a |
for a wired environment |
as networks continue to |
consistently across a wide |
a wired environment might |
networks continue to grow |
across a wide range |
wired environment might perform |
continue to grow in |
to grow in complexity |
environment might perform poorly |
a wide range of |
might perform poorly or |
perform poorly or fail |
poorly or fail under |
it will become increasingly |
wide range of datasets |
or fail under such |
will become increasingly difficult |
fail under such conditions |
become increasingly difficult for |
increasingly difficult for network |
difficult for network operators |
for network operators to |
network operators to provide |
operators to provide this |
to provide this flexibility |
a scalable and tcp |
if there is a |
provide this flexibility without |
the law of large |
friendly congestion control for |
this flexibility without the |
there is a way |
is a way to |
congestion control for high |
flexibility without the support |
law of large numbers |
a way to solve |
without the support of |
way to solve the |
the support of proper |
to solve the problem |
support of proper tools |
of proper tools and |
proper tools and infrastructure |
large scale server systems |
there is a way |
scale server systems process |
is a way to |
a way to build |
way to build the |
to build the desired |
build the desired mashup |
server systems process incredibly |
provide both the control |
systems process incredibly large |
both the control and |
throughout the above we |
the above we noted |
above we noted requirements |
process incredibly large request |
the control and monitoring |
incredibly large request loads |
control and monitoring components |
and monitoring components necessary |
monitoring components necessary to |
we now summarize them |
components necessary to automatically |
now summarize them below |
necessary to automatically adapt |
to automatically adapt the |
automatically adapt the network |
adapt the network to |
directing these to a |
the network to the |
these to a small |
network to the needs |
these needs are seen |
to a small fraction |
to the needs of |
needs are seen in |
are seen in many |
the needs of the |
needs of the applications |
seen in many settings |
a small fraction of |
small fraction of the |
because both systems use |
both systems use a |
systems use a language |
fraction of the total |
we believe them to |
of the total number |
believe them to be |
the total number of |
them to be typical |
they have rigorous semantics |
total number of disks |
to be typical of |
have rigorous semantics that |
be typical of most |
rigorous semantics that can |
typical of most soc |
semantics that can be |
of most soc applications |
that can be formally |
can be formally defined |
the fraction that is |
fraction that is in |
we would like to |
would like to enable |
like to enable a |
to enable a non |
they provide predictable operational |
provide predictable operational behavior |
request queued request send |
that is in high |
programmer to rapidly develop |
queued request send reply |
third international workshop on |
to rapidly develop a |
request send reply queued |
international workshop on protocols |
they allow for the |
rapidly develop a new |
send reply queued reply |
workshop on protocols for |
allow for the rigorous |
develop a new collaborative |
reply queued reply send |
on protocols for fast |
for the rigorous expression |
a new collaborative application |
protocols for fast long |
the rigorous expression of |
new collaborative application by |
can significantly raise the |
rigorous expression of algorithms |
collaborative application by composing |
significantly raise the probability |
expression of algorithms for |
application by composing together |
e dd e dd |
of algorithms for monitoring |
raise the probability of |
by composing together and |
dd e dd f |
algorithms for monitoring or |
for monitoring or managing |
composing together and customizing |
e dd f edd |
the probability of error |
monitoring or managing sdn |
together and customizing preexisting |
dd f edd f |
probability of error and |
or managing sdn networks |
and customizing preexisting components |
f edd f g |
of error and failure |
edd f g fg |
f g fg e |
we would like to |
g fg e ed |
would like to be |
fg e ed e |
like to be able |
e ed e e |
to be able to |
ed e e d |
be able to overlay |
the fact that the |
e e d f |
fact that the disks |
able to overlay data |
e d f eed |
that the disks used |
to overlay data from |
d f eed f |
the disks used in |
overlay data from multiple |
f eed f g |
packet recovery in high |
data from multiple sources |
eed f g fg |
f g fg e |
g fg e d |
speed networks using coding |
fg e d e |
e d e d |
d e d f |
potentially in different formats |
disks used in these |
networks using coding and |
using coding and buffer |
coding and buffer management |
this work was supported |
used in these contexts |
obtained using different protocols |
in these contexts are |
using different protocols and |
different protocols and inconsistent |
protocols and inconsistent interfaces |
by a grant from |
a grant from the |
grant from the darpa |
from the darpa mrc |
the darpa mrc program |
these contexts are typically |
we would like to |
contexts are typically low |
would like to be |
like to be able |
to be able to |
be able to dynamically |
able to dynamically customize |
to dynamically customize the |
dynamically customize the application |
customize the application at |
the application at runtime |
end with relatively weak |
with relatively weak reliability |
relatively weak reliability guarantees |
bcq pcb c bq |
pcb c bq pcb |
c bq pcb cbqpcb |
by incorporating new data |
bq pcb cbqpcb n |
incorporating new data sources |
pcb cbqpcb n n |
new data sources or |
cbqpcb n n on |
data sources or changing |
online measurement of large |
n n on n |
sources or changing the |
measurement of large traffic |
performance evaluation of forward |
n on n c |
on n c bc |
or changing the way |
of large traffic aggregates |
evaluation of forward error |
of forward error correction |
n c bc bonn |
changing the way data |
large traffic aggregates on |
traffic aggregates on commodity |
forward error correction in |
c bc bonn c |
the way data is |
way data is presented |
aggregates on commodity switches |
error correction in atm |
bc bonn c bc |
bonn c bc b |
c bc b cbcb |
as we shall see |
correction in atm networks |
and without disrupting system |
without disrupting system operation |
we would like to |
would like to be |
like to be able |
to be able to |
be able to accommodate |
our solution alleviates this |
able to accommodate new |
solution alleviates this problem |
to accommodate new types |
alleviates this problem by |
accommodate new types of |
this problem by making |
new types of data |
problem by making sure |
c bc b cbcb |
by making sure that |
types of data sources |
making sure that the |
sure that the live |
that the live subset |
new formats or protocols |
the live subset of |
formats or protocols that |
live subset of disks |
or protocols that we |
subset of disks is |
protocols that we may |
of disks is not |
that we may not |
c bc b cbcb |
disks is not constant |
we may not have |
may not have anticipated |
not have anticipated at |
have anticipated at the |
anticipated at the time |
at the time the |
the time the system |
time the system was |
the system was released |
c bc b cbcb |
the rest of this |
data might be published |
rest of this paper |
might be published by |
be published by the |
of this paper is |
published by the individual |
by the individual users |
this paper is organized |
paper is organized as |
is organized as follows |
and it might be |
a compositional architecture for |
it might be necessary |
compositional architecture for gossip |
c b cb a |
might be necessary for |
architecture for gossip protocols |
b cb a a |
be necessary for the |
cb a a k |
necessary for the users |
a a k k |
for the users to |
a k k j |
the users to exchange |
k k j jk |
users to exchange their |
to exchange their data |
exchange their data without |
describes some of the |
their data without access |
some of the solutions |
data without access to |
of the solutions explored |
without access to a |
c bc b cbcb |
access to a centralized |
to a centralized repository |
bc b cbcb kk |
the solutions explored in |
b cbcb kk j |
data may be obtained |
solutions explored in the |
cbcb kk j m |
may be obtained using |
explored in the first |
kk j m lkjj |
be obtained using different |
obtained using different types |
j m lkjj ml |
in the first three |
using different types of |
m lkjj ml ml |
efficient erasure correcting codes |
the first three quadrants |
different types of network |
lkjj ml ml c |
ieee transactions on information |
types of network protocols |
first three quadrants mentioned |
ml ml c b |
transactions on information theory |
three quadrants mentioned above |
ml c b c |
and the type of |
c b c b |
the type of the |
b c b cb |
type of the physical |
c b cb kj |
of the physical network |
b cb kj ih |
the physical network or |
cb kj ih i |
physical network or protocols |
kj ih i h |
network or protocols may |
ih i h ih |
i h ih j |
or protocols may not |
protocols may not be |
may not be known |
not be known in |
be known in advance |
presents and analyzes our |
and analyzes our solution |
it should be possible |
should be possible to |
be possible to rapidly |
possible to rapidly compose |
to rapidly compose the |
rapidly compose the application |
compose the application using |
the application using whatever |
application using whatever communication |
using whatever communication infrastructure |
whatever communication infrastructure is |
communication infrastructure is currently |
infrastructure is currently available |
users may be mobile |
managing the network with |
the network with merlin |
may be mobile or |
be mobile or temporarily |
mobile or temporarily disconnected |
discusses our evaluation methodology |
our evaluation methodology and |
c bc b c |
evaluation methodology and results |
bc b c bc |
b c bc b |
and the topology of |
c bc b cbcb |
the topology of the |
bc b cbcb rpc |
topology of the network |
b cbcb rpc times |
cbcb rpc times at |
of the network and |
rpc times at low |
times at low bandwidth |
the network and its |
we conclude in section |
network and its characteristics |
and its characteristics might |
its characteristics might change |
characteristics might change over |
might change over time |
request queued request send |
queued request send reply |
request send reply queued |
send reply queued reply |
reply queued reply send |
the system should be |
queued reply send total |
system should be easily |
reply send total time |
should be easily reconfigurable |
the requirements outlined above |
requirements outlined above might |
outlined above might seem |
above might seem hard |
might seem hard to |
seem hard to satisfy |
the solution is surprisingly |
solution is surprisingly simple |
based solutions the concept |
our analysis motivates a |
analysis motivates a component |
solutions the concept of |
the concept of a |
concept of a memory |
rd annual ieee symposium |
in which the web |
of a memory hierarchy |
annual ieee symposium on |
which the web services |
a memory hierarchy arose |
ieee symposium on foundations |
memory hierarchy arose as |
the web services and |
hierarchy arose as a |
symposium on foundations of |
web services and hosted |
arose as a result |
on foundations of computer |
as a result of |
services and hosted content |
a result of the |
foundations of computer science |
and hosted content are |
result of the natural |
hosted content are modeled |
a language for provisioning |
language for provisioning network |
content are modeled as |
of the natural tradeoff |
for provisioning network resources |
are modeled as reusable |
the natural tradeoff between |
modeled as reusable overlayed |
natural tradeoff between memory |
as reusable overlayed information |
tradeoff between memory speed |
reusable overlayed information layers |
between memory speed and |
overlayed information layers backed |
memory speed and memory |
information layers backed by |
layers backed by customizable |
backed by customizable transport |
by customizable transport layers |
rpc traffic with varying |
speed and memory cost |
traffic with varying bandwidth |
a graph of components |
a collaborative application is |
collaborative application is a |
application is a forest |
a set of such |
set of such graphs |
our vision demands a |
vision demands a new |
demands a new kind |
ieee transactions on information |
a new kind of |
transactions on information theory |
new kind of soc |
kind of soc standard |
in order to facilitate |
show the time spent |
order to facilitate the |
the time spent on |
to facilitate the side |
time spent on rpcs |
spent on rpcs during |
on rpcs during an |
rpcs during an execution |
during an execution of |
an execution of the |
execution of the simultaneous |
of the simultaneous writeback |
software defined traffic measurement |
side coexistence of components |
the simultaneous writeback test |
defined traffic measurement with |
coexistence of components that |
simultaneous writeback test from |
traffic measurement with opensketch |
of components that might |
writeback test from section |
that there exists a |
components that might today |
there exists a similar |
exists a similar tradeoff |
that might today be |
a similar tradeoff between |
might today be implemented |
similar tradeoff between performance |
today be implemented as |
tradeoff between performance and |
be implemented as proprietary |
between performance and power |
implemented as proprietary minibrowsers |
with the bandwidth varying |
the bandwidth varying according |
bandwidth varying according to |
varying according to the |
according to the curve |
if we enable components |
to the curve in |
we enable components to |
enable components to talk |
components to talk to |
to talk to oneanother |
we need to agree |
performance disks and low |
need to agree on |
rpcs are labelled as |
to agree on the |
are labelled as follows |
agree on the events |
on the events and |
the events and representation |
events and representation that |
and representation that the |
representation that the dialog |
performance disks such as |
that the dialog will |
the dialog will employ |
disks such as laptop |
such as laptop disks |
the decoupling of functionality |
decoupling of functionality into |
of functionality into layers |
functionality into layers also |
into layers also suggests |
layers also suggests a |
also suggests a need |
suggests a need for |
a need for a |
need for a standardized |
for a standardized layering |
they explore the possibility |
explore the possibility of |
in the examples above |
the possibility of setting |
possibility of setting up |
one can identify at |
of setting up a |
can identify at least |
identify at least four |
demand fetch to raise |
setting up a disk |
fetch to raise priority |
to raise priority of |
up a disk hierarchy |
raise priority of a |
a disk hierarchy by |
priority of a prefetch |
of a prefetch rpc |
disk hierarchy by using |
the linkage layer that |
hierarchy by using high |
linkage layer that talks |
layer that talks to |
that talks to the |
talks to the underlying |
to the underlying data |
the underlying data source |
the update generating and |
update generating and interpreting |
the time spent on |
generating and interpreting layer |
time spent on rpcs |
spent on rpcs is |
on rpcs is shown |
rpcs is shown with |
is shown with prefetching |
shown with prefetching enabled |
performance disks in conjunction |
and the transport protocol |
disks in conjunction with |
in conjunction with each |
conjunction with each other |
we propose that this |
propose that this decoupling |
that this decoupling be |
this decoupling be done |
decoupling be done using |
be done using event |
in a related vein |
a natural way of |
natural way of thinking |
way of thinking about |
of thinking about components |
thinking about components that |
about components that dates |
components that dates back |
that dates back to |
note that rpc interactions |
dates back to smalltalk |
that rpc interactions can |
rpc interactions can overlap |
interactions can overlap so |
can overlap so the |
overlap so the quantities |
so the quantities for |
the quantities for different |
quantities for different rpc |
for different rpc types |
different rpc types are |
rather than having the |
rpc types are not |
than having the data |
types are not additive |
having the data center |
the data center developer |
data center developer offer |
center developer offer content |
developer offer content through |
for some rpc types |
offer content through proprietary |
content through proprietary minibrowser |
through proprietary minibrowser interface |
the time spent on |
time spent on particular |
spent on particular activities |
on particular activities is |
particular activities is negligible |
activities is negligible in |
is negligible in proportion |
she would define an |
negligible in proportion to |
would define an event |
in proportion to the |
proportion to the overall |
to the overall time |
propose dynamic rotations per |
based interface between transport |
dynamic rotations per minute |
interface between transport and |
between transport and information |
transport and information layers |
attribute requests are small |
the visual events delivered |
requests are small and |
visual events delivered by |
are small and have |
events delivered by the |
small and have a |
delivered by the transport |
and have a very |
by the transport could |
have a very low |
the miner s dilemma |
the transport could then |
a very low transmission |
miner s dilemma ittay |
transport could then be |
s dilemma ittay eyal |
very low transmission time |
dilemma ittay eyal cornell |
could then be delivered |
ittay eyal cornell university |
whereby disks can be |
low transmission time relative |
disks can be run |
eyal cornell university abstract |
can be run at |
transmission time relative to |
be run at multiple |
cornell university abstract an |
run at multiple speeds |
time relative to their |
at multiple speeds depending |
university abstract an open |
then be delivered to |
relative to their queueing |
to their queueing delays |
abstract an open distributed |
be delivered to an |
multiple speeds depending on |
an open distributed system |
delivered to an information |
such users happen to |
speeds depending on whether |
open distributed system can |
to an information layer |
users happen to be |
depending on whether power |
distributed system can be |
an information layer responsible |
happen to be working |
on whether power or |
system can be secured |
information layer responsible for |
to be working on |
whether power or performance |
can be secured by |
layer responsible for visualizing |
be working on the |
power or performance takes |
be secured by requiring |
responsible for visualizing them |
working on the same |
on the same element |
secured by requiring participants |
or performance takes precedence |
the same element of |
by requiring participants to |
same element of the |
requiring participants to present |
element of the design |
participants to present proof |
to present proof of |
present proof of work |
proof of work and |
it is clear that |
of work and rewarding |
user mouse and keyboard |
is clear that satisfying |
work and rewarding them |
mouse and keyboard events |
clear that satisfying a |
and rewarding them for |
and keyboard events and |
that satisfying a request |
rewarding them for participation |
keyboard events and pass |
satisfying a request from |
events and pass them |
a request from stale |
and pass them down |
request from stale data |
poses a significant engineering |
a significant engineering challenge |
whether in from the |
in from the cache |
significant engineering challenge whose |
the bitcoin digital currency |
with this type of |
this type of event |
or on a server |
bitcoin digital currency introduced |
digital currency introduced this |
on a server that |
engineering challenge whose feasibility |
currency introduced this mechanism |
a server that has |
either layer could easily |
challenge whose feasibility is |
server that has yet |
which is adopted by |
whose feasibility is far |
layer could easily be |
that has yet to |
is adopted by almost |
feasibility is far from |
could easily be replaced |
has yet to see |
adopted by almost all |
is far from obvious |
easily be replaced with |
yet to see a |
by almost all contemporary |
be replaced with a |
to see a delayed |
almost all contemporary digital |
replaced with a different |
see a delayed writeback |
all contemporary digital currencies |
with a different one |
another approach is proposed |
contemporary digital currencies and |
would be visible to |
be visible to the |
digital currencies and related |
currencies and related services |
visible to the user |
to the user and |
the user and costly |
approach is proposed by |
a natural process leads |
is proposed by colarelli |
natural process leads participants |
peer protocols would also |
process leads participants of |
proposed by colarelli et |
protocols would also be |
strong cache consistency is |
leads participants of such |
would also be encapsulated |
cache consistency is certainly |
participants of such systems |
also be encapsulated within |
consistency is certainly achievable |
of such systems to |
be encapsulated within their |
is certainly achievable in |
such systems to form |
encapsulated within their respective |
certainly achievable in distributed |
systems to form pools |
within their respective transport |
achievable in distributed file |
their respective transport layers |
in distributed file systems |
where members aggregate their |
members aggregate their power |
aggregate their power and |
their power and share |
power and share the |
and share the rewards |
experience with bitcoin shows |
with bitcoin shows that |
bitcoin shows that the |
one version of a |
shows that the largest |
version of a transport |
that the largest pools |
of a transport layer |
the largest pools are |
a transport layer could |
but must be implemented |
largest pools are often |
transport layer could fetch |
must be implemented with |
pools are often open |
layer could fetch data |
be implemented with synchronous |
implemented with synchronous rpcs |
could fetch data directly |
using massive arrays of |
allowing anyone to join |
fetch data directly from |
and requires either readers |
data directly from a |
massive arrays of inexpensive |
requires either readers or |
directly from a server |
it has long been |
arrays of inexpensive disks |
either readers or writers |
from a server in |
has long been known |
readers or writers to |
a server in a |
long been known that |
or writers to incur |
server in a data |
in a data center |
writers to incur a |
been known that a |
to incur a delay |
known that a member |
incur a delay to |
whereas a different version |
that a member can |
a delay to ensure |
a different version might |
a member can sabotage |
delay to ensure that |
different version might use |
member can sabotage an |
to ensure that only |
they propose the use |
version might use a |
might use a peer |
ensure that only the |
propose the use of |
can sabotage an open |
that only the latest |
the use of a |
sabotage an open pool |
only the latest version |
use of a small |
an open pool by |
the latest version of |
of a small number |
open pool by seemingly |
a reliable multicast protocol |
latest version of a |
a small number of |
pool by seemingly joining |
version of a file |
it could leverage different |
by seemingly joining it |
of a file is |
a file is accessed |
could leverage different type |
seemingly joining it but |
small number of cache |
leverage different type of |
joining it but never |
number of cache disks |
different type of hardware |
it but never sharing |
of cache disks in |
as we have noted |
type of hardware or |
but never sharing its |
cache disks in addition |
we have noted in |
of hardware or be |
never sharing its proofs |
sharing its proofs of |
its proofs of work |
hardware or be optimized |
disks in addition to |
have noted in section |
or be optimized for |
in addition to the |
the pool shares its |
be optimized for different |
addition to the maid |
pool shares its revenue |
optimized for different types |
sending file updates to |
shares its revenue with |
for different types of |
different types of workloads |
file updates to a |
its revenue with the |
revenue with the attacker |
updates to a server |
to the maid disks |
to a server asynchronously |
provided that the different |
and so each of |
that the different versions |
a server asynchronously has |
so each of its |
the different versions of |
server asynchronously has two |
the data in these |
different versions of the |
data in these cache |
asynchronously has two potential |
in these cache disks |
versions of the transport |
each of its participants |
has two potential benefits |
these cache disks is |
of the transport layer |
of its participants earns |
its participants earns less |
the process modifying the |
cache disks is updated |
the transport layer conform |
process modifying the file |
disks is updated to |
transport layer conform to |
modifying the file need |
we define and analyze |
is updated to reflect |
layer conform to the |
the file need not |
define and analyze a |
updated to reflect the |
conform to the same |
file need not wait |
and analyze a game |
to reflect the workload |
to the same standardized |
need not wait for |
analyze a game where |
reflect the workload that |
the same standardized event |
not wait for the |
a game where pools |
the workload that is |
wait for the write |
game where pools use |
workload that is currently |
for the write to |
where pools use some |
that is currently being |
the write to complete |
the application could then |
pools use some of |
is currently being accessed |
application could then switch |
use some of their |
could then switch between |
some of their participants |
then switch between them |
if the update is |
of their participants to |
switch between them as |
the maid disks can |
the update is delayed |
their participants to infiltrate |
between them as conditions |
them as conditions demand |
update is delayed in |
participants to infiltrate other |
maid disks can then |
is delayed in the |
to infiltrate other pools |
disks can then be |
delayed in the log |
infiltrate other pools and |
can then be powered |
in the log for |
other pools and perform |
then be powered down |
the log for some |
pools and perform such |
log for some interval |
users interact through live |
and perform such an |
perform such an attack |
interact through live objects |
for some interval before |
and need only be |
through live objects that |
with any number of |
live objects that transform |
some interval before being |
any number of pools |
need only be spun |
objects that transform actions |
interval before being written |
before being written back |
that transform actions into |
only be spun up |
transform actions into updates |
be spun up when |
actions into updates that |
it may be superseded |
spun up when a |
into updates that are |
may be superseded by |
attacks is not a |
is not a nash |
not a nash equilibrium |
be superseded by a |
superseded by a later |
by a later update |
up when a cache |
updates that are communicated |
when a cache miss |
we study the special |
that are communicated in |
and therefore can be |
therefore can be omitted |
can be omitted entirely |
are communicated in the |
a cache miss occurs |
study the special cases |
research edition where the |
communicated in the form |
edition where the academic |
the special cases where |
where the academic knights |
special cases where either |
the academic knights meet |
cases where either two |
academic knights meet the |
where either two pools |
knights meet the evil |
upon which their contents |
in the form of |
these benefits come at |
either two pools or |
meet the evil empire |
which their contents are |
the form of events |
benefits come at the |
two pools or any |
the evil empire werner |
their contents are copied |
form of events that |
come at the cost |
pools or any number |
evil empire werner vogels |
contents are copied onto |
of events that are |
at the cost of |
or any number of |
empire werner vogels the |
events that are shared |
are copied onto the |
the cost of reduced |
any number of identical |
werner vogels the rivalry |
that are shared via |
copied onto the cache |
cost of reduced cache |
number of identical pools |
vogels the rivalry in |
are shared via the |
shared via the transport |
via the transport layer |
of identical pools play |
the rivalry in the |
onto the cache disks |
of reduced cache consistency |
identical pools play the |
rivalry in the operating |
the protocol implemented by |
pools play the game |
in the operating system |
protocol implemented by the |
this approach has several |
since the version of |
the operating system market |
play the game and |
implemented by the transport |
approach has several of |
the version of the |
operating system market place |
the game and the |
by the transport layer |
has several of the |
version of the file |
system market place has |
game and the rest |
the transport layer might |
several of the weaknesses |
of the file stored |
market place has a |
and the rest of |
transport layer might replicate |
layer might replicate the |
the file stored at |
place has a severe |
the rest of the |
of the weaknesses that |
might replicate the event |
file stored at the |
has a severe impact |
rest of the participants |
of the participants are |
deliver it to the |
a severe impact on |
the weaknesses that memory |
stored at the server |
the participants are uninvolved |
it to the tablets |
severe impact on the |
impact on the academic |
at the server is |
to the tablets of |
weaknesses that memory caches |
in both of these |
on the academic world |
both of these cases |
the tablets of our |
tablets of our rescue |
the server is inconsistent |
of these cases there |
that memory caches suffer |
of our rescue workers |
server is inconsistent during |
where in the old |
these cases there exists |
is inconsistent during the |
cases there exists an |
in the old days |
there exists an equilibrium |
only on a larger |
exists an equilibrium that |
the old days intellection |
an equilibrium that constitutes |
inconsistent during the time |
equilibrium that constitutes a |
and report it through |
old days intellection quality |
on a larger scale |
during the time that |
that constitutes a tragedy |
report it through the |
constitutes a tragedy of |
the time that the |
days intellection quality and |
it through the event |
if the cache disks |
time that the update |
intellection quality and careful |
a tragedy of the |
the cache disks are |
that the update remains |
quality and careful deliberation |
based interface back to |
tragedy of the commons |
cache disks are insufficient |
the update remains queued |
and careful deliberation would |
interface back to the |
of the commons where |
disks are insufficient to |
update remains queued for |
careful deliberation would prevail |
back to the information |
the commons where the |
are insufficient to store |
remains queued for transmission |
to the information layer |
commons where the participating |
insufficient to store the |
nowadays discussions about operating |
the information layer at |
where the participating pools |
to store the entire |
even though asynchronous writes |
discussions about operating systems |
information layer at which |
the participating pools attack |
store the entire working |
though asynchronous writes in |
about operating systems research |
layer at which the |
participating pools attack one |
the entire working set |
asynchronous writes in mfs |
operating systems research appear |
at which the event |
pools attack one another |
entire working set of |
writes in mfs are |
systems research appear to |
which the event has |
attack one another and |
working set of the |
in mfs are not |
research appear to be |
the event has originated |
one another and earn |
set of the current |
mfs are not delayed |
appear to be more |
another and earn less |
of the current workload |
are not delayed to |
to be more like |
and earn less than |
not delayed to aggregate |
the transport layer with |
be more like the |
earn less than they |
delayed to aggregate updates |
transport layer with the |
more like the battlefield |
less than they would |
layer with the embedded |
like the battlefield of |
than they would have |
with considerable latency penalties |
with the embedded distributed |
a burst of updates |
the battlefield of a |
burst of updates to |
the embedded distributed protocol |
they would have if |
battlefield of a holy |
of a holy war |
embedded distributed protocol would |
would have if none |
of updates to a |
distributed protocol would behave |
have if none had |
updates to a sequence |
with objectivity as its |
objectivity as its main |
protocol would behave very |
if none had attacked |
to a sequence of |
the cache disks represent |
as its main victim |
would behave very much |
a sequence of files |
cache disks represent a |
behave very much like |
we have tried to |
disks represent a significant |
the decision whether or |
very much like an |
have tried to side |
sequence of files may |
represent a significant added |
decision whether or not |
much like an object |
tried to side step |
of files may flood |
a significant added cost |
whether or not to |
like an object in |
to side step the |
files may flood the |
significant added cost in |
or not to attack |
an object in smalltalk |
side step the emotional |
may flood the link |
added cost in themselves |
not to attack is |
step the emotional current |
flood the link to |
it would consume events |
to attack is the |
the link to the |
would consume events and |
disk management solutions pinheiro |
and select an operating |
link to the server |
attack is the miner |
consume events and respond |
events and respond with |
select an operating system |
to the server and |
is the miner s |
the miner s dilemma |
and respond with events |
an operating system that |
the server and increase |
management solutions pinheiro and |
an instance of the |
this motivates thinking about |
operating system that could |
solutions pinheiro and bianchini |
server and increase the |
instance of the iterative |
motivates thinking about communication |
system that could bring |
and increase the delay |
of the iterative prisoner |
thinking about communication protocols |
that could bring our |
increase the delay before |
the iterative prisoner s |
about communication protocols as |
could bring our research |
the delay before updates |
iterative prisoner s dilemma |
communication protocols as objects |
bring our research into |
delay before updates towards |
our research into the |
before updates towards the |
research into the next |
and indeed in treating |
updates towards the end |
indeed in treating them |
into the next century |
in treating them as |
towards the end of |
treating them as objects |
the game is played |
suggest that if data |
the end of the |
them as objects much |
game is played daily |
based on objective technical |
that if data is |
end of the burst |
as objects much as |
is played daily by |
on objective technical and |
if data is laid |
of the burst are |
objects much as we |
played daily by the |
objective technical and organizational |
technical and organizational criteria |
the burst are committed |
much as we treat |
daily by the active |
data is laid out |
as we treat any |
by the active bitcoin |
any other client accessing |
this paper describes how |
we treat any other |
the active bitcoin pools |
is laid out on |
other client accessing the |
paper describes how this |
treat any other kind |
which apparently choose not |
any other kind of |
describes how this evaluation |
laid out on disks |
client accessing the file |
apparently choose not to |
other kind of object |
how this evaluation lead |
out on disks according |
choose not to attack |
kind of object in |
this evaluation lead to |
cache consistency will access |
on disks according to |
of object in a |
evaluation lead to the |
consistency will access the |
if this balance breaks |
disks according to frequency |
object in a language |
lead to the insight |
will access the stale |
access the stale version |
in a language like |
to the insight that |
the revenue of open |
according to frequency of |
a language like java |
the insight that microsoft |
revenue of open pools |
rather than one which |
to frequency of access |
language like java or |
insight that microsoft s |
of open pools might |
than one which incorporates |
like java or in |
that microsoft s windows |
open pools might diminish |
with the most popular |
java or in a |
microsoft s windows nt |
one which incorporates the |
making them unattractive to |
or in a runtime |
s windows nt is |
which incorporates the pending |
incorporates the pending update |
them unattractive to participants |
in a runtime environment |
windows nt is the |
the most popular files |
a runtime environment like |
nt is the operating |
we therefore refer to |
most popular files being |
runtime environment like jini |
is the operating system |
therefore refer to this |
popular files being located |
environment like jini or |
the operating system that |
refer to this as |
files being located in |
operating system that is |
to this as a |
being located in one |
system that is best |
is a digital currency |
this as a hidden |
located in one set |
that is best prepared |
doing so unifies apparently |
a digital currency that |
as a hidden upstudies |
in one set of |
is best prepared for |
so unifies apparently distinct |
digital currency that is |
a hidden upstudies of |
one set of disks |
best prepared for the |
unifies apparently distinct approaches |
currency that is gaining |
hidden upstudies of distributed |
prepared for the future |
that is gaining acceptance |
and the least popular |
upstudies of distributed file |
just as a remotely |
the least popular ones |
of distributed file systems |
as a remotely hosted |
introduction until recently there |
least popular ones in |
distributed file systems have |
a remotely hosted form |
until recently there was |
popular ones in another |
file systems have largely |
remotely hosted form of |
recently there was no |
systems have largely concluded |
hosted form of content |
there was no doubt |
have largely concluded that |
then the latter set |
was no doubt in |
form of content such |
largely concluded that file |
concluded that file date |
no doubt in academia |
of content such as |
the latter set of |
with an estimated market |
and the cache consistency |
content such as a |
latter set of disks |
doubt in academia which |
an estimated market capitalization |
the cache consistency problem |
such as a map |
set of disks could |
in academia which operating |
estimated market capitalization of |
cache consistency problem caused |
as a map or |
of disks could be |
academia which operating system |
market capitalization of over |
consistency problem caused by |
a map or an |
disks could be powered |
which operating system to |
problem caused by asynchronous |
map or an image |
could be powered down |
operating system to use |
caused by asynchronous sharing |
or an image of |
be powered down to |
system to use for |
by asynchronous sharing is |
an image of a |
powered down to conserve |
to use for systems |
asynchronous sharing is infrequent |
image of a raincloud |
down to conserve energy |
use for systems research |
sharing is infrequent in |
of a raincloud can |
is infrequent in general |
a raincloud can be |
raincloud can be modeled |
can be modeled as |
be modeled as an |
modeled as an object |
their scheme is called |
whether it was a |
scheme is called popular |
it was a bsd |
is called popular data |
was a bsd or |
so can network protocols |
called popular data concentration |
a bsd or system |
can network protocols be |
bsd or system v |
or system v derivative |
network protocols be treated |
bitcoin s security stems |
protocols be treated as |
s security stems from |
be treated as objects |
was the predominant choice |
security stems from a |
stems from a robust |
from a robust incentive |
and they implement and |
a robust incentive system |
they implement and evaluate |
writes as the hidden |
as the hidden update |
the hidden update problem |
participants are required to |
which had its roots |
p systems try to |
implement and evaluate a |
are required to provide |
had its roots in |
systems try to make |
and evaluate a prototype |
required to provide expensive |
its roots in research |
try to make everything |
we have identified a |
evaluate a prototype file |
to provide expensive proofs |
to make everything a |
have identified a class |
a prototype file server |
was used since its |
provide expensive proofs of |
make everything a p |
identified a class of |
prototype file server called |
used since its inception |
expensive proofs of work |
a class of cache |
file server called nomad |
since its inception to |
but in the examples |
and they are rewarded |
class of cache consistency |
its inception to investigate |
in the examples we |
the examples we ve |
they are rewarded according |
of cache consistency scenarmobile |
inception to investigate fundamental |
server called nomad fs |
examples we ve seen |
are rewarded according to |
rewarded according to their |
cache consistency scenarmobile file |
to investigate fundamental system |
according to their efforts |
several kinds of content |
investigate fundamental system research |
consistency scenarmobile file systems |
kinds of content would |
this architecture has proved |
scenarmobile file systems such |
which runs on top |
of content would more |
architecture has proved both |
file systems such as |
and the accumulated knowledge |
runs on top of |
content would more naturally |
has proved both stable |
systems such as coda |
the accumulated knowledge in |
on top of the |
would more naturally be |
proved both stable and |
accumulated knowledge in academia |
top of the file |
more naturally be hosted |
both stable and scalable |
knowledge in academia about |
of the file system |
in academia about its |
the file system and |
and it is used |
academia about its internals |
file system and monitors |
it is used by |
rely on optimistic conios |
about its internals and |
d images of terrain |
is used by most |
on optimistic conios as |
system and monitors data |
its internals and operations |
images of terrain and |
used by most contemporary |
optimistic conios as being |
and monitors data layout |
internals and operations was |
of terrain and buildings |
by most contemporary digital |
conios as being of |
monitors data layout on |
and operations was significant |
most contemporary digital currencies |
as being of high |
data layout on disks |
contemporary digital currencies and |
other available operating systems |
being of high importance |
digital currencies and related |
available operating systems such |
of high importance and |
their findings are that |
operating systems such as |
currencies and related services |
high importance and inadequately |
findings are that if |
on the other hand |
systems such as vms |
importance and inadequately served |
and inadequately served by |
such as vms and |
as vms and mvs |
inadequately served by ex |
soc applications are likely |
are that if the |
applications are likely to |
had their roots in |
that if the low |
are likely to embody |
currency control to resolve |
their roots in the |
likely to embody quite |
roots in the commercial |
control to resolve the |
in the commercial world |
access disks are powered |
to embody quite a |
to resolve the conflicts |
the commercial world and |
disks are powered down |
embody quite a range |
resolve the conflicts generated |
commercial world and knowledge |
quite a range of |
a range of p |
world and knowledge about |
the conflicts generated by |
this results in a |
and knowledge about these |
conflicts generated by hidden |
results in a considerable |
knowledge about these systems |
generated by hidden upisting |
in a considerable performance |
each separate video object |
about these systems never |
by hidden upisting mobile |
a considerable performance hit |
these systems never accumulated |
hidden upisting mobile file |
systems never accumulated to |
upisting mobile file systems |
never accumulated to the |
accumulated to the critical |
they suggest instead that |
to the critical mass |
suppose that a complex |
suggest instead that they |
the critical mass were |
that a complex engineering |
may have its own |
instead that they be |
critical mass were these |
a complex engineering dates |
have its own associated |
that they be run |
mass were these systems |
its own associated update |
own associated update stream |
were these systems could |
they be run at |
an alternative approach is |
these systems could be |
if one thinks of |
alternative approach is to |
be run at low |
systems could be considered |
one thinks of these |
approach is to use |
run at low speed |
our results apply to |
thinks of these as |
is to use a |
could be considered for |
results apply to all |
of these as topics |
while their idea is |
be considered for widespread |
apply to all such |
to use a variant |
these as topics in |
as topics in publish |
considered for widespread research |
to all such incentive |
use a variant of |
their idea is sound |
for widespread research tasks |
all such incentive systems |
a variant of callbacks |
variant of callbacks to |
an application could have |
of callbacks to design |
it is not clear |
although new research operating |
application could have many |
but we use bitcoin |
callbacks to design is |
is not clear whether |
new research operating systems |
could have many such |
we use bitcoin terminology |
to design is maintained |
not clear whether this |
research operating systems have |
have many such topics |
use bitcoin terminology and |
design is maintained on |
clear whether this scheme |
operating systems have been |
bitcoin terminology and examples |
is maintained on a |
whether this scheme would |
systems have been developed |
and the application instance |
terminology and examples since |
maintained on a server |
this scheme would adapt |
none have found the |
and examples since it |
on a server and |
the application instance running |
scheme would adapt to |
have found the following |
examples since it serves |
a server and updated |
application instance running on |
would adapt to different |
found the following that |
since it serves as |
server and updated by |
instance running on a |
adapt to different workloads |
the following that the |
it serves as an |
and updated by teams |
running on a given |
following that the established |
serves as an active |
updated by teams of |
by teams of de |
that the established unix |
as an active and |
on a given user |
the established unix s |
an active and archetypal |
a given user s |
established unix s received |
active and archetypal example |
allow a client to |
given user s machine |
propose another data layout |
a client to replay |
user s machine could |
another data layout management |
bitcoin implements its incentive |
client to replay writes |
s machine could simultaneously |
data layout management scheme |
implements its incentive systems |
to replay writes asynchronously |
machine could simultaneously display |
freebsd and others continue |
layout management scheme to |
but retain strong signers |
could simultaneously display data |
and others continue to |
its incentive systems with |
management scheme to optimize |
simultaneously display data from |
others continue to dominate |
incentive systems with a |
scheme to optimize disk |
display data from several |
continue to dominate the |
systems with a data |
to optimize disk access |
data from several topics |
to dominate the academic |
with a data structure |
optimize disk access patterns |
dominate the academic landscape |
a data structure called |
we have previously said |
data structure called the |
have previously said that |
structure called the blockchain |
previously said that we |
but slowly but surely |
said that we d |
slowly but surely windows |
the echo file system |
that we d like |
the blockchain is a |
but surely windows nt |
we d like to |
blockchain is a serialization |
surely windows nt is |
d like to think |
is a serialization of |
windows nt is now |
like to think of |
a serialization of all |
nt is now entering |
to think of protocols |
serialization of all bitcoin |
is now entering the |
their approach uses finer |
of all bitcoin transactions |
think of protocols as |
now entering the academic |
of protocols as objects |
site supervisors work from |
grained control over data |
it is a single |
entering the academic world |
supervisors work from those |
control over data layout |
is a single global |
it now becomes clear |
the academic world as |
work from those designs |
over data layout on |
a single global ledger |
now becomes clear that |
academic world as a |
world as a viable |
data layout on disk |
single global ledger maintained |
becomes clear that further |
from those designs using |
global ledger maintained by |
clear that further precision |
alternative platform for research |
those designs using mobile |
ledger maintained by an |
that further precision is |
further precision is needed |
maintained by an open |
tuning it on a |
although academia looked with |
by an open distributed |
the objects aren t |
it on a per |
academia looked with fascination |
we thank larry felser |
an open distributed system |
objects aren t merely |
looked with fascination at |
thank larry felser and |
aren t merely protocols |
with fascination at dave |
larry felser and his |
since anyone can join |
fascination at dave cutler |
applications are instrumented and |
anyone can join the |
but in fact are |
felser and his team |
at dave cutler s |
are instrumented and then |
can join the open |
in fact are individual |
and his team at |
dave cutler s attempt |
instrumented and then profiled |
join the open system |
fact are individual protocol |
his team at autodesk |
cutler s attempt to |
and then profiled to |
the open system and |
are individual protocol instances |
team at autodesk for |
s attempt to build |
then profiled to obtain |
open system and participate |
at autodesk for their |
attempt to build a |
profiled to obtain array |
system and participate in |
our system will need |
autodesk for their help |
to build a new |
to obtain array access |
and participate in maintaining |
system will need to |
for their help in |
build a new operating |
obtain array access sequences |
participate in maintaining the |
will need to simultaneously |
their help in understanddevices |
a new operating system |
in maintaining the blockchain |
need to simultaneously support |
new operating system from |
which their system then |
to simultaneously support potentially |
operating system from the |
these supervisors read from |
bitcoin uses a proof |
their system then uses |
simultaneously support potentially large |
system from the ground |
supervisors read from the |
uses a proof of |
read from the server |
support potentially large numbers |
from the ground up |
system then uses to |
a proof of work |
from the server and |
potentially large numbers of |
then uses to determine |
proof of work mechanism |
the server and may |
large numbers of transport |
uses to determine optimal |
of work mechanism to |
server and may also |
numbers of transport objects |
to determine optimal disk |
work mechanism to deter |
and may also ing |
of transport objects running |
determine optimal disk layouts |
mechanism to deter attacks |
all expected that windows |
transport objects running concurrently |
expected that windows nt |
may also ing the |
that windows nt would |
also ing the file |
optimal disk layouts by |
participation requires exerting significant |
objects running concurrently in |
windows nt would go |
ing the file access |
nt would go the |
the file access patterns |
would go the same |
disk layouts by computing |
requires exerting significant compute |
running concurrently in the |
file access patterns that |
go the same way |
layouts by computing optimal |
exerting significant compute resources |
concurrently in the end |
access patterns that arise |
the same way as |
by computing optimal stripe |
patterns that arise in |
same way as the |
computing optimal stripe factor |
a participant that proves |
that arise in collaborative |
way as the other |
participant that proves she |
in support of a |
arise in collaborative work |
as the other commercially |
that proves she has |
support of a variety |
in collaborative work applications |
the other commercially designed |
proves she has exerted |
of a variety of |
collaborative work applications for |
other commercially designed operating |
she has exerted enough |
a variety of applications |
work applications for very |
commercially designed operating systems |
has exerted enough resources |
variety of applications and |
the wisdom of marrying |
designed operating systems before |
exerted enough resources with |
applications for very change |
of applications and uses |
wisdom of marrying the |
operating systems before it |
enough resources with a |
for very change the |
all of this leads |
systems before it and |
resources with a proof |
very change the design |
of marrying the disk |
of this leads to |
before it and remain |
with a proof of |
marrying the disk layout |
for example to reflect |
it and remain in |
a proof of work |
this leads to new |
leads to new challenges |
example to reflect one |
and remain in the |
proof of work is |
the disk layout to |
the obvious one was |
remain in the dark |
of work is allowed |
to reflect one of |
disk layout to the |
obvious one was mentioned |
in the dark corner |
work is allowed to |
reflect one of the |
layout to the application |
one was mentioned earlier |
the dark corner from |
is allowed to take |
one of the contingencies |
to the application seems |
dark corner from a |
allowed to take a |
of the contingencies large |
today s web services |
the application seems questionable |
corner from a research |
to take a step |
the contingencies large architectural |
s web services don |
from a research use |
take a step in |
contingencies large architectural and |
web services don t |
a research use point |
a step in the |
large architectural and engineering |
services don t support |
research use point of |
step in the protocol |
proposed by zhu et |
architectural and engineering design |
don t support p |
use point of view |
in the protocol by |
and engineering design firms |
the protocol by generating |
protocol by generating a |
by generating a block |
about four years ago |
contemporary web services solutions |
web services solutions presume |
services solutions presume a |
solutions presume a client |
not long after the |
participants are compensated for |
long after the final |
are compensated for their |
after the final major |
compensated for their efforts |
server style of interaction |
the final major release |
for their efforts with |
encountered and resolved only |
final major release of |
their efforts with newly |
and resolved only as |
combines a number of |
major release of academic |
efforts with newly minted |
with data relayed through |
resolved only as construction |
a number of ideas |
release of academic version |
with newly minted bitcoins |
data relayed through a |
only as construction proceeds |
of academic version of |
it assumes multispeed disks |
relayed through a message |
academic version of the |
the process of creating |
version of the unix |
process of creating a |
of the unix operating |
and computes online the |
as we have seen |
of creating a block |
even if clients are |
computes online the optimal |
we have seen earlier |
the unix operating system |
creating a block is |
if clients are connected |
clients are connected to |
high traffic can cause |
online the optimal speed |
a block is called |
are connected to one |
traffic can cause delays |
the optimal speed that |
block is called mining |
can cause delays in |
optimal speed that each |
cause delays in the |
delays in the round |
and the participants miners |
speed that each disk |
if they lose connectivity |
that each disk should |
they lose connectivity to |
trip time for small |
time for small rpcs |
in order to win |
lose connectivity to the |
connectivity to the broker |
order to win the |
to win the reward |
each disk should run |
disk should run at |
they can t collaborate |
many miners try to |
miners try to generate |
try to generate blocks |
data rpcs have a |
to minimize speed transition |
another serious issue arises |
rpcs have a higher |
minimize speed transition overheads |
the system automatically adjusts |
serious issue arises if |
have a higher outgoing |
the farewell of the |
system automatically adjusts the |
issue arises if the |
a higher outgoing queueing |
farewell of the berkeley |
automatically adjusts the difficulty |
disks maintain their speeds |
arises if the clients |
higher outgoing queueing delay |
of the berkeley systems |
adjusts the difficulty of |
maintain their speeds for |
if the clients don |
outgoing queueing delay in |
the berkeley systems werner |
the difficulty of block |
their speeds for a |
the clients don t |
queueing delay in the |
berkeley systems werner vogels |
difficulty of block generation |
speeds for a fixed |
clients don t trust |
delay in the absence |
systems werner vogels is |
such that one block |
in the absence of |
don t trust the |
werner vogels is a |
that one block is |
the absence of prefetching |
t trust the data |
vogels is a research |
one block is added |
trust the data center |
is a research scientist |
block is added every |
this is due to |
a research scientist at |
is due to the |
they call this the |
sensitive data will need |
research scientist at the |
due to the majority |
call this the coarse |
data will need to |
scientist at the department |
to the majority of |
minutes to the blockchain |
will need to be |
at the department of |
the majority of the |
need to be encrypted |
the department of computer |
majority of the competing |
this means that each |
department of computer science |
of the competing rpcs |
means that each miner |
of computer science of |
the problem here is |
the competing rpcs being |
that each miner seldom |
computer science of cornell |
problem here is that |
hibernator includes a file |
competing rpcs being high |
includes a file server |
science of cornell university |
here is that web |
each miner seldom generates |
rpcs being high priority |
being high priority fetch |
is that web services |
miner seldom generates a |
seldom generates a block |
his research targets high |
that web services security |
a file server that |
web services security standards |
although its revenue may |
availability in distributed systems |
services security standards tend |
file server that sits |
its revenue may be |
security standards tend to |
these rpcs are mostly |
server that sits on |
revenue may be positive |
with a particular focus |
standards tend to trust |
rpcs are mostly replaced |
that sits on top |
may be positive in |
a particular focus on |
tend to trust the |
are mostly replaced by |
mostly replaced by prefetches |
be positive in expectation |
particular focus on enterprise |
to trust the web |
sits on top of |
which operate at a |
trust the web services |
focus on enterprise cluster |
on enterprise cluster systems |
operate at a lower |
a miner may have |
the web services platform |
web services platform itself |
at a lower priority |
miner may have to |
on top of the |
a lower priority than |
may have to wait |
top of the file |
president of reliable network |
of the file system |
have to wait for |
lower priority than store |
the standards offer no |
of reliable network solutions |
the file system and |
to wait for an |
standards offer no help |
file system and manipulates |
until any point where |
offer no help at |
wait for an extended |
system and manipulates data |
any point where a |
no help at all |
for an extended period |
which specializes in building |
and manipulates data layout |
point where a concurrent |
help at all if |
an extended period to |
specializes in building solutions |
manipulates data layout to |
where a concurrent demand |
at all if we |
extended period to create |
in building solutions for |
data layout to put |
a concurrent demand fetch |
all if we need |
period to create a |
building solutions for very |
solutions for very large |
concurrent demand fetch rpc |
if we need to |
to create a block |
layout to put the |
scale reliable distributed systems |
we need to provide |
create a block and |
demand fetch rpc raises |
to put the most |
need to provide end |
a block and earn |
fetch rpc raises their |
block and earn the |
rpc raises their priorities |
and earn the actual |
raises their priorities to |
earn the actual bitcoins |
accessed data on the |
their priorities to the |
end encryption mechanisms while |
data on the highest |
priorities to the fetch |
encryption mechanisms while also |
on the highest speed |
miners form mining pools |
mechanisms while also preventing |
the highest speed disks |
usenix windows nt symposium |
while also preventing the |
where all members mine |
also preventing the hosted |
a comparison of fetch |
all members mine concurrently |
the authors address the |
his personal homepage is |
preventing the hosted services |
data and prefetch rpcs |
authors address the issue |
personal homepage is at |
members mine concurrently and |
the hosted services from |
and prefetch rpcs reveals |
address the issue of |
homepage is at http |
mine concurrently and they |
hosted services from seeing |
prefetch rpcs reveals the |
the issue of performance |
concurrently and they share |
services from seeing the |
rpcs reveals the effect |
issue of performance guarantees |
and they share their |
from seeing the keys |
reveals the effect of |
of performance guarantees by |
they share their revenue |
the effect of the |
performance guarantees by stipulating |
share their revenue whenever |
effect of the bandwidth |
we encounter debilitating latency |
their revenue whenever one |
of the bandwidth decrease |
guarantees by stipulating that |
encounter debilitating latency and |
revenue whenever one of |
by stipulating that if |
debilitating latency and throughput |
whenever one of them |
stipulating that if performance |
latency and throughput issues |
one of them creates |
the test run with |
that if performance drops |
of them creates a |
hosted services will be |
services will be performance |
test run with prefetching |
them creates a block |
if performance drops below |
run with prefetching performs |
performance drops below some |
limiting bottlenecks when used |
with prefetching performs a |
prefetching performs a fetch |
pools are typically implemented |
bottlenecks when used in |
drops below some threshold |
are typically implemented as |
data rpc to get |
when used in settings |
typically implemented as a |
rpc to get the |
used in settings with |
then all disks are |
implemented as a pool |
to get the first |
group and the early |
in settings with large |
all disks are spun |
as a pool manager |
get the first file |
and the early demise |
settings with large numbers |
disks are spun up |
a pool manager and |
which triggers prefetching from |
with large numbers of |
large numbers of clients |
the early demise of |
pool manager and a |
triggers prefetching from its |
are spun up to |
as we will see |
manager and a cohort |
prefetching from its file |
from its file group |
spun up to their |
we will see in |
and a cohort of |
early demise of mach |
because of the large |
will see in our |
a cohort of miners |
up to their highest |
demise of mach as |
of the large delay |
see in our experimental |
in our experimental section |
of mach as the |
the pool manager joins |
the large delay between |
to their highest speed |
we are left with |
pool manager joins the |
large delay between file |
mach as the last |
are left with a |
manager joins the bitcoin |
caching solutions zhu et |
as the last of |
left with a mixture |
delay between file accesses |
joins the bitcoin system |
the last of the |
with a mixture of |
the bitcoin system as |
prefetches complete entirely without |
a mixture of good |
last of the research |
bitcoin system as a |
complete entirely without any |
mixture of good and |
of the research operating |
system as a single |
as a single miner |
of good and bad |
the research operating systems |
entirely without any overlapping |
good and bad news |
instead of generating proof |
without any overlapping demand |
the operating system research |
observe that the storage |
web services standardize client |
any overlapping demand fetches |
operating system research world |
of generating proof of |
generating proof of work |
services standardize client access |
system research world was |
that the storage cache |
standardize client access to |
it outsources the work |
research world was at |
world was at a |
over the course of |
client access to hosted |
outsources the work to |
the work to the |
work to the miners |
the course of the |
access to hosted services |
the storage cache management |
was at a crossroads |
course of the second |
to hosted services and |
in order to evaluate |
storage cache management policy |
intel based personal computers |
cache management policy is |
order to evaluate the |
of the second period |
hosted services and data |
based personal computers were |
management policy is pivotal |
to evaluate the miners |
the second period of |
personal computers were becoming |
computers were becoming ubiquitous |
evaluate the miners efforts |
second period of time |
we can easily build |
policy is pivotal in |
and a myriad of |
can easily build some |
bandwidth becomes insufficient for |
easily build some form |
the pool manager accepts |
is pivotal in determining |
a myriad of unix |
becomes insufficient for a |
build some form of |
pool manager accepts partial |
pivotal in determining the |
myriad of unix operating |
in determining the sequence |
some form of multiframed |
manager accepts partial proof |
insufficient for a prefetch |
of unix operating systems |
determining the sequence of |
form of multiframed web |
accepts partial proof of |
for a prefetch to |
unix operating systems was |
the sequence of requests |
of multiframed web page |
partial proof of work |
a prefetch to complete |
operating systems was available |
sequence of requests that |
multiframed web page that |
proof of work and |
prefetch to complete during |
systems was available for |
of requests that access |
web page that could |
of work and estimates |
to complete during the |
was available for this |
available for this platform |
page that could host |
work and estimates each |
requests that access disks |
that could host each |
and estimates each miner |
eventually many moved to |
could host each kind |
estimates each miner s |
many moved to use |
moved to use linux |
each miner s power |
host each kind of |
miner s power according |
s delay between accesses |
each kind of information |
s power according to |
a popular architectural clone |
kind of information in |
and raisepriority rpcs are |
power according to the |
popular architectural clone of |
cache management policies could |
of information in its |
raisepriority rpcs are triggered |
according to the rate |
architectural clone of the |
clone of the traditional |
of the traditional unix |
rpcs are triggered by |
to the rate with |
management policies could be |
information in its own |
at the computer science |
the rate with which |
policies could be tailored |
are triggered by the |
in its own minibrowser |
the computer science department |
rate with which it |
could be tailored to |
triggered by the consequent |
computer science department at |
with which it submits |
be tailored to change |
by the consequent cache |
when connectivity is adequate |
science department at cornell |
which it submits such |
tailored to change the |
the consequent cache misses |
department at cornell university |
it submits such partial |
to change the average |
relaying data via a |
at cornell university we |
submits such partial proof |
such partial proof of |
partial proof of work |
data via a hosted |
cornell university we made |
change the average idle |
as the bandwidth decreases |
via a hosted service |
when a miner generates |
a hosted service has |
university we made the |
the average idle time |
a miner generates a |
hosted service has many |
we made the decision |
the queueing delays increase |
average idle time between |
miner generates a full |
service has many of |
made the decision to |
queueing delays increase as |
idle time between disk |
generates a full proof |
has many of the |
the decision to conduct |
delays increase as a |
time between disk requests |
a full proof of |
many of the benefits |
decision to conduct our |
increase as a proportion |
full proof of work |
of the benefits of |
to conduct our research |
as a proportion of |
the benefits of a |
thus providing more opportunities |
conduct our research on |
a proportion of the |
benefits of a publishsubscribe |
it sends it to |
providing more opportunities for |
our research on windows |
research on windows nt |
of a publishsubscribe architecture |
sends it to the |
more opportunities for reducing |
proportion of the total |
it to the pool |
such as robustness as |
by that time we |
of the total time |
opportunities for reducing disk |
to the pool manager |
as robustness as the |
that time we had |
the total time spent |
for reducing disk energy |
the pool manager which |
robustness as the set |
time we had learned |
total time spent on |
time spent on prefetches |
pool manager which publishes |
as the set of |
we had learned enough |
reducing disk energy consumption |
manager which publishes this |
the set of clients |
had learned enough from |
which publishes this proof |
set of clients changes |
learned enough from the |
publishes this proof of |
enough from the early |
this proof of work |
the modifying client to |
from the early design |
proof of work to |
the natural way to |
modifying client to flush |
cache policies that are |
the early design of |
of work to the |
natural way to think |
client to flush its |
policies that are aware |
early design of windows |
work to the bitcoin |
way to think of |
to flush its updates |
that are aware of |
design of windows nt |
to the bitcoin system |
to think of our |
flush its updates whenever |
are aware of the |
of windows nt to |
think of our application |
its updates whenever another |
aware of the underlying |
windows nt to realize |
the pool manager thus |
of our application is |
updates whenever another client |
of the underlying disk |
nt to realize that |
pool manager thus receives |
our application is as |
whenever another client accesses |
the underlying disk management |
to realize that it |
manager thus receives the |
application is as an |
another client accesses the |
underlying disk management schemes |
realize that it was |
thus receives the full |
is as an object |
client accesses the file |
that it was a |
receives the full revenue |
it was a major |
the full revenue of |
was a major step |
full revenue of the |
a major step forward |
revenue of the block |
which disks are running |
major step forward in |
but web services provide |
of the block and |
disks are running at |
step forward in operating |
web services provide no |
the block and distributes |
are running at which |
forward in operating system |
separates invalidating a file |
services provide no support |
block and distributes it |
running at which speeds |
in operating system design |
invalidating a file from |
provide no support for |
and distributes it fairly |
a file from transmitting |
no support for this |
distributes it fairly according |
it would provide us |
support for this kind |
file from transmitting its |
it fairly according to |
would provide us with |
for this kind of |
from transmitting its update |
can make more intelligent |
fairly according to its |
provide us with a |
this kind of client |
us with a platform |
according to its members |
we have implemented a |
make more intelligent replacement |
kind of client application |
to its members power |
with a platform on |
have implemented a similar |
a platform on which |
of client application development |
platform on which we |
implemented a similar scheme |
on which we could |
many of the pools |
more intelligent replacement decisions |
a similar scheme in |
which we could perform |
of the pools are |
we could perform research |
similar scheme in mfs |
could perform research more |
the pools are open |
perform research more effectively |
the authors present both |
our solution may perform |
in which an access |
research more effectively and |
authors present both offline |
pools are open they |
solution may perform very |
which an access to |
more effectively and it |
present both offline and |
are open they allow |
may perform very poorly |
an access to a |
effectively and it would |
both offline and online |
open they allow any |
access to a file |
and it would allows |
offline and online power |
they allow any miner |
or fail if the |
to a file which |
it would allows us |
allow any miner to |
fail if the hosted |
a file which has |
would allows us to |
any miner to join |
aware cache replacement algorithms |
if the hosted services |
file which has an |
allows us to focus |
miner to join them |
cache replacement algorithms to |
the hosted services are |
which has an uncommitted |
us to focus on |
to join them using |
replacement algorithms to optimize |
hosted services are inaccessible |
has an uncommitted update |
to focus on the |
join them using a |
algorithms to optimize read |
an uncommitted update at |
focus on the future |
them using a public |
to optimize read accesses |
all data will probably |
uncommitted update at a |
on the future directions |
using a public internet |
data will probably be |
update at a different |
they also show through |
a public internet interface |
will probably be visible |
the future directions without |
at a different client |
also show through experiments |
probably be visible to |
such open pools are |
a different client will |
show through experiments the |
future directions without having |
be visible to the |
open pools are susceptible |
different client will force |
through experiments the somewhat |
directions without having to |
visible to the hosted |
pools are susceptible to |
client will force the |
will force the writeback |
without having to worry |
to the hosted services |
are susceptible to the |
experiments the somewhat obvious |
having to worry whether |
the hosted services unless |
susceptible to the classical |
the somewhat obvious fact |
the mfs consistency algorithm |
to worry whether the |
hosted services unless the |
to the classical block |
somewhat obvious fact that |
mfs consistency algorithm differs |
worry whether the operating |
services unless the developer |
the classical block withholding |
obvious fact that for |
consistency algorithm differs in |
whether the operating system |
unless the developer uses |
classical block withholding attack |
fact that for write |
algorithm differs in its |
the operating system was |
the developer uses some |
that for write accesses |
differs in its incorporation |
operating system was capable |
developer uses some sort |
in its incorporation of |
system was capable of |
uses some sort of |
its incorporation of file |
was capable of supporting |
some sort of non |
incorporation of file access |
capable of supporting innovation |
of file access information |
back policies offer more |
policies offer more opportunities |
offer more opportunities to |
by now our complete |
rather than enforce the |
now our complete educational |
where a miner sends |
more opportunities to save |
than enforce the same |
our complete educational operation |
a miner sends only |
opportunities to save power |
enforce the same level |
complete educational operation and |
miner sends only partial |
to save power than |
the same level of |
educational operation and the |
sends only partial proof |
save power than write |
same level of consistency |
operation and the majority |
only partial proof of |
using live objects for |
level of consistency for |
and the majority of |
partial proof of work |
live objects for soc |
of consistency for all |
the majority of our |
proof of work to |
objects for soc applications |
consistency for all files |
majority of our research |
of work to the |
in the context of |
of our research projects |
for soc applications cornell |
work to the pool |
the context of write |
our research projects have |
mfs differentiates between private |
soc applications cornell s |
to the pool manager |
research projects have switched |
differentiates between private files |
applications cornell s live |
the pool manager and |
projects have switched to |
cornell s live objects |
a very natural candidate |
have switched to using |
pool manager and discards |
s live objects platform |
which have recently only |
very natural candidate is |
switched to using windows |
manager and discards full |
live objects platform supports |
have recently only been |
natural candidate is the |
to using windows nt |
and discards full proof |
objects platform supports componentized |
recently only been accessed |
candidate is the log |
discards full proof of |
only been accessed by |
full proof of work |
been accessed by a |
layered mashup creation and |
accessed by a single |
mashup creation and sharing |
by a single client |
due to the partial |
to the partial proof |
the partial proof of |
partial proof of work |
and overcomes limitations of |
proof of work it |
overcomes limitations of existing |
of work it sends |
as it now officially |
which are accessed by |
work it sends to |
limitations of existing web |
it now officially has |
are accessed by multiple |
it sends to the |
of existing web technologies |
now officially has been |
accessed by multiple clients |
sends to the pool |
officially has been christened |
the major design aspects |
major design aspects are |
design aspects are as |
we now give a |
enforcing cache consistency between |
aspects are as follows |
the miner is considered |
now give a brief |
cache consistency between clients |
miner is considered a |
the developer starts by |
consistency between clients necessarily |
give a brief overview |
is considered a regular |
developer starts by creating |
between clients necessarily requires |
a brief overview of |
considered a regular pool |
clients necessarily requires that |
or gaining access to |
brief overview of the |
a regular pool member |
the ride has been |
necessarily requires that shared |
a collection of components |
regular pool member and |
ride has been rocky |
overview of the log |
requires that shared files |
pool member and the |
has been rocky and |
each component is an |
that shared files are |
member and the pool |
been rocky and fascinating |
component is an object |
shared files are kept |
structured file system before |
and the pool can |
is an object that |
files are kept highly |
file system before describing |
the pool can estimate |
in this article i |
are kept highly consistent |
this article i want |
an object that supports |
pool can estimate its |
system before describing the |
article i want to |
object that supports live |
i want to share |
before describing the power |
but modifications to private |
can estimate its power |
that supports live functionality |
want to share some |
modifications to private files |
to share some of |
saving opportunity it represents |
to private files can |
share some of the |
and exposes eventbased interfaces |
private files can be |
the attacker shares the |
some of the reasoning |
exposes eventbased interfaces by |
files can be written |
attacker shares the revenue |
of the reasoning behind |
eventbased interfaces by which |
can be written back |
shares the revenue obtained |
the reasoning behind our |
interfaces by which it |
be written back to |
the revenue obtained by |
reasoning behind our choice |
by which it interacts |
written back to the |
revenue obtained by the |
behind our choice for |
which it interacts with |
back to the server |
obtained by the other |
our choice for windows |
it interacts with other |
structured file system the |
to the server less |
by the other pool |
choice for windows nt |
interacts with other components |
file system the log |
the server less aggressively |
the other pool members |
for windows nt and |
windows nt and to |
nt and to share |
and to share some |
to share some our |
the technique of using |
but does not contribute |
share some our experiences |
components representing hosted content |
technique of using file |
some our experiences with |
representing hosted content sensors |
of using file access |
hosted content sensors and |
it reduces the revenue |
content sensors and actuators |
was motivated by a |
using file access patterns |
our experiences with windows |
reduces the revenue of |
sensors and actuators renderers |
motivated by a need |
and actuators renderers that |
experiences with windows nt |
actuators renderers that graphically |
file access patterns to |
renderers that graphically depict |
access patterns to adjust |
that graphically depict events |
by a need to |
the revenue of the |
with windows nt as |
patterns to adjust a |
graphically depict events replication |
a need to optimize |
revenue of the other |
windows nt as a |
to adjust a cache |
depict events replication protocols |
need to optimize the |
of the other members |
nt as a research |
adjust a cache consistency |
events replication protocols synchronization |
to optimize the latency |
but also its own |
a cache consistency protocol |
replication protocols synchronization protocols |
as a research platform |
optimize the latency of |
cache consistency protocol has |
protocols synchronization protocols folders |
we provide necessary background |
the latency of write |
os research as religion |
synchronization protocols folders containing |
provide necessary background on |
consistency protocol has been |
research as religion the |
protocols folders containing sets |
necessary background on the |
protocol has been used |
as religion the biggest |
folders containing sets of |
background on the bitcoin |
has been used in |
writing a block of |
religion the biggest hurdle |
containing sets of objects |
on the bitcoin protocol |
been used in the |
a block of data |
the biggest hurdle in |
sets of objects display |
used in the sprite |
block of data to |
biggest hurdle in starting |
pools and the classical |
of objects display interfaces |
in the sprite distributed |
of data to a |
hurdle in starting research |
and the classical block |
objects display interfaces that |
the sprite distributed operation |
data to a seagate |
in starting research on |
the classical block withholding |
display interfaces that visualize |
sprite distributed operation system |
to a seagate barracuda |
starting research on windows |
classical block withholding attack |
interfaces that visualize folders |
a seagate barracuda disk |
research on windows nt |
block withholding attack in |
seagate barracuda disk costs |
on windows nt was |
mashups of components are |
barracuda disk costs about |
withholding attack in section |
windows nt was not |
of components are represented |
attack in section ii |
nt was not technical |
components are represented as |
are represented as a |
represented as a kind |
as a kind of |
though in sprite changes |
a kind of xml |
and specify our model |
in sprite changes in |
it was to overcome |
kind of xml web |
was to overcome the |
sprite changes in caching |
specify our model in |
of xml web pages |
to overcome the skepticism |
changes in caching policy |
our model in section |
ms in seek time |
overcome the skepticism of |
each describing a recipe |
model in section iii |
in seek time and |
in caching policy were |
the skepticism of our |
describing a recipe for |
caching policy were made |
skepticism of our colleagues |
a recipe for obtaining |
for a broader view |
policy were made when |
of our colleagues who |
recipe for obtaining and |
a broader view of |
were made when a |
our colleagues who were |
for obtaining and parameterizing |
broader view of the |
made when a file |
colleagues who were convinced |
obtaining and parameterizing components |
view of the protocol |
when a file was |
who were convinced that |
and parameterizing components that |
of the protocol and |
a file was opened |
were convinced that it |
parameterizing components that will |
the protocol and ecosystem |
file was opened simultaneously |
convinced that it would |
kb in transmission time |
protocol and ecosystem the |
was opened simultaneously at |
components that will serve |
that it would not |
and ecosystem the reader |
opened simultaneously at different |
that will serve as |
it would not be |
ecosystem the reader may |
the key observation here |
simultaneously at different clients |
will serve as layers |
would not be possible |
the reader may refer |
key observation here is |
serve as layers of |
not be possible to |
reader may refer to |
while mfs uses longer |
observation here is that |
as layers of the |
be possible to use |
may refer to the |
here is that seek |
layers of the composed |
possible to use windows |
refer to the survey |
is that seek time |
of the composed mashup |
to use windows nt |
to the survey by |
the remainder of this |
that seek time is |
use windows nt as |
the survey by bonneau |
remainder of this section |
seek time is a |
windows nt as a |
we call such an |
survey by bonneau et |
of this section describes |
time is a large |
nt as a good |
call such an xml |
by bonneau et al |
this section describes our |
is a large and |
as a good platform |
such an xml page |
section describes our consistency |
a large and constant |
a good platform for |
an xml page a |
describes our consistency algorithm |
large and constant term |
good platform for research |
xml page a live |
our consistency algorithm in |
and constant term in |
page a live object |
consistency algorithm in detail |
constant term in latency |
a live object reference |
the predictions were fascinating |
term in latency computation |
and an evaluation of |
an evaluation of its |
evaluation of its effectiveness |
we would turn into |
in this work we |
of its effectiveness in |
references can be distributed |
to eliminate this term |
this work we analyze |
its effectiveness in reducing |
would turn into a |
can be distributed as |
work we analyze block |
effectiveness in reducing cache |
turn into a bug |
be distributed as files |
we analyze block withholding |
the lfs replaces write |
in reducing cache inconsistencies |
analyze block withholding attacks |
lfs replaces write operations |
block withholding attacks among |
microsoft would sue the |
replaces write operations by |
host reader writer parameter |
would sue the department |
http or other means |
withholding attacks among pools |
write operations by append |
reader writer parameter delay |
sue the department for |
an soc application is |
writer parameter delay between |
operations by append operations |
parameter delay between accessing |
a pool that employs |
delay between accessing modules |
the department for every |
between accessing modules operations |
pool that employs the |
accessing modules operations per |
that employs the pool |
soc application is created |
secondary storage is treated |
department for every technical |
modules operations per module |
employs the pool block |
operations per module delay |
the pool block withholding |
per module delay between |
application is created by |
storage is treated as |
for every technical publication |
pool block withholding attack |
module delay between operations |
is created by building |
is treated as a |
block withholding attack registers |
delay between operations delay |
created by building a |
treated as a large |
microsoft would hide the |
withholding attack registers with |
between operations delay between |
by building a forest |
as a large append |
would hide the pieces |
attack registers with the |
operations delay between accessing |
building a forest consisting |
hide the pieces of |
registers with the victim |
only log and writes |
a forest consisting of |
the pieces of buggy |
delay between accessing modules |
with the victim pool |
log and writes always |
forest consisting of graphs |
pieces of buggy code |
between accessing modules operations |
the victim pool as |
and writes always go |
consisting of graphs of |
of buggy code from |
accessing modules operations per |
victim pool as a |
writes always go to |
of graphs of references |
buggy code from us |
modules operations per module |
pool as a regular |
as a regular miner |
graphs of references that |
code from us or |
operations per module delay |
always go to the |
it receives tasks from |
from us or bill |
per module delay between |
of references that are |
go to the log |
receives tasks from the |
us or bill gates |
module delay between operations |
references that are mashed |
that are mashed together |
tasks from the victim |
or bill gates would |
delay between operations size |
to the log head |
from the victim pool |
bill gates would personally |
between operations size of |
the victim pool and |
gates would personally tell |
operations size of external |
victim pool and transfers |
an automated tool lets |
would personally tell us |
automated tool lets the |
size of external files |
tool lets the developer |
seek time is thus |
personally tell us where |
pool and transfers them |
of external files value |
lets the developer drag |
time is thus eliminated |
tell us where and |
and transfers them to |
the developer drag and |
us where and how |
transfers them to some |
developer drag and drop |
and write latency becomes |
drag and drop to |
write latency becomes purely |
them to some of |
where and how we |
and drop to combine |
latency becomes purely a |
to some of its |
and how we should |
drop to combine references |
becomes purely a function |
some of its own |
how we should do |
to combine references for |
purely a function of |
of its own miners |
we should do our |
combine references for individual |
a function of the |
should do our research |
references for individual objects |
function of the disk |
we call these infiltrating |
for individual objects into |
of the disk bandwidth |
call these infiltrating miners |
individual objects into an |
objects into an xml |
into an xml mashup |
an xml mashup of |
xml mashup of references |
how do reads in |
and the mining power |
mashup of references describing |
do reads in the |
the mining power spent |
of references describing a |
reads in the lfs |
mining power spent by |
references describing a graph |
the operating systems research |
power spent by a |
in the lfs work |
describing a graph of |
operating systems research community |
spent by a pool |
a graph of objects |
systems research community has |
by a pool the |
in the same way |
research community has not |
a pool the infiltration |
the same way as |
community has not remained |
pool the infiltration rate |
same way as in |
checks mashups to verify |
has not remained untouched |
way as in conventional |
mashups to verify that |
when the attacking pool |
as in conventional file |
not remained untouched by |
to verify that they |
the attacking pool s |
in conventional file systems |
remained untouched by the |
verify that they compose |
attacking pool s infiltrating |
untouched by the market |
that they compose correctly |
pool s infiltrating miners |
by the market place |
s infiltrating miners deliver |
the market place rivalry |
infiltrating miners deliver partial |
market place rivalry between |
miners deliver partial proofs |
place rivalry between microsoft |
deliver partial proofs of |
rivalry between microsoft and |
partial proofs of work |
and hence do not |
between microsoft and the |
hence do not avoid |
microsoft and the group |
do not avoid seek |
and the group lead |
d visualization of an |
the attacker transfers them |
the group lead by |
visualization of an airplane |
attacker transfers them to |
group lead by sun |
of an airplane may |
transfers them to the |
lead by sun microsystems |
an airplane may need |
them to the victim |
airplane may need to |
to the victim pool |
it is even more |
may need to be |
the assumption is that |
is even more unfortunate |
need to be connected |
assumption is that with |
even more unfortunate that |
to be connected to |
is that with good |
letting the attacked pool |
be connected to a |
more unfortunate that the |
that with good caching |
the attacked pool estimate |
connected to a source |
unfortunate that the positions |
with good caching mechanisms |
attacked pool estimate their |
to a source of |
that the positions taken |
configuration parameters for the |
pool estimate their power |
a source of gps |
the positions taken are |
parameters for the cache |
reads will be a |
source of gps and |
positions taken are not |
when the infiltrating miners |
will be a small |
of gps and other |
for the cache consistency |
taken are not based |
the infiltrating miners deliver |
be a small fraction |
gps and other orientation |
the cache consistency evaluation |
are not based on |
infiltrating miners deliver a |
a small fraction of |
and other orientation data |
not based on intellectual |
miners deliver a full |
small fraction of disk |
individual instances are uniformally |
based on intellectual deliberation |
deliver a full proof |
fraction of disk accesses |
instances are uniformally distributed |
on intellectual deliberation but |
which in turn needs |
a full proof of |
are uniformally distributed within |
intellectual deliberation but purely |
in turn needs to |
full proof of work |
as can be imagined |
uniformally distributed within the |
deliberation but purely on |
turn needs to run |
distributed within the listed |
but purely on emotional |
the attacking pool discards |
needs to run over |
within the listed ranges |
space reclamation is a |
purely on emotional grounds |
attacking pool discards it |
to run over a |
reclamation is a tricky |
run over a data |
is a tricky problem |
many see microsoft operating |
over a data replication |
a tricky problem in |
this attack affects the |
see microsoft operating systems |
if the file is |
tricky problem in log |
attack affects the revenues |
a data replication protocol |
microsoft operating systems as |
the file is shared |
problem in log structured |
affects the revenues of |
data replication protocol with |
operating systems as the |
file is shared and |
in log structured file |
the revenues of the |
replication protocol with specific |
systems as the evil |
is shared and no |
log structured file systems |
revenues of the pools |
protocol with specific reliability |
as the evil empire |
shared and no other |
of the pools in |
and no other shared |
the pools in several |
no other shared update |
ordering or security properties |
pools in several ways |
out to squash every |
other shared update is |
excellent solutions have been |
to squash every attempt |
shared update is being |
the victim pool s |
when activated on a |
squash every attempt at |
solutions have been proposed |
update is being sent |
victim pool s effective |
activated on a user |
every attempt at innovation |
have been proposed to |
pool s effective mining |
on a user s |
been proposed to solve |
the thread begins transmitting |
s effective mining rate |
a user s machine |
proposed to solve it |
and working with them |
thread begins transmitting the |
effective mining rate is |
working with them is |
begins transmitting the update |
and one such is |
an xml mashup yields |
with them is seen |
mining rate is unchanged |
transmitting the update at |
one such is of |
xml mashup yields a |
them is seen as |
the update at the |
such is of interest |
mashup yields a graph |
is seen as collaboration |
but its total revenue |
update at the store |
is of interest to |
yields a graph of |
seen as collaboration with |
its total revenue is |
of interest to us |
a graph of interconnected |
as collaboration with the |
total revenue is divided |
if another shared update |
collaboration with the enemy |
graph of interconnected proxies |
revenue is divided among |
another shared update is |
the disk is divided |
with the enemy of |
is divided among more |
shared update is being |
disk is divided into |
the enemy of free |
a proxy is a |
divided among more miners |
update is being written |
is divided into large |
enemy of free academic |
proxy is a piece |
is being written back |
the attacker s mining |
of free academic speech |
is a piece of |
divided into large log |
attacker s mining power |
a piece of running |
into large log segments |
a synchronous forward invalidation |
s mining power is |
piece of running code |
the pros and cons |
synchronous forward invalidation rpc |
mining power is reduced |
of running code that |
pros and cons are |
once a log segment |
forward invalidation rpc is |
running code that may |
since some of its |
a log segment gets |
invalidation rpc is made |
and cons are often |
code that may render |
some of its miners |
log segment gets filled |
rpc is made to |
cons are often discussed |
of its miners are |
is made to the |
are often discussed with |
its miners are used |
made to the server |
a new log segment |
often discussed with a |
or transform visual content |
miners are used for |
to the server at |
new log segment is |
discussed with a righteous |
encapsulate a protocol stack |
the server at the |
log segment is allocated |
are used for block |
with a righteous zeal |
server at the highest |
segment is allocated and |
used for block withholding |
a righteous zeal that |
at the highest priority |
is allocated and the |
righteous zeal that is |
allocated and the log |
but it earns additional |
zeal that is frightening |
and the log head |
component in the xml |
it earns additional revenue |
and then the update |
the log head moves |
in the xml mashup |
earns additional revenue through |
then the update is |
our own experiences with |
log head moves to |
the xml mashup produces |
additional revenue through its |
the update is queued |
own experiences with microsoft |
head moves to the |
xml mashup produces an |
revenue through its infiltration |
update is queued for |
experiences with microsoft can |
moves to the new |
mashup produces an associated |
through its infiltration of |
is queued for later |
with microsoft can only |
to the new segment |
produces an associated proxy |
its infiltration of the |
queued for later high |
microsoft can only be |
infiltration of the other |
can only be described |
the hierarchy of proxies |
of the other pool |
when some threshold of |
only be described as |
hierarchy of proxies reflects |
some threshold of a |
be described as extremely |
of proxies reflects the |
threshold of a segment |
a forward invalidation is |
proxies reflects the hierarchical |
described as extremely positive |
of a segment gets |
forward invalidation is only |
the total effective mining |
reflects the hierarchical structure |
a segment gets invalidated |
invalidation is only made |
total effective mining power |
the hierarchical structure of |
never before have we |
is only made if |
its valid data is |
hierarchical structure of the |
before have we had |
effective mining power in |
only made if the |
valid data is moved |
structure of the xml |
have we had such |
mining power in the |
made if the update |
data is moved to |
of the xml mashup |
we had such a |
power in the system |
if the update cannot |
is moved to another |
had such a positive |
in the system is |
the update cannot be |
moved to another segment |
such a positive relation |
an object proxy can |
update cannot be transmitted |
the system is reduced |
a positive relation with |
replacing that segment s |
cannot be transmitted immediately |
object proxy can initialize |
positive relation with a |
that segment s invalid |
proxy can initialize itself |
causing the bitcoin protocol |
relation with a vendor |
segment s invalid data |
can initialize itself by |
in practice it can |
the bitcoin protocol to |
initialize itself by copying |
practice it can therefore |
without any pressure from |
bitcoin protocol to reduce |
itself by copying the |
it can therefore be |
and it is then |
protocol to reduce the |
by copying the state |
any pressure from their |
can therefore be omitted |
it is then added |
to reduce the difficulty |
copying the state from |
pressure from their side |
therefore be omitted at |
is then added to |
the state from some |
be omitted at high |
then added to the |
state from some active |
taking all these factors |
omitted at high bandwidth |
added to the pool |
we can only conclude |
from some active proxy |
can only conclude that |
at high bandwidth or |
only conclude that the |
all these factors into |
conclude that the reasons |
high bandwidth or when |
that the reasons for |
these factors into account |
the reasons for the |
to the pool of |
reasons for the controversy |
our platform assists with |
for the controversy must |
the pool of free |
bandwidth or when traffic |
platform assists with this |
we observe that a |
the controversy must be |
pool of free log |
controversy must be found |
assists with this sort |
must be found in |
or when traffic is |
be found in a |
observe that a pool |
with this sort of |
of free log segments |
when traffic is low |
found in a sort |
that a pool might |
this sort of state |
in a sort of |
a pool might be |
sort of state transfer |
a sort of traditional |
pool might be able |
this process results in |
sort of traditional emotional |
might be able to |
sending a forward invalidation |
process results in a |
of traditional emotional bonding |
be able to increase |
a forward invalidation rpc |
results in a natural |
the object proxies then |
traditional emotional bonding of |
able to increase its |
forward invalidation rpc without |
in a natural division |
object proxies then become |
emotional bonding of academia |
to increase its revenue |
invalidation rpc without requiring |
a natural division of |
proxies then become active |
bonding of academia with |
increase its revenue by |
rpc without requiring the |
natural division of allocated |
of academia with the |
its revenue by attacking |
without requiring the modifying |
requiring the modifying process |
academia with the underdog |
revenue by attacking other |
by attacking other pools |
the modifying process to |
for example by relaying |
division of allocated segments |
with the underdog and |
modifying process to wait |
each pool therefore makes |
of allocated segments into |
the underdog and that |
example by relaying events |
process to wait introduces |
pool therefore makes a |
allocated segments into stable |
underdog and that no |
by relaying events from |
therefore makes a choice |
and that no real |
relaying events from sensors |
makes a choice of |
that no real experiences |
events from sensors into |
a choice of whether |
no real experiences drive |
from sensors into a |
choice of whether to |
the consistency maintenance algorithm |
real experiences drive the |
sensors into a replica |
of whether to attack |
consistency maintenance algorithm a |
consisting almost entirely of |
experiences drive the discussion |
whether to attack each |
maintenance algorithm a transient |
almost entirely of data |
to attack each of |
algorithm a transient inconsistency |
entirely of data that |
gaining knowledge the foremost |
or by receiving events |
knowledge the foremost reasons |
of data that is |
the foremost reasons why |
by receiving events and |
foremost reasons why unix |
attack each of the |
reasons why unix was |
when the server receives |
why unix was such |
the server receives a |
each of the other |
receiving events and reacting |
data that is rarely |
unix was such a |
server receives a forward |
was such a powerhouse |
events and reacting to |
such a powerhouse in |
of the other pools |
receives a forward invalidation |
that is rarely invalidated |
and reacting to them |
a powerhouse in operating |
the other pools in |
powerhouse in operating system |
a forward invalidation for |
other pools in the |
in operating system research |
forward invalidation for a |
pools in the system |
operating system research was |
invalidation for a shared |
system research was the |
for a shared the |
research was the great |
by redisplaying an aircraft |
a shared the mfs |
and with what infiltration |
which need to be |
shared the mfs cache |
was the great amount |
with what infiltration rate |
need to be constantly |
the mfs cache consistency |
the great amount of |
our approach shares certain |
mfs cache consistency algorithm |
to be constantly cleaned |
great amount of knowledge |
approach shares certain similarities |
this gives rise to |
cache consistency algorithm is |
amount of knowledge accumulated |
shares certain similarities with |
gives rise to the |
consistency algorithm is intended |
of knowledge accumulated over |
we will see how |
rise to the pool |
algorithm is intended to |
certain similarities with the |
knowledge accumulated over the |
will see how this |
to the pool game |
is intended to achieve |
similarities with the existing |
accumulated over the years |
see how this feature |
intended to achieve a |
we specify this game |
over the years about |
how this feature can |
with the existing web |
to achieve a file |
specify this game and |
the years about the |
this feature can be |
the existing web development |
this game and provide |
or begins receiving an |
feature can be used |
existing web development model |
years about the internal |
game and provide initial |
begins receiving an update |
can be used to |
about the internal operation |
in the sense that |
receiving an update for |
be used to save |
and provide initial analysis |
the internal operation of |
the sense that it |
an update for a |
update for a file |
provide initial analysis in |
internal operation of the |
sense that it uses |
used to save power |
initial analysis in section |
operation of the operating |
that it uses hierarchical |
it records the idenhigh |
analysis in section iv |
of the operating system |
it uses hierarchical xml |
records the idenhigh degree |
uses hierarchical xml documents |
the idenhigh degree of |
hierarchical xml documents to |
idenhigh degree of consistency |
many of us had |
xml documents to define |
in section v we |
of us had become |
documents to define the |
section v we analyze |
subject to the constraints |
us had become gurus |
to define the content |
v we analyze the |
to the constraints imposed |
had become gurus about |
we analyze the scenario |
the constraints imposed by |
become gurus about some |
on the other hand |
analyze the scenario where |
constraints imposed by tity |
gurus about some part |
the scenario where exactly |
imposed by tity of |
saving opportunity we shall |
about some part of |
we depart from some |
scenario where exactly two |
by tity of the |
opportunity we shall now |
some part of the |
depart from some of |
where exactly two of |
tity of the writer |
we shall now argue |
part of the os |
from some of the |
exactly two of the |
shall now argue that |
marks the file as |
now argue that there |
the file as dirty |
argue that there remains |
file as dirty and |
that there remains an |
of the os kernel |
some of the de |
two of the pools |
as dirty and issues |
there remains an unexplored |
the os kernel and |
of the pools take |
dirty and issues callbacks |
remains an unexplored quadrant |
facto stylistic standards that |
os kernel and could |
the pools take part |
and issues callbacks to |
an unexplored quadrant in |
stylistic standards that have |
kernel and could recite |
pools take part in |
issues callbacks to file |
unexplored quadrant in this |
standards that have emerged |
and could recite the |
take part in the |
callbacks to file semantics |
quadrant in this solution |
could recite the fields |
part in the game |
to file semantics and |
in this solution space |
for example if one |
recite the fields of |
in the game and |
file semantics and the |
example if one pulls |
caches are used to |
the game and only |
semantics and the desirability |
are used to minimize |
if one pulls a |
game and only one |
the fields of an |
and the desirability of |
used to minimize accesses |
one pulls a minibrowser |
and only one can |
fields of an i |
the desirability of minimising |
to minimize accesses to |
pulls a minibrowser from |
only one can attack |
desirability of minimising overhead |
node structure at late |
a minibrowser from google |
one can attack the |
minimize accesses to disk |
structure at late night |
we all the clients |
at late night meetings |
minibrowser from google earth |
can attack the other |
all the clients caching |
late night meetings or |
good caching algorithms practically |
the clients caching it |
night meetings or discuss |
it expects to interact |
caching algorithms practically eliminate |
meetings or discuss which |
expects to interact directly |
the attacker can always |
if one of these |
or discuss which data |
one of these clients |
discuss which data structures |
of these clients fetches |
algorithms practically eliminate read |
to interact directly with |
attacker can always increase |
which data structures to |
these clients fetches the |
practically eliminate read accesses |
interact directly with the |
can always increase its |
data structures to modify |
clients fetches the file |
eliminate read accesses to |
directly with the end |
always increase its revenue |
structures to modify to |
fetches the file have |
read accesses to disk |
with the end user |
increase its revenue by |
to modify to add |
the file have opted |
its revenue by attacking |
modify to add a |
and includes embedded javascript |
file have opted for |
to add a new |
includes embedded javascript that |
have opted for a |
add a new protocol |
we conclude that in |
embedded javascript that handles |
opted for a compromise |
a new protocol at |
conclude that in the |
whether synchronous or not |
javascript that handles such |
for a compromise which |
new protocol at runtime |
that in the general |
that handles such interactions |
a compromise which results |
protocol at runtime over |
in the general case |
must still eventually access |
compromise which results in |
at runtime over an |
still eventually access the |
which results in a |
runtime over an early |
eventually access the disk |
with any number of |
results in a small |
over an early morning |
the same functionality would |
any number of pools |
same functionality would be |
an early morning cappuccino |
functionality would be represented |
in a small overhead |
would be represented as |
a small overhead before |
be represented as a |
small overhead before the |
represented as a mashup |
disk access will be |
as a mashup of |
overhead before the update |
many of us were |
access will be write |
of us were and |
before the update has |
us were and still |
a mashup of a |
were and still are |
mashup of a component |
attacks is not a |
the update has been |
and still are afraid |
of a component that |
still are afraid to |
update has been committed |
are afraid to leave |
is not a nash |
afraid to leave this |
putting a disk management |
to leave this bastion |
not a nash equilibrium |
leave this bastion of |
a disk management layer |
the server sends highbut |
a component that fetches |
this bastion of safety |
disk management layer on |
server sends highbut admits |
component that fetches maps |
bastion of safety behind |
section vi deals with |
of safety behind and |
that fetches maps and |
management layer on top |
sends highbut admits the |
vi deals with the |
safety behind and trade |
fetches maps and similar |
layer on top of |
highbut admits the possibility |
deals with the case |
behind and trade it |
maps and similar content |
on top of the |
admits the possibility of |
with the case of |
and trade it in |
and similar content with |
top of the file |
the possibility of a |
the case of two |
trade it in for |
similar content with a |
possibility of a transient |
case of two pools |
it in for working |
content with a second |
of a transient inconsistency |
system to optimize data |
in for working on |
with a second component |
to optimize data layout |
where each can attack |
priority server pull rpcs |
a second component that |
optimize data layout for |
for working on an |
each can attack the |
server pull rpcs to |
second component that provides |
data layout for writes |
working on an operating |
can attack the other |
on an operating system |
component that provides the |
layout for writes is |
pull rpcs to the |
an operating system that |
that provides the visualization |
for writes is only |
rpcs to the clients |
operating system that at |
provides the visualization interface |
writes is only halfway |
analysis becomes more complicated |
to the clients with |
system that at first |
is only halfway to |
becomes more complicated in |
the clients with outstanding |
that at first sight |
only halfway to the |
although the term mashup |
more complicated in two |
clients with outstanding upthe |
at first sight had |
halfway to the solution |
the term mashup may |
complicated in two ways |
with outstanding upthe algorithm |
first sight had nothing |
term mashup may sound |
to take this idea |
sight had nothing in |
outstanding upthe algorithm requires |
mashup may sound static |
take this idea to |
had nothing in common |
upthe algorithm requires information |
this idea to its |
nothing in common with |
the revenue of each |
algorithm requires information about |
idea to its logical |
in the sense of |
in common with our |
revenue of each pool |
requires information about client |
to its logical conclusion |
the sense of having |
common with our beloved |
of each pool affects |
information about client accesses |
sense of having its |
it is necessary to |
each pool affects the |
about client accesses in |
with our beloved unix |
of having its components |
is necessary to rethink |
pool affects the revenue |
client accesses in dates |
having its components predetermined |
necessary to rethink the |
affects the revenue of |
and our annotated version |
to rethink the file |
which causes them to |
our annotated version of |
the revenue of the |
rethink the file the |
this is not necessarily |
causes them to raise |
annotated version of the |
revenue of the other |
the file the disk |
is not necessarily the |
them to raise the |
version of the unix |
of the other through |
not necessarily the case |
to raise the priority |
of the unix version |
the other through the |
management policies described in |
raise the priority of |
one kind of live |
policies described in the |
other through the infiltrating |
described in the related |
kind of live object |
in the related works |
through the infiltrating miners |
the priority of any |
of live object could |
the related works section |
priority of any store |
live object could be |
object could be a |
related works section essentially |
we prove that for |
could be a folder |
prove that for a |
data order to divide |
order to divide files |
be a folder including |
that for a static |
for a static choice |
a static choice of |
to divide files according |
a folder including a |
wouldn t be of |
works section essentially attack |
t be of much |
divide files according their |
be of much help |
static choice of infiltration |
of much help any |
folder including a set |
much help any more |
section essentially attack the |
choice of infiltration rates |
files according their status |
including a set of |
help any more either |
essentially attack the problem |
any more either it |
a set of objects |
either shared or unrpcs |
attack the problem by |
more either it took |
of infiltration rates the |
either it took more |
the problem by trying |
it took more then |
problem by trying to |
took more then a |
infiltration rates the pool |
more then a year |
shared or unrpcs to |
then a year of |
for example extracted from |
rates the pool revenues |
by trying to predict |
or unrpcs to expedite |
a year of immersion |
example extracted from a |
the pool revenues converge |
trying to predict in |
unrpcs to expedite transmission |
year of immersion in |
extracted from a directory |
to predict in advance |
of immersion in the |
from a directory in |
predict in advance which |
a fetch rpc for |
immersion in the technology |
once one pool changes |
in advance which disk |
fetch rpc for an |
a directory in a |
in the technology to |
one pool changes its |
advance which disk any |
rpc for an unshared |
directory in a file |
the technology to get |
pool changes its infiltration |
which disk any given |
for an unshared file |
in a file system |
technology to get a |
changes its infiltration rate |
disk any given access |
an unshared file shared |
a file system or |
to get a level |
its infiltration rate of |
any given access will |
file system or pulled |
get a level where |
infiltration rate of the |
given access will go |
system or pulled from |
since the file server |
a level where i |
rate of the other |
access will go to |
or pulled from a |
the file server always |
level where i felt |
pulled from a database |
file server always assumes |
where i felt confident |
they optimize the data |
from a database in |
server always assumes that |
the latter may prefer |
always assumes that an |
optimize the data layout |
assumes that an unshared |
i felt confident again |
that an unshared which |
a database in response |
an unshared which is |
latter may prefer to |
unshared which is already |
the data layout on |
database in response to |
in response to a |
may prefer to change |
which is already cached |
data layout on disks |
is already cached by |
layout on disks to |
already cached by a |
felt confident again to |
response to a query |
prefer to change its |
on disks to ensure |
cached by a different |
confident again to direct |
to change its infiltration |
disks to ensure that |
by a different client |
to ensure that accesses |
when the folder contents |
change its infiltration rate |
again to direct others |
a different client always |
ensure that accesses are |
the folder contents change |
its infiltration rate of |
to direct others in |
different client always triggers |
that accesses are localized |
the mashup is dynamically |
accesses are localized to |
client always triggers a |
infiltration rate of the |
direct others in our |
mashup is dynamically updated |
are localized to some |
always triggers a file |
rate of the former |
others in our research |
localized to some fraction |
triggers a file has |
in our research group |
as might occur when |
to some fraction of |
a file has an |
therefore the game itself |
might occur when a |
some fraction of the |
file has an uncommitted |
the game itself takes |
occur when a rescue |
together with the overall |
fraction of the disks |
has an uncommitted write |
game itself takes multiple |
when a rescue worker |
with the overall organizational |
an uncommitted write when |
the overall organizational issues |
a rescue worker enters |
itself takes multiple rounds |
uncommitted write when it |
overall organizational issues i |
rescue worker enters a |
organizational issues i think |
so that only these |
write when it is |
takes multiple rounds to |
worker enters a building |
issues i think we |
that only these need |
when it is accessed |
multiple rounds to converge |
enters a building or |
i think we lost |
only these need be |
it is accessed by |
a building or turns |
think we lost one |
these need be powered |
is accessed by an |
we show analytically that |
building or turns a |
we lost one and |
need be powered up |
accessed by an addiserver |
show analytically that the |
or turns a corner |
lost one and a |
by an addiserver pull |
analytically that the game |
one and a half |
that the game has |
and a half year |
the game has a |
a half year worth |
game has a single |
since the server has |
live objects can easily |
has a single nash |
half year worth of |
the server has no |
objects can easily support |
a single nash equilibrium |
year worth of research |
these are all probabilistic |
server has no way |
can easily support applications |
single nash equilibrium and |
worth of research time |
are all probabilistic models |
has no way of |
easily support applications that |
nash equilibrium and numerically |
of research time to |
no way of knowing |
support applications that dynamically |
equilibrium and numerically study |
research time to make |
way of knowing if |
a new access has |
and numerically study the |
new access has some |
applications that dynamically recompute |
access has some probability |
time to make the |
has some probability of |
of knowing if the |
some probability of not |
numerically study the equilibrium |
to make the switch |
that dynamically recompute the |
knowing if the file |
probability of not fitting |
study the equilibrium points |
make the switch in |
dynamically recompute the set |
if the file has |
of not fitting this |
the equilibrium points for |
the switch in the |
recompute the set of |
the file has tional |
not fitting this model |
equilibrium points for different |
switch in the most |
the set of visible |
file has tional client |
fitting this model and |
points for different pool |
in the most fundamental |
set of visible objects |
this model and needing |
for different pool sizes |
the most fundamental way |
incorrect information about the |
model and needing to |
information about the status |
as a function of |
for pools smaller than |
about the status of |
and needing to access |
a function of location |
others are making the |
the status of a |
needing to access a |
function of location and |
are making the switch |
status of a file |
to access a powered |
of location and orientation |
making the switch more |
of a file only |
the switch more gradually |
a file only outstanding |
switch more gradually and |
at the equilibrium point |
and dynamically add or |
file only outstanding updates |
more gradually and are |
the equilibrium point both |
dynamically add or remove |
gradually and are experiencing |
equilibrium point both pools |
add or remove them |
and are experiencing a |
affects the efficiency of |
point both pools earn |
or remove them from |
are experiencing a more |
the efficiency of the |
both pools earn less |
remove them from the |
experiencing a more smooth |
efficiency of the algorithm |
pools earn less than |
them from the mashup |
a more smooth transition |
earn less than they |
less than they would |
detection of such a |
than they would have |
of such a misfinally |
they would have in |
a rescuer would automatically |
disk layout becomes tied |
all operation systems are |
would have in the |
operation systems are created |
layout becomes tied to |
since updates to shared |
rescuer would automatically and |
have in the nonequilibrium |
systems are created equal |
becomes tied to particular |
updates to shared and |
would automatically and instantly |
in the nonequilibrium no |
are created equal our |
tied to particular applications |
to shared and unshared |
automatically and instantly be |
created equal our experiences |
shared and unshared files |
and instantly be shown |
equal our experiences with |
and unshared files are |
instantly be shown the |
our experiences with switching |
unshared files are writclassification |
be shown the avatars |
experiences with switching to |
two applications that have |
files are writclassification results |
since pools can decide |
shown the avatars of |
with switching to windows |
are writclassification results in |
applications that have completely |
pools can decide to |
the avatars of others |
switching to windows nt |
writclassification results in the |
can decide to start |
that have completely different |
avatars of others who |
to windows nt have |
results in the file |
decide to start or |
of others who are |
have completely different access |
windows nt have made |
in the file being |
to start or stop |
others who are already |
nt have made us |
completely different access patterns |
the file being marked |
different access patterns might |
who are already working |
have made us somewhat |
start or stop attacking |
file being marked as |
are already working at |
access patterns might require |
made us somewhat more |
or stop attacking at |
patterns might require completely |
already working at that |
might require completely different |
being marked as shared |
require completely different data |
us somewhat more philosophical |
completely different data layouts |
stop attacking at any |
different data layouts on |
somewhat more philosophical about |
data layouts on disk |
attacking at any point |
layouts on disk leading |
working at that site |
more philosophical about the |
ten back to the |
on disk leading to |
this can be modeled |
disk leading to conflicts |
philosophical about the nature |
back to the server |
can be modeled as |
and be able to |
leading to conflicts that |
about the nature of |
to the server at |
be modeled as the |
be able to participate |
to conflicts that reduce |
the nature of operation |
the server at different |
modeled as the miner |
able to participate in |
conflicts that reduce possible |
nature of operation systems |
server at different priorities |
as the miner s |
to participate in conference |
that reduce possible powersavings |
the miner s dilemma |
the most fundamental observation |
the original order of |
miner s dilemma an |
original order of the |
most fundamental observation is |
s dilemma an instance |
order of the status |
fundamental observation is that |
dilemma an instance of |
since all writes in |
of the status of |
an instance of the |
point dialog with them |
all writes in an |
the status of files |
instance of the iterative |
when stripped to their |
writes in an lfs |
status of files can |
of the iterative prisoner |
stripped to their core |
through chat objects that |
in an lfs are |
of files can be |
the iterative prisoner s |
all operating systems are |
an lfs are to |
files can be specified |
chat objects that run |
iterative prisoner s dilemma |
operating systems are equal |
lfs are to the |
can be specified by |
objects that run over |
are to the log |
the functionality of the |
that run over multicast |
attacking is the dominant |
be specified by the |
to the log head |
functionality of the windows |
run over multicast protocol |
is the dominant strategy |
specified by the user |
of the windows nt |
over multicast protocol objects |
the dominant strategy in |
by the user or |
the windows nt kernel |
we know in advance |
the user or by |
this model can support |
windows nt kernel is |
know in advance which |
dominant strategy in each |
user or by applithe |
model can support a |
nt kernel is just |
in advance which disk |
strategy in each iteration |
or by applithe sequence |
can support a wide |
kernel is just as |
advance which disk they |
by applithe sequence of |
support a wide variety |
is just as all |
but if the pools |
which disk they will |
applithe sequence of updates |
a wide variety of |
just as all other |
if the pools can |
disk they will access |
sequence of updates is |
wide variety of collaboration |
as all other kernels |
the pools can agree |
of updates is no |
variety of collaboration and |
pools can agree not |
updates is no longer |
it abstracts the hardware |
this gives us the |
of collaboration and coordination |
is no longer entirely |
can agree not to |
abstracts the hardware in |
gives us the perfect |
collaboration and coordination paradigms |
no longer entirely preserved |
agree not to attack |
the hardware in the |
us the perfect prediction |
hardware in the usual |
the perfect prediction mechanism |
both benefit in the |
in the usual sense |
benefit in the long |
in the long run |
the live objects platform |
or can be inferred |
live objects platform makes |
can be inferred by |
objects platform makes it |
be inferred by the |
platform makes it easy |
at least for writeaccesses |
inferred by the file |
makes it easy for |
process and threads hide |
we address in section |
by the file server |
it easy for a |
and threads hide the |
address in section vii |
the file server according |
easy for a non |
threads hide the cpu |
in section vii the |
file server according to |
hide the cpu complexity |
section vii the case |
server according to how |
programmer to create the |
vii the case where |
this prediction mechanism is |
according to how it |
to create the needed |
the case where the |
prediction mechanism is also |
to how it dates |
create the needed soc |
case where the participants |
file systems and files |
mechanism is also entirely |
how it dates to |
the needed soc application |
where the participants are |
systems and files hide |
is also entirely application |
it dates to shared |
the participants are an |
and files hide the |
dates to shared files |
the rescue coordinator pulls |
participants are an arbitrary |
files hide the storage |
to shared files form |
rescue coordinator pulls prebuilt |
are an arbitrary number |
hide the storage devices |
shared files form a |
coordinator pulls prebuilt object |
an arbitrary number of |
files form a subsequence |
pulls prebuilt object references |
protocols hide the network |
form a subsequence of |
if most accesses to |
prebuilt object references from |
arbitrary number of identical |
a subsequence of the |
shared memory and messages |
object references from a |
number of identical pools |
most accesses to disks |
subsequence of the original |
memory and messages are |
references from a folder |
accesses to disks were |
of the original updates |
there exists a symmetric |
to disks were writes |
and messages are used |
each corresponding to a |
exists a symmetric equilibrium |
messages are used to |
corresponding to a desired |
a symmetric equilibrium in |
are used to allow |
to a desired kind |
symmetric equilibrium in which |
we could power down |
used to allow sharing |
a desired kind of |
equilibrium in which each |
could power down every |
to allow sharing of |
automatic inference should incorpoas |
desired kind of information |
in which each participating |
power down every disk |
allow sharing of resources |
inference should incorpoas do |
which each participating pool |
down every disk but |
should incorpoas do the |
each participating pool attacks |
every disk but the |
incorpoas do the updates |
what we often call |
participating pool attacks each |
disk but the one |
do the updates to |
we often call operating |
pool attacks each of |
but the one that |
the updates to unshared |
often call operating systems |
attacks each of the |
the one that the |
updates to unshared files |
would correspond to objects |
each of the other |
one that the log |
call operating systems has |
correspond to objects that |
of the other participating |
that the log head |
operating systems has nothing |
to objects that point |
the other participating pools |
the log head resides |
systems has nothing to |
objects that point to |
implicit dependenrate a heuristic |
log head resides on |
has nothing to do |
that point to a |
dependenrate a heuristic for |
as in the minority |
nothing to do with |
point to a web |
a heuristic for the |
in the minority two |
to do with the |
to a web service |
heuristic for the sharing |
do with the real |
a web service over |
for the sharing status |
with the real core |
web service over the |
the sharing status of |
the real core of |
service over the network |
sharing status of new |
here too at equilibrium |
real core of the |
is an ideal case |
status of new files |
too at equilibrium all |
core of the system |
an ideal case scenario |
at equilibrium all pools |
equilibrium all pools earn |
and a mechacies between |
all pools earn less |
a mechacies between file |
pools earn less than |
unix for most of |
mechacies between file updates |
peer objects would implement |
earn less than with |
our view is that |
for most of us |
between file updates are |
objects would implement chat |
less than with the |
most of us is |
file updates are preserved |
would implement chat windows |
than with the no |
of us is a |
with a good caching |
us is a collection |
a good caching algorithm |
is a collection of |
since the combination of |
a collection of shell |
the combination of nism |
collection of shell commands |
combination of nism for |
of shell commands and |
of nism for converting |
shell commands and development |
nism for converting shared |
our results imply that |
commands and development libraries |
event interfaces allow such |
aware caching algorithms described |
results imply that block |
for converting shared files |
interfaces allow such objects |
converting shared files to |
imply that block withholding |
shared files to be |
caching algorithms described in |
allow such objects to |
david korn s uwin |
that block withholding by |
files to be unshared |
algorithms described in the |
such objects to coexist |
block withholding by pools |
to be unshared if |
described in the related |
objects to coexist in |
withholding by pools leads |
be unshared if they |
in the related works |
to coexist in a |
by pools leads to |
unshared if they cease |
the related works section |
coexist in a shared |
pools leads to an |
and softway s interix |
related works section are |
in a shared display |
if they cease to |
leads to an unfavorable |
works section are good |
a shared display window |
they cease to forward |
to an unfavorable equilibrium |
section are good candidates |
shared display window that |
cease to forward invalidations |
display window that can |
to forward invalidations and |
window that can pan |
forward invalidations and compulsory |
invalidations and compulsory server |
both show that you |
due to the anonymity |
and compulsory server pull |
show that you can |
to the anonymity of |
compulsory server pull rpcs |
that you can give |
reads to disk can |
the anonymity of miners |
server pull rpcs for |
jump to new locations |
you can give users |
to disk can be |
pull rpcs for unbe |
a single pool might |
disk can be minimized |
can give users and |
rpcs for unbe accessed |
single pool might be |
give users and developers |
for unbe accessed by |
pool might be tempted |
users and developers a |
the relative advantages and |
unbe accessed by more |
might be tempted to |
and only a small |
relative advantages and disadvantages |
only a small fraction |
be tempted to attack |
a small fraction of |
advantages and disadvantages of |
small fraction of the |
accessed by more than |
fraction of the disks |
and disadvantages of our |
by more than a |
leading the other pools |
of the disks need |
disadvantages of our model |
more than a single |
the other pools to |
the disks need be |
of our model can |
unix experience including x |
than a single client |
other pools to attack |
disks need be powered |
our model can be |
pools to attack as |
need be powered on |
model can be summarized |
the current implemenshared files |
be powered on in |
to attack as well |
can be summarized as |
current implemenshared files prevents |
powered on in order |
be summarized as follows |
implemenshared files prevents a |
the implications might be |
files prevents a client |
on in order to |
while running on an |
implications might be devastating |
prevents a client from |
in order to serve |
running on an windows |
like other modern web |
might be devastating for |
a client from accessing |
order to serve all |
on an windows nt |
other modern web development |
be devastating for open |
client from accessing new |
to serve all writes |
an windows nt kernel |
modern web development tools |
devastating for open pools |
from accessing new versions |
serve all writes as |
accessing new versions of |
windows nt for most |
our platform supports drag |
new versions of files |
all writes as well |
nt for most of |
if their revenues are |
versions of files tation |
writes as well as |
for most of us |
their revenues are reduced |
of files tation in |
as well as reads |
most of us is |
drop style of development |
files tation in mfs |
of us is the |
miners will prefer to |
tation in mfs assumes |
us is the windows |
will prefer to form |
in mfs assumes that |
is the windows explorer |
prefer to form closed |
easy creation of content |
the windows explorer and |
mfs assumes that every |
to form closed pools |
what about the performance |
windows explorer and point |
assumes that every new |
form closed pools that |
about the performance and |
that every new file |
closed pools that cannot |
the performance and power |
the resulting solutions are |
every new file is |
pools that cannot be |
performance and power costs |
resulting solutions are easy |
new file is unshared |
that cannot be attacked |
and power costs of |
solutions are easy to |
and according to microsoft |
cannot be attacked in |
power costs of log |
are easy to share |
according to microsoft it |
and monin contravention of |
be attacked in this |
costs of log cleaning |
to microsoft it includes |
by selecting appropriate transport |
attacked in this manner |
monin contravention of their |
microsoft it includes a |
selecting appropriate transport layers |
contravention of their update |
it includes a web |
al present some optimizations |
of their update order |
includes a web browser |
though this may be |
present some optimizations in |
functionality such as coordination |
this may be conceived |
such as coordination between |
itors client accesses to |
as coordination between searchers |
client accesses to a |
although i have not |
accesses to a file |
coordination between searchers can |
may be conceived as |
i have not seen |
to a file according |
between searchers can remain |
a file according to |
to hide the performance |
file according to an |
be conceived as bad |
searchers can remain active |
have not seen a |
hide the performance penalty |
according to an overlapping |
conceived as bad news |
can remain active even |
not seen a complete |
the performance penalty of |
to an overlapping series |
as bad news for |
remain active even if |
seen a complete re |
performance penalty of log |
an overlapping series of |
bad news for public |
active even if connectivity |
penalty of log cleaning |
overlapping series of time |
news for public mining |
even if connectivity to |
of log cleaning even |
implementation of the explorer |
series of time periods |
for public mining pools |
if connectivity to the |
log cleaning even when |
of the explorer for |
of time periods to |
connectivity to the data |
cleaning even when the |
on the whole it |
time periods to ensure |
to the data center |
the explorer for unix |
even when the workload |
the whole it may |
periods to ensure that |
the data center is |
when the workload allows |
whole it may be |
to ensure that files |
data center is disrupted |
the workload allows little |
it may be good |
ensure that files which |
workload allows little idle |
allows little idle time |
that files which are |
may be good news |
compatible libraries from mainsoft |
files which are regularly |
streams of video or |
be good news to |
of video or sensor |
which are regularly accessed |
video or sensor data |
good news to the |
or sensor data can |
are regularly accessed remain |
the power costs of |
news to the bitcoin |
sensor data can travel |
regularly accessed remain shared |
power costs of log |
used in the port |
to the bitcoin system |
data can travel directly |
costs of log cleaning |
can travel directly and |
since the mfs file |
the mfs file monitoring |
of log cleaning are |
in the port of |
log cleaning are a |
mfs file monitoring component |
which prefers small pools |
travel directly and won |
the port of internet |
cleaning are a little |
port of internet explorer |
directly and won t |
file monitoring component op |
are a little more |
of internet explorer show |
we examine the practicality |
and won t be |
a little more tricky |
internet explorer show that |
examine the practicality of |
won t be delayed |
little more tricky to |
t be delayed by |
the practicality of the |
explorer show that you |
more tricky to justify |
show that you do |
practicality of the attack |
be delayed by the |
that you do not |
of the attack in |
delayed by the need |
you do not need |
the attack in section |
experimental setup erates on |
by the need to |
do not need a |
this is where the |
attack in section viii |
setup erates on a |
the need to ricochet |
not need a windows |
is where the natural |
in section viii and |
erates on a larger |
need to ricochet off |
need a windows nt |
where the natural division |
section viii and discuss |
on a larger time |
to ricochet off a |
a windows nt kernel |
the natural division of |
viii and discuss implications |
a larger time scale |
ricochet off a remote |
windows nt kernel to |
natural division of segments |
and discuss implications and |
larger time scale than |
off a remote and |
nt kernel to get |
division of segments into |
discuss implications and model |
time scale than the |
a remote and potentially |
kernel to get to |
of segments into stable |
implications and model extensions |
scale than the experiments |
remote and potentially inaccessible |
to get to the |
segments into stable and |
and model extensions in |
than the experiments considered |
and potentially inaccessible server |
get to the same |
into stable and volatile |
model extensions in section |
the experiments considered in |
to the same user |
stable and volatile ones |
extensions in section ix |
experiments considered in at |
the same user experience |
and volatile ones that |
considered in at the |
volatile ones that the |
based interoperability standards are |
in at the start |
ones that the log |
many see the rich |
at the start of |
interoperability standards are needed |
that the log cleaning |
our contributions are the |
the start of this |
see the rich win |
the log cleaning process |
log cleaning process results |
cleaning process results in |
contributions are the following |
start of this section |
of this section we |
this section we identified |
we could lose access |
section we identified large |
could lose access to |
programming interface as the |
after a significant fraction |
interface as the native |
lose access to some |
a significant fraction of |
access to some of |
scale collaborative this paper |
significant fraction of segments |
as the native programming |
to some of the |
fraction of segments on |
some of the sophisticated |
the native programming model |
of segments on a |
definition of the pool |
of the sophisticated proprietary |
we omit its details |
native programming model for |
segments on a disk |
of the pool game |
the sophisticated proprietary interactive |
omit its details for |
programming model for windows |
on a disk have |
the pool game where |
sophisticated proprietary interactive functionality |
its details for brevity |
model for windows nt |
a disk have been |
pool game where pools |
proprietary interactive functionality optimized |
disk have been classified |
game where pools in |
interactive functionality optimized for |
have been classified as |
been classified as stable |
engineering design as an |
and although most windows |
functionality optimized for proprietary |
where pools in a |
design as an example |
although most windows applications |
optimized for proprietary minibrowser |
pools in a proof |
as an example of |
most windows applications are |
we power the disk |
an example of a |
windows applications are designed |
power the disk on |
based solutions with an |
example of a scenario |
applications are designed using |
the disk on and |
ofwork secured system attack |
solutions with an embedded |
of a scenario which |
are designed using this |
disk on and copy |
secured system attack one |
with an embedded javascript |
a scenario which features |
designed using this interface |
on and copy the |
system attack one another |
scenario which features when |
and copy the stable |
attack one another with |
which features when a |
copy the stable segments |
one another with a |
it is not the |
features when a process |
the stable segments to |
another with a pool |
is not the windows |
peer communication can be |
stable segments to a |
communication can be much |
when a process modifies |
not the windows nt |
segments to a stable |
to a stable disk |
can be much harder |
a process modifies a |
the windows nt kernel |
with a pool block |
be much harder to |
process modifies a file |
volatile segments to a |
windows nt kernel interface |
a pool block withholding |
much harder to use |
segments to a volatile |
harder to use than |
to a volatile disk |
to use than relaying |
pool block withholding attack |
an update is scheduled |
almost no applications are |
use than relaying data |
disk is kept on |
than relaying data through |
no applications are built |
update is scheduled to |
relaying data through a |
applications are built using |
is scheduled to be |
data through a hosted |
and the entire disk |
are built using the |
scheduled to be a |
through a hosted service |
the entire disk is |
built using the kernel |
to be a high |
a hosted service that |
entire disk is freed |
using the kernel interface |
be a high degree |
hosted service that uses |
disk is freed for |
is freed for reuse |
service that uses an |
a high degree of |
and you would have |
this is similar to |
in the general case |
that uses an enterprise |
high degree of read |
you would have a |
is similar to the |
would have a hard |
uses an enterprise service |
similar to the log |
have a hard time |
an enterprise service bus |
to the log cleaning |
a hard time finding |
the log cleaning scheme |
hard time finding the |
log cleaning scheme described |
cleaning scheme described in |
time finding the complete |
at present we have |
finding the complete documentation |
present we have evalappended |
the complete documentation for |
we have evalappended to |
complete documentation for all |
have evalappended to the |
attacks is not an |
documentation for all the |
evalappended to the log |
is not an equilibrium |
for all the system |
all the system calls |
the lack of a |
which uses a hidden |
lack of a one |
uses a hidden structure |
of a one size |
and the process continues |
describing windows nt as |
the process continues executing |
a hidden structure embedded |
a one size fits |
windows nt as a |
hidden structure embedded in |
process continues executing withuated |
one size fits all |
continues executing withuated the |
structure embedded in the |
nt as a micro |
size fits all publish |
executing withuated the mfs |
embedded in the log |
in the log to |
with two minority pools |
withuated the mfs cache |
the log to track |
log to track segment |
to track segment utilization |
the mfs cache consistency |
subscribe substrate forces the |
two minority pools participating |
mfs cache consistency algorithm |
substrate forces the developers |
cleaning an entire disk |
as the kernel is |
forces the developers to |
cache consistency algorithm using |
an entire disk amortizes |
consistency algorithm using a |
the developers to become |
the only nash equilibrium |
the kernel is certainly |
entire disk amortizes the |
algorithm using a synthetic |
developers to become familiar |
only nash equilibrium is |
kernel is certainly not |
disk amortizes the cost |
using a synthetic out |
to become familiar with |
nash equilibrium is when |
is certainly not small |
amortizes the cost of |
a synthetic out having |
become familiar with and |
equilibrium is when the |
the cost of powering |
synthetic out having to |
familiar with and choose |
is when the pools |
cost of powering the |
of powering the disk |
powering the disk on |
with and choose between |
when the pools attack |
out having to wait |
number of accesses number |
and choose between a |
the pools attack one |
but it is does |
having to wait for |
it is does describe |
choose between a range |
is does describe the |
of accesses number of |
to wait for the |
pools attack one another |
between a range of |
does describe the abstraction |
accesses number of files |
wait for the server |
a range of different |
number of files touched |
describe the abstraction correctly |
for the server to |
the abstraction correctly in |
of files touched number |
and both earn less |
range of different and |
the server to be |
files touched number of |
abstraction correctly in which |
both earn less than |
correctly in which the |
server to be contacted |
touched number of bytes |
of different and incompatible |
earn less than if |
in which the kernel |
number of bytes touched |
different and incompatible options |
less than if none |
which the kernel provides |
of bytes touched average |
the kernel provides base |
than if none had |
bytes touched average number |
an wrong choice of |
if none had attacked |
touched average number of |
average number of bytes |
though we are hoping |
wrong choice of transport |
kernel provides base services |
we are hoping to |
choice of transport could |
provides base services and |
miners therefore face the |
are hoping to obtain |
of transport could result |
base services and the |
therefore face the miner |
hoping to obtain real |
transport could result in |
services and the specific |
face the miner s |
to obtain real data |
could result in degraded |
and the specific application |
the miner s dilemma |
obtain real data from |
result in degraded qos |
the specific application context |
real data from such |
specific application context is |
data from such an |
an instance of the |
application context is provided |
from such an thread |
instance of the iterative |
context is provided through |
such an thread then |
or even data loss |
of the iterative prisoner |
is provided through subsystem |
an thread then checks |
the iterative prisoner s |
provided through subsystem servers |
thread then checks the |
iterative prisoner s dilemma |
through subsystem servers or |
then checks the status |
subsystem servers or personalities |
checks the status of |
the status of the |
second life as a |
status of the file |
repeatedly choosing between attack |
life as a soc |
of the file the |
choosing between attack and |
as a soc application |
the file the update |
between attack and no |
based simulator of a |
simulator of a log |
file the update modifies |
a soc application up |
is one of the |
soc application up to |
one of the personalities |
given a trace of |
application up to now |
of the personalities running |
a trace of read |
if the environment in |
the personalities running on |
trace of read and |
the environment in the |
personalities running on top |
of read and write |
read and write requests |
environment in the future |
running on top of |
we have focused on |
on top of windows |
logsim returns the observed |
have focused on a |
top of windows nt |
returns the observed access |
the observed access latencies |
focused on a small |
with multiple pools of |
multiple pools of equal |
the update is queued |
pools of equal size |
update is queued for |
of equal size there |
is queued for transmission |
equal size there is |
queued for transmission at |
size there is a |
but our longer term |
for transmission at the |
for the chosen set |
there is a symmetric |
our longer term goal |
and posix are others |
transmission at the reg |
the chosen set of |
is a symmetric nash |
longer term goal is |
posix are others delivered |
chosen set of configuration |
set of configuration parameters |
term goal is to |
are others delivered by |
a symmetric nash equilibrium |
goal is to support |
others delivered by microsoft |
world traces for our |
is to support a |
traces for our simulations |
to support a large |
for our simulations from |
our simulations from a |
simulations from a web |
where all pools earn |
one can run windows |
all pools earn less |
scale nextgeneration collaboration system |
server that serves images |
can run windows nt |
pools earn less than |
nextgeneration collaboration system similar |
that serves images from |
run windows nt without |
earn less than if |
collaboration system similar to |
serves images from a |
images from a database |
less than if none |
system similar to second |
windows nt without these |
than if none had |
similar to second life |
nt without these standard |
if none had attacked |
without these standard personalities |
these standard personalities and |
standard personalities and build |
a virtual reality immersion |
personalities and build your |
virtual reality immersion system |
and build your own |
reality immersion system created |
immersion system created by |
system created by linden |
created by linden labs |
describes the characteristics of |
the characteristics of a |
characteristics of a sample |
what is an operating |
of a sample trace |
is an operating system |
while a true evaluation |
a true evaluation of |
inefficient equilibria for open |
true evaluation of the |
second life is implemented |
equilibria for open pools |
this question seems to |
evaluation of the feasibility |
life is implemented with |
for open pools may |
question seems to be |
of the feasibility and |
is implemented with a |
open pools may serve |
seems to be on |
the feasibility and efficacy |
implemented with a data |
pools may serve the |
to be on the |
feasibility and efficacy of |
with a data center |
may serve the system |
be on the mind |
and efficacy of our |
a data center including |
serve the system by |
on the mind of |
efficacy of our solution |
data center including a |
the system by reducing |
the mind of many |
of our solution can |
center including a large |
system by reducing their |
mind of many people |
our solution can only |
including a large number |
by reducing their attraction |
of many people these |
solution can only be |
a large number of |
reducing their attraction and |
many people these days |
can only be achieved |
large number of servers |
their attraction and pushing |
only be achieved through |
number of servers storing |
attraction and pushing miners |
infused by the microsoft |
of servers storing the |
be achieved through an |
and pushing miners towards |
by the microsoft trial |
servers storing the state |
achieved through an actual |
pushing miners towards smaller |
storing the state of |
through an actual implementation |
miners towards smaller closed |
academics in general have |
the state of the |
towards smaller closed pools |
in general have taken |
state of the virtual |
simulation provides an elegant |
general have taken a |
of the virtual world |
provides an elegant way |
have taken a very |
the classical block withholding |
an elegant way to |
taken a very narrow |
classical block withholding attack |
elegant way to identify |
a very narrow view |
the locations of all |
block withholding attack is |
way to identify and |
very narrow view of |
locations of all users |
withholding attack is old |
to identify and explore |
narrow view of what |
attack is old as |
identify and explore some |
view of what an |
is old as pools |
and explore some of |
of what an operating |
old as pools themselves |
explore some of the |
some of the cost |
what an operating system |
an operating system is |
benefit tradeoffs in a |
tradeoffs in a scaled |
but its use by |
its use by pools |
use by pools has |
down version of our |
version of our system |
by pools has not |
david faber at microsoft |
pools has not been |
the mechanism we simulate |
faber at microsoft trial |
has not been suggested |
at microsoft trial defined |
then move about and |
microsoft trial defined an |
not been suggested until |
mechanism we simulate is |
move about and interact |
trial defined an operating |
been suggested until recently |
we simulate is as |
simulate is as follows |
defined an operating system |
about and interact with |
an operating system as |
and interact with others |
we overview related attacks |
operating system as the |
overview related attacks and |
system as the software |
related attacks and prior |
disks are assumed to |
as the software that |
attacks and prior work |
are assumed to begin |
the software that controls |
and prior work in |
assumed to begin in |
one can create a |
software that controls the |
prior work in section |
to begin in the |
can create a cybercaf |
that controls the execution |
work in section x |
begin in the on |
in the on state |
controls the execution of |
the execution of programs |
execution of programs on |
and an access count |
and conclude with final |
of programs on computer |
conclude with final remarks |
programs on computer systems |
with final remarks in |
on computer systems and |
is maintained for each |
maintained for each disk |
computer systems and may |
final remarks in section |
systems and may provide |
the user specifies the |
remarks in section xi |
and may provide low |
user specifies the maximum |
specifies the maximum percentage |
as other second life |
other second life users |
second life users enter |
life users enter the |
users enter the room |
level services such as |
services such as resource |
of disks that are |
such as resource allocation |
p reliminaries b itcoin |
disks that are kept |
they can interact with |
reliminaries b itcoin and |
that are kept powered |
are kept powered on |
b itcoin and p |
can interact with the |
itcoin and p ooled |
interact with the environment |
and p ooled m |
with the environment and |
output control in a |
p ooled m ining |
control in a form |
the environment and one |
ooled m ining bitcoin |
in a form which |
m ining bitcoin is |
a form which is |
ining bitcoin is a |
form which is sufficiently |
bitcoin is a distributed |
which is sufficiently simple |
is sufficiently simple and |
in the second life |
sufficiently simple and general |
the second life architecture |
a disk check process |
simple and general so |
disk check process scans |
and general so that |
check process scans the |
general so that these |
process scans the access |
whenever an avatar moves |
so that these services |
scans the access count |
an avatar moves or |
that these services are |
the access count for |
number of rpcs average |
avatar moves or performs |
these services are broadly |
access count for each |
of rpcs average time |
moves or performs some |
services are broadly useful |
count for each disk |
or performs some action |
are broadly useful to |
for each disk and |
performs some action in |
broadly useful to software |
each disk and powers |
some action in the |
useful to software developers |
disk and powers down |
action in the virtual |
and powers down all |
in the virtual world |
powers down all but |
down all but the |
all but the most |
a request describing this |
request describing this event |
describing this event is |
this event is passed |
as well as any |
event is passed to |
well as any disk |
is passed to the |
as any disk which |
passed to the hosting |
any disk which does |
to the hosting data |
disk which does not |
the hosting data center |
which does not have |
hik j ihkj m |
in research community this |
does not have at |
research community this strict |
j ihkj m l |
hosting data center and |
not have at least |
community this strict distinction |
ihkj m l ml |
data center and processed |
have at least t |
this strict distinction serves |
m l ml cb |
center and processed by |
and processed by servers |
strict distinction serves to |
l ml cb c |
at least t access |
least t access count |
distinction serves to distinguish |
ml cb c b |
processed by servers running |
serves to distinguish the |
cb c b cbcb |
by servers running there |
to distinguish the real |
c b cbcb ed |
distinguish the real men |
b cbcb ed f |
the real men from |
cbcb ed f gf |
clients do perform a |
miss results in an |
real men from the |
ed f gf cb |
do perform a variety |
results in an access |
men from the boys |
clients use the system |
perform a variety of |
in an access to |
f gf cb c |
use the system by |
a variety of decoding |
researchers and hackers that |
gf cb c b |
the system by issuing |
an access to a |
access to a powered |
and hackers that work |
cb c b yx |
system by issuing transactions |
variety of decoding and |
hackers that work in |
c b yx cbcb |
of decoding and rendering |
that work in the |
then this disk is |
decoding and rendering functions |
work in the area |
this disk is spun |
disk is spun up |
and rendering functions locally |
z eded f f |
in the area defined |
to remain powered on |
eded f f gfgf |
and the system s |
the area defined by |
the system s only |
f f gfgf cb |
remain powered on until |
but the data center |
area defined by this |
system s only task |
f gfgf cb b |
s only task is |
the data center must |
defined by this narrow |
powered on until the |
gfgf cb b on |
only task is to |
data center must be |
task is to serialize |
on until the next |
cb b on yxyx |
by this narrow definition |
center must be in |
until the next disk |
must be in the |
b on yxyx cbb |
this narrow definition of |
is to serialize transactions |
the next disk check |
be in the loop |
narrow definition of operating |
to serialize transactions in |
in the loop to |
definition of operating systems |
serialize transactions in a |
the loop to ensure |
and there is a |
z eded f f |
transactions in a single |
loop to ensure that |
there is a corresponding |
in a single ledger |
consider themselves part of |
to ensure that all |
is a corresponding latency |
a single ledger and |
themselves part of the |
ensure that all users |
a corresponding latency penalty |
single ledger and reject |
part of the select |
that all users observe |
ledger and reject transactions |
of the select circle |
all users observe consistent |
judicious choice of the |
and reject transactions that |
the select circle of |
users observe consistent state |
choice of the parameters |
gfgf c c b |
reject transactions that cannot |
select circle of people |
of the parameters m |
c c b on |
when the number of |
circle of people working |
the parameters m and |
transactions that cannot be |
c b on yx |
the number of users |
of people working on |
parameters m and t |
that cannot be serialized |
b on yx ccb |
number of users in |
people working on the |
m and t minimizes |
cannot be serialized due |
on yx ccb qp |
of users in a |
working on the core |
and t minimizes the |
be serialized due to |
users in a scenario |
on the core of |
t minimizes the probability |
serialized due to conflicts |
in a scenario isn |
the core of the |
minimizes the probability of |
due to conflicts with |
a scenario isn t |
core of the systems |
the probability of this |
gf cb b c |
to conflicts with previous |
scenario isn t huge |
of the systems area |
probability of this occurrence |
cb b c onon |
conflicts with previous transactions |
the systems area of |
b c onon yxxy |
systems area of computer |
c onon yxxy cbbc |
second life can easily |
area of computer science |
onon yxxy cbbc qpqp |
life can easily keep |
bitcoin transactions are protected |
can easily keep up |
transactions are protected with |
easily keep up using |
are protected with cryptographic |
once you are in |
z eded r f |
protected with cryptographic techniques |
keep up using a |
eded r f f |
up using a standard |
with cryptographic techniques that |
you are in this |
r f f srs |
using a standard workload |
cryptographic techniques that ensure |
are in this circle |
a standard workload partitioning |
methodology we have proposed |
standard workload partitioning scheme |
in this circle you |
techniques that ensure that |
we have proposed the |
workload partitioning scheme in |
this circle you will |
that ensure that only |
have proposed the use |
partitioning scheme in which |
circle you will become |
gfgf c b onon |
proposed the use of |
scheme in which different |
ensure that only the |
c b onon yx |
you will become part |
the use of lfs |
in which different servers |
that only the rightful |
b onon yx cb |
use of lfs in |
will become part of |
which different servers handle |
only the rightful owner |
onon yx cb qp |
of lfs in lieu |
become part of the |
different servers handle different |
the rightful owner of |
lfs in lieu of |
part of the secret |
servers handle different portions |
of the secret society |
in lieu of ffs |
rightful owner of a |
handle different portions of |
z ed r f |
the secret society that |
or other conventional file |
different portions of the |
ed r f r |
owner of a bitcoin |
other conventional file systems |
secret society that practices |
portions of the virtual |
society that practices the |
of a bitcoin can |
that practices the black |
of the virtual world |
practices the black art |
gf invalidations and server |
the black art of |
center scenarios to achieve |
black art of os |
invalidations and server pulls |
a bitcoin can transfer |
scenarios to achieve power |
art of os research |
and server pulls mfs |
of os research and |
to achieve power conservation |
os research and will |
bitcoin can transfer it |
research and will start |
and will start to |
for this idea to |
for example because large |
this idea to be |
idea to be accepted |
the transaction ledger is |
will start to regard |
example because large numbers |
transaction ledger is stored |
start to regard any |
because large numbers of |
two questions need to |
ledger is stored by |
diff synchronous average time |
to regard any other |
large numbers of users |
questions need to be |
is stored by a |
regard any other activity |
numbers of users want |
need to be answered |
stored by a network |
any other activity of |
of users want to |
to be answered in |
by a network of |
other activity of systems |
users want to enter |
be answered in the |
a network of miners |
activity of systems development |
want to enter the |
answered in the affirmative |
network of miners in |
of systems development as |
to enter the same |
of miners in a |
systems development as irrelevant |
enter the same virtual |
miners in a data |
development as irrelevant to |
the same virtual discotheque |
in a data structure |
as irrelevant to the |
a data structure caller |
irrelevant to the future |
does this new scheme |
data structure caller the |
the servers can become |
servers can become overwhelmed |
this new scheme result |
can become overwhelmed and |
structure caller the blockchain |
to the future of |
new scheme result in |
become overwhelmed and are |
the future of computer |
scheme result in significant |
overwhelmed and are forced |
future of computer science |
result in significant power |
and are forced to |
in significant power savings |
are forced to reject |
revenue for proof of |
forced to reject some |
for a long time |
for proof of work |
to reject some of |
a long time the |
proof of work the |
reject some of the |
long time the line |
of work the blockchain |
some of the users |
time the line was |
work the blockchain records |
of the users or |
the line was drawn |
the blockchain records the |
the users or reduce |
line was drawn at |
does this new scheme |
blockchain records the transactions |
users or reduce their |
was drawn at the |
this new scheme provide |
records the transactions in |
or reduce their frame |
drawn at the kernel |
new scheme provide comparable |
the transactions in units |
scheme provide comparable performance |
transactions in units of |
provide comparable performance to |
in units of blocks |
rendering rates and resolution |
comparable performance to existing |
performance to existing schemes |
and one could only |
one could only consider |
could only consider himself |
the answers to these |
only consider himself a |
dubbed the genesis block |
consider himself a true |
answers to these questions |
himself a true os |
second life might seem |
to these questions must |
a true os researcher |
life might seem jumpy |
true os researcher after |
is defined as part |
these questions must be |
might seem jumpy and |
os researcher after having |
defined as part of |
questions must be largely |
seem jumpy and unrealistic |
researcher after having developed |
as part of the |
must be largely applicationindependent |
after having developed at |
part of the protocol |
having developed at least |
second life as a |
developed at least two |
and must apply to |
life as a live |
at least two device |
must apply to a |
as a live objects |
a valid block contains |
least two device drivers |
apply to a generic |
a live objects application |
valid block contains the |
two device drivers and |
to a generic data |
live objects application poses |
block contains the hash |
device drivers and hacked |
a generic data center |
objects application poses some |
contains the hash of |
drivers and hacked on |
generic data center model |
application poses some new |
the hash of the |
and hacked on the |
poses some new challenges |
hash of the previous |
hacked on the terminal |
to address these questions |
of the previous block |
on the terminal driver |
on the one hand |
the terminal driver of |
we present a simulator |
terminal driver of the |
driver of the bsd |
the hash of the |
hash of the transactions |
many aspects of the |
of the transactions in |
aspects of the application |
the transactions in the |
logsim consists of less |
of the application can |
transactions in the current |
consists of less than |
the application can be |
in the current block |
of less than a |
application can be addressed |
less than a thousand |
can be addressed in |
than a thousand lines |
be addressed in the |
a thousand lines of |
and a bitcoin address |
addressed in the same |
thousand lines of java |
in modern operating systems |
a bitcoin address which |
in the same manner |
lines of java code |
modern operating systems such |
bitcoin address which is |
the same manner we |
of java code and |
operating systems such as |
address which is to |
same manner we ve |
java code and is |
systems such as windows |
which is to be |
manner we ve outlined |
code and is a |
such as windows nt |
is to be credited |
we ve outlined for |
and is a single |
to be credited with |
ve outlined for the |
be credited with a |
outlined for the search |
the notion of where |
credited with a reward |
for the search and |
notion of where exactly |
with a reward for |
the search and rescue |
of where exactly operating |
a reward for generating |
search and rescue application |
where exactly operating systems |
reward for generating the |
we must turn off |
exactly operating systems services |
for generating the block |
must turn off some |
operating systems services are |
one could use microsoft |
turn off some percentage |
systems services are located |
could use microsoft virtual |
off some percentage of |
services are located is |
any miner may add |
use microsoft virtual earth |
some percentage of disks |
are located is not |
miner may add a |
percentage of disks in |
located is not that |
may add a valid |
of disks in the |
is not that simple |
add a valid block |
disks in the storage |
not that simple any |
a valid block to |
as a source of |
in the storage system |
that simple any more |
valid block to the |
block to the chain |
to the chain by |
d textures representing landscapes |
there are two opposing |
fundamental services are split |
are two opposing forces |
services are split between |
two opposing forces at |
are split between kernel |
opposing forces at play |
split between kernel and |
forces at play here |
between kernel and user |
proving that it has |
kernel and user space |
that it has spent |
and user space in |
it has spent a |
a large number of |
has spent a certain |
user space in attempts |
large number of powered |
in standards for creating |
space in attempts to |
spent a certain amount |
standards for creating mashups |
on disks results in |
for creating mashups could |
in attempts to optimise |
a certain amount of |
disks results in good |
certain amount of work |
attempts to optimise their |
creating mashups could be |
results in good performance |
amount of work and |
to optimise their efficiency |
mashups could be used |
of work and publishing |
optimise their efficiency and |
could be used to |
work and publishing the |
their efficiency and avoid |
be used to identify |
and publishing the block |
but also low power |
publishing the block with |
efficiency and avoid uncontrolled |
used to identify sensors |
also low power savings |
the block with the |
and avoid uncontrolled growth |
to identify sensors and |
block with the proof |
avoid uncontrolled growth of |
identify sensors and other |
on the other hand |
with the proof over |
uncontrolled growth of kernel |
sensors and other data |
the proof over an |
growth of kernel services |
and other data sources |
decreasing the number of |
the number of powered |
proof over an overlay |
over an overlay network |
an overlay network to |
overlay network to all |
on disks incurs two |
which could then be |
the pervasiveness of distributed |
network to all other |
disks incurs two possible |
could then be wrapped |
pervasiveness of distributed services |
to all other miners |
incurs two possible penalties |
then be wrapped as |
of distributed services in |
be wrapped as live |
distributed services in modern |
wrapped as live objects |
services in modern systems |
as live objects and |
when a miner creates |
in modern systems can |
live objects and incorporated |
a miner creates a |
modern systems can be |
objects and incorporated into |
miner creates a block |
systems can be considered |
and incorporated into live |
can be considered a |
incorporated into live scenes |
transitions consume power and |
be considered a threat |
it is compensated for |
consume power and thus |
considered a threat to |
on top of this |
power and thus counter |
is compensated for its |
a threat to the |
and thus counter the |
compensated for its efforts |
threat to the traditional |
thus counter the potential |
for its efforts with |
streaming media sources such |
to the traditional notion |
media sources such as |
its efforts with bitcoins |
counter the potential savings |
the traditional notion of |
sources such as video |
the potential savings achieved |
such as video cameras |
traditional notion of operating |
this compensation includes a |
as video cameras mounted |
potential savings achieved by |
video cameras mounted at |
compensation includes a per |
notion of operating systems |
savings achieved by powered |
cameras mounted at street |
mounted at street level |
at street level in |
transaction fee paid by |
street level in places |
many support services are |
fee paid by the |
level in places such |
support services are required |
to find the optimal |
paid by the users |
in places such as |
services are required to |
find the optimal percentage |
by the users whose |
places such as tokyo |
are required to make |
the optimal percentage of |
the users whose transactions |
such as tokyo s |
required to make distributed |
optimal percentage of disks |
users whose transactions are |
as tokyo s ginza |
to make distributed systems |
percentage of disks to |
whose transactions are included |
tokyo s ginza can |
make distributed systems work |
of disks to be |
s ginza can be |
distributed systems work efficiently |
disks to be powered |
ginza can be added |
systems work efficiently and |
to be powered down |
and an amount of |
can be added to |
work efficiently and effectively |
an amount of minted |
be added to create |
efficiently and effectively and |
we ran a set |
amount of minted bitcoins |
added to create realistic |
and effectively and these |
ran a set of |
of minted bitcoins that |
to create realistic experience |
effectively and these services |
a set of simulations |
minted bitcoins that are |
set of simulations on |
bitcoins that are thus |
of simulations on logsim |
that are thus introduced |
the more complex issue |
simulations on logsim and |
more complex issue is |
are thus introduced into |
such as security and |
on logsim and varied |
complex issue is that |
thus introduced into the |
as security and directory |
security and directory services |
issue is that a |
introduced into the system |
logsim and varied the |
and directory services or |
is that a search |
and varied the number |
that a search and |
directory services or distributed |
varied the number of |
the work which a |
services or distributed object |
a search and rescue |
the number of disks |
search and rescue application |
or distributed object support |
work which a miner |
number of disks that |
which a miner is |
distributed object support and |
and rescue application can |
of disks that we |
a miner is required |
object support and cluster |
rescue application can be |
disks that we kept |
miner is required to |
support and cluster management |
application can be imagined |
that we kept powered |
is required to do |
can be imagined as |
we kept powered up |
required to do is |
be imagined as a |
kept powered up from |
are not part of |
to do is to |
imagined as a situational |
powered up from none |
not part of a |
do is to repeatedly |
as a situational state |
part of a traditional |
is to repeatedly calculate |
a situational state fully |
of a traditional view |
to repeatedly calculate a |
situational state fully replicated |
a traditional view of |
repeatedly calculate a a |
state fully replicated across |
traditional view of operating |
calculate a a hash |
fully replicated across all |
view of operating systems |
a a hash function |
replicated across all of |
a hash function specifically |
across all of its |
hash function specifically the |
all of its users |
but they are essential |
function specifically the sha |
they are essential to |
are essential to the |
essential to the operation |
to the operation of |
the operation of modern |
operation of modern operating |
of modern operating systems |
all machines would see |
machines would see all |
out of a total |
would see all the |
of a total of |
this results in that |
see all the state |
results in that an |
all the state updates |
in that an operating |
that an operating system |
an operating system no |
operating system no longer |
system no longer is |
even if the user |
no longer is a |
if the user is |
longer is a simple |
the user is zoomed |
is a simple division |
user is zoomed into |
a simple division between |
is zoomed into some |
simple division between kernel |
zoomed into some particular |
division between kernel and |
into some particular spot |
between kernel and user |
some particular spot within |
kernel and user space |
particular spot within the |
spot within the overall |
within the overall scene |
of a block header |
but consist of a |
consist of a myriad |
of a myriad of |
a myriad of services |
to indicate that he |
indicate that he has |
that he has performed |
one can contemplate such |
he has performed this |
of which some are |
can contemplate such an |
has performed this work |
which some are kernilized |
contemplate such an approach |
such an approach because |
an approach because the |
average duration of reader |
duration of reader fetch |
the miner provides a |
some are local and |
approach because the aggregate |
miner provides a probabilistic |
are local and others |
because the aggregate amount |
provides a probabilistic proof |
local and others are |
the aggregate amount of |
a probabilistic proof as |
and others are remote |
aggregate amount of information |
probabilistic proof as follows |
amount of information might |
of information might not |
information might not be |
might not be that |
operating systems that address |
disks were kept powered |
not be that large |
systems that address the |
the generated block has |
were kept powered up |
that address the needs |
generated block has a |
address the needs of |
block has a nonce |
the needs of current |
has a nonce field |
needs of current and |
second life conceptually is |
of current and future |
life conceptually is a |
current and future clients |
conceptually is a whole |
and future clients and |
which can contain any |
is a whole universe |
future clients and informatik |
can contain any value |
clients and informatik informatique |
unbounded in size and |
in size and hence |
size and hence with |
the miner places different |
and hence with different |
miner places different values |
hence with different users |
places different values in |
with different users in |
different values in this |
different users in very |
values in this field |
users in very distinct |
in this field and |
in very distinct parts |
this field and calculates |
very distinct parts of |
field and calculates the |
distinct parts of the |
and calculates the hash |
parts of the space |
calculates the hash for |
the hash for each |
hash for each value |
it would make no |
would make no sense |
make no sense for |
no sense for every |
if the result of |
sense for every user |
the result of the |
operating systems servers no |
for every user to |
systems servers no longer |
result of the hash |
every user to see |
servers no longer span |
of the hash is |
user to see every |
no longer span a |
the hash is smaller |
to see every event |
longer span a single |
hash is smaller than |
span a single computer |
is smaller than a |
a single computer and |
smaller than a target |
single computer and they |
than a target value |
computer and they abstract |
we would solve this |
and they abstract services |
would solve this problem |
they abstract services away |
solve this problem using |
the nonce is considered |
abstract services away from |
this problem using the |
nonce is considered a |
services away from physical |
problem using the dynamic |
is considered a solution |
away from physical nodes |
using the dynamic database |
from physical nodes allowing |
the dynamic database querying |
physical nodes allowing user |
dynamic database querying approach |
nodes allowing user to |
and the block is |
database querying approach outlined |
allowing user to be |
the block is valid |
querying approach outlined in |
user to be part |
approach outlined in section |
to be part of |
be part of a |
part of a larger |
the number of attempts |
number of attempts to |
of attempts to find |
attempts to find a |
to find a single |
potential global operating environment |
find a single hash |
each user would see |
a single hash is |
user would see only |
single hash is therefore |
would see only the |
will the real dinosaur |
hash is therefore random |
see only the objects |
the real dinosaur please |
is therefore random with |
only the objects within |
real dinosaur please come |
therefore random with a |
the objects within some |
dinosaur please come forward |
random with a geometric |
objects within some range |
with a geometric distribution |
until the spring of |
or within line of |
within line of sight |
as each attempt is |
each attempt is a |
attempt is a bernoulli |
as a user moves |
is a bernoulli trial |
a user moves about |
a bernoulli trial with |
bernoulli trial with a |
trial with a success |
with a success probability |
the platform would recompute |
a success probability determined |
platform would recompute the |
we were deeply committed |
success probability determined by |
would recompute the query |
were deeply committed to |
probability determined by the |
recompute the query result |
deeply committed to sunos |
determined by the target |
cdf number of accesses |
by the target value |
and then update the |
then update the display |
and other bsd derivatives |
update the display accordingly |
at the existing huge |
the existing huge hashing |
existing huge hashing rates |
huge hashing rates and |
hashing rates and small |
rates and small target |
and small target values |
at that moment its |
that moment its vendor |
moment its vendor was |
its vendor was discontinuing |
vendor was discontinuing the |
was discontinuing the operating |
that since some live |
discontinuing the operating system |
the time to find |
since some live objects |
time to find a |
some live objects uses |
to find a single |
live objects uses p |
and had designated solaris |
find a single hash |
a single hash can |
single hash can be |
hash can be approximated |
p protocols that might |
can be approximated by |
which had its root |
protocols that might organize |
on onon yxyx p |
that might organize user |
had its root in |
onon yxyx p p |
yxyx p p qpqp |
might organize user s |
its root in at |
be approximated by an |
organize user s machines |
z onon yxyx p |
user s machines into |
onon yxyx p p |
yxyx p p qppq |
s machines into groups |
approximated by an exponential |
t s system v |
machines into groups forwarding |
by an exponential distribution |
s system v as |
z on yx p |
on yx p qp |
system v as the |
into groups forwarding streams |
z onon yxxy p |
onon yxxy p p |
yxxy p p qpqp |
v as the successor |
groups forwarding streams of |
the average time for |
forwarding streams of data |
z on yx p |
on yx p qp |
streams of data to |
average time for a |
of data to one |
this event forced us |
time for a miner |
event forced us to |
for a miner to |
forced us to take |
z time spent on |
time spent on invalidations |
a miner to find |
data to one another |
us to take a |
miner to find a |
to take a step |
to find a solution |
take a step back |
find a solution is |
we end up in |
a step back and |
a solution is therefore |
end up in a |
step back and evaluate |
solution is therefore proportional |
up in a situation |
back and evaluate our |
is therefore proportional to |
average store rpc duration |
in a situation where |
and evaluate our research |
therefore proportional to its |
a situation where each |
evaluate our research directions |
proportional to its hashing |
situation where each user |
our research directions and |
to its hashing rate |
where each user belongs |
research directions and our |
its hashing rate or |
each user belongs to |
directions and our expectations |
hashing rate or mining |
user belongs to a |
and our expectations with |
rate or mining power |
belongs to a potentially |
our expectations with respect |
to a potentially large |
expectations with respect to |
a potentially large number |
with respect to the |
to maintain a constant |
potentially large number of |
respect to the operating |
maintain a constant rate |
large number of such |
to the operating systems |
a constant rate of |
number of such groups |
the operating systems to |
constant rate of bitcoin |
operating systems to use |
rate of bitcoin generation |
and the groups that |
the groups that one |
if one issue in |
groups that one user |
one issue in our |
and as part of |
that one user is |
issue in our discussions |
as part of its |
one user is a |
in our discussions was |
part of its defense |
user is a part |
our discussions was dominant |
of its defense against |
is a part of |
its defense against denial |
a part of might |
defense against denial of |
part of might be |
it was the fact |
against denial of service |
of might be very |
was the fact that |
denial of service and |
v v w w |
might be very different |
the fact that most |
of service and other |
v w w ut |
be very different from |
fact that most of |
service and other attacks |
w w ut v |
very different from the |
that most of the |
w ut v wv |
most of the operating |
different from the groups |
ut v wv ut |
the system normalizes the |
from the groups that |
of the operating systems |
system normalizes the rate |
the groups that other |
the operating systems we |
normalizes the rate of |
groups that other users |
operating systems we were |
the rate of block |
that other users belong |
systems we were looking |
rate of block generation |
other users belong to |
we were looking at |
were looking at were |
looking at were actually |
at were actually very |
were actually very old |
actually very old fashioned |
to support such a |
support such a model |
the protocol deterministically defines |
we need to be |
protocol deterministically defines the |
need to be able |
in structure and in |
deterministically defines the target |
to be able to |
effect of increasing percentage |
defines the target value |
structure and in implementation |
be able to support |
of increasing percentage of |
the target value for |
able to support very |
increasing percentage of powered |
graphs for cache consistency |
most of these operating |
to support very large |
for cache consistency trace |
target value for each |
of these operating systems |
these graphs show various |
up disks on performance |
value for each block |
support very large numbers |
these operating systems had |
graphs show various features |
show various features of |
very large numbers of |
operating systems had their |
for each block according |
various features of the |
each block according to |
systems had their conception |
features of the performance |
of the performance results |
effect of increasing percentage |
block according to the |
had their conception in |
large numbers of publish |
async denotes asynchronous invalidations |
according to the time |
their conception in the |
of increasing percentage of |
to the time required |
and none no invalidations |
increasing percentage of powered |
the time required to |
time required to generate |
diff denotes differentiated writeback |
required to generate recent |
denotes differentiated writeback priorities |
up disks on power |
and with different users |
to generate recent blocks |
differentiated writeback priorities for |
disks on power consumption |
with different users subscribed |
writeback priorities for shared |
on power consumption both |
different users subscribed to |
priorities for shared and |
power consumption both its |
users subscribed to very |
for shared and unshared |
consumption both its performance |
s and did not |
subscribed to very different |
shared and unshared files |
and did not change |
as well as its |
well as its power |
and unif denotes uniform |
unif denotes uniform priorities |
did not change much |
to very different sets |
is updated once every |
not change much in |
cc is the mfs |
is the mfs cache |
the mfs cache consistency |
mfs cache consistency algorithm |
change much in structure |
very different sets of |
the former is measured |
much in structure since |
different sets of topics |
former is measured using |
in structure since then |
is measured using the |
measured using the observed |
the height of a |
using the observed access |
height of a bar |
the observed access latencies |
of a bar counts |
a bar counts the |
up to now we |
bar counts the number |
counts the number of |
the number of invalidations |
linux could be seen |
to now we have |
while the latter is |
could be seen as |
the white portion counts |
be seen as an |
blocks such that the |
now we have been |
the latter is measured |
white portion counts the |
latter is measured by |
such that the average |
we have been fairly |
portion counts the number |
counts the number of |
the number of server |
that the average time |
seen as an exception |
is measured by comparing |
have been fairly negative |
the average time for |
as an exception since |
measured by comparing the |
our experimental setup consisting |
experimental setup consisting of |
setup consisting of three |
consisting of three hosts |
by comparing the cumulative |
been fairly negative about |
average time for each |
an exception since it |
comparing the cumulative percentage |
fairly negative about the |
and a writer client |
exception since it was |
the cumulative percentage of |
time for each block |
negative about the trend |
since it was developed |
cumulative percentage of time |
the bandwidth from the |
for each block to |
about the trend to |
it was developed in |
percentage of time the |
bandwidth from the reader |
each block to be |
the trend to standardize |
was developed in the |
trend to standardize client |
from the reader to |
the reader to the |
to standardize client access |
developed in the second |
standardize client access to |
of time the disks |
reader to the server |
to the server was |
the server was fixed |
server was fixed at |
time the disks are |
block to be found |
in the second half |
client access to hosted |
the disks are kept |
access to hosted content |
the second half of |
to hosted content through |
disks are kept powered |
are kept powered on |
second half of the |
hosted content through web |
to be found is |
and the bandwidth from |
content through web minibrowsers |
as well as the |
through web minibrowsers that |
the bandwidth from the |
well as the number |
web minibrowsers that make |
bandwidth from the writer |
as the number of |
the number of mode |
from the writer to |
minibrowsers that make the |
the writer to the |
writer to the server |
that make the javascript |
to the server was |
the server was varied |
make the javascript running |
server was varied according |
was varied according to |
varied according to the |
according to the experiment |
the javascript running on |
javascript running on a |
the writer was configured |
running on a user |
note that the exponential |
writer was configured in |
but its structure mirrored |
that the exponential distribution |
on a user s |
was configured in one |
configured in one of |
in one of seven |
one of seven different |
of seven different ways |
the exponential distribution is |
a user s machine |
its structure mirrored that |
exponential distribution is memoryless |
user s machine virtually |
show the results of |
structure mirrored that of |
synchronous or no invalidations |
s machine virtually inseparable |
the results of these |
mirrored that of the |
machine virtually inseparable from |
results of these simulations |
and differentiated or uniform |
if all miners mine |
that of the traditional |
virtually inseparable from the |
differentiated or uniform priorities |
all miners mine for |
of the traditional unix |
inseparable from the data |
or uniform priorities for |
miners mine for block |
the traditional unix systems |
from the data center |
uniform priorities for writing |
mine for block number |
priorities for writing back |
of the disks powered |
the disks powered on |
for writing back shared |
writing back shared and |
back shared and unshared |
shared and unshared files |
our core criticism was |
for block number b |
and as such it |
core criticism was that |
the mfs concurrency control |
mfs concurrency control algorithm |
criticism was that for |
as such it could |
was that for most |
such it could be |
once the block is |
that for most soc |
corresponds to asynchronous invalidations |
the block is found |
it could be considered |
of the disks can |
to asynchronous invalidations with |
block is found at |
for most soc applications |
could be considered one |
the disks can be |
asynchronous invalidations with differentiated |
disks can be spun |
be considered one of |
is found at time |
invalidations with differentiated priority |
can be spun down |
considered one of them |
found at time t |
a minibrowser approach would |
with differentiated priority for |
minibrowser approach would lack |
be spun down while |
differentiated priority for shared |
spun down while still |
priority for shared files |
approach would lack the |
all miners switch to |
down while still maintaining |
the significant advances made |
would lack the flexibility |
miners switch to mine |
both clients access a |
while still maintaining performance |
significant advances made in |
lack the flexibility to |
switch to mine for |
clients access a shared |
still maintaining performance comparable |
advances made in academic |
the flexibility to seamlessly |
to mine for the |
access a shared repository |
maintaining performance comparable to |
made in academic computer |
flexibility to seamlessly combine |
mine for the subsequent |
a shared repository of |
performance comparable to that |
in academic computer science |
to seamlessly combine content |
for the subsequent block |
shared repository of files |
comparable to that of |
seamlessly combine content from |
the subsequent block b |
repository of files stored |
to that of a |
combine content from different |
in os research and |
of files stored on |
that of a conventional |
content from different sources |
os research and in |
files stored on the |
stored on the file |
on the file server |
of a conventional file |
research and in system |
a conventional file system |
and in system software |
at t without changing |
and to customize the |
in system software engineering |
t without changing their |
each module has a |
without changing their probability |
to customize the underlying |
the performance of our |
module has a descriptor |
changing their probability distribution |
customize the underlying communication |
the underlying communication substrate |
has a descriptor file |
a descriptor file and |
descriptor file and a |
file and a set |
and a set of |
their probability distribution of |
performance of our system |
have had only minimal |
probability distribution of finding |
of our system depends |
had only minimal impact |
distribution of finding a |
our earlier concerns carry |
only minimal impact on |
module descriptor files are |
descriptor files are about |
earlier concerns carry over |
our system depends very |
of finding a block |
minimal impact on the |
concerns carry over to |
kb in size and |
in size and the |
finding a block after |
impact on the design |
carry over to the |
system depends very heavily |
a block after t |
on the design and |
over to the second |
depends very heavily on |
member files take up |
files take up an |
take up an average |
up an average of |
the design and implementation |
to the second life |
very heavily on its |
design and implementation of |
the second life scenario |
heavily on its cache |
and implementation of commercial |
on its cache configuration |
the probability that a |
implementation of commercial operating |
probability that a miner |
the total size of |
of commercial operating systems |
since cache optimization is |
that a miner i |
total size of all |
cache optimization is an |
size of all the |
a miner i with |
optimization is an orthogonal |
of all the files |
all the files in |
is an orthogonal issue |
d texture representing terrain |
the files in the |
files in the collection |
in the collection is |
an orthogonal issue that |
texture representing terrain in |
the design of all |
miner i with mining |
design of all unix |
i with mining power |
the writer workload consists |
representing terrain in some |
of all unix systems |
orthogonal issue that comprises |
writer workload consists of |
with mining power mi |
terrain in some region |
all unix systems violates |
issue that comprises an |
unix systems violates almost |
mining power mi finds |
workload consists of the |
that comprises an entire |
systems violates almost all |
consists of the writer |
power mi finds the |
comprises an entire field |
mi finds the next |
violates almost all of |
of the writer updating |
an entire field of |
finds the next block |
the writer updating modules |
the next block is |
in a minibrowser approach |
writer updating modules in |
updating modules in a |
modules in a random |
in a random order |
almost all of the |
entire field of research |
all of the software |
field of research in |
of the software engineering |
an update to a |
the software engineering principles |
the minibrowser generates the |
next block is its |
of research in itself |
update to a module |
software engineering principles presented |
minibrowser generates the texture |
block is its ratio |
to a module consists |
is its ratio out |
generates the texture from |
it is important to |
a module consists of |
module consists of a |
consists of a sequence |
of a sequence of |
a sequence of operations |
engineering principles presented to |
its ratio out of |
the texture from hosted |
is important to isolate |
principles presented to first |
ratio out of the |
texture from hosted data |
of which are reads |
which are reads and |
out of the total |
important to isolate its |
presented to first year |
of the total mining |
to isolate its effect |
to first year s |
the total mining power |
isolate its effect on |
are writes to a |
writes to a file |
to a file in |
a file in the |
file in the module |
total mining power m |
its effect on performance |
first year s computer |
mining power m in |
year s computer science |
power m in the |
s computer science students |
consist of writes to |
we implemented an ideal |
of writes to unshared |
writes to unshared external |
implemented an ideal cache |
an ideal cache algorithm |
m in the system |
to unshared external files |
this model makes it |
the design is monolithic |
which are each created |
design is monolithic with |
model makes it difficult |
which we term the |
are each created with |
each created with a |
created with a unique |
with a unique name |
we term the oracle |
is monolithic with almost |
monolithic with almost no |
there is a pause |
with almost no modular |
this data point represents |
is a pause between |
data point represents the |
almost no modular structure |
miner miner miner pool |
a pause between each |
point represents the best |
to superimpose other content |
pause between each operation |
represents the best performance |
superimpose other content over |
between each operation and |
the best performance we |
and the internal kernel |
other content over the |
miner miner miner pool |
each operation and a |
best performance we could |
the internal kernel interfaces |
content over the texture |
operation and a longer |
performance we could achieve |
internal kernel interfaces are |
and a longer pause |
kernel interfaces are not |
we could achieve since |
interfaces are not strictly |
a longer pause between |
could achieve since an |
longer pause between updates |
achieve since an oracle |
pause between updates to |
since an oracle has |
between updates to modules |
are not strictly enforced |
an oracle has future |
not strictly enforced which |
we would need to |
the reader workload is |
would need to rely |
strictly enforced which introduces |
reader workload is similar |
enforced which introduces dependencies |
need to rely on |
oracle has future knowledge |
which introduces dependencies on |
but an access to |
introduces dependencies on the |
has future knowledge and |
to rely on a |
an access to a |
rely on a hosting |
future knowledge and is |
access to a module |
dependencies on the actual |
on a hosting system |
knowledge and is able |
to a module consists |
a module consists of |
module consists of a |
and is able to |
on the actual implementation |
a hosting system s |
consists of a series |
hosting system s mashup |
the actual implementation of |
of a series of |
a series of reads |
system s mashup technology |
actual implementation of data |
and external files are |
implementation of data structures |
is able to replace |
external files are never |
files are never accessed |
able to replace items |
s mashup technology to |
to replace items accessed |
mashup technology to do |
the configuration parameters used |
replace items accessed furthest |
making it impossible to |
configuration parameters used to |
it impossible to upgrade |
items accessed furthest in |
impossible to upgrade or |
parameters used to generate |
to upgrade or replace |
accessed furthest in the |
furthest in the future |
used to generate the |
to generate the reader |
technology to do this |
upgrade or replace modules |
generate the reader and |
or replace modules without |
the reader and writer |
replace modules without also |
reader and writer workload |
and one miner mines |
modules without also redesigning |
and writer workload are |
writer workload are listed |
workload are listed in |
are listed in table |
without also redesigning several |
one miner mines solo |
also redesigning several other |
if we wanted to |
we also wish to |
we wanted to blend |
the writer workload has |
wanted to blend weather |
also wish to provide |
to blend weather information |
writer workload has a |
blend weather information from |
wish to provide a |
workload has a nominal |
has a nominal duration |
a nominal duration of |
nominal duration of two |
duration of two minutes |
weather information from the |
pools datacenters are built |
while the reader workload |
datacenters are built around |
information from the national |
to provide a performance |
redesigning several other modules |
the reader workload is |
are built around the |
from the national hurricane |
provide a performance comparison |
reader workload is extended |
built around the world |
the national hurricane center |
a performance comparison of |
workload is extended to |
for example to replace |
national hurricane center with |
example to replace the |
is extended to terminate |
performance comparison of our |
hurricane center with a |
extended to terminate at |
to replace the scheduler |
comparison of our system |
center with a google |
to terminate at the |
of our system against |
replace the scheduler in |
with a google map |
the scheduler in any |
our system against conventional |
terminate at the same |
scheduler in any of |
at the same time |
the same time as |
in any of the |
same time as the |
time as the writer |
any of the bsd |
as the writer workload |
the writer workload actually |
writer workload actually finishes |
of the bsd s |
the google map service |
since low bandwidth could |
google map service would |
mining is only profitable |
as an approximation of |
the bsd s one |
low bandwidth could extend |
map service would need |
is only profitable using |
an approximation of such |
bsd s one needs |
bandwidth could extend its |
service would need to |
only profitable using dedicated |
approximation of such a |
could extend its running |
extend its running time |
its running time beyond |
running time beyond two |
time beyond two minutes |
s one needs to |
would need to explicitly |
profitable using dedicated hardware |
of such a system |
one needs to spend |
need to explicitly support |
using dedicated hardware in |
we implemented a random |
dedicated hardware in cutting |
needs to spend two |
to explicitly support this |
implemented a random placement |
hardware in cutting edge |
analysis of the results |
of the results figure |
explicitly support this sort |
a random placement algorithm |
in cutting edge mining |
to spend two weeks |
support this sort of |
which maps each block |
cutting edge mining rigs |
spend two weeks searching |
shows graphs of some |
two weeks searching for |
maps each block to |
graphs of some selected |
of some selected results |
some selected results from |
each block to a |
this sort of embedding |
weeks searching for all |
selected results from the |
searching for all dependencies |
otherwise the energy costs |
results from the experiments |
block to a random |
to a random disk |
the energy costs exceed |
for all dependencies and |
energy costs exceed the |
while synchronous writes provide |
synchronous writes provide strong |
all disks are kept |
costs exceed the expected |
all dependencies and fixing |
writes provide strong concurrency |
provide strong concurrency control |
exceed the expected revenue |
dependencies and fixing other |
they resulted in the |
in our second life |
disks are kept powered |
are kept powered up |
resulted in the lowest |
our second life scenario |
and fixing other sources |
in the lowest rate |
although expected revenue from |
the lowest rate of |
lowest rate of completed |
rate of completed writes |
of completed writes in |
completed writes in all |
writes in all the |
in all the tests |
expected revenue from mining |
at the top of |
since the writer had |
the top of our |
having set the context |
the writer had no |
writer had no possibility |
had no possibility of |
no possibility of over |
revenue from mining is |
let us examine fig |
from mining is proportional |
top of our long |
the visible portion of |
mining is proportional to |
lapping think time with |
is proportional to the |
visible portion of the |
think time with asynchronous |
time with asynchronous writeback |
portion of the scene |
of our long wish |
at all bandwidth levels |
our long wish list |
proportional to the power |
all bandwidth levels the |
bandwidth levels the mfs |
long wish list for |
to the power of |
cc algorithm outperformed synchronous |
the power of the |
of the scene the |
algorithm outperformed synchronous writes |
outperformed synchronous writes by |
the scene the part |
the additional two data |
wish list for an |
synchronous writes by at |
writes by at least |
scene the part of |
list for an ideal |
the part of the |
points described above are |
power of the mining |
for an ideal research |
described above are represented |
part of the texture |
of the mining rigs |
of the texture being |
above are represented in |
and was among the |
an ideal research operating |
the mining rigs used |
are represented in fig |
the texture being displayed |
was among the options |
among the options with |
the options with the |
options with the highest |
with the highest write |
the highest write throughput |
ideal research operating system |
texture being displayed will |
a single home miner |
this is clear from |
single home miner using |
is clear from graph |
home miner using a |
being displayed will often |
were three important general |
miner using a small |
displayed will often be |
which shows the average |
using a small rig |
three important general points |
shows the average time |
will often be controlled |
a small rig is |
the average time to |
small rig is unlikely |
often be controlled by |
average time to complete |
rig is unlikely to |
time to complete store |
be controlled by events |
is unlikely to mine |
to complete store rpcs |
complete store rpcs initiated |
store rpcs initiated by |
rpcs initiated by the |
initiated by the writer |
controlled by events generated |
unlikely to mine a |
by events generated by |
to mine a block |
the design and implementation |
events generated by other |
mine a block for |
design and implementation of |
generated by other live |
cc outperforms all of |
outperforms all of the |
all of the alternatives |
by other live objects |
a block for years |
this is because of |
other live objects that |
and implementation of the |
is because of the |
implementation of the operating |
live objects that share |
because of the reduced |
of the operating system |
if we imagine a |
objects that share the |
of the reduced number |
the reduced number of |
we imagine a line |
imagine a line at |
a line at y |
reduced number of invalidations |
number of invalidations it |
of invalidations it generates |
that share the display |
the operating system should |
share the display window |
operating system should comply |
in contrast to most |
contrast to most of |
to most of the |
most of the other |
of the other schemes |
system should comply with |
should comply with modern |
it is able to |
perhaps under control of |
is able to take |
under control of users |
able to take advantage |
control of users running |
to take advantage of |
take advantage of both |
advantage of both differentiated |
of both differentiated writeback |
of users running on |
comply with modern software |
miners often organize themselves |
users running on machines |
with modern software engineering |
often organize themselves into |
pull rpcs to raise |
rpcs to raise the |
to raise the priority |
raise the priority of |
the priority of its |
priority of its writes |
organize themselves into mining |
running on machines elsewhere |
modern software engineering principles |
themselves into mining pools |
on machines elsewhere in |
shows the performance from |
the performance from the |
performance from the reader |
from the reader s |
the reader s perspective |
of the accesses live |
machines elsewhere in the |
the accesses live above |
allowing researchers to introduce |
elsewhere in the network |
while the writer is |
accesses live above this |
live above this line |
the writer is able |
a pool is a |
researchers to introduce new |
writer is able to |
pool is a group |
to introduce new components |
is able to decrease |
these remote sources won |
is a group of |
able to decrease its |
a group of miners |
to decrease its time |
group of miners that |
disks on is the |
remote sources won t |
and replace core components |
decrease its time spent |
its time spent performing |
on is the third |
sources won t fit |
replace core components without |
of miners that share |
time spent performing store |
miners that share their |
won t fit into |
core components without redesigning |
spent performing store rpcs |
is the third best |
the third best configuration |
t fit into the |
the reader s average |
fit into the interaction |
components without redesigning the |
that share their revenues |
reader s average time |
share their revenues when |
next only to the |
without redesigning the complete |
into the interaction model |
s average time spent |
their revenues when one |
only to the oracle |
redesigning the complete system |
the interaction model expected |
average time spent on |
revenues when one of |
to the oracle and |
interaction model expected by |
time spent on fetches |
when one of them |
model expected by the |
spent on fetches increases |
one of them successfully |
expected by the minibrowser |
on fetches increases sharply |
of them successfully mines |
fetches increases sharply when |
them successfully mines a |
increases sharply when the |
successfully mines a block |
sharply when the file |
the overall structure of |
when the file in |
overall structure of the |
the file in question |
file in question must |
structure of the operating |
for each block found |
the performance degradation in |
in question must be |
question must be pulled |
performance degradation in going |
of the operating system |
must be pulled from |
be pulled from the |
pulled from the writer |
degradation in going from |
the revenue is distributed |
the size and shape |
revenue is distributed among |
user and kernel space |
size and shape of |
is distributed among the |
this cost must be |
distributed among the pool |
and shape of the |
and kernel space components |
cost must be weighed |
among the pool members |
shape of the display |
must be weighed against |
the pool members in |
be weighed against the |
of the display window |
pool members in proportion |
weighed against the benefit |
should be designed towards |
the display window and |
members in proportion to |
against the benefit of |
be designed towards the |
disks on is negligibly |
on is negligibly small |
in proportion to their |
the benefit of substantially |
benefit of substantially increased |
of substantially increased writer |
substantially increased writer throughput |
designed towards the future |
display window and other |
proportion to their mining |
window and other elements |
differentiated writeback succeeds in |
to their mining power |
writeback succeeds in reducing |
and other elements of |
for the system under |
succeeds in reducing the |
the system under test |
other elements of the |
in reducing the time |
reducing the time the |
elements of the runtime |
the time the reader |
the optimal configuration is |
time the reader has |
of the runtime environment |
optimal configuration is to |
the reader has to |
the expected revenue of |
the runtime environment should |
configuration is to fig |
reader has to wait |
expected revenue of a |
runtime environment should be |
has to wait when |
revenue of a pool |
should be pervasive throughout |
to wait when accessing |
wait when accessing a |
when accessing a shared |
accessing a shared file |
environment should be inherited |
shows an estimate of |
should be inherited from |
be pervasive throughout the |
of a pool member |
an estimate of the |
be inherited from the |
pervasive throughout the whole |
a pool member is |
estimate of the actual |
inherited from the hierarchy |
throughout the whole system |
pool member is therefore |
of the actual power |
show statistics for invalidations |
from the hierarchy structure |
member is therefore the |
the actual power savings |
statistics for invalidations and |
the hierarchy structure of |
is therefore the same |
actual power savings achieved |
for invalidations and serverpull |
hierarchy structure of the |
therefore the same as |
power savings achieved by |
invalidations and serverpull rpcs |
structure of the object |
the same as its |
savings achieved by our |
and serverpull rpcs for |
of the object mashup |
the operating system vendor |
same as its revenue |
achieved by our solution |
serverpull rpcs for those |
the object mashup used |
operating system vendor should |
as its revenue had |
rpcs for those writer |
object mashup used to |
system vendor should be |
we assume the following |
its revenue had it |
for those writer configurations |
mashup used to create |
vendor should be open |
assume the following disk |
revenue had it mined |
those writer configurations which |
used to create the |
should be open to |
the following disk specifications |
had it mined solo |
writer configurations which make |
configurations which make use |
which make use of |
make use of them |
be open to innovation |
to create the application |
cc significantly reduces the |
significantly reduces the number |
reduces the number of |
the number of invalidations |
our experiences in the |
due to the large |
thus our texture should |
experiences in the past |
our texture should learn |
in the past had |
texture should learn its |
the past had been |
should learn its size |
past had been that |
learn its size and |
had been that vendors |
number of invalidations it |
to the large power |
its size and orientation |
been that vendors always |
of invalidations it must |
the large power of |
size and orientation and |
that vendors always ignored |
invalidations it must transmit |
large power of the |
and orientation and even |
vendors always ignored important |
it must transmit by |
always ignored important research |
orientation and even the |
power of the pool |
must transmit by putting |
ignored important research results |
and even the gps |
transmit by putting off |
even the gps coordinates |
important research results and |
by putting off invalidating |
research results and only |
it finds blocks at |
the gps coordinates on |
putting off invalidating a |
gps coordinates on which |
finds blocks at a |
results and only followed |
off invalidating a file |
coordinates on which to |
blocks at a much |
and only followed very |
invalidating a file until |
on which to center |
at a much higher |
only followed very narrow |
a file until it |
file until it is |
until it is added |
it is added to |
is added to the |
added to the log |
avg time for transition |
which to center from |
a much higher rate |
yet the effect of |
to center from the |
followed very narrow paths |
the effect of this |
center from the parent |
very narrow paths of |
effect of this policy |
from the parent object |
and so the frequency |
narrow paths of incremental |
of this policy on |
we see that turning |
the parent object that |
so the frequency of |
paths of incremental improvements |
this policy on the |
see that turning off |
parent object that hosts |
the frequency of revenue |
policy on the number |
object that hosts it |
frequency of revenue collection |
windows nt was the |
on the number of |
nt was the only |
the number of serverpull |
was the only operating |
of the disks results |
the only operating system |
number of serverpull rpcs |
only operating system that |
the disks results in |
and similarly until we |
of serverpull rpcs is |
similarly until we reach |
operating system that came |
serverpull rpcs is minor |
of revenue collection is |
until we reach the |
system that came close |
revenue collection is higher |
we reach the root |
that came close to |
reach the root object |
which differs from mfs |
came close to matching |
allowing for a stable |
cc in omitting differentiated |
in omitting differentiated writeback |
the root object hosting |
for a stable daily |
makes more invalidations and |
more invalidations and incurs |
invalidations and incurs more |
and incurs more server |
root object hosting the |
close to matching most |
a stable daily or |
with all the disks |
object hosting the display |
to matching most of |
stable daily or weekly |
all the disks off |
because its store rpcs |
hosting the display window |
matching most of our |
daily or weekly income |
its store rpcs must |
most of our requirements |
store rpcs must compete |
rpcs must compete with |
while maintaining acceptable performance |
must compete with the |
a minibrowser isn t |
compete with the rpcs |
minibrowser isn t a |
with the rpcs to |
with a handful of |
the rpcs to write |
a handful of operating |
rpcs to write back |
handful of operating systems |
most pools are controlled |
to write back external |
write back external files |
of operating systems such |
pools are controlled by |
shows some of the |
this increases the commit |
isn t a component |
are controlled by a |
operating systems such as |
some of the tradeoffs |
increases the commit delay |
controlled by a centralized |
systems such as qnx |
of the tradeoffs involved |
the commit delay for |
by a centralized pool |
it runs the show |
such as qnx and |
commit delay for each |
as qnx and utah |
note that the y |
a centralized pool manager |
delay for each file |
despite all of the |
qnx and utah s |
for each file and |
all of the above |
axis represents three different |
and utah s os |
each file and the |
of the above criticism |
represents three different quantities |
file and the likelihood |
and the likelihood of |
the likelihood of it |
miners register with the |
likelihood of it being |
register with the pool |
of it being accessed |
the cumulative percentage of |
with the pool manager |
it being accessed by |
minibrowsers retain one potential |
none of the unix |
the pool manager and |
being accessed by the |
cumulative percentage of time |
retain one potential advantage |
of the unix based |
pool manager and mine |
accessed by the reader |
percentage of time the |
one potential advantage over |
the unix based operating |
manager and mine on |
by the reader while |
of time the disks |
potential advantage over the |
unix based operating systems |
and mine on its |
the reader while it |
time the disks are |
advantage over the layered |
based operating systems came |
mine on its behalf |
reader while it is |
the disks are powered |
disks are powered on |
operating systems came close |
while it is being |
it is being written |
is being written back |
over the layered architecture |
the pool manager generates |
the total duration of |
pool manager generates tasks |
the layered architecture we |
these experiments demonstrate that |
layered architecture we proposed |
manager generates tasks and |
systems came close to |
total duration of the |
experiments demonstrate that for |
architecture we proposed earlier |
generates tasks and the |
came close to fulfilling |
duration of the simulation |
demonstrate that for the |
tasks and the miners |
close to fulfilling our |
that for the trace |
since all aspects of |
to fulfilling our requirements |
for the trace we |
the trace we have |
trace we have examined |
and the miners search |
all aspects of the |
the miners search for |
and the cumulative number |
aspects of the view |
the mfs algorithm of |
miners search for solutions |
the cumulative number of |
cumulative number of mode |
of the view are |
mfs algorithm of asynchronous |
search for solutions based |
as noted before the |
the view are optimized |
algorithm of asynchronous invalidations |
for solutions based on |
transitions that the disks |
that the disks undergo |
view are optimized to |
of asynchronous invalidations and |
solutions based on these |
noted before the core |
are optimized to run |
asynchronous invalidations and differentiated |
based on these tasks |
before the core of |
optimized to run together |
invalidations and differentiated writeback |
on these tasks that |
the core of those |
both the total duration |
and differentiated writeback is |
these tasks that can |
core of those operating |
the total duration of |
differentiated writeback is able |
tasks that can serve |
of those operating systems |
the interaction controls might |
total duration of the |
writeback is able to |
that can serve as |
those operating systems is |
duration of the experiment |
interaction controls might be |
is able to maintain |
can serve as proof |
operating systems is based |
controls might be far |
able to maintain cache |
serve as proof of |
as well as the |
systems is based on |
might be far more |
to maintain cache consistency |
as proof of work |
well as the number |
as the number of |
maintain cache consistency between |
be far more sophisticated |
the number of mode |
cache consistency between the |
far more sophisticated and |
once they find a |
consistency between the two |
they find a solution |
more sophisticated and perform |
between the two clients |
the two clients and |
increase as the percentage |
two clients and to |
as the percentage of |
sophisticated and perform potentially |
they send it to |
and perform potentially much |
clients and to allow |
the percentage of disks |
send it to the |
perform potentially much better |
and to allow the |
percentage of disks that |
it to the pool |
year old designs and |
potentially much better than |
to allow the writer |
of disks that is |
to the pool manager |
old designs and these |
much better than a |
allow the writer to |
disks that is powered |
designs and these operating |
better than a solution |
the writer to write |
than a solution resulting |
and these operating systems |
the pool manager behaves |
that is powered on |
writer to write back |
a solution resulting from |
these operating systems still |
pool manager behaves as |
is powered on is |
to write back changes |
solution resulting from mashing |
operating systems still treat |
manager behaves as a |
powered on is decreased |
write back changes to |
resulting from mashing up |
systems still treat computers |
behaves as a single |
back changes to the |
from mashing up together |
still treat computers as |
as a single miner |
changes to the stored |
mashing up together multiple |
treat computers as single |
a single miner in |
to the stored data |
up together multiple layers |
computers as single entities |
single miner in the |
the stored data faster |
together multiple layers developed |
we see that keeping |
as single entities without |
miner in the bitcoin |
stored data faster than |
multiple layers developed independently |
single entities without a |
in the bitcoin system |
data faster than is |
entities without a coherent |
faster than is possible |
disks on strikes an |
than is possible with |
on strikes an acceptable |
is possible with the |
strikes an acceptable balance |
possible with the alternative |
with the alternative schemes |
once it obtains a |
it obtains a legitimate |
in many realistic examples |
we intend to further |
obtains a legitimate block |
many realistic examples event |
intend to further evaluate |
conclusion in this paper |
a legitimate block from |
to further evaluate the |
further evaluate the perfor |
legitimate block from one |
we point out a |
based interfaces could get |
block from one of |
point out a new |
interfaces could get fairly |
references mance of the |
from one of its |
out a new opportunity |
could get fairly complex |
mance of the algorithm |
one of its miners |
a new opportunity for |
of the algorithm to |
new opportunity for saving |
the algorithm to determine |
opportunity for saving power |
algorithm to determine its |
for saving power in |
saving power in large |
to determine its effectiveness |
and difficult for most |
determine its effectiveness under |
difficult for most developers |
its effectiveness under other |
effectiveness under other workloads |
for most developers to |
the idea is elegant |
the block transfers the |
and with more clients |
block transfers the revenue |
most developers to work |
idea is elegant in |
transfers the revenue to |
developers to work with |
is elegant in its |
elegant in its simplicity |
the revenue to the |
revenue to the control |
to the control of |
the control of the |
log structured file systems |
this observation highlights the |
control of the pool |
observation highlights the importance |
structured file systems write |
highlights the importance of |
of the pool manager |
file systems write only |
the importance of developing |
systems write only to |
evaluation of an adaptive |
of an adaptive transport |
write only to the |
importance of developing component |
an adaptive transport protocol |
of developing component interface |
the pool manager then |
only to the log |
in proceedings of the |
pool manager then distributes |
to the log head |
manager then distributes the |
developing component interface and |
then distributes the revenue |
component interface and event |
nd annual joint conference |
distributes the revenue among |
if read accesses are |
annual joint conference of |
feet high windows nt |
the revenue among the |
read accesses are served |
interface and event standards |
joint conference of the |
high windows nt looked |
revenue among the miners |
accesses are served by |
and event standards for |
conference of the ieee |
windows nt looked like |
among the miners according |
are served by the |
served by the cache |
of the ieee computer |
nt looked like the |
the miners according to |
event standards for the |
the ieee computer and |
looked like the proverbial |
miners according to their |
then write accesses touch |
standards for the layered |
ieee computer and communications |
like the proverbial dinosaur |
according to their mining |
write accesses touch only |
for the layered architecture |
computer and communications societies |
to their mining power |
a closer look revealed |
the layered architecture we |
accesses touch only the |
closer look revealed a |
layered architecture we ve |
touch only the log |
only the log head |
the architecture is illustrated |
look revealed a truly |
the log head disk |
architecture we ve outlined |
architecture is illustrated in |
revealed a truly modern |
potentially allowing us to |
is illustrated in figure |
a truly modern operating |
allowing us to power |
truly modern operating system |
the task isn t |
us to power down |
task isn t really |
in order to estimate |
to power down all |
isn t really all |
order to estimate the |
power down all the |
object oriented design is |
t really all that |
to estimate the mining |
down all the other |
all the other disks |
really all that daunting |
estimate the mining power |
oriented design is pervasive |
existing solutions like disk |
the mining power of |
design is pervasive through |
solutions like disk management |
mining power of a |
conclusion the growing use |
the designers of microsoft |
like disk management solutions |
is pervasive through the |
power of a miner |
the growing use of |
designers of microsoft s |
pervasive through the system |
growing use of mobile |
of microsoft s object |
through the system including |
use of mobile computers |
microsoft s object linking |
the pool manager sets |
the system including the |
of mobile computers and |
s object linking and |
pool manager sets a |
system including the kernel |
mobile computers and wireless |
object linking and embedding |
manager sets a partial |
computers and wireless networks |
sets a partial target |
and wireless networks has |
a partial target for |
wireless networks has greatly |
there is a complete |
partial target for each |
is a complete distributed |
the working set model |
networks has greatly increased |
standard faced similar challenges |
a complete distributed strategy |
working set model for |
target for each member |
has greatly increased the |
set model for program |
complete distributed strategy with |
greatly increased the scope |
model for program behavior |
increased the scope for |
distributed strategy with at |
the scope for adapting |
scope for adapting data |
for adapting data access |
their ole interfaces are |
adapting data access to |
ole interfaces are pervasively |
data access to vary |
interfaces are pervasively used |
strategy with at its |
are pervasively used to |
with at its core |
pervasively used to support |
at its core a |
used to support thousands |
its core a distributed |
to support thousands of |
core a distributed object |
support thousands of plugins |
a distributed object technology |
thousands of plugins that |
distributed object technology and |
of plugins that implement |
object technology and includes |
plugins that implement context |
than the target of |
technology and includes a |
that implement context menus |
the target of the |
and includes a complete |
target of the bitcoin |
includes a complete integration |
of the bitcoin system |
virtual folders and various |
a complete integration of |
folders and various namespace |
complete integration of distributed |
and various namespace extensions |
integration of distributed services |
each miner is required |
of distributed services such |
miner is required to |
distributed services such as |
this paper has explored |
paper has explored applying |
has explored applying and |
explored applying and j |
is required to send |
and drag and drop |
services such as security |
required to send the |
drag and drop technologies |
to send the pool |
send the pool manager |
measurements of a distributed |
the pool manager blocks |
of a distributed file |
pool manager blocks that |
a distributed file the |
lacking the needed standards |
manager blocks that are |
distributed file the technique |
time disks on num |
blocks that are correct |
file the technique of |
that are correct according |
transitions total time of |
the technique of modeless |
are correct according to |
total time of run |
technique of modeless adaptation |
the live objects platform |
correct according to the |
of modeless adaptation to |
and last no but |
live objects platform supports |
according to the partial |
modeless adaptation to a |
last no but least |
objects platform supports both |
to the partial target |
adaptation to a distributed |
platform supports both options |
to a distributed file |
a distributed file system |
supports both options today |
distributed file system system |
there is a real |
the partial target is |
is a real desire |
partial target is chosen |
in proceedings of the |
in addition to allowing |
target is chosen to |
a real desire by |
addition to allowing hosted |
is chosen to be |
real desire by the |
to allowing hosted content |
th acm symposium to |
chosen to be large |
desire by the vendor |
allowing hosted content to |
acm symposium to improve |
symposium to improve its |
to improve its performance |
by the vendor to |
hosted content to be |
the vendor to continuously |
such that partial solutions |
improving the performance of |
that partial solutions arrive |
vendor to continuously innovate |
content to be pulled |
the cache manager for |
the performance of log |
partial solutions arrive frequently |
to continuously innovate its |
to be pulled in |
structured file systems with |
be pulled in and |
continuously innovate its operating |
cache manager for our |
solutions arrive frequently enough |
file systems with adaptive |
arrive frequently enough for |
innovate its operating system |
manager for our mfs |
systems with adaptive methods |
pulled in and exposed |
frequently enough for the |
its operating system and |
for our mfs on |
in and exposed via |
enough for the manager |
operating system and the |
our mfs on operating |
and exposed via event |
for the manager to |
system and the overall |
mfs on operating systems |
on operating systems principles |
the manager to accurately |
and the overall services |
exposed via event interfaces |
manager to accurately estimate |
to accurately estimate the |
accurately estimate the power |
estimate the power of |
the power of the |
microsoft doesn t hesitate |
power of the miner |
doesn t hesitate to |
components developed by some |
t hesitate to incorporate |
developed by some of |
hesitate to incorporate academic |
by some of our |
to incorporate academic results |
some of our users |
pacific file system incorporates |
of our users also |
incorporate academic results into |
file system incorporates features |
our users also use |
academic results into operating |
system incorporates features that |
to reduce management overhead |
results into operating system |
users also use embedded |
incorporates features that are |
also use embedded minibrowsers |
features that are not |
use embedded minibrowsers to |
that are not present |
as the value of |
embedded minibrowsers to gain |
are not present in |
not present in existing |
present in existing grove |
minibrowsers to gain access |
and is open for |
the value of bitcoin |
to gain access to |
is open for new |
value of bitcoin rose |
gain access to a |
open for new directions |
access to a wide |
to a wide range |
a wide range of |
wide range of platforms |
bitcoin mining has become |
mining has become a |
innovation as a life |
has become a rapidly |
as a life style |
become a rapidly advancing |
a life style microsoft |
file systems for mobile |
systems for mobile hosts |
life style microsoft is |
a rapidly advancing industry |
style microsoft is not |
reducing energy consumption of |
microsoft is not conservative |
adaptation to bandwidth variation |
energy consumption of disk |
is not conservative in |
to bandwidth variation through |
consumption of disk storage |
not conservative in its |
technological advancements lead to |
bandwidth variation through the |
of disk storage using |
disk storage using power |
advancements lead to ever |
variation through the use |
conservative in its os |
lead to ever more |
through the use of |
in its os development |
to ever more efficient |
the use of prioritised |
use of prioritised communication |
ever more efficient hashing |
more efficient hashing asics |
while most vendors only |
most vendors only consider |
vendors only consider changes |
only consider changes to |
consider changes to their |
changes to their core |
to their core os |
their core os services |
core os services under |
performance evaluation central to |
os services under extreme |
evaluation central to our |
services under extreme market |
central to our argument |
under extreme market pressure |
to our argument is |
our argument is the |
argument is the assertion |
is the assertion that |
o hint genercache consistency |
the core of windows |
the assertion that hosted |
hint genercache consistency protocol |
assertion that hosted event |
core of windows nt |
genercache consistency protocol using |
of windows nt has |
that hosted event notification |
this is a simplification |
consistency protocol using file |
windows nt has changed |
hosted event notification solutions |
is a simplification that |
protocol using file access |
nt has changed significantly |
event notification solutions scale |
a simplification that is |
using file access information |
has changed significantly over |
notification solutions scale poorly |
simplification that is sufficient |
file access information to |
changed significantly over the |
solutions scale poorly and |
that is sufficient for |
access information to imation |
significantly over the past |
scale poorly and stand |
is sufficient for our |
information to imation through |
over the past years |
poorly and stand as |
sufficient for our analysis |
to imation through speculative |
imation through speculative execution |
and stand as a |
the past years to |
stand as a barrier |
past years to accommodate |
the intricacies of reward |
as a barrier to |
in operating systems prove |
operating systems prove performance |
intricacies of reward systems |
a barrier to collaboration |
years to accommodate the |
of reward systems are |
barrier to collaboration applications |
to accommodate the demands |
reward systems are explained |
accommodate the demands of |
systems are explained in |
the demands of modern |
demands of modern computing |
and that developers will |
that developers will want |
effect of increasing percentage |
developers will want to |
of increasing percentage of |
will want to combine |
increasing percentage of powered |
want to combine hosted |
especially the upcoming release |
to combine hosted content |
the upcoming release of |
up disks on power |
disks on power and |
on power and time |
combine hosted content with |
we have evaluated the |
upcoming release of windows |
hosted content with p |
have evaluated the effect |
and caching solutions are |
evaluated the effect of |
caching solutions are typically |
the effect of these |
solutions are typically application |
effect of these features |
of these features on |
p protocols to overcome |
these features on performance |
protocols to overcome these |
features on performance at |
to overcome these problems |
on performance at varying |
a notable exception is |
performance at varying bandwidth |
notable exception is p |
on the other hand |
at varying bandwidth levels |
varying bandwidth levels and |
in this section we |
bandwidth levels and under |
is applicable to any |
this section we present |
levels and under both |
applicable to any cacheable |
section we present data |
and under both synthetic |
to any cacheable dataset |
formerly known as windows |
we present data to |
under both synthetic and |
both synthetic and real |
present data to support |
known as windows nt |
since existing solutions are |
data to support our |
existing solutions are typically |
to support our claims |
solutions are typically layered |
are typically layered on |
typically layered on top |
layered on top of |
on top of the |
top of the file |
some of the results |
makes that the microsoft |
that the microsoft takes |
the microsoft takes the |
they could be used |
which we discuss in |
microsoft takes the operating |
could be used in |
we discuss in section |
takes the operating system |
be used in conjunction |
discuss in section ix |
the operating system functionality |
used in conjunction with |
operating system functionality to |
in conjunction with our |
system functionality to the |
conjunction with our solution |
forks block propagation in |
functionality to the next |
with our solution to |
block propagation in the |
including a workload emulating |
our solution to take |
are drawn from a |
propagation in the overlay |
a workload emulating collaborative |
to the next level |
solution to take advantage |
drawn from a widely |
in the overlay network |
workload emulating collaborative data |
to take advantage of |
take advantage of application |
the overlay network takes |
from a widely cited |
the advances in windows |
overlay network takes seconds |
a widely cited industry |
we also provide some |
widely cited industry whitepaper |
also provide some initial |
provide some initial simulation |
some initial simulation results |
initial simulation results that |
simulation results that validate |
therefore it is possible |
results that validate our |
that validate our claim |
validate our claim that |
our claim that power |
it is possible for |
performance measurements access with |
is possible for two |
savings are possible using |
possible for two distant |
measurements access with high |
are possible using a |
possible using a log |
access with high read |
for two distant miners |
are too numerous to |
two distant miners to |
too numerous to enumerate |
distant miners to generate |
and found that while |
while simulations can never |
numerous to enumerate here |
miners to generate competing |
found that while the |
and were obtained using |
to generate competing blocks |
simulations can never provide |
that while the of |
were obtained using a |
can never provide conclusive |
obtained using a testing |
they range from a |
while the of automatic |
never provide conclusive evidence |
using a testing methodology |
range from a file |
both of which name |
the of automatic prefetching |
provide conclusive evidence for |
conclusive evidence for the |
from a file system |
of which name the |
a testing methodology and |
evidence for the feasibility |
a file system cache |
in proceedings of the |
which name the same |
testing methodology and setup |
for the feasibility of |
the feasibility of a |
proceedings of the isca |
name the same block |
methodology and setup developed |
file system cache for |
feasibility of a system |
of the isca interadditional |
the same block as |
and setup developed and |
they are an effective |
the isca interadditional costs |
same block as their |
system cache for disconnected |
setup developed and published |
are an effective means |
isca interadditional costs imposed |
block as their predecessor |
cache for disconnected operation |
developed and published by |
an effective means to |
interadditional costs imposed are |
costs imposed are mostly |
effective means to identify |
and published by sonic |
imposed are mostly hidden |
means to identify promising |
to identify promising solutions |
which was originally developed |
published by sonic software |
was originally developed at |
they can have benenational |
our principal contribution in |
originally developed at cmu |
can have benenational conference |
are rare since the |
developed at cmu in |
principal contribution in this |
have benenational conference on |
rare since the average |
at cmu in the |
contribution in this paper |
benenational conference on parallel |
since the average mining |
cmu in the coda |
in this paper is |
conference on parallel and |
the average mining interval |
in the coda project |
this paper is in |
on parallel and distributed |
average mining interval is |
paper is in having |
parallel and distributed computfits |
is in having shown |
and distributed computfits which |
in having shown a |
distributed computfits which are |
to a remote storage |
having shown a new |
a remote storage service |
computfits which are very |
shown a new fit |
which are very visible |
remote storage service that |
a new fit for |
new fit for an |
fit for an old |
for an old idea |
the remainder was produced |
storage service that automatically |
remainder was produced in |
and they occur on |
we believe that the |
believe that the log |
was produced in our |
modal nature of ing |
nature of ing systems |
service that automatically moves |
produced in our own |
they occur on average |
structured file system shows |
that automatically moves old |
in our own experiments |
occur on average once |
file system shows promise |
automatically moves old data |
on average once every |
system shows promise as |
moves old data from |
shows promise as a |
old data from your |
promise as a powersaving |
data from your hard |
as a powersaving opportunity |
from your hard disk |
a powersaving opportunity for |
your hard disk to |
powersaving opportunity for large |
hard disk to remote |
adaptation in mfs allows |
disk to remote servers |
in mfs allows clients |
to remote servers if |
mfs allows clients to |
remote servers if you |
acknowledgments this work was |
allows clients to adapt |
servers if you are |
this work was partially |
clients to adapt quickly |
if you are running |
work was partially funded |
to adapt quickly to |
from the industry white |
you are running out |
was partially funded by |
adapt quickly to a |
the industry white paper |
are running out of |
partially funded by intel |
quickly to a variety |
running out of disk |
funded by intel corporation |
to a variety of |
out of disk space |
by intel corporation and |
a variety of bandwidth |
analyzes the performance of |
intel corporation and the |
variety of bandwidth conditions |
the system has a |
the performance of several |
corporation and the national |
of bandwidth conditions without |
system has a mechanism |
performance of several commercial |
and the national science |
the national science foundation |
bandwidth conditions without substantial |
has a mechanism to |
of several commercial enterprise |
special thanks to saikat |
conditions without substantial changes |
a mechanism to solve |
from tight security integration |
several commercial enterprise service |
thanks to saikat guha |
without substantial changes in |
substantial changes in operation |
commercial enterprise service bus |
to saikat guha for |
mechanism to solve forks |
saikat guha for his |
to solve forks when |
guha for his input |
solve forks when they |
for his input in |
forks when they do |
his input in the |
input in the simulator |
in the simulator design |
when they do occur |
as the dominant security |
the dominant security provider |
we also wish to |
also wish to thank |
wish to thank our |
to thank our anonymous |
shown is the maximum |
thank our anonymous reviewers |
causing one of the |
is the maximum throughput |
our anonymous reviewers for |
one of the blocks |
to a complete integration |
anonymous reviewers for their |
a complete integration of |
of the blocks to |
reviewers for their valuable |
for their valuable feedback |
the blocks to be |
complete integration of network |
blocks to be discarded |
integration of network quality |
of network quality of |
network quality of services |
quality of services tools |
of services tools including |
we ignore bifurcations for |
services tools including data |
ignore bifurcations for the |
tools including data transmission |
bifurcations for the sake |
including data transmission shapers |
for the sake of |
our evaluation has included |
data transmission shapers and |
the sake of simplicity |
evaluation has included comparisons |
transmission shapers and priority |
has included comparisons of |
shapers and priority scheduling |
included comparisons of mfs |
and priority scheduling and |
conserving disk energy in |
comparisons of mfs to |
of mfs to cache |
mfs to cache manm |
disk energy in network |
energy in network servers |
priority scheduling and queuing |
since the choice of |
the experiment varies the |
the choice of the |
experiment varies the number |
choice of the discarded |
varies the number of |
and from attributed based |
the number of subscribers |
from attributed based distributed |
number of subscribers while |
of the discarded block |
attributed based distributed component |
of subscribers while using |
the discarded block on |
based distributed component programming |
th international conference on |
subscribers while using a |
discarded block on bifurcation |
distributed component programming to |
international conference on supercomputing |
while using a single |
ager configurations corresponding to |
using a single publisher |
component programming to indexing |
block on bifurcation is |
configurations corresponding to prior |
corresponding to prior work |
programming to indexing support |
on bifurcation is random |
a single publisher that |
and confirmed scale and |
single publisher that communicates |
to indexing support integrated |
confirmed scale and performance |
publisher that communicates through |
indexing support integrated in |
scale and performance in |
one may incorporate this |
that communicates through a |
support integrated in the |
and performance in a |
may incorporate this event |
communicates through a single |
integrated in the file |
performance in a distributed |
in a distributed file |
a distributed file system |
in the file system |
incorporate this event into |
through a single hosted |
this event into the |
acm that there are |
a single hosted message |
event into the probability |
that there are situations |
single hosted message broker |
into the probability of |
there are situations in |
hosted message broker on |
the case for massive |
the probability of finding |
are situations in which |
message broker on a |
case for massive arrays |
probability of finding a |
situations in which mfs |
we are witnesses of |
broker on a single |
for massive arrays of |
of finding a block |
in which mfs would |
which mfs would outperform |
mfs would outperform afs |
massive arrays of idle |
arrays of idle disks |
on a single topic |
are witnesses of a |
transactions on computer systems |
witnesses of a unique |
and consider instead the |
of a unique process |
consider instead the probability |
instead the probability of |
the probability of finding |
probability of finding a |
of finding a block |
figured for message durability |
finding a block that |
never before have we |
a block that is |
before have we seen |
block that is not |
conference on file and |
have we seen such |
even if a subscriber |
that is not discarded |
on file and storage |
file and storage technologies |
if a subscriber experiences |
we seen such a |
a subscriber experiences a |
seen such a radical |
pools often charge a |
subscriber experiences a transient |
such a radical overhaul |
often charge a small |
experiences a transient loss |
a radical overhaul of |
charge a small percentage |
a transient loss of |
radical overhaul of an |
a small percentage of |
transient loss of connectivity |
overhaul of an operating |
small percentage of the |
of an operating system |
percentage of the revenue |
an operating system targeted |
of the revenue as |
the publisher retains and |
operating system targeted for |
the revenue as fee |
coda and little work |
publisher retains and hence |
system targeted for the |
retains and hence can |
targeted for the enterprise |
and hence can replay |
for the enterprise market |
helping disk arrays sleep |
we discuss in section |
hence can replay all |
these earlier systems were |
disk arrays sleep through |
arrays sleep through the |
sleep through the winter |
earlier systems were designed |
discuss in section ix |
can replay all messages |
systems were designed for |
in general this market |
in section ix the |
were designed for a |
general this market is |
section ix the implications |
designed for a mobile |
as the number of |
this market is very |
ix the implications of |
for a mobile environment |
proceedings of the twentieth |
the number of subscribers |
market is very conservative |
the implications of such |
a mobile environment which |
of the twentieth acm |
number of subscribers increases |
is very conservative and |
implications of such fees |
mobile environment which is |
the twentieth acm symposium |
very conservative and not |
of such fees to |
environment which is substantially |
twentieth acm symposium on |
conservative and not interested |
such fees to our |
which is substantially different |
acm symposium on operating |
and not interested in |
fees to our analysis |
symposium on operating systems |
on operating systems principles |
not interested in taking |
interested in taking risks |
latency will also soars |
will also soars because |
many pools are open |
also soars because the |
pools are open and |
soars because the amount |
however the problems of |
are open and accept |
because the amount of |
the problems of scale |
open and accept any |
the amount of time |
partially connected operafrom that |
and accept any interested |
connected operafrom that available |
amount of time the |
accept any interested miner |
operafrom that available today |
management and distribution are |
of time the broker |
and distribution are asking |
time the broker needs |
distribution are asking for |
mfs is able to |
are asking for radical |
the broker needs to |
is able to provide |
broker needs to spend |
asking for radical solutions |
interplay of energy and |
able to provide tion |
a pool interface is |
needs to spend sending |
for radical solutions to |
of energy and performance |
radical solutions to get |
to spend sending a |
pool interface is typically |
energy and performance for |
interface is typically comprised |
spend sending a single |
solutions to get to |
and performance for disk |
is typically comprised of |
sending a single message |
to get to a |
performance for disk arrays |
typically comprised of a |
a single message increases |
get to a computing |
for disk arrays running |
comprised of a web |
single message increases linearly |
to a computing base |
disk arrays running transaction |
of a web interface |
message increases linearly with |
a computing base that |
arrays running transaction processing |
a web interface for |
increases linearly with the |
computing base that can |
running transaction processing workloads |
web interface for registration |
linearly with the number |
base that can bring |
interface for registration and |
with the number of |
in ieee international symposium |
that can bring us |
for registration and a |
the number of subscribers |
ieee international symposium on |
can bring us into |
registration and a miner |
international symposium on performance |
bring us into the |
and a miner interface |
symposium on performance analysis |
us into the next |
a miner interface for |
improved performance in periods |
into the next century |
durability is often not |
miner interface for the |
performance in periods of |
on performance analysis of |
is often not required |
interface for the mining |
in periods of high |
one of the markets |
for the mining software |
performance analysis of systems |
periods of high network |
of the markets where |
analysis of systems and |
of high network contention |
of systems and software |
the markets where we |
in order to mine |
high network contention by |
markets where we will |
order to mine for |
where we will see |
to mine for a |
we will see the |
mine for a pool |
will see the main |
see the main competitive |
shows throughput in an |
the main competitive battle |
throughput in an experiment |
main competitive battle between |
a miner registers with |
in an experiment in |
competitive battle between microsoft |
miner registers with the |
an experiment in which |
battle between microsoft and |
registers with the web |
experiment in which the |
between microsoft and others |
with the web interface |
in which the publisher |
microsoft and others will |
which the publisher does |
and others will be |
the publisher does not |
others will be that |
supplies a bitcoin address |
publisher does not log |
will be that of |
mofavouring cache validation and |
a bitcoin address to |
does not log data |
be that of the |
cache validation and rpcs |
bitcoin address to receive |
reducing disk power consumption |
that of the e |
validation and rpcs to |
address to receive its |
disk power consumption in |
and rpcs to retrieve |
to receive its future |
power consumption in servers |
a disconnected subscriber would |
receive its future shares |
rpcs to retrieve files |
consumption in servers with |
in servers with drpm |
its future shares of |
to retrieve files over |
disconnected subscriber would experience |
web farms with hundreds |
future shares of the |
retrieve files over other |
subscriber would experience a |
farms with hundreds of |
shares of the revenue |
files over other bile |
would experience a loss |
with hundreds of nodes |
over other bile computing |
other bile computing with |
bile computing with the |
computing with the rover |
with the rover toolkit |
and receives from the |
we find that while |
with support services for |
ieee transactypes of traffic |
find that while the |
receives from the pool |
support services for load |
we have not compared |
from the pool credentials |
that while the maximum |
services for load balancing |
have not compared mfs |
the pool credentials for |
while the maximum throughput |
not compared mfs with |
pool credentials for mining |
the maximum throughput is |
compared mfs with lbfs |
maximum throughput is much |
mfs with lbfs since |
hiding in plain sight |
throughput is much higher |
with lbfs since tions |
then he feeds his |
lbfs since tions on |
google seeks more power |
he feeds his credentials |
since tions on computers |
the degradation of performance |
feeds his credentials and |
in the new york |
the new york times |
his credentials and the |
degradation of performance is |
special issue on mobile |
issue on mobile computing |
of performance is even |
credentials and the pool |
performance is even more |
their approaches are orthogonal |
and the pool s |
is even more dramatic |
the pool s address |
distributed and single image |
pool s address to |
and single image management |
s address to its |
address to its mining |
to its mining rig |
developers of collaboration applications |
of collaboration applications that |
collaboration applications that need |
applications that need good |
that need good scalability |
need good scalability might |
the mining rig obtains |
good scalability might discover |
are really pushing the |
mining rig obtains its |
scalability might discover that |
really pushing the envelope |
rig obtains its tasks |
might discover that hosted |
pushing the envelope of |
obtains its tasks from |
discover that hosted esb |
the envelope of all |
berkeley db java edition |
db java edition architecture |
its tasks from the |
envelope of all operating |
that hosted esb options |
tasks from the pool |
of all operating systems |
an oracle white paper |
hosted esb options won |
from the pool and |
all operating systems that |
esb options won t |
the pool and sends |
operating systems that are |
options won t achieve |
pool and sends partial |
systems that are currently |
won t achieve this |
and sends partial and |
that are currently on |
t achieve this goal |
sends partial and full |
not present in the |
are currently on the |
partial and full proof |
present in the earlier |
currently on the market |
and full proof of |
in the earlier systems |
full proof of work |
the earlier systems we |
earlier systems we have |
we report on some |
systems we have compared |
we have compared against |
windows nt is still |
report on some experiments |
typically with the stratum |
nt is still considered |
on some experiments we |
we anticipate that implementing |
with the stratum protocol |
is still considered to |
some experiments we conducted |
anticipate that implementing lbfs |
still considered to be |
experiments we conducted on |
that implementing lbfs file |
considered to be the |
we conducted on our |
implementing lbfs file chunks |
to be the new |
conducted on our own |
lbfs file chunks in |
be the new kid |
on our own at |
file chunks in mfs |
chunks in mfs would |
our own at cornell |
the new kid on |
new kid on the |
kid on the block |
eduardo pinheiro and ricardo |
pinheiro and ricardo bianchini |
on the block in |
focusing on scalability of |
the block in the |
on scalability of event |
as it finds blocks |
energy conservation techniques for |
conservation techniques for disk |
techniques for disk array |
block in the internet |
scalability of event notification |
in the internet services |
of event notification platforms |
the internet services world |
the pool manager credits |
event notification platforms that |
pool manager credits the |
notification platforms that leverage |
manager credits the miner |
platforms that leverage peer |
but it is clear |
credits the miner s |
it is clear that |
the miner s account |
is clear that the |
miner s account according |
clear that the risks |
th annual international conference |
s account according to |
annual international conference on |
international conference on supercomputing |
account according to its |
peer techniques for dissemination |
that the risks that |
further improve performance its |
according to its share |
techniques for dissemination and |
improve performance its performance |
the risks that are |
to its share of |
risks that are taken |
and performance in a |
performance in a wide |
for dissemination and recovery |
its share of the |
that are taken now |
share of the work |
are taken now are |
in proceedin future work |
taken now are the |
on the first graph |
now are the right |
we plan to investigate |
are the right moves |
and transfers these funds |
plan to investigate the |
the right moves to |
transfers these funds either |
to investigate the performance |
right moves to prepare |
these funds either on |
investigate the performance of |
moves to prepare the |
funds either on request |
characteristics of file system |
either on request or |
the performance of ings |
of file system workloads |
to prepare the operating |
on request or automatically |
performance of ings of |
prepare the operating system |
we compare the maximum |
request or automatically to |
of ings of the |
the operating system for |
compare the maximum throughput |
or automatically to the |
ings of the first |
operating system for operation |
the maximum throughput of |
automatically to the aforementioned |
of the first usenix |
system for operation in |
maximum throughput of two |
to the aforementioned bitcoin |
the first usenix conference |
for operation in these |
throughput of two decentralized |
the aforementioned bitcoin address |
first usenix conference on |
operation in these emerging |
of two decentralized reliable |
usenix conference on file |
in these emerging massive |
two decentralized reliable multicast |
too big pools despite |
these emerging massive computing |
conference on file and |
decentralized reliable multicast protocols |
big pools despite their |
mendel rosenblum and john |
rosenblum and john k |
on file and storage |
pools despite their important |
emerging massive computing environments |
file and storage modeless |
despite their important role |
and storage modeless adaptation |
their important role of |
the design and implementation |
storage modeless adaptation and |
important role of enabling |
design and implementation of |
modeless adaptation and mfs |
the bugs innovation comes |
role of enabling small |
and implementation of a |
adaptation and mfs in |
and mfs in wide |
implementation of a log |
bugs innovation comes at |
innovation comes at a |
comes at a price |
area and more web |
acm transactions on computer |
transactions on computer systems |
pools can constitute a |
one of the costs |
can constitute a threat |
of the costs of |
a single topic and |
the costs of introducing |
constitute a threat to |
single topic and a |
costs of introducing a |
a threat to the |
topic and a single |
of introducing a significant |
threat to the bitcoin |
and a single publisher |
introducing a significant amount |
to the bitcoin system |
a significant amount of |
the bitcoin system if |
significant amount of new |
unlike in the previous |
bitcoin system if their |
amount of new code |
in the previous tests |
system if their size |
of new code is |
as well as further |
if their size is |
new code is the |
well as further evaluating |
their size is too |
code is the number |
as further evaluating the |
size is too large |
is the number of |
further evaluating the performance |
the number of software |
evaluating the performance of |
number of software defects |
the performance of the |
of software defects per |
performance of the mfs |
if one pool controls |
software defects per lines |
of the mfs cache |
these experiments used a |
one pool controls the |
defects per lines of |
the mfs cache consistency |
mfs cache consistency algorithm |
per lines of codes |
pool controls the majority |
lines of codes increases |
controls the majority of |
we also intend to |
also intend to use |
the majority of mining |
majority of mining power |
while measurements actually let |
measurements actually let us |
actually let us believe |
the system becomes unstable |
let us believe that |
us believe that microsoft |
believe that microsoft products |
this limits the peak |
that microsoft products are |
disk layout optimization for |
limits the peak performance |
microsoft products are quite |
layout optimization for reducing |
the peak performance to |
products are quite reliable |
optimization for reducing energy |
are quite reliable at |
disconnected operamfs to further |
for reducing energy consumption |
quite reliable at operating |
operamfs to further examine |
reliable at operating systems |
to further examine the |
further examine the benefits |
examine the benefits achievable |
the benefits achievable from |
benefits achievable from the |
achievable from the autotion |
from the autotion in |
the autotion in the |
autotion in the coda |
in the coda file |
the coda file system |
acm transactions on commatic |
transactions on commatic generation |
on commatic generation of |
commatic generation of caching |
th annual international conference |
generation of caching policies |
annual international conference on |
of caching policies for |
caching policies for files |
international conference on supercomputing |
thousand lines of code |
fresh code has a |
code has a disastrous |
has a disastrous effect |
a disastrous effect on |
disastrous effect on this |
effect on this number |
achieves stable high throughput |
the outlook becomes even |
outlook becomes even more |
becomes even more worrisome |
even more worrisome when |
warns that the system |
more worrisome when we |
that the system is |
worrisome when we realize |
the system is unstable |
when we realize that |
system is unstable with |
we realize that microsoft |
is unstable with even |
realize that microsoft is |
unstable with even smaller |
that microsoft is not |
with even smaller pools |
microsoft is not only |
is not only introducing |
runs at about a |
not only introducing new |
at about a fifth |
only introducing new code |
about a fifth that |
a fifth that speed |
but is also changing |
collapsing as the number |
is also changing all |
as the number of |
also changing all of |
in realistic scenarios of |
the number of subscribers |
changing all of its |
realistic scenarios of the |
number of subscribers increases |
all of its old |
automated hoarding for mobile |
hoarding for mobile computers |
of its old code |
scenarios of the bitcoin |
of the bitcoin system |
in proceedings of the |
the bitcoin system no |
proceedings of the sixteenth |
bitcoin system no pool |
of the sixteenth acm |
at small loss rates |
an automated process is |
system no pool controls |
the sixteenth acm symposium |
automated process is converting |
no pool controls a |
sixteenth acm symposium on |
process is converting all |
latency in qsm is |
pool controls a majority |
acm symposium on operating |
is converting all of |
in qsm is at |
controls a majority of |
symposium on operating systems |
on operating systems principles |
qsm is at the |
a majority of the |
converting all of the |
is at the level |
majority of the mining |
all of the windows |
at the level of |
of the mining power |
of the windows nt |
the windows nt code |
windows nt code to |
nt code to be |
for one day in |
one day in june |
ms irrespectively of the |
irrespectively of the number |
of the number of |
the number of subscribers |
thousand lines of code |
when the number of |
lines of code per |
the number of topics |
a single pool called |
of code per day |
number of topics is |
single pool called ghash |
acknowledgements we would like |
code per day and |
of topics is varied |
we would like to |
per day and is |
would like to thank |
day and is believed |
like to thank robbert |
to thank robbert van |
thank robbert van renesse |
qsm maintains its high |
and is believed to |
maintains its high performance |
is believed to catch |
believed to catch all |
to catch all pointer |
emin gu n sirer |
catch all pointer arithmetic |
gu n sirer and |
on the second graph |
all pointer arithmetic cases |
n sirer and paul |
of the blocks in |
sirer and paul francis |
the blocks in the |
and paul francis for |
blocks in the bitcoin |
paul francis for comments |
an important question is |
in the bitcoin main |
francis for comments and |
important question is whether |
the bitcoin main chain |
for comments and suggestions |
question is whether the |
comments and suggestions regarding |
is whether the introduced |
and suggestions regarding mfs |
whether the introduced functionality |
we report performance for |
the bitcoin community backlashed |
the introduced functionality is |
we also thank rimon |
also thank rimon barr |
introduced functionality is worth |
bitcoin community backlashed at |
functionality is worth the |
community backlashed at the |
is worth the unavoidable |
backlashed at the pool |
and kevin walsh for |
worth the unavoidable initial |
kevin walsh for helpful |
the unavoidable initial instability |
walsh for helpful discussions |
unavoidable initial instability that |
for helpful discussions and |
which has done nothing |
initial instability that is |
but performance for other |
helpful discussions and corrections |
has done nothing worse |
instability that is bound |
performance for other group |
discussions and corrections to |
and corrections to this |
corrections to this paper |
for other group sizes |
done nothing worse than |
that is bound to |
other group sizes is |
nothing worse than being |
is bound to occur |
group sizes is similar |
worse than being extremely |
than being extremely successful |
whenever taking risks to |
jgroups performance was higher |
taking risks to achieve |
a scalable services architecture |
performance was higher with |
risks to achieve major |
scalable services architecture tudor |
was higher with smaller |
to achieve major improvements |
services architecture tudor marian |
higher with smaller group |
architecture tudor marian ken |
with smaller group sizes |
tudor marian ken birman |
io reduced its relative |
marian ken birman department |
reduced its relative mining |
ken birman department of |
there is always the |
its relative mining power |
birman department of computer |
but erodes as the |
is always the down |
relative mining power and |
department of computer science |
erodes as the number |
always the down side |
mining power and publicly |
of computer science cornell |
a coherent distributed file |
the down side that |
power and publicly committed |
down side that there |
computer science cornell university |
coherent distributed file cache |
as the number of |
and publicly committed to |
side that there is |
distributed file cache with |
the number of topics |
publicly committed to stay |
file cache with directory |
cache with directory write |
number of topics increases |
committed to stay away |
that there is some |
to stay away from |
there is some change |
stay away from the |
jgroups failed when we |
is some change of |
acm transactions on computer |
transactions on computer systems |
failed when we attempted |
some change of failure |
when we attempted to |
change of failure and |
we attempted to configure |
of failure and it |
attempted to configure it |
failure and it is |
to configure it with |
and it is likely |
configure it with more |
it is likely that |
it with more than |
is likely that we |
likely that we will |
that we will see |
we will see a |
will see a number |
see a number of |
block withholding and its |
edu abstract data centers |
a number of components |
withholding and its detection |
abstract data centers constructed |
number of components of |
and its detection classical |
data centers constructed as |
of components of nt |
its detection classical block |
centers constructed as clusters |
components of nt coming |
detection classical block withholding |
constructed as clusters of |
of nt coming under |
we look at two |
as clusters of inexpensive |
nt coming under intense |
look at two scalable |
clusters of inexpensive machines |
coming under intense scrutiny |
at two scalable protocols |
of inexpensive machines have |
under intense scrutiny from |
two scalable protocols under |
inexpensive machines have compelling |
machines have compelling cost |
scalable protocols under conditions |
intense scrutiny from industry |
protocols under conditions of |
scrutiny from industry and |
under conditions of stress |
from industry and academia |
but developing services to |
is an attack performed |
developing services to run |
an attack performed by |
services to run on |
with a focus on |
attack performed by a |
to run on them |
run on them can |
on them can be |
them can be challenging |
performed by a pool |
a focus on delivery |
such as the directory |
this paper reports on |
paper reports on a |
reports on a new |
on a new framework |
focus on delivery latency |
by a pool member |
as the directory services |
a pool member against |
the scalable services architecture |
pool member against the |
member against the other |
against the other pool |
the other pool members |
may become a performance |
as a fixed message |
become a performance bottleneck |
a fixed message rate |
a lowbandwidth network file |
which helps developers develop |
the attacking miner registers |
fixed message rate is |
lowbandwidth network file system |
a performance bottleneck in |
helps developers develop scalable |
attacking miner registers with |
in proceedings of the |
miner registers with the |
developers develop scalable clustered |
registers with the pool |
performance bottleneck in the |
proceedings of the seventeenth |
message rate is spread |
develop scalable clustered applications |
with the pool and |
bottleneck in the overall |
of the seventeenth acm |
the work is focused |
work is focused on |
is focused on nontransactional |
focused on nontransactional high |
the seventeenth acm symposium |
the pool and apparently |
in the overall distributed |
rate is spread over |
seventeenth acm symposium on |
pool and apparently starts |
the overall distributed operation |
is spread over varying |
acm symposium on operating |
these are poorly supported |
are poorly supported in |
poorly supported in existing |
symposium on operating systems |
and apparently starts mining |
or the wide spread |
supported in existing platforms |
the wide spread security |
spread over varying numbers |
apparently starts mining honestly |
on operating systems principles |
wide spread security integration |
over varying numbers of |
a primary goal was |
starts mining honestly it |
spread security integration could |
varying numbers of topics |
primary goal was to |
mining honestly it regularly |
security integration could introduce |
goal was to keep |
honestly it regularly sends |
integration could introduce a |
was to keep the |
it regularly sends the |
could introduce a critical |
to keep the ssa |
regularly sends the pool |
introduce a critical dependency |
keep the ssa as |
sends the pool partial |
a critical dependency on |
the ssa as small |
subscribers each join some |
the pool partial proof |
critical dependency on the |
ssa as small and |
each join some number |
pool partial proof of |
dependency on the high |
as small and simple |
small and simple as |
and simple as possible |
join some number of |
partial proof of work |
some number of topics |
key elements include a |
elements include a tcp |
availability of the security |
of the security servers |
based chain replication mechanism |
chain replication mechanism and |
replication mechanism and a |
mechanism and a gossip |
a publisher sends data |
publisher sends data at |
from a research point |
sends data at a |
based subsystem for managing |
the attacking miner sends |
a research point of |
data at a rate |
subsystem for managing configuration |
attacking miner sends only |
research point of view |
at a rate of |
for managing configuration data |
miner sends only partial |
managing configuration data and |
sends only partial proof |
configuration data and repairing |
these problems do not |
only partial proof of |
caching in the sprite |
data and repairing inconsistencies |
problems do not really |
partial proof of work |
in the sprite network |
and repairing inconsistencies after |
repairing inconsistencies after faults |
the sprite network file |
sprite network file system |
do not really bother |
not really bother us |
our experimental results confirm |
if it finds a |
experimental results confirm the |
it finds a full |
acm transactions on computer |
results confirm the effectiveness |
finds a full solution |
transactions on computer systems |
a full solution that |
the advantage of performing |
confirm the effectiveness of |
the effectiveness of the |
effectiveness of the approach |
selecting the topic in |
advantage of performing research |
full solution that constitutes |
the topic in which |
of performing research on |
solution that constitutes a |
topic in which to |
introduction large computing systems |
performing research on a |
that constitutes a full |
in which to send |
large computing systems are |
research on a system |
constitutes a full proof |
which to send at |
computing systems are often |
a full proof of |
to send at random |
systems are often structured |
full proof of work |
are often structured as |
which has distribution at |
proof of work it |
often structured as service |
has distribution at its |
of work it discards |
structured as service oriented |
as service oriented architectures |
work it discards the |
distribution at its core |
it discards the solution |
at its core greatly |
its core greatly outweighs |
core greatly outweighs the |
greatly outweighs the consequences |
for example using web |
reducing the pool s |
outweighs the consequences of |
example using web services |
using web services platforms |
the consequences of working |
the pool s total |
consequences of working with |
pool s total revenue |
of working with a |
clients access services in |
access services in a |
services in a request |
working with a cutting |
with a cutting edge |
we see that ricochet |
a cutting edge operating |
cutting edge operating system |
each service is self |
this attack is illustrated |
attack is illustrated in |
is illustrated in figure |
offers its own api |
however i must admit |
and handles its own |
handles its own quality |
i must admit that |
its own quality of |
own quality of service |
quality of service or |
of service or availability |
service or availability guarantees |
must admit that at |
the attacker does not |
a cornelldeveloped protocol for |
attacker does not change |
for example by arranging |
admit that at more |
cornelldeveloped protocol for low |
does not change the |
example by arranging to |
that at more then |
not change the pool |
by arranging to be |
at more then one |
arranging to be restarted |
to be restarted after |
be restarted after a |
more then one occasion |
restarted after a failure |
change the pool s |
then one occasion my |
the pool s effective |
while many services need |
one occasion my students |
pool s effective mining |
many services need to |
s effective mining power |
occasion my students had |
services need to maintain |
need to maintain availability |
my students had to |
to maintain availability in |
maintain availability in the |
availability in the face |
and does not affect |
students had to control |
in the face of |
does not affect directly |
the face of challenging |
had to control their |
not affect directly the |
face of challenging operating |
affect directly the revenue |
to control their murderous |
of challenging operating conditions |
perspectives on optimistically replicated |
control their murderous intentions |
on optimistically replicated peer |
directly the revenue of |
their murderous intentions towards |
as the number of |
the revenue of other |
the number of topics |
murderous intentions towards the |
revenue of other pools |
number of topics increases |
intentions towards the iis |
software practice and experience |
building services with these |
towards the iis or |
of topics increases to |
services with these properties |
with these properties is |
these properties is difficult |
the iis or mts |
iis or mts developers |
existing web services platforms |
web services platforms offer |
or mts developers or |
services platforms offer load |
the attacked pool shares |
mts developers or were |
attacked pool shares its |
developers or were they |
balancing and restart mechanisms |
pool shares its revenue |
and restart mechanisms for |
or were they kept |
shares its revenue with |
were they kept their |
restart mechanisms for transactional |
its revenue with the |
they kept their good |
mechanisms for transactional services |
revenue with the attacker |
for transactional services implemented |
kept their good spirits |
transactional services implemented using |
services implemented using a |
implemented using a three |
their good spirits by |
therefore each miner earns |
good spirits by contemplating |
each miner earns less |
spirits by contemplating the |
but not for services |
by contemplating the horrible |
not for services implemented |
contemplating the horrible tortures |
for services implemented using |
the horrible tortures one |
as the same revenue |
services implemented using other |
implemented using other technologies |
horrible tortures one could |
the same revenue is |
latency soars when we |
tortures one could perform |
same revenue is distributed |
developers of nontransactional web |
soars when we repeat |
one could perform on |
revenue is distributed among |
of nontransactional web services |
when we repeat this |
could perform on the |
is distributed among more |
nontransactional web services must |
we repeat this with |
perform on the person |
distributed among more miners |
web services must implement |
repeat this with the |
on the person that |
services must implement their |
this with the industrystandard |
the person that had |
must implement their own |
with the industrystandard scalable |
recall that the proof |
person that had designed |
implement their own mechanisms |
the industrystandard scalable reliable |
that the proof of |
that had designed the |
their own mechanisms for |
own mechanisms for replicating |
mechanisms for replicating data |
had designed the com |
industrystandard scalable reliable multicast |
the proof of work |
designed the com security |
tracking membership and live |
proof of work is |
the com security architecture |
membership and live this |
of work is only |
and live this work |
work is only valid |
live this work was |
this work was supported |
work was supported by |
was supported by darpa |
is only valid for |
informed prefetching and caching |
windows research there are |
ipto under the srs |
research there are some |
only valid for a |
widely used for event |
under the srs program |
in proceedings of the |
there are some properties |
valid for a specific |
used for event notification |
the srs program and |
proceedings of the fifteenth |
are some properties of |
for a specific block |
for event notification in |
srs program and by |
of the fifteenth acm |
some properties of windows |
event notification in their |
program and by the |
the fifteenth acm symposium |
properties of windows nt |
notification in their datacenters |
and by the rome |
as it is the |
fifteenth acm symposium on |
of windows nt that |
by the rome air |
as can be seen |
acm symposium on operating |
windows nt that make |
it is the nonce |
the rome air force |
is the nonce with |
symposium on operating systems |
nt that make it |
rome air force research |
air force research laboratory |
the nonce with which |
on operating systems principles |
that make it particularly |
can be seen in |
nonce with which the |
make it particularly suitable |
be seen in the |
with which the block |
it particularly suitable for |
seen in the graph |
under the prometheus program |
which the block s |
particularly suitable for research |
the block s hash |
suitable for research purposes |
srm s recovery latency |
block s hash is |
additional support was provided |
support was provided by |
was provided by the |
provided by the nsf |
s hash is smaller |
s recovery latency rises |
hash is smaller than |
the operating system kernel |
recovery latency rises linearly |
is smaller than its |
operating system kernel for |
latency rises linearly in |
smaller than its target |
robbert van renesse ness |
system kernel for example |
rises linearly in the |
kernel for example is |
linearly in the figure |
redirecting requests during failures |
for example is designed |
the attacking miner cannot |
requests during failures to |
example is designed with |
attacking miner cannot use |
during failures to minimize |
failures to minimize client |
to minimize client disruption |
is designed with extensibility |
miner cannot use it |
scalability of commercial esbs |
designed with extensibility in |
and detecting and repairing |
detecting and repairing inconsistencies |
with extensibility in mind |
of commercial esbs figure |
our premise in this |
premise in this paper |
in this paper is |
this paper is that |
paper is that for |
is that for many |
that for many services |
although the term block |
to allow developers of |
the term block withholding |
allow developers of hardware |
the transactional model is |
scalability of commercial esbs |
developers of hardware based |
term block withholding has |
transactional model is a |
of commercial esbs number |
of hardware based services |
block withholding has become |
model is a poor |
commercial esbs number of |
withholding has become canonical |
is a poor fit |
esbs number of topics |
a poor fit and |
new protocols and file |
poor fit and hence |
protocols and file systems |
fit and hence that |
and file systems to |
and hence that tools |
file systems to add |
hence that tools aimed |
that tools aimed at |
tools aimed at non |
systems to add their |
note that the block |
design and implementation of |
transactional web services systems |
web services systems will |
services systems will be |
systems will be needed |
that the block is |
to add their functionality |
and implementation of the |
the block is discarded |
add their functionality to |
we recognize that this |
implementation of the sun |
block is discarded and |
their functionality to the |
recognize that this is |
that this is debatable |
is discarded and never |
functionality to the system |
of the sun network |
the sun network file |
sun network file system |
vendors have generally argued |
discarded and never introduced |
to the system without |
have generally argued that |
and never introduced into |
our experiments confirm that |
in proceedings of usenix |
the system without much |
generally argued that only |
never introduced into the |
proceedings of usenix summer |
of usenix summer conference |
hosted enterprise service bus |
introduced into the system |
system without much effort |
argued that only transactional |
enterprise service bus architectures |
into the system as |
that only transactional systems |
service bus architectures can |
the system as the |
only transactional systems offer |
bus architectures can achieve |
system as the name |
all kernel code is |
transactional systems offer the |
kernel code is developed |
as the name block |
architectures can achieve high |
systems offer the hooks |
code is developed following |
the name block withholding |
can achieve high levels |
offer the hooks needed |
is developed following a |
name block withholding implies |
achieve high levels of |
the hooks needed to |
developed following a strict |
high levels of publish |
hooks needed to support |
needed to support automated |
to support automated scalability |
following a strict object |
the evolution of coda |
a strict object oriented |
subscribe performance for small |
strict object oriented paradigm |
performance for small numbers |
miners miners miners pool |
acm transactions on computer |
repair and restart mechanisms |
object oriented paradigm and |
for small numbers of |
transactions on computer systems |
key to this argument |
small numbers of subscribers |
oriented paradigm and its |
to this argument is |
paradigm and its functionality |
this argument is the |
and its functionality can |
argument is the ease |
but performance degrades very |
its functionality can only |
is the ease with |
classical block withholding attack |
functionality can only be |
performance degrades very sharply |
the ease with which |
can only be accessed |
degrades very sharply as |
ease with which interrupted |
only be accessed through |
a group of miners |
very sharply as the |
with which interrupted transactions |
be accessed through interfaces |
group of miners attack |
sharply as the number |
which interrupted transactions can |
of miners attack pool |
as the number of |
interrupted transactions can be |
none of its implementation |
the number of subscribers |
transactions can be rolled |
can be rolled back |
with a block withholding |
of its implementation is |
number of subscribers or |
a block withholding attack |
its implementation is visible |
of subscribers or topics |
subscribers or topics grows |
denoted by a dashed |
and the relative simplicity |
by a dashed red |
the jgroups and srm |
the relative simplicity of |
one of the designs |
a dashed red arrow |
jgroups and srm platforms |
relative simplicity of cleaning |
of the designs abstractions |
simplicity of cleaning up |
of cleaning up a |
the designs abstractions of |
cleaning up a database |
which don t leverage |
up a database after |
a database after a |
database after a crash |
designs abstractions of the |
don t leverage peer |
abstractions of the windows |
this attack reduces the |
yet the transactional programming |
attack reduces the attacker |
of the windows nt |
the transactional programming model |
the windows nt kernel |
reduces the attacker s |
determinism and asynchrony of |
the attacker s revenue |
transactional programming model also |
windows nt kernel i |
and asynchrony of set |
attacker s revenue compared |
programming model also brings |
nt kernel i find |
asynchrony of set iterators |
scale poorly in the |
s revenue compared to |
model also brings constraints |
also brings constraints and |
of set iterators to |
poorly in the number |
revenue compared to solo |
kernel i find it |
brings constraints and overheads |
set iterators to reduce |
in the number of |
compared to solo mining |
i find it particularly |
iterators to reduce aggregrate |
to reduce aggregrate file |
were this not the |
this not the case |
find it particularly fascinating |
the number of subscribers |
reduce aggregrate file i |
the transactional model would |
it particularly fascinating to |
number of subscribers or |
to solo mining or |
transactional model would long |
particularly fascinating to work |
in proceedings of the |
solo mining or honest |
model would long ago |
of subscribers or topics |
fascinating to work with |
proceedings of the sixteenth |
mining or honest pool |
would long ago have |
long ago have become |
of the sixteenth acm |
or honest pool participation |
to work with is |
ago have become universal |
the sixteenth acm symposium |
work with is the |
sixteenth acm symposium on |
with is the device |
acm symposium on operating |
some of these constraints |
is the device object |
it suffers from the |
symposium on operating system |
of these constraints relate |
scale well in these |
suffers from the reduced |
on operating system principles |
a device object in |
well in these dimensions |
from the reduced revenue |
these constraints relate to |
device object in an |
the reduced revenue like |
constraints relate to the |
object in an instance |
reduced revenue like the |
ricochet achieved the best |
relate to the challenges |
in an instance created |
revenue like the other |
achieved the best recovery |
to the challenges of |
an instance created by |
like the other pool |
the best recovery latency |
the challenges of maintaining |
instance created by driver |
the other pool participants |
best recovery latency when |
challenges of maintaining a |
created by driver objects |
recovery latency when message |
of maintaining a clean |
latency when message loss |
and its revenue is |
maintaining a clean separation |
its revenue is less |
which encapsulates a unit |
a clean separation of |
clean separation of code |
separation of code and |
of code and data |
when message loss is |
revenue is less than |
encapsulates a unit of |
message loss is an |
is less than its |
not all applications can |
a unit of kernel |
loss is an issue |
less than its share |
all applications can be |
unit of kernel based |
than its share of |
applications can be structured |
can be structured in |
be structured in this |
structured in this manner |
but at relatively high |
its share of the |
of kernel based software |
at relatively high overhead |
transactional rollback and restart |
rollback and restart can |
and restart can be |
restart can be costly |
share of the total |
of the total mining |
whether this is a |
file system usage in |
system usage in windows |
and restarting a database |
not shown on these |
this is a device |
the total mining power |
usage in windows nt |
restarting a database after |
shown on these graphs |
is a device driver |
total mining power in |
a database after a |
mining power in the |
database after a crash |
power in the system |
after a crash incurs |
a network protocol or |
a crash incurs delays |
network protocol or a |
in proceedings of the |
crash incurs delays while |
qsm at small loss |
protocol or a file |
proceedings of the seventeenth |
incurs delays while cleanup |
delays while cleanup code |
while cleanup code runs |
or a file system |
of the seventeenth acm |
this attack can therefore |
at small loss rates |
a file system filter |
the seventeenth acm symposium |
high availability is difficult |
attack can therefore only |
small loss rates achieves |
seventeenth acm symposium on |
availability is difficult to |
can therefore only be |
loss rates achieves similar |
acm symposium on operating |
is difficult to acheive |
these objects have the |
therefore only be used |
rates achieves similar average |
symposium on operating systems |
difficult to acheive in |
objects have the interesting |
only be used for |
achieves similar average latency |
on operating systems principles |
to acheive in the |
have the interesting property |
be used for sabotage |
similar average latency with |
acheive in the transactional |
in the transactional model |
average latency with considerably |
the interesting property that |
latency with considerably lower |
the fastest database replication |
fastest database replication schemes |
interesting property that they |
with considerably lower network |
at a cost to |
property that they can |
considerably lower network overheads |
a cost to the |
that they can be |
suffer from failure scenarios |
cost to the attacker |
they can be attached |
from failure scenarios that |
but if a packet |
can be attached to |
failure scenarios that can |
if a packet is |
a packet is lost |
scenarios that can require |
be attached to other |
that can require intervention |
attached to other device |
can require intervention by |
it may take several |
to other device objects |
even if a pool |
require intervention by a |
may take several seconds |
if a pool detects |
intervention by a human |
by a human operator |
and as such can |
take several seconds to |
a pool detects that |
as such can intercept |
several seconds to recover |
yet the higher fidelity |
pool detects that it |
such can intercept and |
seconds to recover it |
the higher fidelity schemes |
detects that it is |
can intercept and manipulate |
higher fidelity schemes require |
that it is under |
making it less appropriate |
fidelity schemes require expensive |
arla a free afs |
a free afs client |
it is under a |
it less appropriate for |
schemes require expensive multi |
in proceedings of the |
is under a block |
less appropriate for time |
intercept and manipulate all |
under a block withholding |
phase commit protocols and |
and manipulate all requests |
a block withholding attack |
commit protocols and hence |
manipulate all requests flowing |
protocols and hence may |
all requests flowing to |
and hence may not |
we don t see |
requests flowing to and |
hence may not give |
it might not be |
don t see any |
flowing to and from |
may not give adequate |
not give adequate performance |
t see any single |
to and from the |
might not be able |
see any single winner |
and from the original |
not be able to |
any single winner here |
from the original device |
clustered threetier database products |
be able to detect |
the original device object |
threetier database products are |
each of the solutions |
able to detect which |
database products are powerful |
products are powerful solutions |
to detect which of |
of the solutions tested |
this way it is |
detect which of its |
way it is relatively |
but they negotiate these |
the solutions tested has |
which of its registered |
it is relatively simple |
they negotiate these potential |
solutions tested has some |
of its registered miners |
is relatively simple to |
negotiate these potential pitfalls |
tested has some advantages |
its registered miners are |
relatively simple to add |
these potential pitfalls in |
simple to add for |
registered miners are the |
has some advantages that |
potential pitfalls in ways |
to add for example |
miners are the perpetrators |
some advantages that its |
pitfalls in ways that |
add for example a |
volume leases for consistency |
for example a file |
in ways that preclude |
example a file system |
a pool can estimate |
leases for consistency in |
ways that preclude important |
advantages that its competitors |
a file system object |
for consistency in large |
file system object that |
that preclude important classes |
that its competitors lack |
pool can estimate its |
system object that compresses |
ieee transactions on knowledge |
object that compresses or |
preclude important classes of |
important classes of applications |
transactions on knowledge and |
on knowledge and data |
knowledge and data engineering |
that compresses or encrypts |
can estimate its expected |
we re currently developing |
estimate its expected mining |
our motivation is to |
its expected mining power |
re currently developing new |
compresses or encrypts data |
motivation is to show |
expected mining power and |
currently developing new p |
or encrypts data before |
is to show that |
mining power and its |
encrypts data before the |
to show that a |
power and its actual |
data before the data |
show that a simple |
and its actual mining |
before the data reaches |
that a simple and |
its actual mining power |
the data reaches the |
a simple and remarkably |
actual mining power by |
data reaches the under |
simple and remarkably inexpensive |
mining power by the |
reaches the under laying |
and remarkably inexpensive infrastructure |
power by the rates |
the under laying file |
remarkably inexpensive infrastructure can |
by the rates of |
it builds an overlay |
inexpensive infrastructure can support |
under laying file system |
the rates of partial |
builds an overlay multicast |
infrastructure can support clustered |
rates of partial proofs |
an overlay multicast tree |
can support clustered execution |
to redirect disk requests |
overlay multicast tree within |
of partial proofs of |
support clustered execution of |
redirect disk requests to |
multicast tree within which |
partial proofs of work |
clustered execution of a |
disk requests to a |
tree within which events |
proofs of work and |
execution of a significant |
requests to a replication |
within which events travel |
of work and full |
of a significant class |
a significant class of |
significant class of non |
to a replication volume |
work and full proofs |
and is capable of |
and full proofs of |
is capable of selforganizing |
full proofs of work |
or to trace device |
capable of selforganizing in |
the work reported here |
to trace device object |
of selforganizing in the |
work reported here focuses |
trace device object interaction |
selforganizing in the presence |
reported here focuses on |
device object interaction during |
in the presence of |
here focuses on services |
object interaction during development |
supplied by its miners |
the presence of firewalls |
focuses on services that |
interaction during development phases |
on services that don |
services that don t |
that don t fit |
don t fit the |
a difference above a |
t fit the transactional |
fit the transactional paradigm |
difference above a set |
the strict object oriented |
above a set confidence |
typically for reasons of |
for reasons of performance |
a set confidence interval |
strict object oriented approach |
set confidence interval indicates |
object oriented approach is |
ones that operate directly |
a separate project is |
oriented approach is very |
that operate directly on |
operate directly on in |
separate project is creating |
approach is very well |
confidence interval indicates an |
memory data structures or |
data structures or simple |
structures or simple non |
interval indicates an attack |
is very well done |
project is creating a |
very well done from |
is creating a protocol |
well done from a |
to detect whether a |
to simplify our task |
creating a protocol suite |
done from a design |
detect whether a single |
we assume that these |
from a design point |
a protocol suite that |
whether a single miner |
assume that these services |
a design point of |
protocol suite that we |
a single miner is |
that these services are |
design point of view |
suite that we call |
single miner is attacking |
these services are capable |
that we call the |
miner is attacking it |
services are capable of |
we call the properties |
are capable of handling |
capable of handling outof |
call the properties framework |
style hacker s heart |
the pool must use |
hacker s heart starts |
pool must use a |
s heart starts bleeding |
and that processes implementing |
heart starts bleeding when |
must use a similar |
that processes implementing them |
starts bleeding when he |
use a similar technique |
processes implementing them experience |
bleeding when he or |
implementing them experience only |
them experience only crash |
experience only crash failures |
when he or she |
he or she realizes |
comparing the estimated mining |
as will be shown |
will be shown below |
the goal is to |
or she realizes that |
the estimated mining power |
our assumptions hold for |
she realizes that he |
goal is to offer |
assumptions hold for a |
estimated mining power of |
realizes that he can |
is to offer strong |
hold for a very |
mining power of the |
that he can no |
to offer strong forms |
for a very large |
power of the attacker |
he can no longer |
offer strong forms of |
a very large group |
very large group of |
large group of applications |
strong forms of reliability |
of the attacker based |
can no longer do |
the ssa was built |
ssa was built using |
was built using epidemic |
no longer do a |
the attacker based on |
forms of reliability that |
longer do a quick |
attacker based on its |
of reliability that can |
communication protocols in conjunction |
based on its partial |
do a quick fix |
reliability that can be |
protocols in conjunction with |
on its partial proof |
that can be customized |
in conjunction with a |
its partial proof of |
can be customized for |
conjunction with a novel |
partial proof of work |
be customized for special |
inspect a few data |
with a novel variant |
a few data structures |
customized for special needs |
proof of work with |
a novel variant of |
few data structures and |
of work with the |
novel variant of the |
data structures and secretly |
work with the fact |
variant of the chain |
structures and secretly swivel |
with the fact it |
of the chain replication |
speed and scalability are |
and secretly swivel some |
the fact it never |
the chain replication scheme |
and scalability are only |
secretly swivel some pointers |
fact it never supplies |
chain replication scheme which |
scalability are only elements |
swivel some pointers to |
it never supplies a |
replication scheme which has |
are only elements of |
some pointers to make |
never supplies a full |
scheme which has evolved |
only elements of a |
pointers to make things |
supplies a full proof |
which has evolved from |
elements of a broader |
to make things work |
a full proof of |
has evolved from the |
of a broader story |
make things work better |
full proof of work |
evolved from the mechanism |
things work better or |
from the mechanism first |
developers will need different |
work better or make |
the mechanism first proposed |
mechanism first proposed in |
better or make more |
will need different solutions |
if the attacker has |
or make more informed |
need different solutions for |
the attacker has a |
make more informed decisions |
different solutions for different |
attacker has a small |
solutions for different purposes |
has a small mining |
a small mining power |
the internal kernel interfaces |
internal kernel interfaces are |
kernel interfaces are elaborate |
by offering a flexible |
gossip based infrastructures are |
offering a flexible yet |
it will send frequent |
based infrastructures are beneficial |
a flexible yet structured |
will send frequent partial |
infrastructures are beneficial because |
but it appears there |
flexible yet structured component |
send frequent partial proofs |
are beneficial because they |
beneficial because they are |
yet structured component mashup |
frequent partial proofs of |
it appears there are |
structured component mashup environment |
partial proofs of work |
simple to implement rapidly |
to implement rapidly self |
appears there are always |
there are always some |
live objects makes it |
are always some things |
but the pool will |
objects makes it possible |
stabilizing after disruptions analytically |
always some things one |
the pool will only |
makes it possible to |
after disruptions analytically appealing |
some things one cannot |
pool will only expect |
it possible to create |
disruptions analytically appealing this |
things one cannot do |
will only expect to |
possible to create applications |
analytically appealing this paper |
one cannot do as |
only expect to see |
to create applications that |
appealing this paper reports |
cannot do as efficient |
expect to see a |
create applications that mix |
this paper reports on |
do as efficient as |
to see a full |
applications that mix hosted |
paper reports on the |
as efficient as possible |
see a full proof |
that mix hosted with |
reports on the architecture |
a full proof of |
mix hosted with p |
on the architecture and |
full proof of work |
the architecture and performance |
proof of work at |
architecture and performance of |
and performance of the |
in four years of |
performance of the platform |
four years of nt |
of work at very |
and that can adapt |
years of nt kernel |
work at very low |
and explores the limitations |
that can adapt their |
of nt kernel hacking |
at very low frequency |
explores the limitations of |
can adapt their behavior |
nt kernel hacking only |
the limitations of its |
kernel hacking only on |
limitations of its underlying |
hacking only on one |
of its underlying techniques |
only on one occasion |
on one occasion we |
one occasion we needed |
it cannot obtain statistically |
to achieve desired properties |
occasion we needed to |
the experiments are designed |
achieve desired properties in |
cannot obtain statistically significant |
we needed to break |
experiments are designed to |
desired properties in a |
obtain statistically significant results |
needed to break through |
are designed to help |
properties in a way |
statistically significant results that |
to break through the |
designed to help us |
in a way matched |
significant results that would |
break through the standard |
to help us fully |
a way matched to |
results that would indicate |
through the standard kernel |
help us fully understand |
way matched to the |
that would indicate an |
the standard kernel interface |
us fully understand the |
matched to the environment |
would indicate an attack |
fully understand the fundamental |
understand the fundamental properties |
the fundamental properties of |
we wanted to add |
fundamental properties of a |
wanted to add a |
properties of a single |
to add a fast |
an attacker can use |
of a single partitioned |
add a fast trap |
scalability of qsm and |
a single partitioned replicated |
attacker can use multiple |
a fast trap into |
of qsm and jgroups |
single partitioned replicated service |
can use multiple small |
fast trap into the |
partitioned replicated service and |
use multiple small block |
trap into the kernel |
replicated service and thus |
throughput for various group |
multiple small block withholding |
into the kernel for |
service and thus gain |
for various group sizes |
small block withholding miners |
the kernel for fast |
and thus gain a |
block withholding miners and |
kernel for fast user |
thus gain a firm |
withholding miners and replace |
gain a firm grasp |
miners and replace them |
a firm grasp on |
and replace them frequently |
firm grasp on the |
grasp on the behavior |
on the behavior of |
prior work the idea |
the behavior of the |
work the idea of |
and the pages which |
behavior of the ssa |
the pages which hold |
a small miner is |
the idea of integrating |
of the ssa s |
the ssa s building |
ssa s building blocks |
pages which hold the |
idea of integrating web |
which hold the trap |
of integrating web services |
hold the trap dispatch |
we defer for future |
integrating web services with |
the trap dispatch tables |
defer for future work |
a miners whose expected |
web services with peer |
trap dispatch tables were |
for future work the |
miners whose expected full |
dispatch tables were protected |
future work the full |
whose expected full proof |
tables were protected after |
work the full scale |
expected full proof of |
were protected after the |
the full scale evaluation |
peer platforms is certainly |
full proof of work |
protected after the system |
full scale evaluation of |
platforms is certainly not |
proof of work frequency |
after the system boot |
scale evaluation of multiple |
is certainly not new |
of work frequency is |
evaluation of multiple services |
work frequency is yearly |
of multiple services deployed |
multiple services deployed and |
another example of what |
services deployed and running |
deployed and running at |
example of what makes |
and running at the |
such a miner will |
running at the same |
at the same time |
a miner will see |
of what makes windows |
miner will see a |
what makes windows nt |
the ssa currently runs |
makes windows nt particular |
will see a non |
windows nt particular suitable |
ssa currently runs on |
currently runs on a |
nt particular suitable for |
runs on a tightly |
particular suitable for research |
on a tightly coupled |
suitable for research is |
negligible average daily revenue |
a tightly coupled cluster |
for research is the |
tightly coupled cluster of |
research is the fundamental |
coupled cluster of blade |
is the fundamental manner |
cluster of blade servers |
the fundamental manner in |
fundamental manner in which |
manner in which advanced |
in which advanced distributed |
we show that developers |
which advanced distributed services |
show that developers can |
advanced distributed services are |
that developers can tune |
distributed services are integrated |
developers can tune parameters |
services are integrated into |
can tune parameters to |
are integrated into windows |
tune parameters to trade |
integrated into windows nt |
parameters to trade overhead |
to trade overhead for |
trade overhead for speed |
overhead for speed of |
for speed of repair |
speed of repair and |
of repair and we |
repair and we believe |
and we believe that |
it allows us to |
we believe that our |
believe that our results |
allows us to rely |
that our results validate |
our results validate the |
us to rely on |
results validate the approach |
to rely on ubiquitous |
rely on ubiquitous support |
on ubiquitous support services |
ubiquitous support services and |
support services and concentrate |
application model our work |
services and concentrate on |
model our work focuses |
and concentrate on advancing |
our work focuses on |
concentrate on advancing the |
work focuses on datacenters |
on advancing the state |
focuses on datacenters supporting |
advancing the state of |
on datacenters supporting one |
the state of the |
datacenters supporting one or |
state of the art |
supporting one or more |
of the art where |
the one issue that |
one or more services |
one issue that unites |
the art where it |
if the attacker replaces |
issue that unites almost |
or more services deployed |
more services deployed within |
the attacker replaces such |
that unites almost all |
art where it is |
services deployed within a |
attacker replaces such a |
unites almost all approaches |
where it is really |
deployed within a cluster |
replaces such a small |
almost all approaches to |
it is really needed |
within a cluster of |
a cluster of compute |
all approaches to distributed |
such a small miner |
cluster of compute nodes |
approaches to distributed computing |
a small miner every |
windows nt security provides |
to distributed computing is |
nt security provides a |
small miner every month |
distributed computing is the |
security provides a complete |
computing is the need |
provides a complete set |
is the need to |
a complete set of |
tailer might implement a |
the need to know |
he will collect about |
complete set of services |
might implement a front |
need to know whether |
will collect about b |
set of services integrated |
end service that builds |
to know whether certain |
of services integrated into |
service that builds web |
know whether certain components |
at the end of |
that builds web pages |
services integrated into all |
whether certain components in |
the end of each |
integrated into all sections |
certain components in the |
parallelizing the task by |
the task by dispatching |
task by dispatching sub |
components in the system |
end of each month |
into all sections of |
in the system have |
tasks to services to |
all sections of the |
the system have failed |
to services to rank |
sections of the operating |
system have failed or |
services to rank product |
to rank product popularity |
of the operating system |
have failed or are |
failed or are otherwise |
or are otherwise unavailable |
the pool must decide |
pool must decide within |
must decide within this |
researchers who are developing |
decide within this month |
when designing and building |
who are developing an |
within this month whether |
designing and building systems |
are developing an advanced |
this month whether the |
and building systems that |
developing an advanced multi |
month whether the miner |
building systems that need |
whether the miner is |
systems that need to |
the miner is an |
end service would probably |
that need to function |
miner is an attacker |
node replicated transaction server |
service would probably just |
need to function at |
to function at a |
would probably just be |
probably just be cloned |
function at a global |
at a global scale |
and revoke its earnings |
replicated transaction server can |
with identical replicas that |
failure management needs to |
identical replicas that build |
replicas that build pages |
management needs to be |
transaction server can use |
needs to be considered |
server can use off |
to be considered a |
or just an unlucky |
the existing work falls |
be considered a fundamental |
just an unlucky honest |
end services might be |
existing work falls roughly |
considered a fundamental building |
a fundamental building block |
services might be partitioned |
work falls roughly into |
an unlucky honest miner |
this paper describes the |
falls roughly into two |
might be partitioned into |
paper describes the development |
roughly into two categories |
be partitioned into subservices |
describes the development of |
since an honest miner |
partitioned into subservices for |
an honest miner of |
the development of a |
honest miner of this |
the first line of |
development of a system |
into subservices for scalability |
miner of this power |
first line of research |
independent failure management service |
of this power is |
subservices for scalability using |
line of research is |
this power is unlikely |
for scalability using some |
scalability using some key |
which allows systems and |
of research is focused |
power is unlikely to |
and encryption mechanisms into |
allows systems and applications |
research is focused on |
is unlikely to find |
encryption mechanisms into their |
systems and applications to |
is focused on the |
unlikely to find a |
mechanisms into their system |
and applications to incorporate |
focused on the use |
to find a full |
into their system without |
applications to incorporate accurate |
and subservices cloned for |
on the use of |
find a full proof |
their system without much |
to incorporate accurate detection |
subservices cloned for faulttolerance |
the use of peer |
a full proof of |
system without much pain |
incorporate accurate detection of |
cloned for faulttolerance and |
full proof of work |
accurate detection of failed |
for faulttolerance and load |
proof of work within |
detection of failed processes |
the use of the |
of work within a |
use of the com |
work within a month |
of the com object |
this is a common |
without the need for |
is a common model |
the com object model |
the need for making |
as a basis for |
com object model in |
jim gray and others |
a basis for scalable |
need for making compromises |
object model in all |
gray and others have |
model in all the |
for making compromises in |
in all the windows |
and others have suggested |
all the windows nt |
making compromises in their |
the windows nt services |
basis for scalable web |
others have suggested that |
according to the exponential |
compromises in their particular |
in their particular design |
for scalable web service |
have suggested that such |
to the exponential distribution |
windows nt services allows |
scalable web service discovery |
suggested that such a |
nt services allows research |
with the advent of |
the advent of ubiquitous |
services allows research projects |
that such a system |
a pool that rejects |
allows research projects to |
the second line of |
such a system be |
pool that rejects miners |
it is becoming clear |
that rejects miners based |
a system be termed |
research projects to import |
second line of research |
is becoming clear that |
rejects miners based on |
system be termed a |
projects to import these |
line of research concentrates |
becoming clear that the |
of research concentrates on |
be termed a farm |
to import these services |
miners based on this |
clear that the systems |
research concentrates on the |
termed a farm consisting |
import these services in |
based on this criterion |
that the systems that |
concentrates on the use |
a farm consisting of |
these services in a |
on this criterion would |
the systems that are |
on the use of |
farm consisting of raps |
services in a very |
this criterion would reject |
systems that are used |
the use of replication |
in a very simple |
criterion would reject the |
that are used today |
reliable array of partitioned |
use of replication protocols |
a very simple manner |
would reject the majority |
are used today in |
used today in local |
of replication protocols at |
reject the majority of |
array of partitioned services |
replication protocols at the |
the majority of its |
can not simply be |
protocols at the web |
the existence of com |
majority of its honest |
not simply be employed |
reliable array of cloned |
existence of com makes |
of its honest miners |
at the web service |
simply be employed in |
array of cloned server |
of cloned server processes |
the web service backend |
be employed in their |
of com makes it |
web service backend to |
employed in their existing |
the alternative of rejecting |
com makes it trivial |
service backend to achieve |
in their existing form |
alternative of rejecting small |
makes it trivial for |
backend to achieve fault |
their existing form or |
of rejecting small miners |
it trivial for research |
existing form or trivially |
trivial for research projects |
form or trivially converted |
or trivially converted for |
trivially converted for wide |
rejecting small miners in |
for research projects to |
small miners in general |
research projects to export |
miners in general or |
up to the present |
projects to export their |
in general or distributing |
to export their interfaces |
general or distributing revenue |
export their interfaces in |
whatever form such systems |
this structure has arisen |
p platforms such as |
or distributing revenue on |
their interfaces in a |
form such systems may |
structure has arisen mostly |
platforms such as jxta |
distributing revenue on a |
interfaces in a language |
such systems may take |
has arisen mostly in |
such as jxta are |
revenue on a yearly |
in a language independent |
systems may take in |
arisen mostly in very |
as jxta are treated |
on a yearly basis |
a language independent manner |
may take in the |
take in the future |
jxta are treated not |
a yearly basis contradicts |
mostly in very large |
whether they are replicated |
yearly basis contradicts the |
are treated not as |
in very large datacenters |
they are replicated databases |
are replicated databases of |
replicated databases of hyper |
treated not as means |
very large datacenters and |
basis contradicts the goal |
the ensemble project for |
not as means of |
ensemble project for example |
contradicts the goal of |
large datacenters and is |
view or virtual synchronous |
project for example has |
the goal of pooled |
as means of collaboration |
datacenters and is supported |
or virtual synchronous groups |
for example has developed |
goal of pooled mining |
means of collaboration or |
and is supported primarily |
virtual synchronous groups or |
example has developed a |
of collaboration or media |
is supported primarily in |
synchronous groups or agents |
has developed a protocol |
collaboration or media carrying |
supported primarily in the |
groups or agents employing |
developed a protocol environment |
m odel and s |
primarily in the context |
odel and s tandard |
or media carrying live |
a protocol environment for |
or agents employing lazy |
in the context of |
the context of three |
media carrying live content |
protocol environment for distributed |
agents employing lazy consistency |
employing lazy consistency schemes |
environment for distributed operations |
and s tandard o |
for distributed operations in |
but rather as a |
s tandard o peration |
one of the key |
distributed operations in the |
we believe that similar |
tandard o peration we |
of the key problems |
o peration we specify |
operations in the ml |
believe that similar architectures |
rather as a supporting |
the key problems that |
peration we specify the |
in the ml programming |
that similar architectures will |
as a supporting infrastructure |
key problems that needs |
problems that needs to |
that needs to be |
needs to be addressed |
a supporting infrastructure at |
we specify the basic |
the ml programming language |
similar architectures will be |
supporting infrastructure at the |
specify the basic model |
is that of the |
architectures will be needed |
infrastructure at the data |
the basic model in |
that of the detection |
will be needed more |
at the data center |
and by using a |
basic model in which |
of the detection and |
be needed more widely |
the data center backend |
by using a com |
model in which participants |
the detection and handling |
because the need to |
in which participants operate |
using a com interface |
detection and handling of |
the need to tolerate |
which participants operate in |
our work is focused |
and handling of faulty |
handling of faulty components |
a com interface are |
participants operate in section |
work is focused on |
need to tolerate heavy |
com interface are the |
operate in section iii |
is focused on blending |
to tolerate heavy loads |
interface are the services |
building distributed systems and |
focused on blending the |
tolerate heavy loads is |
are the services offered |
distributed systems and applications |
on blending the content |
heavy loads is increasingly |
the services offered by |
systems and applications today |
blending the content available |
loads is increasingly ubiquitous |
services offered by ensemble |
proceed to describe how |
and applications today is |
the content available through |
and economic considerations favor |
to describe how honest |
applications today is done |
offered by ensemble available |
content available through p |
economic considerations favor clustered |
describe how honest miners |
today is done using |
by ensemble available to |
considerations favor clustered solutions |
how honest miners operate |
is done using a |
ensemble available to c |
p and web service |
honest miners operate in |
done using a variety |
and web service protocols |
miners operate in this |
using a variety of |
game servers require scalability |
operate in this environment |
a variety of systems |
neither technology is subordinate |
in this environment in |
servers require scalability for |
variety of systems ranging |
technology is subordinate with |
this environment in sections |
require scalability for situations |
of systems ranging from |
is subordinate with respect |
environment in sections iii |
java and vb programmers |
scalability for situations in |
systems ranging from the |
subordinate with respect to |
for situations in which |
ranging from the bare |
with respect to the |
this allowed the researchers |
from the bare bone |
situations in which there |
respect to the other |
allowed the researchers to |
the bare bone protocols |
in which there are |
the researchers to side |
bare bone protocols interfaces |
which there are many |
and how the classical |
bone protocols interfaces like |
how the classical block |
there are many users |
technologies that use peer |
protocols interfaces like bsd |
the classical block withholding |
step the time consuming |
military systems require scalability |
classical block withholding attack |
interfaces like bsd sockets |
like bsd sockets and |
bsd sockets and the |
sockets and the tdi |
the time consuming development |
systems require scalability to |
block withholding attack is |
peer protocols to support |
time consuming development of |
require scalability to support |
withholding attack is implemented |
to rpc based systems |
protocols to support live |
consuming development of native |
scalability to support new |
attack is implemented with |
rpc based systems such |
to support live and |
development of native language |
to support new generations |
is implemented with our |
based systems such as |
support live and interactive |
of native language interfaces |
support new generations of |
implemented with our model |
systems such as dce |
live and interactive content |
new generations of integrated |
with our model in |
such as dce and |
and interactive content have |
generations of integrated applications |
it helps of course |
our model in section |
as dce and to |
interactive content have existed |
helps of course to |
model in section iii |
dce and to more |
hospital automation is putting |
content have existed earlier |
of course to have |
and to more advanced |
automation is putting new |
course to have all |
to more advanced distributed |
an excellent example of |
to have all the |
is putting new demands |
more advanced distributed support |
excellent example of such |
have all the tools |
putting new demands on |
advanced distributed support systems |
example of such technology |
new demands on medical |
distributed support systems such |
support systems such as |
systems such as isis |
demands on medical information |
model the system is |
of such technology is |
on medical information subsystems |
the system is comprised |
such technology is the |
system is comprised of |
technology is the croquet |
in a wide range |
operating system versions and |
is comprised of the |
a wide range of |
system versions and their |
comprised of the bitcoin |
wide range of everyday |
versions and their source |
of the bitcoin network |
range of everyday settings |
and their source code |
the bitcoin network and |
their source code available |
bitcoin network and nodes |
the rollout of soas |
network and nodes with |
rollout of soas and |
and nodes with unique |
of soas and the |
microsoft is very generous |
in which the entire |
nodes with unique ids |
soas and the ease |
is very generous to |
which the entire state |
and the ease of |
very generous to academia |
the entire state of |
the ease of application |
and progresses in steps |
generous to academia and |
entire state of a |
ease of application integration |
to academia and makes |
state of a virtual |
of application integration they |
academia and makes all |
a node i generates |
application integration they support |
after years of experience |
and makes all their |
node i generates tasks |
integration they support will |
d world is stored |
years of experience with |
makes all their tools |
i generates tasks which |
they support will place |
world is stored in |
of experience with building |
all their tools from |
generates tasks which are |
support will place services |
is stored in a |
experience with building these |
their tools from operating |
tasks which are associated |
will place services under |
stored in a peer |
with building these systems |
tools from operating systems |
which are associated with |
place services under growing |
building these systems and |
these systems and applications |
are associated with its |
services under growing load |
from operating systems to |
it is clear that |
peer fashion and updated |
associated with its id |
operating systems to compilers |
is clear that failure |
fashion and updated using |
with its id i |
our goal is to |
clear that failure management |
and updated using a |
goal is to make |
that failure management is |
updated using a two |
including tons of documentation |
is to make it |
failure management is not |
a node can work |
tons of documentation as |
to make it easy |
management is not just |
node can work on |
of documentation as well |
make it easy to |
is not just a |
can work on a |
other work in this |
documentation as well as |
it easy to build |
not just a essential |
work on a task |
work in this direction |
as well as subscriptions |
easy to build raps |
just a essential tool |
on a task for |
in this direction includes |
well as subscriptions to |
to build raps and |
a essential tool for |
a task for the |
as subscriptions to the |
build raps and racs |
essential tool for group |
tool for group oriented |
for group oriented systems |
raps and racs from |
task for the duration |
subscriptions to the developer |
and racs from traditional |
for the duration of |
all which have built |
to the developer network |
the duration of a |
duration of a step |
but that it is |
that it is a |
available to the departments |
web service applications designed |
none of these systems |
to the departments free |
the result of this |
it is a fundamental |
result of this work |
of these systems supports |
the departments free of |
service applications designed for |
is a fundamental service |
of this work is |
these systems supports the |
departments free of charge |
applications designed for quick |
a fundamental service that |
this work is a |
systems supports the sorts |
designed for quick responsiveness |
fundamental service that should |
work is a set |
supports the sorts of |
source code availability turned |
service that should be |
code availability turned out |
the sorts of componentized |
is a set of |
that should be placed |
availability turned out to |
we also want to |
a set of partial |
should be placed among |
turned out to be |
also want to build |
layered architectures that we |
set of partial proofs |
be placed among such |
out to be not |
want to build the |
architectures that we have |
of partial proofs of |
placed among such established |
to be not crucial |
to build the simplest |
that we have advocated |
partial proofs of work |
among such established basic |
build the simplest platform |
we have advocated here |
proofs of work and |
such established basic services |
the simplest platform capable |
and was only once |
of work and a |
established basic services as |
basic services as naming |
the types of peer |
was only once used |
work and a set |
simplest platform capable of |
only once used to |
and a set of |
platform capable of accomplishing |
once used to make |
a set of full |
capable of accomplishing this |
used to make actual |
service brokerage and ipc |
peer protocols these systems |
set of full proofs |
of accomplishing this task |
to make actual changes |
this paper reports on |
of full proofs of |
protocols these systems can |
make actual changes to |
paper reports on an |
full proofs of work |
reports on an ongoing |
actual changes to the |
on an ongoing research |
these systems can leverage |
an ongoing research effort |
a set of racs |
changes to the operating |
ongoing research effort to |
to the operating systems |
and the types of |
research effort to abstract |
the number of proofs |
effort to abstract the |
number of proofs in |
to abstract the failure |
the types of a |
abstract the failure handling |
types of a traditional |
the failure handling strategies |
von eicken et al |
of proofs in each |
of a traditional hosted |
failure handling strategies from |
proofs in each set |
a traditional hosted content |
handling strategies from a |
in each set has |
traditional hosted content they |
strategies from a variety |
each set has a |
hosted content they can |
from a variety of |
set has a poisson |
content they can blend |
a variety of popular |
has a poisson distribution |
they can blend with |
variety of popular distributed |
can blend with their |
of popular distributed systems |
blend with their p |
popular distributed systems and |
distributed systems and to |
partial proofs with a |
the source is extremely |
systems and to develop |
proofs with a large |
source is extremely useful |
and to develop a |
with a large mean |
is extremely useful as |
to develop a basic |
a large mean and |
extremely useful as additional |
develop a basic failure |
large mean and full |
useful as additional documentation |
a basic failure management |
mean and full proofs |
our platform is designed |
basic failure management service |
and full proofs with |
platform is designed from |
failure management service that |
full proofs with a |
to examine unexpected behaviour |
is designed from ground |
management service that can |
proofs with a small |
examine unexpected behaviour or |
designed from ground up |
gossip traffic chain figure |
service that can be |
with a small mean |
unexpected behaviour or to |
from ground up with |
that can be used |
behaviour or to provide |
ground up with extensibility |
can be used by |
or to provide templates |
up with extensibility in |
nodes that work on |
be used by any |
to provide templates for |
with extensibility in mind |
that work on tasks |
used by any distributed |
provide templates for similar |
work on tasks are |
by any distributed system |
templates for similar projects |
on tasks are called |
every part of it |
any distributed system regardless |
elements of the model |
tasks are called a |
part of it can |
as one can perform |
of the model a |
one can perform complete |
distributed system regardless of |
of it can be |
are called a miners |
the model a service |
can perform complete source |
system regardless of the |
perform complete source code |
model a service is |
it can be replaced |
regardless of the purpose |
complete source code level |
miners have identical power |
a service is simply |
can be replaced and |
of the purpose of |
source code level debugging |
service is simply an |
be replaced and customized |
the purpose of that |
code level debugging of |
is simply an application |
and hence identical probabilities |
purpose of that system |
level debugging of all |
debugging of all parts |
hence identical probabilities to |
of that system or |
and different components within |
simply an application that |
of all parts of |
identical probabilities to generate |
that system or the |
different components within a |
an application that provides |
all parts of the |
probabilities to generate proofs |
system or the techniques |
or the techniques used |
application that provides interfaces |
parts of the operating |
to generate proofs of |
components within a single |
the strategies employed by |
of the operating system |
generate proofs of work |
that provides interfaces that |
within a single mashup |
strategies employed by this |
the operating system including |
provides interfaces that manipulate |
a single mashup application |
employed by this basic |
operating system including the |
interfaces that manipulate objects |
the bitcoin network pays |
single mashup application can |
by this basic service |
system including the kernel |
that manipulate objects of |
bitcoin network pays for |
mashup application can leverage |
this basic service are |
manipulate objects of unspecified |
network pays for full |
application can leverage different |
source codes helps us |
objects of unspecified nature |
pays for full proofs |
basic service are specifically |
can leverage different transport |
codes helps us to |
for full proofs of |
service are specifically targeted |
leverage different transport protocols |
a query operation reads |
helps us to develop |
full proofs of work |
are specifically targeted towards |
query operation reads some |
us to develop experimental |
specifically targeted towards applications |
prior work on typed |
operation reads some object |
to develop experimental services |
targeted towards applications that |
work on typed component |
to acquire this payoff |
reads some object and |
acquire this payoff an |
towards applications that need |
this payoff an entity |
develop experimental services faster |
some object and returns |
on typed component architectures |
applications that need to |
payoff an entity publishes |
experimental services faster and |
object and returns a |
typed component architectures includes |
that need to operate |
an entity publishes a |
services faster and in |
and returns a computed |
component architectures includes a |
need to operate on |
entity publishes a task |
faster and in tune |
returns a computed value |
architectures includes a tremendous |
to operate on a |
operate on a global |
on a global scale |
includes a tremendous variety |
an update operation modifies |
and in tune with |
publishes a task task |
a tremendous variety of |
update operation modifies one |
in tune with existing |
to build a successful |
tremendous variety of programming |
operation modifies one or |
a task task and |
tune with existing functionality |
build a successful service |
variety of programming languages |
modifies one or more |
task task and its |
a successful service the |
of programming languages and |
one or more objects |
students are free to |
successful service the following |
are free to work |
task and its corresponding |
programming languages and platforms |
service the following goals |
the following goals were |
following goals were set |
one unusual assumption made |
free to work with |
and its corresponding proof |
including early languages such |
to work with the |
unusual assumption made in |
work with the source |
design a failure management |
early languages such as |
its corresponding proof of |
assumption made in our |
with the source code |
a failure management system |
the source code and |
corresponding proof of work |
made in our work |
languages such as smalltalk |
failure management system that |
source code and are |
proof of work to |
in our work is |
such as smalltalk alongside |
management system that is |
code and are not |
of work to the |
our work is that |
as smalltalk alongside modern |
system that is independent |
and are not prohibited |
work to the network |
work is that many |
smalltalk alongside modern component |
that is independent of |
are not prohibited in |
is that many services |
is independent of the |
not prohibited in any |
based environments such as |
independent of the distributed |
the payoff goes to |
that many services can |
prohibited in any way |
environments such as java |
of the distributed systems |
payoff goes to the |
many services can process |
in any way from |
the distributed systems packages |
goes to the id |
services can process updates |
any way from applying |
distributed systems packages in |
to the id associated |
can process updates out |
way from applying the |
systems packages in use |
specialized component architectures such |
process updates out of |
from applying the knowledge |
the id associated with |
packages in use and |
component architectures such figure |
updates out of order |
applying the knowledge they |
id associated with task |
in use and provide |
the knowledge they gained |
use and provide failure |
knowledge they gained in |
and provide failure detection |
they gained in their |
provide failure detection of |
scalability qsm and jgroups |
gained in their later |
the bitcoin protocol normalizes |
we focus on services |
bitcoin protocol normalizes revenue |
throughput for various numbers |
protocol normalizes revenue such |
focus on services that |
normalizes revenue such that |
failure detection of processes |
revenue such that the |
in their later careers |
on services that can |
for various numbers of |
such that the average |
services that can respond |
various numbers of topics |
improve the accuracy of |
that can respond correctly |
that the average total |
interactions with the evil |
the average total revenue |
the accuracy of detection |
with the evil empire |
average total revenue distributed |
can respond correctly to |
accuracy of detection of |
the evil empire microsoft |
total revenue distributed in |
respond correctly to queries |
of detection of process |
evil empire microsoft realizes |
revenue distributed in each |
correctly to queries even |
detection of process and |
empire microsoft realizes the |
distributed in each step |
to queries even if |
of process and node |
in each step is |
microsoft realizes the potential |
queries even if some |
for srm and ricochet |
realizes the potential of |
process and node failure |
even if some updates |
each step is a |
srm and ricochet with |
and node failure through |
the potential of widespread |
if some updates are |
step is a constant |
and ricochet with varying |
node failure through systems |
some updates are temporarily |
potential of widespread adoption |
is a constant throughout |
ricochet with varying numbers |
failure through systems support |
updates are temporarily missing |
of widespread adoption of |
a constant throughout the |
with varying numbers of |
widespread adoption of windows |
converge into a state |
constant throughout the execution |
varying numbers of topics |
design support for failure |
into a state determined |
adoption of windows nt |
throughout the execution of |
of windows nt for |
a state determined entirely |
support for failure detectors |
the execution of the |
as mit s argus |
windows nt for research |
state determined entirely by |
nt for research purposes |
execution of the system |
for research purposes and |
for failure detectors to |
determined entirely by the |
mit s argus system |
failure detectors to work |
research purposes and there |
entirely by the set |
detectors to work in |
any node can transact |
by the set of |
purposes and there is |
to work in large |
flexible protocol composition stacks |
node can transact bitcoins |
the set of updates |
work in large scale |
in large scale systems |
protocol composition stacks such |
can transact bitcoins to |
and there is dedicated |
so that if two |
transact bitcoins to another |
while maintaining a high |
composition stacks such as |
there is dedicated academic |
that if two members |
is dedicated academic relations |
maintaining a high level |
dedicated academic relations team |
bitcoins to another node |
if two members of |
stacks such as bast |
a high level of |
high level of accuracy |
to another node by |
two members of some |
academic relations team whose |
another node by issuing |
members of some subservice |
provide support for the |
relations team whose single |
node by issuing a |
of some subservice receive |
support for the detection |
team whose single task |
by issuing a bitcoin |
some subservice receive the |
for the detection of |
whose single task it |
issuing a bitcoin transaction |
subservice receive the same |
the detection of partitions |
single task it is |
receive the same updates |
detection of partitions in |
oriented architectures such as |
task it is to |
the same updates they |
of partitions in networks |
architectures such as juni |
nodes that generate tasks |
it is to facilitate |
same updates they will |
that generate tasks but |
build a comprehensive software |
is to facilitate the |
updates they will be |
generate tasks but outsource |
a comprehensive software package |
to facilitate the technology |
they will be in |
tasks but outsource the |
comprehensive software package that |
facilitate the technology transfer |
will be in equivalent |
but outsource the work |
software package that can |
the technology transfer between |
be in equivalent states |
outsource the work are |
has been used in |
package that can be |
technology transfer between microsoft |
the work are called |
been used in the |
that can be easily |
transfer between microsoft and |
even if those updates |
work are called pools |
used in the context |
can be easily integrated |
between microsoft and academia |
if those updates were |
in the context of |
be easily integrated into |
microsoft and academia and |
those updates were delivered |
the context of integrating |
pools send tasks to |
easily integrated into various |
and academia and vice |
updates were delivered in |
context of integrating service |
send tasks to miners |
integrated into various distributed |
academia and vice versa |
were delivered in different |
tasks to miners over |
into various distributed systems |
delivered in different orders |
to miners over the |
various distributed systems packages |
miners over the network |
distributed systems packages and |
source licensing is very |
systems packages and applications |
licensing is very liberal |
discussion of component integration |
a reissued query or |
is very liberal compared |
of component integration systems |
very liberal compared to |
reissued query or update |
liberal compared to other |
the miners receive the |
component integration systems and |
the resulting system is |
query or update returns |
compared to other os |
miners receive the tasks |
integration systems and their |
resulting system is implemented |
or update returns an |
to other os vendors |
systems and their relation |
system is implemented and |
update returns an equivalent |
other os vendors and |
and their relation to |
is implemented and is |
returns an equivalent result |
and send the partial |
their relation to live |
implemented and is under |
os vendors and several |
send the partial and |
relation to live objects |
and is under test |
vendors and several institutions |
what this amounts to |
the partial and full |
is under test in |
under test in a |
test in a wide |
partial and full proofs |
and several institutions are |
this amounts to is |
and full proofs of |
is beyond the scope |
several institutions are involved |
amounts to is that |
full proofs of work |
beyond the scope of |
in a local setting |
institutions are involved in |
to is that the |
proofs of work to |
the scope of this |
a local setting of |
are involved in active |
is that the ssa |
of work to the |
scope of this paper |
local setting of a |
involved in active exchanges |
that the ssa should |
work to the pool |
setting of a mix |
more details can be |
the ssa should deliver |
of a mix of |
a mix of high |
details can be found |
ssa should deliver updates |
in active exchanges with |
can be found in |
apart from working on |
should deliver updates as |
speed and traditional networks |
active exchanges with product |
from working on tasks |
deliver updates as soon |
and traditional networks and |
exchanges with product and |
updates as soon as |
traditional networks and in |
networks and in the |
and in the internet |
with product and research |
as soon as it |
product and research groups |
soon as it can |
a first software release |
and research groups within |
as it can even |
first software release is |
research groups within microsoft |
it can even if |
software release is planned |
can even if they |
release is planned for |
even if they are |
is planned for the |
if they are not |
joint projects are in |
planned for the autumn |
much relevant prior work |
they are not in |
are not in order |
for the autumn of |
relevant prior work consists |
projects are in progress |
and receipt are instantaneous |
prior work consists of |
work consists of the |
consists of the scripting |
of the scripting languages |
the scripting languages mentioned |
joint papers are starting |
we assume that the |
papers are starting to |
assume that the number |
are starting to appear |
one way that an |
scripting languages mentioned in |
that the number of |
starting to appear and |
way that an application |
languages mentioned in the |
the number of miners |
external failure detector modules |
number of miners is |
that an application might |
mentioned in the discussion |
to appear and academics |
failure detector modules originate |
of miners is large |
an application might process |
in the discussion above |
appear and academics frequently |
detector modules originate in |
miners is large enough |
application might process out |
and academics frequently present |
modules originate in asynchronous |
is large enough such |
might process out of |
academics frequently present cutting |
originate in asynchronous distributed |
large enough such that |
process out of order |
frequently present cutting edge |
in asynchronous distributed systems |
enough such that mining |
out of order updates |
present cutting edge result |
such that mining power |
of order updates is |
where they were introduced |
they were introduced to |
were introduced to de |
order updates is simply |
cutting edge result to |
that mining power can |
updates is simply to |
edge result to microsoft |
couple the mechanism by |
mining power can be |
our belief is that |
is simply to delay |
result to microsoft developers |
the mechanism by which |
power can be split |
belief is that even |
simply to delay processing |
to microsoft developers and |
mechanism by which failures |
can be split arbitrarily |
is that even though |
to delay processing them |
microsoft developers and researchers |
by which failures are |
be split arbitrarily without |
that even though these |
delay processing them until |
which failures are detected |
split arbitrarily without resolution |
even though these languages |
processing them until it |
failures are detected from |
arbitrarily without resolution constraints |
though these languages are |
them until it can |
are detected from the |
these languages are intended |
until it can sort |
detected from the protocols |
languages are intended for |
it can sort them |
from the protocols used |
denote the number of |
are intended for fairly |
can sort them into |
the protocols used to |
the number of pools |
intended for fairly general |
sort them into order |
protocols used to tolerate |
used to tolerate those |
to tolerate those failures |
number of pools with |
for fairly general use |
of pools with p |
but we believe that |
we believe that for |
believe that for many |
that for many uses |
they have evolved to |
have evolved to focus |
the total number of |
evolved to focus on |
chandra and toueg successfully |
total number of mining |
it will be possible |
to focus on minibrowser |
and toueg successfully show |
operating systems there is |
number of mining power |
will be possible to |
focus on minibrowser situations |
toueg successfully show that |
systems there is a |
of mining power in |
be possible to act |
on minibrowser situations in |
successfully show that it |
there is a direct |
mining power in the |
possible to act on |
minibrowser situations in which |
show that it is |
is a direct impact |
power in the system |
to act on an |
situations in which the |
that it is possible |
a direct impact of |
in the system with |
act on an update |
in which the application |
it is possible to |
direct impact of academia |
the system with m |
on an update or |
which the application lives |
is possible to develop |
impact of academia on |
system with m and |
an update or query |
the application lives within |
possible to develop consensus |
of academia on microsoft |
with m and the |
update or query immediately |
application lives within a |
to develop consensus algorithms |
academia on microsoft products |
m and the miners |
or query immediately upon |
lives within a dedicated |
develop consensus algorithms using |
and the miners participating |
query immediately upon receiving |
through involvement in the |
consensus algorithms using failure |
involvement in the strategy |
within a dedicated browser |
immediately upon receiving it |
algorithms using failure detectors |
the miners participating in |
in the strategy phases |
a dedicated browser frame |
miners participating in pool |
the strategy phases of |
the ssa can support |
even if these failure |
participating in pool i |
interacts directly with the |
ssa can support raps |
if these failure detectors |
strategy phases of products |
directly with the user |
these failure detectors make |
phases of products as |
failure detectors make frequent |
of products as well |
detectors make frequent mistakes |
a raps of racs |
and cannot be mixed |
products as well as |
make frequent mistakes in |
cannot be mixed with |
as well as through |
frequent mistakes in their |
be mixed with content |
a service that can |
well as through academic |
mistakes in their observations |
mixed with content from |
we use a quasistatic |
as through academic knowledge |
service that can be |
with content from other |
use a quasistatic analysis |
through academic knowledge transfer |
that can be structured |
content from other sources |
a quasistatic analysis where |
academic knowledge transfer into |
can be structured as |
from other sources in |
quasistatic analysis where miner |
knowledge transfer into products |
be structured as a |
other sources in a |
analysis where miner participation |
transfer into products and |
structured as a raps |
sources in a layered |
where miner participation in |
into products and design |
as a raps must |
in a layered fashion |
the failure detector work |
miner participation in a |
products and design groups |
a raps must have |
failure detector work is |
raps must have a |
live objects can support |
participation in a pool |
detector work is extended |
must have a partitioning |
microsoft also provides research |
in a pool does |
work is extended to |
objects can support minibrowsers |
have a partitioning function |
also provides research funding |
a pool does not |
is extended to systems |
can support minibrowsers as |
a partitioning function that |
provides research funding for |
pool does not change |
extended to systems that |
support minibrowsers as objects |
partitioning function that can |
research funding for some |
does not change over |
to systems that also |
function that can be |
funding for some relevant |
not change over time |
systems that also take |
but we ve argued |
that can be used |
we ve argued that |
that also take network |
ve argued that by |
can be used to |
argued that by modeling |
also take network failure |
that by modeling hosted |
be used to map |
for some relevant groups |
take network failure into |
by modeling hosted content |
used to map each |
solo mining a solo |
some relevant groups and |
network failure into account |
modeling hosted content at |
to map each operation |
mining a solo miner |
relevant groups and fellowship |
hosted content at a |
map each operation to |
a solo miner is |
groups and fellowship and |
content at a lower |
each operation to the |
off in designing practical |
solo miner is a |
and fellowship and research |
at a lower level |
operation to the subservice |
in designing practical distributed |
miner is a node |
fellowship and research internships |
a lower level as |
to the subservice that |
designing practical distributed systems |
is a node that |
and research internships for |
lower level as components |
the subservice that should |
practical distributed systems based |
a node that generates |
research internships for students |
level as components that |
subservice that should execute |
distributed systems based on |
node that generates its |
as components that interact |
that should execute it |
systems based on the |
that generates its own |
components that interact via |
summary four years of |
based on the theory |
four years of research |
that interact via events |
existing systems typically implement |
generates its own tasks |
on the theory developed |
years of research on |
interact via events and |
systems typically implement partitioning |
the theory developed for |
of research on windows |
via events and focusing |
typically implement partitioning functions |
theory developed for asynchronous |
in every step it |
research on windows nt |
events and focusing on |
implement partitioning functions in |
developed for asynchronous systems |
every step it generates |
on windows nt have |
and focusing on the |
partitioning functions in one |
for asynchronous systems is |
step it generates a |
windows nt have taught |
focusing on the multi |
functions in one of |
asynchronous systems is where |
it generates a task |
nt have taught us |
in one of two |
systems is where and |
have taught us that |
one of two ways |
layered style of mashups |
works on it for |
taught us that we |
is where and how |
style of mashups as |
on it for the |
us that we made |
where and how to |
of mashups as opposed |
it for the duration |
that we made the |
and how to introduce |
the service exports its |
mashups as opposed to |
for the duration of |
we made the right |
how to introduce the |
service exports its partitioning |
as opposed to the |
the duration of the |
made the right choice |
to introduce the notion |
exports its partitioning function |
opposed to the standard |
duration of the step |
the right choice in |
introduce the notion of |
to the standard tiled |
of the step and |
right choice in leaving |
the notion of time |
so that clients are |
the standard tiled model |
the step and if |
choice in leaving the |
that clients are able |
step and if it |
in leaving the unix |
traditionally failure detectors have |
clients are able to |
and if it finds |
leaving the unix behind |
failure detectors have been |
are able to locally |
if it finds a |
detectors have been implemented |
able to locally implement |
it finds a full |
windows nt is an |
to locally implement the |
have been implemented using |
finds a full proof |
nt is an exiting |
conclusions to build ambitious |
locally implement the logic |
been implemented using time |
a full proof of |
to build ambitious collaboration |
implement the logic mapping |
full proof of work |
build ambitious collaboration application |
the logic mapping requests |
out mechanisms in the |
logic mapping requests to |
years ahead of its |
mechanisms in the transport |
the web services community |
ahead of its competition |
mapping requests to subservices |
it publishes this proof |
in the transport layer |
web services community will |
publishes this proof of |
the transport layer that |
services community will need |
this proof of work |
transport layer that implements |
community will need ways |
proof of work to |
layer that implements inter |
will need ways to |
the cluster might control |
of work to earn |
need ways to combine |
cluster might control the |
in its implementation and |
work to earn the |
might control the dns |
its implementation and in |
to earn the payoff |
implementation and in the |
outs remain an important |
and in the actual |
content from multiple sources |
remain an important tool |
or could influence the |
in the actual services |
an important tool in |
could influence the creation |
these include hosted sources |
important tool in the |
the actual services offered |
influence the creation of |
include hosted sources that |
tool in the failure |
pools a pool is |
the creation of web |
hosted sources that run |
in the failure manager |
a pool is a |
creation of web pages |
pool is a node |
sources that run in |
the failure manager described |
it took quite some |
of web pages by |
is a node that |
that run in data |
a node that serves |
took quite some time |
web pages by modifying |
failure manager described in |
run in data centers |
node that serves as |
quite some time to |
pages by modifying urls |
manager described in this |
in data centers and |
that serves as a |
some time to reach |
described in this paper |
so that clients will |
serves as a coordinator |
time to reach the |
data centers and support |
that clients will be |
as a coordinator and |
to reach the same |
centers and support web |
the mechanism is integrated |
clients will be directed |
a coordinator and multiple |
reach the same level |
and support web services |
the same level of |
will be directed to |
coordinator and multiple miners |
mechanism is integrated into |
support web services interfaces |
same level of knowledge |
be directed to an |
and multiple miners can |
is integrated into a |
level of knowledge and |
directed to an appropriate |
multiple miners can register |
integrated into a more |
but also direct peer |
of knowledge and insight |
to an appropriate subservice |
miners can register to |
into a more comprehensive |
knowledge and insight we |
can register to a |
a more comprehensive approach |
and insight we used |
peer protocols capable of |
more comprehensive approach that |
register to a pool |
insight we used to |
protocols capable of transporting |
comprehensive approach that treats |
the servers might export |
to a pool and |
we used to have |
capable of transporting audio |
approach that treats failure |
servers might export actual |
a pool and work |
used to have of |
that treats failure detection |
might export actual code |
pool and work for |
to have of unix |
treats failure detection using |
export actual code that |
and work for it |
have of unix systems |
whiteboard data and other |
failure detection using methods |
actual code that the |
data and other content |
detection using methods based |
code that the client |
and other content at |
using methods based on |
in every step it |
that the client runs |
but now that we |
other content at high |
methods based on an |
every step it generates |
now that we have |
content at high data |
based on an analogy |
step it generates a |
that we have arrived |
at high data rates |
on an analogy with |
an analogy with fault |
the partitioning logic is |
we have arrived at |
it generates a task |
partitioning logic is situated |
detection techniques used in |
generates a task for |
have arrived at that |
a further need is |
techniques used in daily |
a task for each |
logic is situated on |
arrived at that same |
further need is to |
used in daily life |
task for each registered |
is situated on a |
at that same knowledge |
need is to allow |
for each registered miner |
situated on a load |
that same knowledge point |
is to allow disconnected |
when trying to contact |
each registered miner and |
trying to contact a |
to allow disconnected collaboration |
on a load balancing |
registered miner and sends |
to contact a person |
is it clear that |
contact a person who |
it clear that our |
a person who has |
miner and sends it |
person who has allegedly |
clear that our research |
a load balancing component |
and sends it over |
back to data centers |
who has allegedly disappeared |
that our research is |
load balancing component resident |
sends it over the |
has allegedly disappeared one |
our research is making |
our review of the |
it over the network |
allegedly disappeared one would |
balancing component resident in |
research is making progress |
review of the performance |
disappeared one would never |
component resident in the |
each miner receives its |
of the performance of |
one would never be |
is making progress faster |
resident in the server |
miner receives its task |
the performance of enterprise |
would never be satisfied |
making progress faster than |
in the server cluster |
receives its task and |
performance of enterprise service |
never be satisfied with |
progress faster than ever |
its task and works |
the load balancer sprays |
be satisfied with making |
faster than ever before |
of enterprise service bus |
task and works on |
enterprise service bus eventing |
satisfied with making repeated |
load balancer sprays requests |
and works on it |
service bus eventing solutions |
with making repeated phone |
balancer sprays requests over |
working with windows nt |
works on it for |
bus eventing solutions in |
making repeated phone calls |
sprays requests over the |
with windows nt requires |
on it for the |
eventing solutions in the |
repeated phone calls to |
requests over the subservices |
windows nt requires certain |
it for the duration |
solutions in the standard |
phone calls to the |
over the subservices in |
nt requires certain level |
for the duration of |
in the standard hosted |
calls to the same |
the standard hosted web |
requires certain level of |
the duration of the |
the subservices in accordance |
to the same location |
standard hosted web services |
certain level of resilience |
duration of the step |
subservices in accordance with |
the same location for |
hosted web services model |
in accordance with server |
same location for half |
web services model made |
at the end of |
not because of flaws |
location for half an |
accordance with server logic |
services model made it |
the end of the |
because of flaws in |
for half an hour |
model made it clear |
the ssa supports the |
of flaws in the |
half an hour and |
end of the step |
made it clear that |
ssa supports the latter |
flaws in the operating |
an hour and then |
it clear that hosted |
supports the latter approach |
the miner sends the |
hour and then declaring |
clear that hosted event |
in the operating system |
miner sends the pool |
and then declaring the |
that hosted event channels |
offering a mechanism that |
sends the pool the |
then declaring the disappearance |
hosted event channels won |
a mechanism that assists |
but because of the |
the pool the full |
declaring the disappearance a |
event channels won t |
mechanism that assists the |
because of the zealous |
pool the full and |
the disappearance a fact |
channels won t have |
that assists the load |
of the zealous attacks |
the full and the |
won t have the |
the zealous attacks by |
full and the partial |
no matter whether the |
t have the scalability |
zealous attacks by colleagues |
balancing component in tracking |
and the partial proofs |
matter whether the phone |
have the scalability and |
attacks by colleagues and |
component in tracking membership |
the partial proofs of |
whether the phone was |
the scalability and latency |
by colleagues and other |
in tracking membership so |
partial proofs of work |
the phone was not |
scalability and latency properties |
colleagues and other researchers |
tracking membership so that |
proofs of work it |
phone was not picked |
and latency properties needed |
membership so that it |
of work it has |
was not picked up |
publishing papers about research |
so that it can |
work it has found |
latency properties needed by |
papers about research performed |
that it can appropriately |
a busy tone was |
properties needed by many |
about research performed on |
it can appropriately route |
research performed on windows |
needed by many applications |
performed on windows nt |
busy tone was heard |
on windows nt is |
the pool receives the |
can appropriately route queries |
tone was heard or |
windows nt is still |
pool receives the proofs |
appropriately route queries and |
p alternatives often achieve |
nt is still quite |
receives the proofs of |
was heard or the |
route queries and updates |
alternatives often achieve far |
is still quite difficult |
the proofs of work |
heard or the phone |
often achieve far better |
still quite difficult as |
proofs of work of |
or the phone was |
achieve far better scalability |
quite difficult as many |
of work of all |
the phone was disconnected |
we assume that processes |
difficult as many of |
work of all its |
assume that processes are |
as many of our |
of all its miners |
in practice one would |
that processes are fail |
many of our peer |
practice one would work |
they also have security |
of our peer still |
one would work to |
also have security advantages |
registers the partial proofs |
our peer still believe |
would work to gain |
the partial proofs of |
peer still believe that |
should a failure occur |
work to gain more |
the data center doesn |
partial proofs of work |
still believe that no |
to gain more confidence |
data center doesn t |
proofs of work and |
believe that no good |
gain more confidence in |
center doesn t get |
of work and publishes |
that no good research |
and will eventually be |
more confidence in such |
doesn t get a |
work and publishes the |
no good research can |
will eventually be detected |
confidence in such a |
t get a chance |
and publishes the full |
good research can be |
eventually be detected as |
in such a decision |
get a chance to |
publishes the full proofs |
research can be performed |
be detected as faulty |
such a decision by |
a chance to see |
can be performed on |
a decision by talking |
be performed on windows |
it calculates its overall |
decision by talking to |
performed on windows nt |
calculates its overall revenue |
by talking to the |
a failure may be |
talking to the landlord |
failure may be transient |
we hope that eventually |
and proceeds to distribute |
the live objects platform |
a process can become |
live objects platform can |
proceeds to distribute it |
hope that eventually the |
the neighbors or others |
process can become temporarily |
objects platform can seamlessly |
to distribute it among |
that eventually the advanced |
neighbors or others that |
can become temporarily unavailable |
platform can seamlessly support |
distribute it among its |
eventually the advanced technical |
or others that may |
can seamlessly support applications |
it among its miners |
the advanced technical nature |
others that may have |
but then restart and |
seamlessly support applications that |
advanced technical nature of |
that may have a |
then restart and recover |
support applications that require |
each miner receives revenue |
technical nature of the |
may have a more |
restart and recover any |
applications that require a |
miner receives revenue proportional |
nature of the operating |
have a more informed |
and recover any missing |
that require a mixture |
receives revenue proportional to |
of the operating system |
a more informed idea |
recover any missing updates |
require a mixture of |
revenue proportional to its |
the operating system will |
more informed idea about |
a mixture of data |
proportional to its success |
operating system will prevail |
informed idea about the |
mixture of data sources |
to its success in |
system will prevail in |
idea about the situation |
its success in the |
will prevail in the |
about the situation of |
success in the current |
including both hosted and |
prevail in the discussion |
the situation of the |
discussion our model is |
in the current step |
both hosted and direct |
situation of the person |
our model is not |
hosted and direct p |
of the person in |
model is not completely |
and that we can |
the person in question |
that we can have |
namely the ratio of |
is not completely general |
we can have a |
the ratio of its |
the failure management described |
can have a community |
ratio of its partial |
failure management described in |
and for this reason |
have a community where |
further benefits include an |
of its partial proofs |
management described in this |
for this reason some |
a community where research |
benefits include an easy |
its partial proofs of |
described in this paper |
this reason some discussion |
community where research results |
include an easy to |
partial proofs of work |
in this paper is |
reason some discussion is |
where research results can |
an easy to use |
proofs of work out |
this paper is capable |
some discussion is needed |
research results can be |
easy to use drag |
of work out of |
paper is capable of |
results can be shared |
work out of all |
is capable of following |
consider the following example |
can be shared without |
out of all partial |
capable of following a |
be shared without sarcasm |
of all partial proofs |
we wish to support |
of following a similar |
shared without sarcasm or |
drop programming style that |
all partial proofs of |
wish to support a |
following a similar strategy |
without sarcasm or the |
programming style that yields |
partial proofs of work |
to support a scalable |
sarcasm or the risk |
style that yields applications |
proofs of work the |
support a scalable inventory |
if a process under |
or the risk of |
that yields applications represented |
of work the pool |
a scalable inventory service |
a process under investigation |
the risk of igniting |
yields applications represented as |
work the pool received |
scalable inventory service that |
process under investigation is |
risk of igniting yet |
applications represented as xml |
inventory service that receives |
under investigation is not |
of igniting yet another |
represented as xml files |
service that receives updates |
we assume that pools |
investigation is not responding |
igniting yet another holy |
that receives updates corresponding |
assume that pools do |
is not responding it |
yet another holy war |
which can be shared |
receives updates corresponding to |
that pools do not |
not responding it will |
can be shared as |
updates corresponding to inventory |
pools do not collect |
responding it will contact |
be shared as files |
corresponding to inventory consumption |
do not collect fees |
it will contact the |
shared as files or |
to inventory consumption and |
not collect fees of |
will contact the operating |
as files or even |
inventory consumption and re |
collect fees of the |
contact the operating system |
files or even via |
fees of the revenue |
the operating system under |
or even via email |
operating system under which |
system under which the |
under which the process |
which the process is |
the process is running |
queries against such a |
pool fees and their |
users that open such |
against such a service |
fees and their implications |
that open such files |
such a service would |
or other nodes on |
and their implications on |
open such files find |
a service would compute |
other nodes on the |
their implications on our |
such files find themselves |
service would compute and |
nodes on the same |
implications on our analysis |
files find themselves immersed |
would compute and return |
on the same sub |
on our analysis are |
find themselves immersed in |
compute and return an |
our analysis are discussed |
themselves immersed in a |
and return an inventory |
net to help reach |
analysis are discussed in |
immersed in a mediarich |
return an inventory count |
to help reach a |
are discussed in section |
in a mediarich collaborative |
an inventory count as |
help reach a decision |
discussed in section ix |
a mediarich collaborative environment |
inventory count as of |
reach a decision in |
mediarich collaborative environment that |
count as of the |
a decision in which |
collaborative environment that also |
as of the time |
decision in which one |
environment that also offers |
of the time the |
in which one can |
block withholding miner a |
the time the query |
that also offers strong |
which one can have |
withholding miner a miner |
time the query was |
also offers strong reliability |
one can have greater |
miner a miner registered |
the query was processed |
can have greater confidence |
a miner registered at |
miner registered at a |
registered at a pool |
at a pool can |
but inventory can change |
a pool can perform |
inventory can change in |
pool can perform the |
in the near future |
most distributed systems in |
can change in real |
can perform the classical |
distributed systems in use |
perform the classical block |
systems in use today |
the classical block withholding |
in use today deal |
classical block withholding attack |
use today deal with |
today deal with failure |
deal with failure of |
with failure of nodes |
most important of all |
failure of nodes or |
reissued a moment later |
an attacker miner operates |
of nodes or networks |
attacker miner operates as |
live objects are real |
nodes or networks in |
miner operates as if |
or networks in some |
might yield a different |
operates as if it |
networks in some way |
yield a different result |
the platform is available |
as if it worked |
a different result and |
platform is available for |
if it worked for |
different result and yet |
in general the problem |
is available for free |
general the problem is |
result and yet both |
the problem is detected |
available for free download |
problem is detected in |
and yet both would |
is detected in the |
for free download from |
detected in the communication |
yet both would be |
it worked for the |
free download from cornell |
in the communication subsystem |
expert testimony of professor |
the communication subsystem where |
both would be correct |
worked for the pool |
testimony of professor david |
communication subsystem where session |
of professor david j |
subsystem where session or |
where session or transport |
session or transport protocols |
it receives its tasks |
responses reflecting a reasonably |
or transport protocols are |
receives its tasks and |
reflecting a reasonably current |
transport protocols are unable |
its tasks and works |
a reasonably current server |
protocols are unable to |
tasks and works on |
reasonably current server state |
are unable to make |
and works on them |
current server state are |
unable to make progress |
server state are acceptable |
to make progress because |
make progress because of |
progress because of the |
because of the lack |
only at the end |
on the other hand |
of the lack of |
at the end of |
the lack of response |
the end of each |
lack of response from |
a response reflecting a |
end of each round |
of response from remote |
response reflecting a very |
of each round it |
response from remote nodes |
reflecting a very stale |
each round it sends |
a very stale state |
round it sends only |
very stale state would |
traditionally packets are being |
lateral error correction for |
it sends only its |
stale state would be |
packets are being retransmitted |
error correction for time |
sends only its partial |
state would be incorrect |
are being retransmitted after |
only its partial proofs |
being retransmitted after a |
its partial proofs of |
retransmitted after a time |
a client should not |
partial proofs of work |
client should not be |
should not be offered |
out period and after |
not be offered a |
period and after a |
be offered a promotional |
and after a retry |
and omits full proofs |
offered a promotional price |
after a retry threshold |
omits full proofs of |
a promotional price on |
a retry threshold is |
full proofs of work |
promotional price on a |
retry threshold is reached |
proofs of work if |
price on a plasma |
threshold is reached the |
of work if it |
on a plasma tv |
is reached the remote |
work if it had |
a plasma tv if |
reached the remote destination |
if it had found |
plasma tv if the |
the remote destination is |
it had found any |
tv if the last |
remote destination is marked |
if the last unit |
destination is marked as |
the last unit was |
is marked as unreachable |
the pool registers the |
last unit was actually |
pool registers the miner |
unit was actually sold |
registers the miner s |
was actually sold hours |
some systems inject additional |
the miner s partial |
actually sold hours ago |
systems inject additional packets |
miner s partial proofs |
inject additional packets into |
additional packets into the |
packets into the data |
into the data stream |
the data stream to |
but cannot distinguish between |
data stream to ensure |
the inventory service should |
cannot distinguish between miners |
stream to ensure timely |
inventory service should reflect |
distinguish between miners running |
to ensure timely detection |
service should reflect as |
between miners running honestly |
ensure timely detection of |
should reflect as many |
miners running honestly and |
timely detection of failures |
reflect as many updates |
running honestly and block |
detection of failures at |
as many updates as |
honestly and block withholding |
of failures at moments |
many updates as possible |
and block withholding miners |
failures at moments when |
updates as possible in |
at moments when the |
as possible in the |
moments when the traffic |
possible in the replies |
when the traffic is |
the implications are that |
in the replies it |
implications are that a |
the traffic is low |
are that a miner |
the replies it gives |
traffic is low or |
that a miner that |
replies it gives to |
is low or unidirectional |
a miner that engages |
it gives to requests |
miner that engages in |
that engages in block |
engages in block withholding |
in block withholding does |
but any reply is |
block withholding does not |
any reply is correct |
withholding does not contribute |
reply is correct provided |
does not contribute to |
is correct provided that |
not contribute to the |
correct provided that it |
contribute to the pool |
provided that it was |
to the pool s |
that it was based |
the pool s overall |
it was based on |
pool s overall mining |
was based on a |
s overall mining power |
based on a recent |
on a recent state |
expect the application to |
the application to handle |
but still shares the |
exploiting gossip for self |
still shares the pool |
application to handle the |
shares the pool s |
we shall see that |
to handle the failure |
management in scalable event |
handle the failure management |
the pool s revenue |
shall see that the |
in scalable event notification |
the failure management as |
uwin unix for windows |
pool s revenue according |
see that the ssa |
scalable event notification systems |
failure management as the |
s revenue according to |
that the ssa allows |
management as the support |
the usenix windows nt |
revenue according to its |
the ssa allows brief |
as the support system |
usenix windows nt workshop |
according to its sent |
ssa allows brief inconsistencies |
the support system does |
to its sent partial |
allows brief inconsistencies but |
support system does not |
its sent partial proofs |
brief inconsistencies but that |
system does not contain |
sent partial proofs of |
inconsistencies but that they |
does not contain any |
partial proofs of work |
but that they can |
not contain any fault |
that they can be |
contain any fault management |
they can be limited |
can be limited to |
be limited to a |
to reason about a |
limited to a few |
often these systems cannot |
reason about a pool |
to a few seconds |
these systems cannot distinguish |
about a pool s |
systems cannot distinguish between |
a pool s efficiency |
cannot distinguish between process |
pool s efficiency we |
semantic integration of web |
operations against the inventory |
s efficiency we define |
node or network failure |
against the inventory service |
integration of web services |
efficiency we define its |
the inventory service happen |
of web services and |
we define its per |
the mechanisms used to |
inventory service happen to |
web services and peer |
mechanisms used to detect |
service happen to be |
used to detect failure |
miner revenue as follows |
happen to be commutative |
to detect failure do |
detect failure do not |
failure do not adapt |
do not adapt to |
not adapt to changing |
peer networks to achieve |
adapt to changing network |
hence the service can |
networks to achieve fault |
to changing network conditions |
the service can process |
service can process updates |
can process updates out |
process updates out of |
updates out of order |
making it almost impossible |
it almost impossible to |
almost impossible to use |
but many kinds of |
impossible to use these |
many kinds of services |
to use these systems |
kinds of services can |
use these systems unmodified |
of services can handle |
these systems unmodified in |
services can handle out |
systems unmodified in wide |
the revenue density of |
can handle out of |
unmodified in wide area |
revenue density of a |
handle out of order |
in wide area systems |
density of a pool |
out of order updates |
wide area systems without |
of a pool is |
area systems without resorting |
a pool is the |
systems without resorting to |
if for no other |
pool is the ratio |
without resorting to heavy |
is the ratio between |
for no other reason |
the ratio between the |
resorting to heavy weight |
ratio between the average |
no other reason than |
to heavy weight solutions |
between the average revenue |
other reason than that |
the average revenue a |
heavy weight solutions like |
reason than that in |
average revenue a pool |
weight solutions like using |
than that in many |
flexible protocol composition in |
revenue a pool member |
solutions like using a |
that in many settings |
protocol composition in bast |
a pool member earns |
like using a tcp |
pool member earns and |
using a tcp connection |
each update is uniquely |
member earns and the |
a tcp connection as |
update is uniquely sequenced |
earns and the average |
tcp connection as the |
is uniquely sequenced by |
and the average revenue |
connection as the preferred |
uniquely sequenced by its |
the average revenue it |
as the preferred transport |
sequenced by its source |
average revenue it would |
the preferred transport method |
revenue it would have |
preferred transport method for |
it would have earned |
transport method for each |
permitting the service to |
would have earned as |
method for each rpc |
the service to sort |
have earned as a |
for each rpc call |
service to sort updates |
earned as a solo |
th edition with source |
to sort updates and |
as a solo miner |
edition with source code |
sort updates and to |
updates and to process |
and to process queries |
especially those designed to |
to process queries against |
those designed to support |
process queries against the |
the revenue density of |
designed to support high |
queries against the sorted |
self organizing live objects |
revenue density of a |
against the sorted database |
density of a solo |
of a solo miner |
our group has held |
group has held discussions |
has held discussions with |
management in a more |
and that of a |
held discussions with operators |
in a more integrated |
that of a miner |
discussions with operators of |
a more integrated way |
of a miner working |
with operators of several |
a miner working with |
operators of several large |
miner working with an |
many of these systems |
of several large datacenters |
working with an unattacked |
of these systems are |
with an unattacked pool |
these systems are structured |
an unattacked pool are |
and concluded that many |
systems are structured as |
unattacked pool are one |
concluded that many services |
are structured as groups |
that many services have |
structured as groups of |
many services have the |
as groups of cooperating |
services have the kinds |
if a pool is |
groups of cooperating processes |
have the kinds of |
a pool is attacked |
of cooperating processes using |
the kinds of properties |
pool is attacked with |
cooperating processes using some |
kinds of properties just |
is attacked with block |
processes using some form |
of properties just cited |
attacked with block withholding |
using some form of |
some form of group |
form of group membership |
jms performance comparison for |
performance comparison for publish |
ability to respond based |
comparison for publish subscribe |
to respond based on |
its revenue density decreases |
for publish subscribe messaging |
respond based on a |
based on a reasonable |
detection to be able |
on a reasonable current |
to be able to |
fiorano software technologies pvt |
a reasonable current state |
be able to reach |
able to reach consensus |
continuous analysis because our |
analysis because our analysis |
and to handle out |
because our analysis will |
various methods are used |
our analysis will be |
analysis will be of |
will be of the |
be of the average |
of the average revenue |
of which fault monitors |
the ssa is a |
ssa is a good |
is a good match |
we will consider proofs |
a good match for |
will consider proofs of |
good match for personalization |
consider proofs of work |
match for personalization services |
both full and partial |
as continuous deterministic sizes |
according to their probability |
work on a task |
on a task therefore |
a task therefore results |
task therefore results in |
therefore results in a |
leveraging collaboration of peer |
results in a deterministic |
in a deterministic fraction |
a deterministic fraction of |
deterministic fraction of proof |
fraction of proof of |
of proof of work |
peer and web services |
are the most popular |
t he p ool |
however in each of |
he p ool g |
in each of these |
the design and implementation |
p ool g ame |
each of these systems |
design and implementation of |
of these systems the |
ool g ame a |
and implementation of the |
these systems the failure |
systems the failure management |
the failure management is |
failure management is an |
the pool block withholding |
management is an integral |
pool block withholding attack |
is an integral part |
block withholding attack just |
an integral part of |
withholding attack just as |
integral part of the |
attack just as a |
part of the particular |
just as a miner |
of the particular membership |
as a miner can |
the particular membership or |
a miner can perform |
particular membership or transport |
miner can perform block |
membership or transport system |
these deal primarily with |
based web service composition |
or transport system and |
can perform block withholding |
deal primarily with weakly |
web service composition with |
transport system and not |
perform block withholding on |
primarily with weakly consistent |
service composition with jade |
system and not available |
block withholding on a |
with weakly consistent data |
composition with jade and |
and not available for |
withholding on a pool |
with jade and jxta |
not available for general |
on a pool j |
available for general use |
although some research groups |
a pool i can |
pool i can use |
and all sorts of |
i can use some |
all sorts of services |
can use some of |
sorts of services in |
use some of its |
of services in which |
some of its mining |
services in which replies |
of its mining power |
in which replies are |
its mining power to |
which replies are intrinsically |
mining power to infiltrate |
replies are intrinsically noisy |
power to infiltrate a |
to infiltrate a pool |
infiltrate a pool j |
a pool j and |
pool j and perform |
such as services that |
j and perform a |
as services that report |
and perform a block |
services that report data |
perform a block withholding |
that report data gathered |
a block withholding attack |
report data gathered from |
block withholding attack on |
are focusing on wide |
data gathered from remote |
what s new in |
focusing on wide area |
withholding attack on j |
gathered from remote sensors |
s new in windows |
on wide area systems |
based architecture for semanticweb |
architecture for semanticweb service |
for semanticweb service automatic |
semanticweb service automatic composition |
denote the amount of |
the majority of the |
the amount of such |
majority of the existing |
a datacenter would also |
amount of such infiltrating |
of the existing failure |
datacenter would also host |
of such infiltrating mining |
the existing failure detectors |
would also host some |
such infiltrating mining power |
existing failure detectors are |
also host some kinds |
infiltrating mining power at |
failure detectors are not |
host some kinds of |
mining power at step |
detectors are not suitable |
some kinds of services |
power at step t |
are not suitable for |
kinds of services ill |
at step t by |
not suitable for use |
step t by xi |
suitable for use in |
for use in large |
use in large scale |
matched to our model |
in large scale systems |
but because we are |
because of their inflexibility |
because we are working |
of their inflexibility or |
we are working with |
their inflexibility or the |
are working with web |
inflexibility or the simplicity |
working with web services |
or the simplicity of |
the simplicity of their |
simplicity of their assumptions |
miners working for pool |
working for pool i |
services running on the |
running on the ssa |
on the ssa can |
the ssa can easily |
ssa can easily interact |
either mining honestly or |
can easily interact with |
mining honestly or used |
building a failure detector |
easily interact with services |
honestly or used for |
interact with services that |
a failure detector that |
or used for infiltrating |
with services that employ |
used for infiltrating pool |
failure detector that is |
services that employ other |
for infiltrating pool j |
that employ other solutions |
detector that is not |
that is not an |
are loyal to pool |
is not an integral |
loyal to pool i |
not an integral part |
an integral part of |
integral part of the |
at the end of |
the end of a |
part of the communication |
end of a round |
of the communication architecture |
consistency semantics the ssa |
the communication architecture permits |
semantics the ssa implements |
the ssa implements stochastic |
and jong hoon ahnn |
pool i aggregates its |
communication architecture permits the |
ssa implements stochastic consistency |
architecture permits the implementation |
i aggregates its revenue |
permits the implementation of |
programming with live distributed |
implements stochastic consistency semantics |
aggregates its revenue from |
with live distributed objects |
its revenue from mining |
the implementation of a |
revenue from mining in |
implementation of a collection |
an application will only |
of a collection of |
from mining in the |
a collection of failure |
application will only observe |
collection of failure detection |
mining in the current |
of failure detection techniques |
will only observe an |
failure detection techniques and |
in the current round |
detection techniques and support |
only observe an inconsistency |
techniques and support for |
the current round and |
observe an inconsistency if |
and support for failure |
current round and from |
an inconsistency if a |
support for failure detection |
round and from its |
inconsistency if a fault |
for failure detection methods |
and from its infiltration |
if a fault occurs |
failure detection methods of |
from its infiltration in |
detection methods of varying |
its infiltration in the |
methods of varying levels |
infiltration in the previous |
and even then only |
of varying levels of |
in the previous round |
even then only for |
varying levels of complexity |
then only for a |
levels of complexity from |
only for a period |
of complexity from which |
for a period of |
complexity from which the |
it distributes the revenue |
a period of time |
from which the system |
achieving reliability through distributed |
distributes the revenue evenly |
period of time associated |
which the system designer |
reliability through distributed data |
the revenue evenly among |
of time associated with |
the system designer can |
through distributed data flows |
revenue evenly among all |
time associated with our |
system designer can choose |
distributed data flows and |
evenly among all its |
associated with our repair |
designer can choose to |
data flows and recursive |
among all its loyal |
with our repair protocol |
can choose to match |
flows and recursive delegation |
all its loyal miners |
choose to match the |
nick vasilatos and werner |
its loyal miners according |
to match the system |
vasilatos and werner vogels |
and only if it |
loyal miners according to |
match the system requirements |
only if it has |
miners according to their |
do you need source |
if it has the |
according to their partial |
you need source with |
it has the bad |
to their partial proofs |
the failure management service |
need source with that |
has the bad luck |
their partial proofs of |
failure management service consists |
the bad luck to |
partial proofs of work |
management service consists of |
panel at the usenix |
bad luck to query |
service consists of three |
at the usenix windows |
luck to query a |
consists of three functional |
the usenix windows nt |
to query a node |
the pool s miners |
of three functional modules |
usenix windows nt workshop |
query a node impacted |
pool s miners are |
a node impacted by |
s miners are oblivious |
node impacted by the |
miners are oblivious to |
impacted by the failure |
are oblivious to their |
oblivious to their role |
to their role and |
their role and they |
role and they operate |
this window can be |
and they operate as |
window can be made |
they operate as regular |
can be made small |
operate as regular honest |
as regular honest miners |
so that applications are |
that applications are unlikely |
applications are unlikely to |
are unlikely to observe |
unlikely to observe a |
a library that implements |
to observe a problem |
library that implements simple |
that implements simple failure |
implements simple failure management |
simple failure management functionality |
failure management functionality and |
or permitted to grow |
management functionality and provide |
revenue convergence note that |
permitted to grow somewhat |
functionality and provide the |
convergence note that pool |
to grow somewhat larger |
and provide the api |
note that pool j |
provide the api to |
that pool j sends |
the api to the |
pool j sends its |
api to the complete |
depending upon the cost |
j sends its revenue |
to the complete service |
upon the cost of |
sends its revenue to |
the cost of inconsistency |
its revenue to infiltrators |
summary in usenix login |
cost of inconsistency and |
a service implementing per |
revenue to infiltrators from |
of inconsistency and the |
service implementing per node |
to infiltrators from pool |
inconsistency and the relative |
implementing per node failure |
infiltrators from pool i |
and the relative value |
per node failure management |
changtao qu and wolfgang |
qu and wolfgang nejdl |
from pool i at |
pool i at the |
i at the end |
at the end of |
combining fault management with |
the end of the |
fault management with other |
end of the step |
management with other local |
with other local nodes |
other local nodes to |
of faster response time |
local nodes to exploit |
peer network with web |
faster response time versus |
and this revenue is |
nodes to exploit locality |
network with web services |
response time versus lower |
this revenue is calculated |
to exploit locality of |
time versus lower risk |
revenue is calculated in |
exploit locality of communication |
versus lower risk of |
is calculated in pool |
locality of communication and |
lower risk of an |
unix application portability to |
calculated in pool i |
of communication and failure |
risk of an observed |
application portability to windows |
in pool i at |
communication and failure patterns |
of an observed fault |
portability to windows nt |
pool i at the |
to windows nt via |
i at the beginning |
an inquiry service closely |
windows nt via an |
at the beginning of |
inquiry service closely coupled |
in the experimental work |
nt via an alternative |
the beginning of the |
service closely coupled with |
the experimental work that |
via an alternative environment |
beginning of the subsequent |
closely coupled with the |
experimental work that follows |
an alternative environment subsystem |
of the subsequent step |
coupled with the operating |
with the operating system |
the operating system which |
the usenix windows nt |
we measure these windows |
usenix windows nt workshop |
measure these windows for |
if there is a |
these windows for scenarios |
there is a chain |
a scalable and ontology |
windows for scenarios representative |
is a chain of |
provides information about the |
for scenarios representative of |
a chain of pools |
information about the state |
scenarios representative of conditions |
chain of pools of |
about the state of |
p infrastructure for semantic |
representative of conditions that |
of pools of length |
the state of local |
infrastructure for semantic web |
for semantic web services |
state of local participating |
of conditions that arise |
of local participating processes |
where each pool infiltrates |
conditions that arise in |
each pool infiltrates the |
that arise in realistic |
pool infiltrates the next |
arise in realistic settings |
the most fundamental operation |
most fundamental operation offered |
the pool revenue will |
fundamental operation offered by |
pool revenue will not |
operation offered by a |
the ssa framework the |
revenue will not be |
offered by a failure |
ssa framework the basic |
will not be static |
by a failure detection |
framework the basic operation |
a failure detection service |
the basic operation of |
failure detection service is |
basic operation of the |
since the revenue from |
detection service is that |
operation of the ssa |
the revenue from infiltration |
service is that of |
of the ssa is |
revenue from infiltration takes |
is that of the |
the ssa is as |
from infiltration takes one |
that of the investigation |
ssa is as follows |
infiltration takes one step |
of the investigation of |
a collaboration system architecture |
takes one step to |
the investigation of a |
as queries or updates |
one step to take |
investigation of a suspected |
queries or updates are |
step to take each |
of a suspected process |
or updates are received |
to take each hop |
updates are received in |
are received in the |
protect the future of |
received in the cluster |
to make use of |
the future of computing |
future of computing technology |
make use of this |
use of this operation |
they are passed through |
of this operation it |
are passed through a |
max is the longest |
this operation it is |
passed through a partition |
is the longest chain |
operation it is not |
sonic performance test suite |
it is not necessary |
the longest chain in |
through a partition mapping |
is not necessary for |
longest chain in the |
a partition mapping component |
not necessary for either |
chain in the system |
necessary for either the |
for either the local |
either the local or |
which directs the request |
the local or remote |
directs the request to |
the revenue stabilizes after |
local or remote process |
the request to an |
or remote process to |
request to an appropriate |
remote process to run |
to an appropriate racs |
process to run any |
to run any of |
run any of the |
any of the heartbeat |
of the heartbeat or |
if there are loops |
the heartbeat or polling |
we will use the |
there are loops in |
heartbeat or polling patterns |
will use the term |
are loops in the |
use the term subservice |
loops in the infiltration |
the term subservice rather |
in the infiltration graph |
the reasons that the |
term subservice rather than |
reasons that the local |
subservice rather than racs |
that the local process |
rather than racs in |
the system will converge |
the local process began |
than racs in the |
system will converge to |
local process began to |
racs in the remainder |
will converge to a |
process began to suspect |
in the remainder of |
converge to a certain |
began to suspect the |
the remainder of the |
to a certain revenue |
to suspect the remote |
remainder of the paper |
suspect the remote process |
the remote process are |
remote process are not |
process are not of |
as stated in the |
to create a subservice |
are not of any |
stated in the following |
create a subservice the |
not of any importance |
in the following lemma |
a subservice the developer |
of any importance to |
subservice the developer must |
any importance to the |
the developer must first |
importance to the failure |
developer must first implement |
to the failure management |
must first implement a |
first implement a non |
this is then cloned |
is then cloned using |
then cloned using the |
cloned using the ssa |
using the ssa platform |
if infiltration rates are |
infiltration rates are constant |
each replica is placed |
a demonstration of collaborative |
replica is placed on |
demonstration of collaborative web |
the pool revenues converge |
is placed on a |
of collaborative web services |
placed on a separate |
collaborative web services and |
web services and peer |
on a separate node |
denote the revenue density |
and the replicas are |
the process at address |
the revenue density of |
the replicas are then |
process at address is |
revenue density of pool |
replicas are then linked |
at address is investigated |
density of pool i |
are then linked using |
address is investigated and |
of pool i at |
then linked using tcp |
is investigated and a |
pool i at the |
linked using tcp to |
investigated and a report |
i at the end |
using tcp to create |
and a report is |
at the end of |
tcp to create a |
a report is returned |
the end of step |
to create a chain |
report is returned within |
end of step t |
is returned within the |
of step t by |
returned within the deadline |
step t by ri |
within the deadline set |
the deadline set by |
deadline set by the |
set by the local |
p network based architecture |
by the local process |
network based architecture for |
based architecture for web |
architecture for web service |
the local process does |
local process does not |
we therefore have a |
process does not have |
and define the revenue |
does not have to |
define the revenue density |
not have to wait |
the revenue density vector |
have to wait for |
revenue density vector r |
to wait for the |
wait for the investigation |
for the investigation to |
the investigation to finish |
mapping between a subservice |
investigation to finish but |
between a subservice and |
to finish but can |
a subservice and a |
finish but can make |
subservice and a chain |
but can make use |
can make use of |
make use of the |
use of the asynch |
of the asynch interface |
the asynch interface to |
asynch interface to collect |
interface to collect the |
to collect the result |
collect the result at |
the result at a |
result at a later |
at a later moment |
the report contains information |
gossip based chain replication |
report contains information on |
based chain replication the |
contains information on whether |
chain replication the replication |
information on whether the |
replication the replication scheme |
on whether the remote |
the replication scheme has |
whether the remote node |
replication scheme has evolved |
the remote node was |
scheme has evolved out |
remote node was reachable |
has evolved out of |
node was reachable within |
evolved out of the |
was reachable within the |
out of the chain |
reachable within the deadline |
of the chain replication |
within the deadline and |
the chain replication mechanism |
the deadline and whether |
chain replication mechanism first |
deadline and whether the |
replication mechanism first introduced |
and whether the process |
mechanism first introduced in |
whether the process under |
the process under investigation |
process under investigation was |
under investigation was still |
investigation was still present |
was still present at |
still present at the |
present at the host |
if the mode parameter |
the mode parameter was |
p in every round |
mode parameter was used |
parameter was used to |
was used to request |
used to request a |
to request a more |
request a more detailed |
pool i uses its |
a more detailed remote |
i uses its mining |
more detailed remote reporting |
the original scheme was |
uses its mining power |
original scheme was developed |
its mining power of |
scheme was developed as |
mining power of m |
was developed as a |
process checkpoint information is |
managed transactional consistency for |
developed as a means |
checkpoint information is returned |
as a means of |
information is returned or |
transactional consistency for web |
a means of obtaining |
is returned or the |
consistency for web caching |
means of obtaining high |
returned or the remote |
for web caching ittay |
of obtaining high throughput |
or the remote process |
web caching ittay eyal |
obtaining high throughput and |
j used for direct |
the remote process is |
caching ittay eyal ken |
high throughput and availability |
used for direct mining |
remote process is interrupted |
ittay eyal ken birman |
throughput and availability for |
for direct mining p |
process is interrupted to |
eyal ken birman robbert |
and availability for query |
is interrupted to provide |
ken birman robbert van |
availability for query and |
interrupted to provide status |
birman robbert van renesse |
for query and update |
to provide status information |
robbert van renesse cornell |
query and update requests |
van renesse cornell university |
and update requests without |
renesse cornell university abstract |
update requests without sacrificing |
cornell university abstract in |
see the section on |
and shares it among |
requests without sacrificing strong |
the section on os |
shares it among its |
without sacrificing strong consistency |
section on os integration |
it among its m |
sacrificing strong consistency guarantees |
only caches are widely |
caches are widely used |
are widely used in |
widely used in cloud |
used in cloud infrastructure |
in cloud infrastructure to |
cloud infrastructure to reduce |
infrastructure to reduce access |
to reduce access latency |
if the node was |
reduce access latency and |
the node was not |
access latency and to |
node was not reachable |
latency and to reduce |
the gossip based chain |
was not reachable and |
gossip based chain replication |
and to reduce load |
based chain replication behaves |
not reachable and the |
to reduce load on |
chain replication behaves in |
reachable and the local |
reduce load on backend |
load on backend databases |
and the local process |
replication behaves in the |
the local process has |
behaves in the following |
local process has requested |
operators view coherent caches |
in the following manner |
process has requested extensive |
all sums are over |
view coherent caches as |
the following manner during |
has requested extensive investigation |
sums are over the |
coherent caches as impractical |
following manner during normal |
are over the range |
caches as impractical at |
manner during normal operation |
as impractical at genuinely |
the failure investigator will |
during normal operation when |
impractical at genuinely large |
failure investigator will try |
normal operation when nodes |
at genuinely large scale |
investigator will try to |
operation when nodes aren |
genuinely large scale and |
will try to contact |
when nodes aren t |
large scale and many |
scale and many client |
nodes aren t failing |
try to contact a |
aren t failing or |
to contact a failure |
t failing or restarting |
facing caches are updated |
contact a failure manager |
caches are updated in |
a failure manager at |
are updated in an |
failure manager at the |
updated in an asynchronous |
manager at the node |
update operations are forwarded |
in an asynchronous manner |
operations are forwarded to |
an asynchronous manner with |
are forwarded to the |
asynchronous manner with best |
forwarded to the head |
to the head of |
the head of the |
head of the chain |
net or within its |
or within its administrative |
existing solutions that support |
denote the direct mining |
where the request is |
solutions that support cache |
within its administrative domain |
the direct mining revenue |
the request is processed |
that support cache consistency |
its administrative domain which |
direct mining revenue density |
request is processed using |
support cache consistency are |
administrative domain which should |
mining revenue density of |
is processed using the |
cache consistency are inapplicable |
domain which should be |
revenue density of each |
processed using the local |
consistency are inapplicable to |
which should be able |
density of each pool |
using the local replica |
are inapplicable to this |
should be able to |
inapplicable to this scenario |
be able to give |
to this scenario since |
able to give a |
this scenario since they |
the state changes are |
to give a more |
scenario since they require |
state changes are passed |
give a more conclusive |
since they require a |
which is a constant |
is a constant factor |
a more conclusive answer |
they require a round |
changes are passed along |
more conclusive answer about |
require a round trip |
are passed along down |
conclusive answer about the |
a round trip to |
passed along down the |
answer about the node |
round trip to the |
along down the chain |
trip to the database |
down the chain to |
to the database on |
the chain to the |
the database on every |
s failure to respond |
chain to the next |
database on every cache |
on every cache transaction |
to the next element |
if network failure is |
network failure is the |
existing incoherent cache technologies |
failure is the cause |
incoherent cache technologies are |
is the cause of |
which in turn updates |
cache technologies are oblivious |
the cause of the |
in turn updates it |
technologies are oblivious to |
cause of the loss |
turn updates it s |
are oblivious to transactional |
of the loss of |
updates it s state |
oblivious to transactional data |
to transactional data access |
it s state and |
the loss of connectivity |
s state and performs |
even if the backend |
state and performs the |
if the backend database |
and performs the same |
the backend database supports |
the report will indicate |
performs the same operation |
backend database supports transactions |
report will indicate which |
the same operation until |
will indicate which part |
same operation until the |
indicate which part of |
operation until the tail |
which part of the |
until the tail is |
part of the path |
the tail is reached |
of the path is |
the path is reachable |
aware cache for read |
path is reachable and |
is reachable and where |
reachable and where the |
and where the suspected |
queries can either be |
can either be directed |
either be directed towards |
be directed towards a |
directed towards a randomly |
cache improves cache consistency |
towards a randomly selected |
improves cache consistency despite |
a randomly selected process |
cache consistency despite asynchronous |
randomly selected process in |
consistency despite asynchronous and |
if the failure investigator |
selected process in the |
despite asynchronous and unreliable |
the failure investigator is |
process in the group |
asynchronous and unreliable communication |
failure investigator is configured |
in the group or |
and unreliable communication between |
process communication primitives for |
investigator is configured with |
the group or to |
unreliable communication between the |
communication primitives for programming |
p the revenue of |
group or to a |
the revenue of pool |
is configured with alternative |
primitives for programming distributed |
communication between the cache |
or to a specific |
revenue of pool i |
configured with alternative outgoing |
for programming distributed systems |
between the cache and |
to a specific one |
of pool i in |
with alternative outgoing paths |
programming distributed systems robbert |
the cache and the |
cache and the database |
distributed systems robbert van |
systems robbert van renesse |
pool i in step |
the strongest consistency guarantee |
these paths are probed |
i in step t |
strongest consistency guarantee is |
department of computer science |
paths are probed to |
in step t taken |
consistency guarantee is acheived |
of computer science cornell |
a variant of serializability |
are probed to see |
step t taken through |
guarantee is acheived if |
computer science cornell university |
variant of serializability that |
probed to see if |
t taken through infiltration |
is acheived if all |
science cornell university category |
of serializability that is |
to see if it |
taken through infiltration from |
acheived if all query |
serializability that is suitable |
see if it is |
through infiltration from pool |
representation the following position |
if all query operations |
that is suitable for |
if it is possible |
infiltration from pool j |
the following position paper |
all query operations are |
is suitable for incoherent |
it is possible to |
from pool j s |
following position paper describes |
query operations are targeted |
suitable for incoherent caches |
is possible to circumvent |
pool j s revenue |
position paper describes a |
operations are targeted at |
possible to circumvent the |
j s revenue in |
paper describes a new |
and prove that with |
are targeted at the |
to circumvent the network |
s revenue in step |
describes a new interprocess |
prove that with unbounded |
targeted at the tail |
circumvent the network failure |
revenue in step t |
a new interprocess communication |
that with unbounded resources |
with unbounded resources t |
the network failure and |
at the tail of |
network failure and in |
the tail of the |
failure and in such |
tail of the chain |
primitive that is designed |
and in such a |
of the chain node |
that is designed to |
in such a way |
is designed to make |
such a way collect |
cache allows the system |
designed to make it |
a way collect information |
allows the system manager |
which is the case |
way collect information about |
to make it easier |
the system manager to |
is the case for |
collect information about the |
make it easier to |
system manager to choose |
manager to choose a |
to choose a trade |
pool i distributes this |
information about the remote |
it easier to program |
the case for the |
i distributes this revenue |
about the remote process |
easier to program distributed |
off between performance and |
between performance and consistency |
distributes this revenue among |
to program distributed algorithms |
case for the vanilla |
our evaluation shows that |
evaluation shows that t |
for the vanilla chain |
the report contains information |
this revenue among its |
cache detects many inconsistencies |
the vanilla chain replication |
report contains information about |
it is largely based |
revenue among its mi |
detects many inconsistencies with |
vanilla chain replication scheme |
contains information about the |
is largely based on |
many inconsistencies with only |
information about the results |
largely based on my |
inconsistencies with only nominal |
with only nominal overhead |
based on my experience |
about the results of |
however this eliminates the |
on my experience in |
i members loyal and |
the results of these |
this eliminates the opportunity |
we use synthetic workloads |
my experience in implementing |
members loyal and infiltrators |
results of these probes |
eliminates the opportunity to |
use synthetic workloads to |
experience in implementing algorithms |
the opportunity to load |
synthetic workloads to demonstrate |
in implementing algorithms such |
define the p p |
workloads to demonstrate the |
implementing algorithms such as |
early triggers many systems |
the p p infiltration |
to demonstrate the efficacy |
algorithms such as distributed |
such as distributed consensus |
p p infiltration matrix |
demonstrate the efficacy of |
faults and node restarts |
p infiltration matrix by |
the efficacy of t |
triggers many systems find |
and node restarts can |
infiltration matrix by its |
many systems find it |
node restarts can disrupt |
subject to your evaluation |
restarts can disrupt the |
systems find it desirable |
matrix by its i |
cache when data accesses |
to your evaluation of |
your evaluation of my |
evaluation of my proposal |
when data accesses are |
can disrupt the primary |
find it desirable to |
i would be happy |
disrupt the primary communication |
data accesses are clustered |
it desirable to detect |
would be happy to |
desirable to detect failure |
accesses are clustered and |
to detect failure of |
be happy to present |
detect failure of remote |
are clustered and its |
the primary communication pattern |
happy to present this |
failure of remote processes |
clustered and its adaptive |
primary communication pattern of |
to present this idea |
of remote processes even |
and its adaptive reaction |
communication pattern of the |
present this idea at |
this idea at the |
its adaptive reaction to |
adaptive reaction to workload |
remote processes even if |
idea at the workshop |
pattern of the ssa |
reaction to workload changes |
i ij and the |
processes even if there |
ipc allows processes to |
ij and the revenue |
even if there is |
allows processes to share |
with workloads based on |
workloads based on the |
based on the real |
processes to share information |
if the head of |
and the revenue vector |
if there is no |
to share information and |
the head of a |
the revenue vector at |
there is no data |
share information and to |
information and to synchronize |
and to synchronize actions |
is no data exchange |
head of a chain |
revenue vector at step |
no data exchange actually |
of a chain fails |
there are two classes |
are two classes of |
two classes of ipc |
data exchange actually under |
vector at step t |
exchange actually under way |
at step t is |
update sources will need |
step t is r |
of the inconsistencies and |
sources will need to |
the inconsistencies and increases |
will need to discover |
inconsistencies and increases the |
systems are free to |
need to discover a |
and increases the rate |
are free to implement |
mc has processes communicate |
increases the rate of |
to discover a new |
has processes communicate send |
free to implement whatever |
the rate of consistent |
to implement whatever scheme |
processes communicate send and |
implement whatever scheme they |
discover a new head |
rate of consistent transactions |
communicate send and receive |
send and receive messages |
of consistent transactions by |
whatever scheme they find |
scheme they find appropriate |
while sm allows processes |
they find appropriate and |
if an inner node |
sm allows processes to |
find appropriate and use |
an inner node crashes |
allows processes to share |
appropriate and use the |
inner node crashes the |
processes to share data |
and use the failure |
node crashes the chain |
to share data directly |
use the failure investigator |
crashes the chain may |
in the pool game |
share data directly while |
the pool game pools |
i ntroduction internet services |
the failure investigator from |
data directly while synchronizing |
the chain may break |
pool game pools try |
ntroduction internet services like |
failure investigator from the |
directly while synchronizing using |
game pools try to |
internet services like online |
investigator from the previous |
while synchronizing using such |
and if the tail |
pools try to optimize |
services like online retailers |
from the previous section |
synchronizing using such primitives |
if the tail crashes |
try to optimize their |
like online retailers and |
the previous section to |
using such primitives as |
to optimize their infiltration |
online retailers and social |
previous section to handle |
such primitives as mutexes |
acks might not be |
optimize their infiltration rates |
retailers and social networks |
section to handle the |
primitives as mutexes and |
as mutexes and condition |
mutexes and condition variables |
and social networks store |
to handle the suspicions |
might not be sent |
their infiltration rates of |
social networks store important |
not be sent back |
infiltration rates of other |
networks store important data |
where processes are physically |
processes are physically separated |
or they can make |
store important data sets |
rates of other pools |
they can make use |
important data sets in |
mc is dominant as |
of other pools to |
can make use of |
data sets in large |
is dominant as e |
other pools to maximize |
make use of two |
sets in large distributed |
dominant as e orts |
pools to maximize their |
use of two standardized |
or some of its |
in large distributed databases |
as e orts to |
to maximize their revenue |
of two standardized schemes |
some of its members |
e orts to support |
two standardized schemes implemented |
orts to support the |
technical challenges have forced |
challenges have forced such |
to support the sm |
standardized schemes implemented by |
have forced such large |
the overall number of |
support the sm paradigm |
overall number of miners |
schemes implemented by the |
processes will miss updates |
system operators to forgo |
number of miners and |
the sm paradigm have |
of miners and the |
will miss updates and |
operators to forgo transactional |
to forgo transactional consistency |
sm paradigm have not |
paradigm have not been |
have not been successful |
implemented by the failure |
providing perobject consistency instead |
miss updates and hence |
examples of sm include |
updates and hence queries |
miners and the number |
of sm include tcp |
and the number of |
and hence queries will |
often with some form |
sm include tcp connections |
by the failure manager |
the number of miners |
hence queries will return |
with some form of |
some form of eventual |
form of eventual consistency |
queries will return outdated |
the failure manager library |
number of miners loyal |
will return outdated results |
of miners loyal to |
the mc and sm |
miners loyal to each |
the first scheme uses |
mc and sm paradigms |
loyal to each pool |
to repair these inconsistencies |
and sm paradigms are |
first scheme uses a |
to each pool remain |
sm paradigms are duals |
scheme uses a heartbeat |
each pool remain constant |
paradigms are duals in |
uses a heartbeat mechanism |
the ssa implements a |
pool remain constant throughout |
are duals in that |
ssa implements a secondary |
remain constant throughout the |
duals in that one |
implements a secondary update |
which sends out i |
constant throughout the game |
in that one can |
a secondary update propagation |
that one can be |
secondary update propagation mechanism |
one can be implememted |
can be implememted using |
be implememted using the |
time progresses in rounds |
implememted using the other |
alive messages to a |
messages to a group |
it uses gossip protocols |
to a group of |
but they also each |
uses gossip protocols to |
let s be a |
a group of processes |
they also each have |
gossip protocols to rapidly |
s be a constant |
group of processes using |
also each have their |
protocols to rapidly detect |
be a constant integer |
support transactions with guarantees |
of processes using multiple |
each have their advantages |
to rapidly detect and |
a constant integer large |
transactions with guarantees such |
processes using multiple point |
have their advantages and |
rapidly detect and repair |
constant integer large enough |
with guarantees such as |
their advantages and disadvantages |
detect and repair inconsistencies |
integer large enough that |
guarantees such as snapshot |
advantages and disadvantages when |
large enough that revenue |
such as snapshot isolation |
and disadvantages when compared |
point messages or a |
enough that revenue can |
as snapshot isolation and |
disadvantages when compared with |
when compared with one |
compared with one another |
that revenue can be |
snapshot isolation and even |
while simultaneously orchestrating repair |
messages or a single |
revenue can be approximated |
isolation and even full |
it is useful to |
simultaneously orchestrating repair of |
or a single ip |
can be approximated as |
and even full transactional |
is useful to consider |
orchestrating repair of the |
be approximated as its |
even full transactional atomicity |
useful to consider how |
repair of the chain |
approximated as its convergence |
to consider how distributed |
our work begins with |
as its convergence limit |
each process keeps track |
the gossip rate can |
process keeps track of |
consider how distributed algorithms |
work begins with the |
begins with the observation |
with the observation that |
how distributed algorithms such |
in each round the |
keeps track of the |
gossip rate can be |
track of the reception |
each round the system |
distributed algorithms such as |
algorithms such as replication |
of the reception times |
it can be difficult |
can be difficult for |
be difficult for client |
round the system takes |
the reception times of |
rate can be tuned |
tier applications to leverage |
reception times of messages |
the system takes s |
applications to leverage the |
times of messages and |
system takes s steps |
with a higher rate |
typically it has much |
of messages and if |
to leverage the transactions |
messages and if a |
a higher rate overheads |
it has much to |
has much to do |
leverage the transactions that |
and if a number |
higher rate overheads rise |
takes s steps and |
much to do with |
to do with progress |
if a number of |
rate overheads rise but |
s steps and then |
the transactions that the |
a number of consecutive |
overheads rise but repair |
in order for some |
steps and then a |
transactions that the databases |
that the databases provide |
rise but repair occurs |
order for some process |
and then a single |
number of consecutive heartbeats |
but repair occurs more |
for some process to |
then a single pool |
their reads are satisfied |
of consecutive heartbeats from |
repair occurs more rapidly |
some process to be |
reads are satisfied primarily |
consecutive heartbeats from a |
process to be able |
are satisfied primarily from |
picked with a round |
heartbeats from a destination |
to be able to |
satisfied primarily from incoherent |
primarily from incoherent cache |
be able to make |
able to make a |
to make a transition |
from a destination is |
the benefits of caching |
benefits of caching are |
of caching are twofold |
repair is slower but |
may change its infiltration |
it needs to know |
a destination is missed |
is slower but overheads |
change its infiltration rates |
needs to know that |
destination is missed a |
slower but overheads drop |
it reduces database load |
its infiltration rates of |
to know that one |
is missed a suspicion |
thereby enabling higher throughput |
know that one or |
infiltration rates of all |
missed a suspicion is |
that one or more |
the subsections that follow |
rates of all other |
the caches are typically |
one or more other |
subsections that follow discuss |
a suspicion is raised |
of all other pools |
caches are typically placed |
or more other processes |
that follow discuss the |
are typically placed close |
more other processes have |
follow discuss the two |
typically placed close to |
other processes have reached |
the total revenue of |
discuss the two core |
placed close to the |
processes have reached a |
have reached a particular |
reached a particular milestone |
close to the clients |
fixed period or an |
the two core mechanisms |
and some data associated |
some data associated with |
total revenue of each |
two core mechanisms in |
period or an exponential |
data associated with that |
associated with that milestone |
core mechanisms in greater |
the problem centers on |
or an exponential back |
revenue of each step |
mechanisms in greater detail |
problem centers on the |
of each step is |
a new leader in |
centers on the asynchronous |
new leader in paxos |
each step is normalized |
on the asynchronous style |
leader in paxos needs |
step is normalized to |
the asynchronous style of |
a second class of |
in paxos needs to |
second class of faults |
asynchronous style of communication |
class of faults are |
paxos needs to know |
of faults are transient |
style of communication used |
fixed or estimated by |
faults are transient and |
needs to know that |
are transient and relate |
or estimated by the |
of communication used between |
so the revenue per |
transient and relate to |
estimated by the system |
to know that a |
communication used between the |
the revenue per round |
and relate to the |
know that a quorum |
used between the database |
revenue per round is |
relate to the behavior |
that a quorum of |
between the database and |
the database and the |
database and the geo |
a quorum of acceptors |
per round is one |
and multiple suspicion levels |
to the behavior of |
quorum of acceptors have |
multiple suspicion levels are |
the behavior of tcp |
the pool taking a |
a cache should not |
suspicion levels are configurable |
of acceptors have progressed |
behavior of tcp when |
pool taking a step |
cache should not access |
levels are configurable by |
acceptors have progressed to |
of tcp when a |
taking a step knows |
should not access the |
are configurable by the |
have progressed to its |
tcp when a node |
a step knows the |
not access the database |
configurable by the application |
progressed to its proposed |
when a node is |
step knows the rate |
access the database on |
to its proposed ballot |
a node is subjected |
knows the rate of |
the database on every |
its proposed ballot and |
the application can provide |
node is subjected to |
the rate of infiltrators |
database on every transaction |
proposed ballot and it |
application can provide application |
is subjected to stress |
rate of infiltrators attacking |
ballot and it needs |
can provide application specific |
any approach requiring a |
of infiltrators attacking it |
and it needs to |
provide application specific data |
approach requiring a high |
it needs to know |
such as a burst |
application specific data to |
requiring a high rate |
needs to know what |
as a burst of |
though not their identity |
specific data to be |
a high rate of |
to know what the |
a burst of traffic |
data to be piggybacked |
high rate of round |
know what the highest |
and the revenue rates |
to be piggybacked on |
what the highest accepted |
the revenue rates of |
be piggybacked on the |
trips to an authoritative |
the highest accepted proposals |
revenue rates of each |
piggybacked on the heartbeats |
to an authoritative backend |
highest accepted proposals from |
rates of each of |
of each of the |
the os tends to |
an authoritative backend database |
the second scheme uses |
each of the other |
os tends to lose |
accepted proposals from those |
tends to lose packets |
second scheme uses a |
of the other pools |
authoritative backend database would |
proposals from those acceptors |
from those acceptors are |
scheme uses a polling |
backend database would cause |
to lose packets and |
this knowledge is required |
database would cause unacceptable |
would cause unacceptable latency |
uses a polling method |
lose packets and the |
knowledge is required to |
many if not all |
a polling method to |
a cache must respond |
cache must respond instantly |
is required to optimize |
if not all distributed |
polling method to collect |
packets and the effect |
required to optimize a |
not all distributed algorithms |
method to collect acknowledgments |
and the effect is |
and asynchronous updates rule |
to optimize a pool |
all distributed algorithms can |
to collect acknowledgments from |
the effect is that |
asynchronous updates rule out |
optimize a pool s |
distributed algorithms can be |
collect acknowledgments from the |
effect is that tcp |
updates rule out cache |
a pool s revenue |
algorithms can be cleanly |
acknowledgments from the peer |
is that tcp will |
rule out cache coherency |
can be cleanly expressed |
be cleanly expressed this |
cleanly expressed this way |
out cache coherency schemes |
from the peer processes |
as we see next |
that tcp will impose |
cache coherency schemes that |
as a collection of |
tcp will impose congestion |
coherency schemes that would |
if no acknowledgments are |
will impose congestion control |
a collection of transition |
schemes that would require |
we explain in section |
no acknowledgments are received |
impose congestion control mechanisms |
collection of transition specifications |
that would require the |
explain in section viii |
acknowledgments are received after |
congestion control mechanisms and |
of transition specifications that |
would require the backend |
in section viii how |
are received after a |
control mechanisms and choke |
transition specifications that specify |
require the backend database |
section viii how a |
received after a number |
mechanisms and choke back |
specifications that specify under |
the backend database to |
viii how a pool |
after a number of |
that specify under which |
backend database to promptly |
how a pool can |
a number of retries |
specify under which conditions |
database to promptly invalidate |
updates will cease to |
a pool can technically |
number of retries a |
under which conditions they |
to promptly invalidate or |
will cease to propagate |
pool can technically obtain |
of retries a suspicion |
which conditions they are |
promptly invalidate or update |
cease to propagate down |
can technically obtain this |
retries a suspicion is |
conditions they are enabled |
invalidate or update cached |
to propagate down the |
technically obtain this knowledge |
a suspicion is raised |
they are enabled and |
or update cached this |
propagate down the chain |
are enabled and what |
update cached this work |
enabled and what state |
and what state they |
cached this work is |
what state they need |
this work is supported |
state they need from |
general analysis recall that |
they need from other |
need from other processes |
analysis recall that mi |
even though most of |
recall that mi is |
though most of the |
that mi is the |
note the similarity to |
by a grant from |
most of the nodes |
mi is the number |
the similarity to knowledge |
a grant from the |
and retransmission limits are |
of the nodes involved |
is the number of |
grant from the darpa |
from the darpa mrc |
the darpa mrc program |
while the sm paradigm |
retransmission limits are configurable |
the nodes involved could |
the number of miners |
the sm paradigm seems |
limits are configurable by |
or even to track |
are configurable by the |
sm paradigm seems the |
configurable by the application |
number of miners loyal |
even to track the |
nodes involved could still |
paradigm seems the best |
by the application or |
of miners loyal to |
to track the locations |
involved could still have |
seems the best fit |
the application or can |
miners loyal to pool |
track the locations at |
could still have ample |
the best fit for |
application or can be |
loyal to pool i |
the locations at which |
still have ample capacity |
best fit for this |
or can be adapted |
locations at which cached |
fit for this model |
can be adapted by |
at which cached objects |
for this model of |
be adapted by the |
which cached objects reside |
this model of distributed |
model of distributed algorithms |
adapted by the failure |
we will show that |
we define a variant |
by the failure manager |
will show that when |
the paradigm is hard |
define a variant of |
the failure manager to |
show that when such |
paradigm is hard to |
is the number of |
a variant of serializability |
failure manager to the |
that when such a |
is hard to make |
hard to make efficient |
variant of serializability called |
manager to the network |
when such a problem |
the number of miners |
of serializability called cacheserializability |
such a problem arises |
and scalable in a |
serializability called cacheserializability that |
instrumenting the operating system |
scalable in a physically |
number of miners used |
called cacheserializability that is |
gossip will route data |
of miners used by |
in a physically distributed |
to achieve greater failure |
will route data around |
miners used by pool |
cacheserializability that is suitable |
a physically distributed system |
achieve greater failure detection |
route data around the |
used by pool i |
that is suitable for |
greater failure detection accuracy |
it is notoriously errorprone |
by pool i to |
is suitable for incoherent |
suitable for incoherent caches |
is notoriously errorprone as |
pool i to infiltrate |
data around the congested |
a wide range of |
wide range of web |
range of web applications |
notoriously errorprone as programmers |
around the congested nodes |
i to infiltrate pool |
it is necessary to |
errorprone as programmers are |
from social networks to |
social networks to online |
networks to online retailers |
as programmers are having |
to infiltrate pool j |
is necessary to instrument |
and will also deliver |
programmers are having difficulty |
infiltrate pool j at |
settle for caches that |
necessary to instrument the |
will also deliver missed |
are having difficulty utilizing |
pool j at step |
for caches that are |
to instrument the operating |
also deliver missed updates |
having difficulty utilizing the |
j at step t |
caches that are oblivious |
instrument the operating environment |
deliver missed updates to |
difficulty utilizing the synchronization |
that are oblivious to |
are oblivious to transactions |
missed updates to the |
utilizing the synchronization primitives |
the operating environment with |
the mining rate of |
updates to the overloaded |
the synchronization primitives correctly |
despite the fact that |
operating environment with support |
mining rate of pool |
to the overloaded nodes |
the fact that an |
the mc paradigm can |
environment with support for |
rate of pool i |
the overloaded nodes when |
fact that an inconsistent |
mc paradigm can be |
with support for process |
of pool i is |
overloaded nodes when the |
that an inconsistent read |
paradigm can be used |
support for process investigation |
pool i is therefore |
nodes when the problem |
an inconsistent read access |
can be used instead |
i is therefore the |
when the problem ends |
inconsistent read access can |
it has always been |
is therefore the number |
be used instead but |
read access can deter |
has always been argued |
therefore the number of |
used instead but is |
access can deter a |
in the original chain |
always been argued that |
the number of its |
instead but is awkward |
can deter a client |
the original chain replication |
been argued that in |
number of its loyal |
but is awkward and |
deter a client and |
original chain replication scheme |
argued that in a |
of its loyal miners |
is awkward and error |
a client and reduce |
client and reduce their |
and reduce their income |
its loyal miners minus |
chain replication scheme the |
that in a distributed |
loyal miners minus the |
prone as well it |
replication scheme the queries |
they cannot afford consistent |
in a distributed system |
miners minus the miners |
as well it requires |
scheme the queries are |
cannot afford consistent cache |
a distributed system it |
minus the miners it |
well it requires the |
the queries are directed |
afford consistent cache techniques |
distributed system it is |
the miners it uses |
it requires the programmer |
queries are directed to |
consistent cache techniques that |
system it is impossible |
miners it uses for |
requires the programmer to |
are directed to the |
cache techniques that require |
it is impossible to |
it uses for infiltration |
the programmer to figure |
directed to the tail |
techniques that require backend |
is impossible to distinguish |
programmer to figure out |
to the tail of |
that require backend accesses |
impossible to distinguish a |
to figure out which |
the tail of the |
this effective mining rate |
require backend accesses on |
to distinguish a crashed |
figure out which processes |
tail of the chain |
effective mining rate is |
backend accesses on every |
accesses on every transaction |
out which processes should |
mining rate is divided |
distinguish a crashed process |
which processes should send |
since there is no |
a crashed process from |
rate is divided by |
processes should send which |
there is no additional |
crashed process from one |
is divided by the |
should send which data |
is no additional epidemic |
a novel caching scheme |
process from one that |
divided by the total |
send which data to |
no additional epidemic communication |
novel caching scheme that |
from one that is |
by the total mining |
which data to which |
caching scheme that improves |
one that is slow |
the total mining rate |
data to which destinations |
scheme that improves consistency |
any update known to |
total mining rate in |
to which destinations at |
that improves consistency at |
update known to the |
mining rate in the |
which destinations at which |
improves consistency at the |
rate in the system |
known to the tail |
destinations at which times |
to the tail is |
consistency at the cache |
the tail is stable |
at which times in |
tail is stable because |
at the cache level |
is stable because it |
which times in order |
namely the number of |
the cache level with |
but with the proper |
stable because it must |
times in order to |
the number of all |
cache level with a |
with the proper system |
because it must first |
in order to ensure |
number of all miners |
level with a nominal |
the proper system support |
it must first have |
order to ensure that |
of all miners that |
with a nominal storage |
proper system support this |
must first have been |
to ensure that recipients |
all miners that do |
a nominal storage and |
system support this is |
first have been seen |
ensure that recipients of |
miners that do not |
nominal storage and communication |
storage and communication tradeoff |
have been seen by |
that recipients of this |
that do not engage |
support this is no |
been seen by all |
recipients of this data |
do not engage in |
cache significantly improves consistency |
seen by all the |
of this data can |
this data can make |
data can make progress |
significantly improves consistency for |
by all the members |
this is no longer |
not engage in block |
improves consistency for workloads |
all the members of |
is no longer true |
engage in block withholding |
sometimes messages are lost |
consistency for workloads where |
the members of the |
messages are lost if |
for workloads where data |
members of the chain |
are lost if the |
if the node is |
workloads where data accesses |
lost if the receiver |
the node is reachable |
denote the direct mining |
if the receiver starts |
where data accesses are |
data accesses are clustered |
the direct mining rate |
to maintain such an |
the receiver starts execution |
node is reachable and |
direct mining rate of |
maintain such an invariant |
which is common in |
receiver starts execution after |
is reachable and operating |
mining rate of pool |
is common in today |
starts execution after the |
reachable and operating correctly |
rate of pool i |
common in today s |
execution after the sender |
the original paper includes |
of pool i at |
in today s large |
after the sender has |
original paper includes mechanisms |
pool i at step |
the operating system can |
the sender has started |
paper includes mechanisms to |
i at step t |
operating system can determine |
sender has started sending |
this is achieved while |
includes mechanisms to ensure |
at step t by |
system can determine whether |
has started sending messages |
is achieved while retaining |
mechanisms to ensure that |
step t by pp |
can determine whether or |
started sending messages to |
achieved while retaining the |
to ensure that a |
t by pp mi |
determine whether or not |
sending messages to it |
while retaining the global |
ensure that a request |
by pp mi j |
whether or not the |
retaining the global scalability |
that a request really |
or not the process |
the global scalability afforded |
a request really reaches |
not the process has |
global scalability afforded by |
request really reaches the |
pray semantics of connectionless |
scalability afforded by executing |
afforded by executing read |
really reaches the head |
semantics of connectionless or |
of connectionless or non |
reaches the head of |
the process has crashed |
only transactions on the |
transactions on the edge |
blocking messaging primitives is |
messaging primitives is one |
primitives is one example |
the head of the |
directly from the cache |
the failure management integrated |
head of the chain |
failure management integrated into |
we do this by |
management integrated into the |
do this by storing |
integrated into the os |
often needless information is |
this by storing dependency |
into the os offers |
needless information is sent |
that updates are passed |
the os offers processes |
by storing dependency information |
information is sent as |
updates are passed down |
os offers processes a |
storing dependency information with |
is sent as more |
are passed down the |
offers processes a mechanism |
dependency information with the |
sent as more recent |
passed down the chain |
processes a mechanism to |
information with the cached |
as more recent information |
down the chain and |
a mechanism to register |
with the cached objects |
more recent information makes |
the chain and applied |
mechanism to register and |
recent information makes old |
k the revenue density |
chain and applied in |
the revenue density of |
and applied in a |
revenue density of pool |
information makes old messages |
to identify possible inconsistencies |
applied in a strictly |
density of pool i |
makes old messages obsolete |
of pool i at |
identify possible inconsistencies without |
pool i at the |
to register and request |
in a strictly fifo |
possible inconsistencies without contacting |
using paxos again as |
paxos again as an |
again as an example |
a strictly fifo manner |
inconsistencies without contacting the |
without contacting the database |
register and request a |
strictly fifo manner even |
i at the end |
in the stream of |
at the end of |
fifo manner even when |
the user can improve |
and request a certain |
the stream of values |
the end of step |
manner even when nodes |
user can improve the |
request a certain level |
stream of values that |
of values that acceptors |
values that acceptors accept |
can improve the level |
a certain level of |
end of step t |
even when nodes fail |
improve the level of |
certain level of service |
only the most recent |
of step t is |
when nodes fail and |
the level of consistency |
the most recent one |
most recent one is |
recent one is of |
level of consistency by |
step t is its |
nodes fail and the |
one is of interest |
of consistency by adjusting |
t is its revenue |
fail and the chain |
but most mc implementations |
is its revenue from |
consistency by adjusting the |
and the chain is |
most mc implementations will |
its revenue from direct |
is a simple binary |
by adjusting the size |
the chain is restructured |
mc implementations will carefully |
revenue from direct mining |
a simple binary test |
adjusting the size of |
implementations will carefully deliver |
from direct mining together |
simple binary test performed |
the size of this |
will carefully deliver each |
direct mining together with |
and that queries are |
binary test performed by |
size of this dependency |
of this dependency data |
mining together with its |
that queries are sent |
test performed by the |
carefully deliver each and |
deliver each and every |
each and every one |
more dependency data leads |
performed by the os |
together with its revenue |
queries are sent to |
dependency data leads to |
data leads to increased |
leads to increased consistency |
delaying delivery of the |
are sent to the |
by the os upon |
with its revenue from |
delivery of the important |
sent to the tail |
the os upon receipt |
to demonstrate the efficacy |
its revenue from infiltrated |
of the important information |
to the tail of |
os upon receipt of |
demonstrate the efficacy of |
revenue from infiltrated pools |
the important information until |
the tail of the |
upon receipt of an |
the efficacy of the |
important information until all |
tail of the chain |
receipt of an inquiry |
efficacy of the proposed |
information until all obsoleted |
divided by the number |
of the proposed scheme |
until all obsoleted information |
by the number of |
strong consistency follows easily |
all obsoleted information has |
consistency follows easily because |
the number of its |
we created a prototype |
indicating whether the process |
obsoleted information has been |
follows easily because query |
number of its loyal |
created a prototype implementation |
whether the process is |
information has been delivered |
the process is still |
of its loyal miners |
a prototype implementation and |
has been delivered as |
been delivered as well |
process is still present |
its loyal miners together |
prototype implementation and exposed |
this leads to wasting |
leads to wasting resources |
loyal miners together with |
easily because query requests |
implementation and exposed it |
potential deadlock situations due |
miners together with block |
because query requests and |
is still present in |
and exposed it to |
deadlock situations due to |
query requests and update |
still present in the |
exposed it to workloads |
situations due to flow |
withholding infiltrators that attack |
requests and update requests |
present in the process |
it to workloads based |
due to flow control |
infiltrators that attack it |
and update requests are |
in the process table |
to workloads based on |
to flow control leading |
update requests are processed |
the process table and |
workloads based on graphically |
flow control leading to |
control leading to deadly |
leading to deadly embrace |
requests are processed serially |
process table and thus |
are processed serially at |
table and thus not |
and also obfuscates how |
also obfuscates how the |
obfuscates how the algorithms |
how the algorithms work |
and thus not has |
processed serially at the |
such as those seen |
as those seen in |
those seen in social |
thus not has crashed |
serially at the tail |
not has crashed or |
at the tail element |
has crashed or voluntary |
a new class of |
new class of ipc |
crashed or voluntary exited |
that that tries to |
that tries to combine |
tries to combine the |
the gossip based chain |
to combine the best |
the two other levels |
two other levels that |
combine the best features |
gossip based chain replication |
other levels that are |
the best features of |
best features of sm |
of the inconsistencies and |
based chain replication weakens |
features of sm and |
of sm and mc |
the inconsistencies and can |
chain replication weakens the |
levels that are currently |
from sm it inherits |
replication weakens the model |
inconsistencies and can increase |
that are currently implemented |
sm it inherits direct |
weakens the model in |
and can increase the |
it inherits direct access |
the model in two |
can increase the ratio |
inherits direct access to |
direct access to and |
increase the ratio of |
provide a remote process |
model in two key |
a remote process with |
the ratio of consistent |
access to and synchro |
in two key respects |
remote process with information |
ratio of consistent transactions |
of consistent transactions by |
process with information about |
nization on state rather |
with information about the |
on state rather than |
information about the progress |
state rather than providing |
about the progress the |
rather than providing a |
the progress the local |
than providing a stream |
progress the local process |
our solution might sometimes |
providing a stream of |
a stream of state |
stream of state updates |
the local process is |
solution might sometimes use |
local process is making |
both with low overhead |
while from mc it |
might sometimes use the |
process is making which |
from mc it inherits |
sometimes use the wrong |
we construct synthetic workloads |
is making which is |
mc it inherits an |
use the wrong head |
construct synthetic workloads and |
making which is useful |
it inherits an efficient |
the wrong head of |
synthetic workloads and observe |
workloads and observe how |
and observe how t |
wrong head of the |
which is useful in |
inherits an efficient implementation |
hereinafter we move to |
head of the chain |
is useful in the |
cache reacts to different |
an efficient implementation over |
we move to a |
useful in the investigation |
reacts to different clustering |
efficient implementation over the |
move to a static |
in the investigation of |
to different clustering levels |
implementation over the existing |
to a static state |
for example if an |
the investigation of processes |
different clustering levels and |
over the existing physical |
the existing physical infrastructure |
example if an update |
investigation of processes that |
clustering levels and how |
a static state analysis |
if an update source |
of processes that are |
levels and how it |
the concept is that |
static state analysis and |
an update source is |
processes that are alive |
and how it adapts |
concept is that processes |
is that processes publish |
that processes publish facts |
how it adapts as |
state analysis and omit |
update source is operating |
but that appear slow |
analysis and omit the |
it adapts as clusters |
which are information about |
source is operating with |
that appear slow or |
and omit the t |
adapts as clusters change |
are information about milestones |
is operating with inaccurate |
appear slow or unresponsive |
omit the t argument |
information about milestones they |
about milestones they have |
with perfectly clustered workloads |
the t argument in |
operating with inaccurate membership |
milestones they have reached |
t argument in the |
with inaccurate membership information |
argument in the expressions |
cache implements full cache |
and subscribe to new |
subscribe to new facts |
at certain intervals the |
certain intervals the process |
intervals the process logs |
the ipc interface is |
the process logs checkpoint |
ipc interface is similar |
to explain this perfect |
updates might sometimes arrive |
interface is similar to |
is similar to topic |
explain this perfect behavior |
might sometimes arrive out |
process logs checkpoint timestamps |
this perfect behavior we |
sometimes arrive out of |
logs checkpoint timestamps with |
perfect behavior we prove |
arrive out of order |
checkpoint timestamps with the |
since the row sums |
behavior we prove a |
but there are several |
we prove a related |
timestamps with the failure |
for example if the |
there are several important |
example if the chain |
the row sums of |
with the failure service |
prove a related claim |
are several important semantic |
if the chain is |
row sums of the |
a related claim we |
several important semantic differences |
the chain is disrupted |
sums of the infiltration |
related claim we show |
which simultaneously logs the |
chain is disrupted by |
of the infiltration matrix |
claim we show that |
simultaneously logs the process |
is disrupted by a |
interface is as follows |
disrupted by a failure |
the infiltration matrix are |
we show that with |
by a failure and |
infiltration matrix are smaller |
show that with unbounded |
a failure and some |
it will be the |
that with unbounded resources |
matrix are smaller than |
failure and some updates |
will be the publishers |
with unbounded resources t |
the response to an |
are smaller than one |
and some updates arrive |
be the publishers that |
response to an inquiry |
some updates arrive via |
its largest eigenvalue is |
to an inquiry request |
the publishers that actively |
updates arrive via the |
largest eigenvalue is smaller |
an inquiry request holds |
publishers that actively try |
arrive via the gossip |
eigenvalue is smaller than |
the contributions of this |
inquiry request holds the |
that actively try to |
via the gossip protocol |
contributions of this work |
according to the perron |
actively try to push |
of this work are |
request holds the last |
try to push new |
holds the last checkpoint |
to push new facts |
these changes substantially simplify |
the last checkpoint timestamp |
push new facts to |
changes substantially simplify the |
new facts to the |
facts to the subscribers |
substantially simplify the algorithm |
the revenues at all |
simplify the algorithm but |
the current local time |
revenues at all pools |
the algorithm but they |
paxos leaders publish new |
a variant of serializability |
at all pools converge |
algorithm but they also |
leaders publish new ballots |
variant of serializability suitable |
all pools converge as |
whether the process has |
but they also weaken |
publish new ballots and |
of serializability suitable for |
pools converge as follows |
the process has been |
they also weaken the |
new ballots and push |
serializability suitable for incoherent |
process has been allocated |
also weaken the properties |
ballots and push these |
suitable for incoherent caches |
has been allocated cpu |
weaken the properties of |
and push these to |
been allocated cpu time |
the properties of the |
push these to acceptors |
allocated cpu time since |
properties of the solution |
these to acceptors as |
cpu time since the |
to acceptors as acceptors |
time since the last |
acceptors as acceptors do |
since the last checkpoint |
as acceptors do not |
a less significant change |
which allows trading off |
acceptors do not necessarily |
less significant change is |
allows trading off efficiency |
and whether the process |
significant change is that |
do not necessarily know |
trading off efficiency and |
off efficiency and transaction |
change is that we |
not necessarily know what |
whether the process has |
is that we load |
necessarily know what the |
the process has consumed |
consistency in large scale |
know what the set |
process has consumed any |
in large scale cache |
balance queries over the |
has consumed any messages |
what the set of |
large scale cache deployments |
queries over the members |
consumed any messages since |
the set of leaders |
set of leaders is |
any messages since the |
over the members of |
messages since the last |
the members of the |
since the last checkpoint |
old ballots are automatically |
members of the chain |
ballots are automatically dropped |
are automatically dropped from |
cache with synthetic workloads |
automatically dropped from the |
dropped from the transmission |
from the transmission queue |
demonstrating its adaptivity and |
its adaptivity and sensitivity |
adaptivity and sensitivity to |
and sensitivity to clustering |
upon receipt of an |
receipt of an inquiry |
but in ways that |
it will be the |
of an inquiry the |
in ways that seem |
will be the subscribers |
an inquiry the operating |
ways that seem to |
be the subscribers that |
inquiry the operating system |
that seem to match |
the subscribers that actively |
cache with workloads based |
seem to match the |
the operating system uses |
subscribers that actively poll |
with workloads based on |
workloads based on graphically |
operating system uses an |
that actively poll the |
to match the class |
system uses an upcall |
actively poll the publishers |
match the class of |
the class of applications |
world data demonstrating detection |
class of applications of |
data demonstrating detection rates |
of applications of interest |
the pool game if |
demonstrating detection rates of |
leaders and learners both |
pool game if no |
and learners both subscribe |
to interrupt the process |
and has the potential |
learners both subscribe to |
game if no pool |
interrupt the process and |
has the potential to |
both subscribe to acceptors |
if no pool engages |
the process and requests |
the potential to greatly |
subscribe to acceptors accepting |
no pool engages in |
process and requests that |
and consistency improvements of |
to acceptors accepting pvalues |
pool engages in block |
potential to greatly improve |
and requests that the |
acceptors accepting pvalues and |
engages in block withholding |
to greatly improve query |
requests that the process |
accepting pvalues and poll |
greatly improve query performance |
that the process prepares |
pvalues and poll for |
the process prepares a |
and poll for these |
poll for these facts |
process prepares a special |
prepares a special response |
this response is returned |
response is returned to |
is returned to the |
returned to the caller |
cache with unbounded resources |
with unbounded resources implements |
epidemic dissemination as noted |
unbounded resources implements cache |
as subscribers that su |
dissemination as noted earlier |
subscribers that su ered |
and we have i |
the previous sections all |
that su ered communication |
previous sections all deal |
su ered communication loss |
sections all deal with |
ssa uses gossip to |
ered communication loss due |
uses gossip to detect |
all deal with provisions |
communication loss due to |
gossip to detect and |
the complexity of implementing |
to detect and repair |
loss due to a |
detect and repair the |
deal with provisions targeted |
complexity of implementing geo |
due to a network |
and repair the inconsistencies |
with provisions targeted towards |
to a network partition |
scale databases with strong |
provisions targeted towards the |
repair the inconsistencies that |
a network partition or |
databases with strong guarantees |
targeted towards the failure |
the inconsistencies that can |
network partition or having |
each miner s revenue |
with strong guarantees initially |
towards the failure management |
inconsistencies that can arise |
partition or having been |
miner s revenue is |
strong guarantees initially led |
the failure management of |
that can arise after |
or having been temporarily |
s revenue is proportional |
guarantees initially led companies |
failure management of processes |
can arise after a |
having been temporarily subscribe |
revenue is proportional to |
initially led companies to |
arise after a failure |
is proportional to its |
led companies to abandon |
companies to abandon cross |
exploiting the close coupled |
proportional to its power |
after a failure or |
the close coupled nature |
a failure or when |
object consistency altogether and |
close coupled nature of |
failure or when a |
consistency altogether and make |
be it in a |
coupled nature of a |
or when a node |
altogether and make do |
it in a pool |
nature of a process |
when a node joins |
due to a user |
in a pool or |
of a process and |
and make do with |
to a user closing |
a pool or working |
a process and the |
make do with weak |
a user closing a |
pool or working solo |
the basic idea is |
process and the operating |
do with weak guarantees |
user closing a laptop |
basic idea is simple |
and the operating system |
with weak guarantees such |
the operating system it |
weak guarantees such as |
recall that difficulty is |
guarantees such as per |
operating system it runs |
that difficulty is only |
will the interface requires |
each process in the |
system it runs under |
difficulty is only adjusted |
the interface requires that |
object atomicity or eventual |
atomicity or eventual consistency |
is only adjusted periodically |
interface requires that the |
to aid accurate detection |
requires that the fact |
aid accurate detection in |
process in the system |
and there are transient |
such systems do repair |
that the fact type |
in the system runs |
accurate detection in the |
there are transient effects |
systems do repair any |
the fact type for |
fact type for a |
type for a par |
are transient effects that |
do repair any problems |
the system runs a |
detection in the case |
transient effects that are |
repair any problems that |
any problems that arise |
continue to poll publishers |
in the case of |
effects that are not |
system runs a periodic |
to poll publishers to |
the case of node |
that are not covered |
runs a periodic local |
poll publishers to receive |
case of node failure |
user is sometimes exposed |
of node failure the |
publishers to receive facts |
are not covered by |
a periodic local timer |
is sometimes exposed to |
node failure the fault |
to receive facts they |
not covered by this |
sometimes exposed to inconsistency |
failure the fault management |
receive facts they have |
covered by this stable |
without synchronization across processes |
the fault management system |
facts they have ticular |
for some applications this |
fault management system implements |
they have ticular topic |
when a timer expires |
management system implements a |
some applications this is |
have ticular topic is |
we discuss this in |
system implements a node |
applications this is acceptable |
ticular topic is totally |
discuss this in section |
a process computes a |
implements a node management |
topic is totally ordered |
this in section viii |
process computes a summary |
and the approach has |
a node management service |
the approach has been |
and those facts will |
miners miners miners a |
approach has been surprisingly |
also called a digest |
those facts will missed |
has been surprisingly successful |
which is based on |
is based on the |
in today s cloud |
all this is invisible |
based on the experience |
controls its infiltration rate |
this is invisible to |
relaxed consistency is something |
its infiltration rate of |
on the experience that |
is invisible to the |
the experience that local |
infiltration rate of pool |
consistency is something of |
invisible to the core |
a list of things |
experience that local failure |
is something of a |
to the core application |
list of things that |
that local failure investigation |
something of a credo |
local failure investigation on |
of things that it |
the core application be |
failure investigation on a |
things that it knows |
core application be delivered |
investigation on a subnet |
application be delivered in |
be delivered in order |
on a subnet is |
a subnet is more |
this summary is sent |
subnet is more accurate |
summary is sent to |
is more accurate than |
is sent to a |
any data can be |
data can be made |
can be made to |
and will choose the |
more accurate than investigation |
sent to a randomly |
will choose the value |
only transactions by accessing |
to a randomly selected |
accurate than investigation over |
choose the value that |
transactions by accessing caches |
a randomly selected peer |
but can be managed |
than investigation over the |
the value that maximizes |
can be managed through |
investigation over the internet |
which receive their values |
value that maximizes the |
be managed through the |
or subset of peers |
receive their values by |
that maximizes the revenue |
managed through the contally |
their values by reading |
maximizes the revenue density |
on a participating subnet |
through the contally ordered |
values by reading from |
a participating subnet one |
the contally ordered by |
by reading from the |
quick delivery is more |
contally ordered by tagging |
participating subnet one or |
reading from the database |
delivery is more important |
ordered by tagging it |
subnet one or more |
is more important than |
by tagging it with |
one or more node |
more important than reliability |
tagging it with a |
or more node failure |
important than reliability for |
on the first round |
it with a sequence |
with a sequence number |
update transactions go directly |
than reliability for gossip |
the first round of |
more node failure monitors |
transactions go directly to |
reliability for gossip messages |
the hope is that |
go directly to the |
first round of the |
hope is that fact |
directly to the database |
round of the pool |
of the pool game |
hence we favor udp |
we favor udp datagrams |
based ipc will simplify |
favor udp datagrams over |
ipc will simplify disbut |
the value of r |
udp datagrams over tcp |
these are simple services |
will simplify disbut often |
datagrams over tcp for |
subsequent cache invalidations can |
is maximized at a |
simplify disbut often times |
over tcp for this |
are simple services capable |
cache invalidations can be |
maximized at a single |
disbut often times facts |
tcp for this kind |
simple services capable of |
invalidations can be delayed |
at a single point |
often times facts such |
for this kind of |
services capable of performing |
can be delayed or |
a single point in |
times facts such as |
this kind of communication |
capable of performing local |
be delayed or even |
single point in the |
facts such as ballots |
of performing local failure |
delayed or even lost |
point in the feasible |
such as ballots are |
the recipient compares the |
performing local failure investigations |
or even lost due |
in the feasible range |
as ballots are totally |
recipient compares the gossiped |
local failure investigations upon |
compares the gossiped information |
ballots are totally ortributed |
even lost due to |
failure investigations upon requests |
the gossiped information with |
are totally ortributed programming |
lost due to race |
due to race conditions |
gossiped information with its |
totally ortributed programming and |
investigations upon requests from |
upon requests from remote |
ortributed programming and make |
information with its own |
leading to a potentially |
requests from remote nodes |
programming and make it |
with its own state |
to a potentially inconsistent |
and make it easier |
a potentially inconsistent view |
make it easier to |
potentially inconsistent view by |
it easier to reason |
inconsistent view by the |
easier to reason dered |
view by the cache |
identifying information known to |
cannot not react to |
by the cache clients |
to reason dered already |
information known to the |
not react to pool |
known to the sender |
multicast to announce their |
to the sender but |
to announce their availability |
the sender but unknown |
given a stream of |
announce their availability within |
sender but unknown to |
a stream of facts |
their availability within the |
this point is the |
but unknown to itself |
stream of facts on |
availability within the organization |
point is the stable |
large internet services store |
of facts on some |
facts on some topic |
is the stable state |
internet services store vast |
within the organization where |
or known to it |
the stable state of |
services store vast amounts |
about safety and liveness |
the organization where their |
known to it but |
stable state of the |
store vast amounts of |
vast amounts of data |
to it but apparently |
state of the system |
the argument for this |
organization where their presence |
online retailers such as |
where their presence is |
it but apparently unknown |
argument for this is |
retailers such as amazon |
their presence is being |
and we denote the |
but apparently unknown to |
for this is only |
such as amazon and |
presence is being tracked |
we denote the value |
apparently unknown to the |
this is only the |
as amazon and ebay |
is being tracked by |
denote the value of |
unknown to the sender |
is only the highest |
amazon and ebay maintain |
being tracked by the |
the value of x |
and ebay maintain product |
tracked by the other |
most recent fact need |
it then sends back |
by the other nfm |
ebay maintain product stocks |
recent fact need be |
then sends back a |
maintain product stocks and |
fact need be delivered |
sends back a gossip |
product stocks and information |
need be delivered that |
be delivered that the |
back a gossip reply |
delivered that the paradigm |
that the paradigm allows |
and social networking sites |
an nfm accepts queries |
the paradigm allows the |
social networking sites such |
nfm accepts queries from |
paradigm allows the programmer |
networking sites such as |
accepts queries from remote |
allows the programmer to |
sites such as facebook |
queries from remote nodes |
the programmer to clearly |
using an unreliable datagram |
such as facebook and |
from remote nodes about |
programmer to clearly eventually |
an unreliable datagram protocol |
as facebook and twitter |
remote nodes about the |
facebook and twitter maintain |
while older facts can |
nodes about the availability |
and twitter maintain graphical |
older facts can be |
facts can be dropped |
twitter maintain graphical databases |
about the availability of |
containing information the sender |
maintain graphical databases representing |
the availability of a |
also specify transitions and |
information the sender might |
graphical databases representing user |
availability of a node |
specify transitions and under |
the sender might find |
databases representing user relations |
of a node within |
transitions and under which |
and the values of |
sender might find useful |
representing user relations and |
a node within its |
and under which conditions |
the values of the |
might find useful and |
user relations and group |
node within its organization |
under which conditions they |
values of the corresponding |
find useful and requesting |
relations and group structures |
which conditions they di |
of the corresponding revenues |
useful and requesting information |
conditions they di erent |
the corresponding revenues of |
and requesting information it |
it will forward this |
they di erent from |
di erent from pub |
requesting information it lacks |
corresponding revenues of the |
will forward this request |
revenues of the pools |
of the pools with |
forward this request to |
such databases are sharded |
the pools with r |
if no more facts |
this request to an |
databases are sharded and |
no more facts are |
request to an nfm |
are sharded and replicated |
to an nfm on |
more facts are enabled |
an nfm on the |
the originator of the |
facts are enabled without |
originator of the exchange |
nfm on the particular |
the vast majority of |
on the particular subnet |
of the exchange will |
substituting the stable value |
are enabled without having |
vast majority of accesses |
the particular subnet which |
the exchange will send |
the stable value x |
enabled without having to |
majority of accesses are |
of accesses are read |
exchange will send a |
without having to worry |
particular subnet which will |
will send a final |
having to worry much |
subnet which will investigate |
send a final message |
to worry much about |
which will investigate the |
a final message containing |
worry much about how |
we obtain the revenues |
will investigate the availability |
final message containing any |
much about how are |
obtain the revenues of |
investigate the availability of |
message containing any data |
about how are published |
the revenues of the |
the availability of the |
containing any data that |
how are published but |
revenues of the two |
of the two pools |
any data that was |
are published but some |
availability of the node |
data that was solicited |
all are given in |
of the node by |
published but some process |
that was solicited by |
are given in figure |
the node by launching |
but some process later |
was solicited by the |
node by launching a |
some process later subscribes |
solicited by the receiver |
by launching a number |
launching a number of |
a number of fault |
number of fault test |
it these conditions are |
of fault test requests |
these conditions are discovered |
gossip messages are bounded |
to reduce database load |
messages are bounded in |
to simplify the expressions |
reduce database load and |
are bounded in size |
will eventually receive the |
database load and to |
if this is support |
eventually receive the most |
load and to reduce |
this is support by |
receive the most recent |
and to reduce access |
to reduce access latency |
is support by the |
the most recent fact |
thus during a round |
these companies employ a |
support by the host |
companies employ a twotier |
assuming both publisher and |
by the host under |
employ a twotier structure |
during a round each |
both publisher and subscriber |
the host under investigation |
placing layers of cache |
publisher and subscriber are |
and subscriber are correct |
host under investigation or |
layers of cache servers |
a round each process |
under investigation or by |
of cache servers in |
round each process will |
investigation or by icmp |
cache servers in front |
these semantics are similar |
each process will send |
or by icmp echo |
servers in front of |
semantics are similar to |
are similar to the |
by icmp echo requests |
in front of the |
front of the database |
similar to the anti |
icmp echo requests if |
process will send a |
echo requests if not |
will send a message |
entropy style of gossip |
style of gossip protocols |
the result of the |
but the underlying implementation |
result of the query |
the underlying implementation can |
the caches of primary |
of the query is |
underlying implementation can be |
perhaps eliciting a reply |
caches of primary interest |
the query is then |
implementation can be anything |
of primary interest to |
query is then returned |
primary interest to us |
is then returned to |
interest to us are |
then returned to the |
there is also a |
and perhaps will respond |
returned to the requesting |
to us are typically |
is also a control |
perhaps will respond to |
to the requesting node |
us are typically situated |
also a control interface |
will respond to that |
are typically situated far |
a control interface that |
respond to that reply |
the nfm also functions |
control interface that controls |
nfm also functions as |
typically situated far from |
interface that controls routing |
that controls routing of |
also functions as proxy |
situated far from the |
functions as proxy for |
controls routing of facts |
as proxy for process |
o ne attacker we |
in the worst case |
far from the backend |
routing of facts for |
proxy for process availability |
ne attacker we begin |
from the backend database |
of facts for a |
for process availability queries |
a round results in |
the backend database systems |
facts for a particular |
for a particular topic |
process availability queries in |
backend database systems to |
availability queries in the |
database systems to reduce |
queries in the case |
systems to reduce latency |
in the case where |
attacker we begin our |
paxos acceptors subscribe to |
the case where a |
we begin our analysis |
companies place caches close |
acceptors subscribe to ballots |
case where a firewall |
begin our analysis with |
place caches close to |
subscribe to ballots and |
where a firewall obstructs |
the load imposed on |
our analysis with a |
caches close to clients |
to ballots and to |
a firewall obstructs the |
load imposed on the |
timeouts are used to |
imposed on the network |
firewall obstructs the free |
analysis with a simplified |
ballots and to new |
are used to ensure |
on the network will |
obstructs the free querying |
with a simplified game |
and to new proposals |
used to ensure that |
the network will thus |
the free querying of |
free querying of the |
to new proposals from |
to ensure that stale |
network will thus be |
a simplified game of |
querying of the nodes |
new proposals from leaders |
ensure that stale cached |
will thus be linear |
simplified game of two |
of the nodes by |
that stale cached objects |
thus be linear in |
game of two pools |
when the leader publishes |
the nodes by their |
stale cached objects will |
be linear in the |
the leader publishes one |
nodes by their peers |
cached objects will eventually |
linear in the number |
leader publishes one of |
objects will eventually be |
will eventually be flushed |
publishes one of these |
in the number of |
the number of processes |
but to achieve a |
it is transmitted to |
to achieve a high |
is transmitted to all |
s are configured with |
achieve a high cache |
transmitted to all subscribers |
are configured with domain |
a high cache hit |
configured with domain and |
high cache hit ratio |
but any individual process |
with domain and acl |
and the underlying communication |
timeout values are generally |
the underlying communication layer |
any individual process will |
values are generally large |
domain and acl mechanisms |
underlying communication layer will |
individual process will see |
to obtain reasonable consistency |
process will see a |
and acl mechanisms to |
communication layer will continue |
will see a constant |
acl mechanisms to control |
the database sends an |
layer will continue retransmission |
see a constant load |
mechanisms to control access |
database sends an asynchronous |
will continue retransmission until |
to control access to |
sends an asynchronous stream |
continue retransmission until either |
control access to the |
an asynchronous stream of |
retransmission until either acknowledged |
independent of system size |
asynchronous stream of invalidation |
access to the information |
until either acknowledged or |
stream of invalidation records |
either acknowledged or another |
of invalidation records or |
acknowledged or another fact |
invalidation records or cache |
or another fact renders |
an extension which is |
records or cache updates |
extension which is under |
another fact renders it |
which is under investigation |
fact renders it obsolete |
is under investigation is |
often using protocols optimized |
under investigation is to |
using protocols optimized for |
the ssa gossips about |
investigation is to have |
protocols optimized for throughput |
ssa gossips about membership |
is to have nodes |
optimized for throughput and |
to have nodes multicast |
for throughput and freshness |
have nodes multicast heartbeats |
throughput and freshness and |
nodes multicast heartbeats with |
and freshness and lacking |
multicast heartbeats with local |
freshness and lacking absolute |
heartbeats with local node |
and lacking absolute guarantees |
with local node information |
lacking absolute guarantees of |
local node information periodically |
absolute guarantees of order |
guarantees of order or |
of order or reliability |
miners outside both pools |
recoveries and application state |
outside both pools mine |
this information can be |
information can be collected |
it is difficult to |
both pools mine solo |
can be collected by |
is difficult to make |
be collected by the |
using this information to |
difficult to make this |
collected by the local |
this information to initiate |
to make this invalidation |
or with closed pools |
information to initiate repairs |
by the local nfm |
make this invalidation mechanism |
this invalidation mechanism reliable |
invalidation mechanism reliable without |
mechanism reliable without hampering |
reliable without hampering database |
without hampering database efficiency |
s and shared in |
with closed pools that |
and shared in compressed |
one form of repair |
the issues are many |
closed pools that do |
shared in compressed form |
form of repair involves |
pools that do not |
the databases are large |
of repair involves disruption |
in compressed form among |
compressed form among the |
repair involves disruption to |
that do not attack |
residing on many servers |
form among the other |
involves disruption to a |
do not attack and |
among the other nfm |
disruption to a chain |
live network streaming with |
network streaming with utilities |
streaming with utilities and |
with utilities and cost |
databases use locks prudently |
use locks prudently in |
s in the organization |
utilities and cost ymir |
and cost ymir vigfusson |
locks prudently in order |
prudently in order to |
in order to maximize |
order to maximize concurrency |
not attack and cannot |
if a fault breaks |
local system management tools |
attack and cannot be |
to the extent that |
a fault breaks a |
system management tools can |
and cannot be attacked |
the extent that the |
fault breaks a chain |
management tools can connect |
extent that the database |
breaks a chain or |
tools can connect to |
that the database keeps |
a chain or disables |
can connect to an |
the database keeps track |
chain or disables the |
connect to an nfm |
database keeps track of |
or disables the head |
to an nfm to |
keeps track of the |
disables the head of |
the head of a |
an nfm to retrieve |
track of the caches |
of the caches that |
head of a chain |
nfm to retrieve the |
this scenario is illustrated |
the caches that hold |
freedman school of computer |
to retrieve the information |
retrieve the information and |
caches that hold a |
school of computer science |
scenario is illustrated in |
gossip is used to |
that hold a copy |
the information and set |
is used to detect |
hold a copy of |
a copy of each |
copy of each object |
information and set trap |
used to detect the |
iceland of computer science |
and set trap conditions |
is illustrated in figure |
it may be possible |
to detect the problem |
may be possible to |
be possible to send |
possible to send an |
to send an invalidation |
detect the problem and |
in distributed systems build |
but tracking the state |
the problem and repair |
distributed systems build on |
tracking the state of |
usa school of electronics |
problem and repair involves |
the dashed red arrow |
and repair involves designating |
school of electronics engineering |
systems build on top |
the state of caches |
state of caches is |
of electronics engineering and |
repair involves designating a |
build on top of |
involves designating a new |
of caches is complicated |
electronics engineering and computer |
engineering and computer science |
on top of a |
caches is complicated and |
designating a new head |
dashed red arrow indicates |
top of a web |
is complicated and hence |
a new head for |
red arrow indicates that |
of a web of |
complicated and hence if |
new head for the |
arrow indicates that x |
a web of interconnected |
and hence if they |
head for the chain |
web of interconnected networks |
hence if they are |
if they are used |
they are used at |
are used at all |
for the chain or |
the chain or establishing |
chain or establishing a |
such systems view invalidations |
or establishing a new |
systems view invalidations as |
view invalidations as a |
establishing a new tcp |
we have to take |
invalidations as a kind |
as a kind of |
a kind of hint |
a new tcp connection |
have to take network |
new tcp connection bridging |
to take network failure |
they could be delayed |
tcp connection bridging the |
take network failure into |
connection bridging the gap |
s mining power infiltrates |
network failure into account |
mining power infiltrates pool |
due to buffering or |
failures at network level |
to buffering or retransmissions |
a second form of |
at network level are |
buffering or retransmissions after |
or retransmissions after message |
retransmissions after message loss |
network level are in |
with a block withholding |
department abstract the growth |
level are in general |
abstract the growth in |
are in general related |
the growth in internet |
a block withholding attack |
second form of repair |
in general related to |
growth in internet traffic |
form of repair involves |
general related to crash |
in internet traffic associated |
of repair involves lost |
related to crash failures |
internet traffic associated with |
repair involves lost updates |
due to an inaccurate |
to crash failures of |
traffic associated with video |
to an inaccurate list |
associated with video streaming |
crash failures of routers |
an inaccurate list of |
inaccurate list of locations |
with video streaming and |
failures of routers and |
of routers and gateways |
video streaming and sharing |
does not engage in |
not engage in block |
streaming and sharing of |
or to severe degradation |
engage in block withholding |
if subservice a has |
and sharing of videos |
to severe degradation of |
subservice a has a |
sharing of videos is |
severe degradation of the |
due to a system |
of videos is so |
a has a member |
degradation of the service |
to a system configuration |
a system configuration change |
videos is so rapid |
has a member m |
of the service level |
the service level due |
is so rapid that |
a member m that |
all of its m |
service level due to |
so rapid that it |
or because of races |
because of races between |
of races between reads |
rapid that it may |
member m that knows |
level due to network |
that it may soon |
due to network congestion |
loyal miners work on |
it may soon dwarf |
may soon dwarf all |
soon dwarf all other |
dwarf all other forms |
all other forms of |
causing minimum performance requirements |
a missing invalidation obviously |
other forms of internet |
forms of internet content |
minimum performance requirements to |
performance requirements to be |
missing invalidation obviously leaves |
we assume that all |
one reason for this |
requirements to be violated |
invalidation obviously leaves the |
obviously leaves the corresponding |
reason for this is |
assume that all forms |
miners work on its |
leaves the corresponding cache |
for this is that |
the failure investigator will |
that all forms of |
work on its behalf |
the corresponding cache entry |
this is that only |
when not able to |
corresponding cache entry stale |
all forms of information |
is that only some |
not able to reach |
forms of information are |
that only some forms |
pitfalls of such invalidation |
able to reach the |
only some forms of |
of information are uniquely |
of such invalidation schemes |
information are uniquely named |
some forms of content |
are uniquely named and |
such invalidation schemes are |
uniquely named and that |
forms of content can |
named and that updates |
invalidation schemes are described |
on the other hand |
of content can be |
content can be cached |
to reach the node |
schemes are described in |
are described in detail |
and that updates are |
reach the node under |
the other hand does |
described in detail by |
that updates are ordered |
the node under investigation |
other hand does not |
in detail by nishita |
updates are ordered separately |
data generated in real |
node under investigation or |
under investigation or a |
detail by nishita et |
by nishita et al |
generated in real time |
in real time such |
investigation or a relevant |
are ordered separately by |
hand does not employ |
real time such as |
or a relevant nfm |
ordered separately by each |
does not employ x |
time such as by |
separately by each update |
such as by live |
perform a path search |
by each update source |
as by live video |
and by bronson et |
by bronson et al |
by live video broadcasts |
a path search to |
path search to find |
search to find the |
to find the trouble |
find the trouble spot |
of update x and |
the trouble spot in |
update x and a |
trouble spot in the |
x and a member |
spot in the network |
and a member m |
of its loyal miners |
a member m that |
iptv or new episodes |
but forgoing transactional consistency |
member m that lacks |
or new episodes of |
it uses the traceroute |
forgoing transactional consistency can |
m that lacks x |
new episodes of popular |
uses the traceroute technique |
transactional consistency can result |
consistency can result in |
episodes of popular tv |
of popular tv shows |
and its direct mining |
can result in undesired |
gossip can be used |
its direct mining power |
the traceroute technique of |
result in undesired behavior |
can be used to |
direct mining power is |
traceroute technique of emitting |
in undesired behavior of |
be used to detect |
immersive virtual reality applications |
used to detect this |
technique of emitting small |
undesired behavior of a |
behavior of a service |
virtual reality applications and |
to detect this and |
of emitting small messages |
emitting small messages with |
reality applications and games |
detect this and m |
mining power is only |
consider a buyer at |
small messages with limited |
applications and games typically |
this and m can |
power is only m |
a buyer at an |
messages with limited ttl |
and games typically can |
and m can then |
buyer at an online |
games typically can t |
m can then send |
at an online site |
typically can t be |
can then send x |
an online site who |
can t be cached |
then send x to |
online site who looks |
t be cached at |
triggering icmp responses from |
send x to m |
site who looks for |
be cached at all |
icmp responses from routers |
x to m directly |
who looks for a |
responses from routers among |
and in today s |
looks for a toy |
from routers among the |
in today s systems |
for a toy train |
routers among the path |
a toy train with |
without waiting for the |
each client may pull |
waiting for the chain |
toy train with its |
client may pull such |
for the chain to |
train with its matching |
if an obstruction is |
may pull such information |
the chain to be |
with its matching tracks |
an obstruction is found |
the bitcoin system normalizes |
pull such information on |
chain to be repaired |
its matching tracks just |
obstruction is found it |
bitcoin system normalizes these |
such information on its |
matching tracks just as |
is found it is |
found it is reported |
information on its own |
tracks just as the |
just as the vendor |
it is reported to |
on its own point |
system normalizes these rates |
as the vendor is |
normalizes these rates by |
gossip is not a |
is reported to the |
the vendor is adding |
vendor is adding them |
is not a particularly |
reported to the caller |
these rates by the |
is adding them to |
stream directly from the |
directly from the data |
adding them to the |
them to the database |
rates by the total |
from the data center |
not a particularly fast |
the failure management library |
by the total number |
even if large numbers |
the total number of |
failure management library offers |
a particularly fast protocol |
the client may see |
if large numbers of |
total number of miners |
management library offers functionality |
client may see only |
large numbers of clients |
may see only the |
library offers functionality to |
number of miners that |
numbers of clients share |
see only the train |
offers functionality to keep |
of miners that publish |
of clients share interest |
only the train in |
functionality to keep the |
miners that publish full |
clients share interest in |
the train in stock |
to keep the obstruction |
that publish full proofs |
share interest in at |
train in stock but |
keep the obstruction under |
interest in at least |
the obstruction under investigation |
rounds of the protocol |
in stock but not |
in at least some |
obstruction under investigation and |
of the protocol to |
stock but not the |
at least some aspects |
least some aspects of |
under investigation and to |
the protocol to reach |
but not the tracks |
namely all miners but |
some aspects of the |
aspects of the data |
protocol to reach n |
not the tracks because |
all miners but x |
investigation and to notify |
to reach n processes |
the tracks because the |
we propose a new |
and to notify the |
tracks because the product |
propose a new system |
to notify the application |
because the product insertion |
a new system called |
on the other hand |
the product insertion transaction |
notify the application once |
new system called g |
product insertion transaction would |
the application once the |
system called g radient |
insertion transaction would often |
application once the obstruction |
called g radient aimed |
transaction would often be |
if rounds occur frequently |
once the obstruction seems |
g radient aimed at |
would often be broken |
the obstruction seems to |
radient aimed at reducing |
often be broken into |
obstruction seems to be |
aimed at reducing the |
the pools direct revenues |
seems to be removed |
be broken into two |
the delay before information |
at reducing the load |
delay before information spreads |
broken into two or |
before information spreads to |
pools direct revenues are |
reducing the load on |
into two or more |
information spreads to all |
this way the process |
direct revenues are therefore |
the load on providers |
two or more atomic |
spreads to all members |
way the process does |
revenues are therefore m |
load on providers of |
or more atomic but |
to all members of |
the process does not |
on providers of such |
more atomic but independent |
all members of a |
process does not need |
providers of such and |
atomic but independent subtransactions |
members of a system |
of such and enabling |
does not need to |
of a system may |
in a social network |
a system may still |
such and enabling scalable |
not need to keep |
system may still be |
an inconsistency with unexpected |
need to keep the |
may still be small |
inconsistency with unexpected results |
bandwidthsensitive streaming service for |
to keep the partitioned |
with unexpected results can |
keep the partitioned processes |
streaming service for heterogeneous |
the partitioned processes under |
unexpected results can occur |
partitioned processes under investigation |
service for heterogeneous consumers |
processes under investigation but |
results can occur if |
under investigation but can |
even in a large |
the core of the |
investigation but can wait |
can occur if a |
in a large system |
core of the system |
but can wait until |
occur if a user |
of the system is |
can wait until the |
if a user x |
the system is an |
wait until the connectivity |
a user x s |
system is an overlay |
until the connectivity is |
user x s record |
is an overlay networking |
the connectivity is restored |
x s record says |
an overlay networking architecture |
connectivity is restored by |
s record says it |
gossip is astonishingly robust |
overlay networking architecture intended |
is restored by simply |
record says it belongs |
networking architecture intended to |
restored by simply monitoring |
says it belongs to |
architecture intended to run |
by simply monitoring the |
it belongs to a |
intended to run directly |
simply monitoring the trouble |
there are exponentially many |
belongs to a certain |
to run directly on |
monitoring the trouble spot |
are exponentially many paths |
to a certain group |
exponentially many paths by |
run directly on a |
many paths by which |
directly on a content |
paths by which information |
in case the network |
on a content hosting |
but that group s |
by which information can |
case the network topology |
a content hosting platform |
that group s record |
which information can pass |
the network topology permits |
group s record does |
information can pass from |
network topology permits it |
s record does not |
and which optimizes aggregate |
can pass from point |
record does not include |
which optimizes aggregate bandwidth |
pass from point a |
does not include x |
optimizes aggregate bandwidth use |
the investigator can be |
from point a to |
aggregate bandwidth use by |
investigator can be configured |
point a to point |
web albums maintain picture |
bandwidth use by transforming |
can be configured to |
a to point b |
albums maintain picture data |
use by transforming in |
be configured to use |
maintain picture data and |
configured to use alternate |
picture data and access |
to use alternate paths |
data and access control |
flight data to match |
hence almost any imaginable |
and access control lists |
almost any imaginable disruption |
data to match the |
any imaginable disruption short |
to match the ideal |
imaginable disruption short of |
match the ideal stream |
disruption short of a |
the ideal stream quality |
and it is important |
short of a lasting |
ideal stream quality expressed |
it is important that |
of a lasting partitioning |
to reach one of |
stream quality expressed as |
is important that acl |
a lasting partitioning failure |
divides its revenue among |
quality expressed as an |
important that acl and |
reach one of the |
lasting partitioning failure can |
its revenue among its |
expressed as an economic |
that acl and album |
one of the destination |
partitioning failure can be |
failure can be overcome |
as an economic utility |
acl and album updates |
of the destination nfm |
revenue among its loyal |
an economic utility of |
and album updates are |
album updates are consistent |
economic utility of the |
utility of the consuming |
the gossip protocols implemented |
of the consuming client |
among its loyal miners |
gossip protocols implemented in |
the classical example involves |
from cornell for example |
its loyal miners and |
protocols implemented in the |
classical example involves removing |
implemented in the ssa |
loyal miners and the |
cornell for example it |
example involves removing one |
introduction recent years have |
in the ssa have |
miners and the miners |
for example it is |
involves removing one s |
recent years have seen |
the ssa have been |
and the miners that |
example it is possible |
removing one s boss |
years have seen skyrocketing |
ssa have been designed |
the miners that infiltrated |
it is possible to |
one s boss from |
have seen skyrocketing demand |
have been designed specifically |
miners that infiltrated it |
is possible to construct |
s boss from the |
seen skyrocketing demand for |
been designed specifically for |
boss from the album |
designed specifically for use |
skyrocketing demand for internet |
specifically for use in |
alternative routes to anywhere |
routes to anywhere in |
from the album acl |
demand for internet bandwidth |
for use in our |
its revenue density is |
to anywhere in california |
the album acl and |
use in our modified |
revenue density is therefore |
increasingly dominated by real |
album acl and then |
in our modified version |
density is therefore r |
time streaming of short |
our modified version of |
acl and then adding |
the request contains sufficient |
modified version of chain |
and then adding unflattering |
request contains sufficient information |
version of chain replication |
then adding unflattering pictures |
contains sufficient information for |
but in many forms |
sufficient information for the |
information for the nfm |
for the nfm to |
and with the goal |
the nfm to construct |
while many of these |
with the goal of |
nfm to construct a |
the goal of running |
many of these systems |
to construct a symmetric |
construct a symmetric return |
of these systems make |
goal of running in |
if trends continue then |
of running in large |
a symmetric return path |
these systems make do |
trends continue then internet |
running in large clusters |
systems make do with |
continue then internet video |
in large clusters or |
protocols that can exploit |
then internet video alone |
make do with weak |
do with weak consistency |
that can exploit this |
internet video alone will |
large clusters or datacenters |
can exploit this type |
video alone will generate |
their utility is reduced |
exploit this type of |
alone will generate almost |
utility is reduced when |
this type of information |
is reduced when their |
type of information are |
reduced when their clients |
let be a group |
of information are under |
when their clients observe |
be a group of |
exabytes per month by |
information are under development |
their clients observe inconsistencies |
a group of processes |
per month by the |
month by the end |
by the end of |
there has been a |
has been a wave |
been a wave of |
a wave of recent |
wave of recent innovations |
of recent innovations within |
recent innovations within the |
and let p be |
innovations within the backend |
let p be a |
p be a process |
be a process in |
a process in that |
offering scalable object stores |
process in that group |
scalable object stores that |
in that group p |
object stores that can |
stores that can efficiently |
that can efficiently support |
can efficiently support transactions |
divides its revenue among |
efficiently support transactions through |
support transactions through snapshot |
percent of all internet |
of all internet traffic |
transactions through snapshot isolation |
through snapshot isolation and |
snapshot isolation and even |
isolation and even full |
and even full atomicity |
its revenue among its |
each process has its |
revenue among its registered |
process has its own |
among its registered miners |
has its own view |
failure investigation of a |
its own view of |
faced with a competitive |
investigation of a process |
own view of the |
with a competitive landscape |
of a process at |
view of the group |
the revenue includes both |
a process at the |
isps and content providers |
process at the same |
at the same sub |
and content providers are |
revenue includes both its |
content providers are exploring |
includes both its direct |
providers are exploring technologies |
net has always been |
both its direct mining |
are exploring technologies to |
has always been viewed |
its direct mining revenue |
exploring technologies to help |
always been viewed as |
direct mining revenue and |
these views can lag |
been viewed as a |
technologies to help satisfy |
mining revenue and the |
views can lag reality |
viewed as a reasonably |
to help satisfy the |
revenue and the revenue |
as a reasonably accurate |
for example if a |
example if a process |
help satisfy the growing |
and the revenue its |
if a process joins |
satisfy the growing demand |
the revenue its infiltrators |
reasons for false suspicions |
a process joins or |
the growing demand alongside |
revenue its infiltrators obtained |
for false suspicions were |
process joins or leaves |
growing demand alongside the |
our challenge is to |
its infiltrators obtained from |
false suspicions were overload |
demand alongside the purchase |
challenge is to improve |
infiltrators obtained from pool |
suspicions were overload in |
alongside the purchase of |
is to improve transaction |
and different members might |
were overload in the |
the purchase of expensive |
to improve transaction consistency |
different members might not |
overload in the receiver |
purchase of expensive infrastructure |
improve transaction consistency at |
members might not have |
in the receiver os |
transaction consistency at the |
might not have consistent |
reducing the bandwidth consumption |
consistency at the cache |
not have consistent views |
the bandwidth consumption of |
at the cache layer |
bandwidth consumption of simultaneous |
consumption of simultaneous replicated |
of simultaneous replicated content |
our work assumes that |
simultaneous replicated content is |
work assumes that the |
even when the cache |
replicated content is a |
assumes that the network |
when the cache cannot |
content is a challenge |
that the network within |
the cache cannot access |
is a challenge which |
the network within a |
cache cannot access the |
a challenge which usually |
network within a cluster |
cannot access the backend |
challenge which usually leverages |
within a cluster does |
which could cause high |
access the backend on |
which usually leverages two |
a cluster does not |
could cause high message |
the backend on each |
backend on each read |
usually leverages two main |
cluster does not partition |
cause high message loss |
the revenue per loyal |
leverages two main tools |
revenue per loyal pool |
although there are low |
or unresponsiveness due to |
unresponsiveness due to application |
caching of content and |
due to application overload |
of content and multicasting |
miner is therefore r |
probability failure patterns that |
failure patterns that could |
patterns that could temporarily |
some forms of video |
today s consistency solutions |
although that could be |
that could temporarily partition |
forms of video content |
s consistency solutions are |
that could be seen |
could temporarily partition some |
consistency solutions are limited |
could be seen as |
temporarily partition some subservice |
solutions are limited to |
such as downloads of |
be seen as a |
partition some subservice in |
are limited to the |
as downloads of unencrypted |
seen as a design |
some subservice in a |
limited to the database |
downloads of unencrypted movies |
as a design error |
subservice in a logical |
to the database backend |
of unencrypted movies or |
in a logical sense |
unencrypted movies or films |
movies or films where |
or films where many |
films where many users |
even when the database |
where many users will |
when the database itself |
many users will share |
although confident about the |
the database itself is |
users will share the |
confident about the result |
database itself is consistent |
will share the same |
share the same encryption |
the same encryption key |
one was never guaranteed |
the vast majority of |
process p chooses a |
was never guaranteed that |
vast majority of operations |
p chooses a random |
never guaranteed that the |
majority of operations are |
chooses a random subset |
guaranteed that the process |
of operations are read |
a random subset of |
a wide variety of |
that the process had |
random subset of a |
wide variety of caching |
the process had truly |
subset of a particular |
variety of caching options |
only transactions issued by |
process had truly crashed |
of a particular size |
of caching options exist |
transactions issued by edge |
a particular size view |
issued by edge clients |
by edge clients and |
using the os failure |
edge clients and are |
the os failure management |
clients and are at |
we obtain the expression |
os failure management extensions |
and are at high |
obtain the expression for |
are at high risk |
the expression for r |
and commences a dialog |
at high risk of |
this assurance is now |
commences a dialog with |
high risk of observing |
assurance is now available |
a dialog with each |
of which is the |
risk of observing inconsistent |
dialog with each process |
which is the akamai |
of observing inconsistent state |
the time needed by |
is the akamai content |
time needed by the |
observing inconsistent state in |
with each process in |
the akamai content distribution |
needed by the failure |
inconsistent state in the |
each process in the |
akamai content distribution network |
by the failure detector |
state in the cache |
process in the set |
the failure detector to |
failure detector to come |
detector to come to |
to come to a |
the outright loss of |
is arguably the most |
the initial message is |
come to a result |
outright loss of cache |
arguably the most famous |
initial message is a |
to a result has |
loss of cache invalidations |
message is a compact |
a result has been |
of cache invalidations emerges |
is a compact state |
result has been greatly |
cache invalidations emerges as |
a compact state digest |
has been greatly reduced |
invalidations emerges as an |
compact state digest summarizing |
been greatly reduced in |
emerges as an especially |
state digest summarizing the |
greatly reduced in the |
as an especially significant |
digest summarizing the state |
reduced in the optimistic |
an especially significant problem |
multicast techniques can reduce |
summarizing the state of |
techniques can reduce the |
especially significant problem if |
the state of the |
can reduce the overall |
significant problem if transactional |
state of the sender |
common case that the |
reduce the overall network |
problem if transactional consistency |
case that the node |
the overall network traffic |
if transactional consistency is |
that the node on |
overall network traffic by |
the follow up dialog |
transactional consistency is required |
the node on which |
network traffic by taking |
follow up dialog consists |
node on which the |
traffic by taking advantage |
up dialog consists of |
an acceptable solution for |
by taking advantage of |
on which the process |
which the process was |
acceptable solution for a |
taking advantage of the |
dialog consists of an |
the process was running |
solution for a consistent |
advantage of the packet |
consists of an explicit |
process was running is |
for a consistent cache |
of the packet replication |
of an explicit request |
was running is reachable |
a consistent cache must |
the packet replication and |
an explicit request of |
consistent cache must maintain |
packet replication and forwarding |
explicit request of missing |
cache must maintain the |
replication and forwarding within |
regardless if the process |
request of missing update |
must maintain the performance |
and forwarding within the |
if the process has |
of missing update operations |
maintain the performance properties |
forwarding within the network |
the process has failed |
the performance properties of |
within the network infrastructure |
process has failed or |
performance properties of the |
several details of the |
has failed or not |
properties of the existing |
details of the epidemic |
of the existing caching |
of the epidemic protocols |
the existing caching tier |
the deployment of the |
the node is able |
the epidemic protocols employed |
deployment of the efficient |
node is able to |
of the efficient network |
epidemic protocols employed in |
is able to indicate |
protocols employed in the |
able to indicate whether |
we need to maintain |
employed in the framework |
to indicate whether or |
in the framework turned |
need to maintain the |
indicate whether or not |
the framework turned out |
to maintain the shielding |
whether or not the |
framework turned out to |
maintain the shielding role |
or not the process |
turned out to be |
the shielding role of |
not the process has |
out to be important |
shielding role of the |
the process has crashed |
to be important determinants |
role of the cache |
area internet has failed |
be important determinants of |
important determinants of system |
determinants of system performance |
in general a single |
general a single round |
the cache hit ratio |
of system performance and |
cache hit ratio should |
hit ratio should be |
system performance and behavior |
ratio should be high |
trip time is sufficient |
time is sufficient at |
is sufficient at the |
sufficient at the local |
and so more expensive |
at the local network |
suppose that a process |
so more expensive application |
the local network to |
that a process disseminates |
local network to get |
a process disseminates information |
level overlays are generally |
only cache access should |
process disseminates information via |
network to get a |
overlays are generally used |
cache access should complete |
disseminates information via epidemics |
to get a result |
access should complete with |
information via epidemics about |
should complete with a |
via epidemics about a |
complete with a single |
epidemics about a subject |
with a single client |
about a subject s |
we analyze this game |
analyze this game numerically |
this game numerically by |
game numerically by finding |
the devices used by |
process p gossips about |
numerically by finding the |
by finding the x |
devices used by content |
p gossips about subject |
area case this time |
used by content subscribers |
gossips about subject s |
trip on cache hits |
case this time is |
by content subscribers have |
about subject s a |
this time is a |
content subscribers have become |
subject s a finite |
this prohibits coherent cache |
time is a function |
subscribers have become increasingly |
s a finite number |
prohibits coherent cache solutions |
is a function of |
have become increasingly heterogeneous |
a finite number of |
coherent cache solutions such |
cache solutions such as |
a function of the |
become increasingly heterogeneous mobile |
finite number of times |
and substituting this value |
substituting this value for |
this value for r |
function of the level |
increasingly heterogeneous mobile devices |
of the level of |
as long as subject |
the level of congestion |
long as subject s |
level of congestion in |
as subject s is |
of congestion in the |
subject s is hot |
are projected to consume |
congestion in the network |
projected to consume over |
in the network path |
we vary the sizes |
after which subject s |
a rchitecture since the |
vary the sizes of |
which subject s is |
rchitecture since the cache |
the sizes of the |
subject s is no |
s is no longer |
is no longer gossiped |
no longer gossiped about |
exabytes of video per |
since the cache is |
sizes of the pools |
explicit requests for copies |
of the pools through |
the cache is required |
the os extensions also |
of video per month |
requests for copies of |
for copies of missed |
cache is required to |
os extensions also improve |
video per month in |
the pools through the |
copies of missed messages |
is required to respond |
extensions also improve the |
pools through the entire |
of missed messages are |
required to respond immediately |
also improve the confidence |
through the entire feasible |
missed messages are limited |
messages are limited in |
are limited in size |
the entire feasible range |
to respond immediately to |
improve the confidence in |
entire feasible range and |
respond immediately to the |
to prevent a process |
the confidence in the |
feasible range and depict |
immediately to the client |
prevent a process that |
confidence in the failure |
range and depict the |
to the client on |
a process that lagged |
in the failure investigation |
and depict the optimal |
depict the optimal x |
implying that a range |
the failure investigation process |
the client on hits |
process that lagged behind |
that a range of |
failure investigation process in |
that lagged behind or |
a range of subscription |
investigation process in the |
lagged behind or just |
range of subscription rates |
process in the wide |
behind or just joined |
cache channel is asynchronous |
of subscription rates and |
or just joined from |
just joined from trying |
subscription rates and policies |
and the corresponding revenues |
joined from trying to |
rates and policies must |
the corresponding revenues in |
we decided to employ |
from trying to catch |
decided to employ a |
corresponding revenues in figure |
using the old strategy |
and policies must be |
trying to catch up |
to catch up all |
catch up all at |
up all at once |
to employ a transactional |
the old strategy of |
policies must be applied |
employ a transactional consistency |
old strategy of simply |
each point in each |
must be applied over |
a transactional consistency that |
which would result in |
strategy of simply polling |
point in each graph |
be applied over the |
transactional consistency that is |
would result in enormous |
of simply polling a |
in each graph represents |
applied over the user |
consistency that is weaker |
result in enormous messages |
simply polling a process |
each graph represents the |
over the user base |
that is weaker than |
in enormous messages and |
polling a process until |
graph represents the equilibrium |
is weaker than the |
enormous messages and serious |
a process until a |
process until a time |
weaker than the full |
messages and serious fluctuations |
and serious fluctuations in |
serious fluctuations in system |
represents the equilibrium point |
than the full acid |
fluctuations in system load |
out occurs gives much |
the full acid model |
the equilibrium point of |
even if multiple users |
occurs gives much less |
such a process may |
gives much less confidence |
equilibrium point of a |
if multiple users are |
a process may need |
much less confidence in |
point of a game |
multiple users are streaming |
process may need to |
less confidence in the |
of a game with |
users are streaming the |
are streaming the same |
confidence in the result |
a game with the |
only transactions and update |
may need to catch |
transactions and update transactions |
in the result of |
game with the corresponding |
need to catch up |
streaming the same event |
and update transactions that |
the result of the |
with the corresponding m |
to catch up over |
such as watching the |
result of the failure |
update transactions that access |
catch up over many |
up over many seconds |
of the failure investigation |
transactions that access the |
as watching the opening |
that access the same |
explicit message requests are |
watching the opening ceremony |
message requests are honored |
the opening ceremony of |
access the same cache |
requests are honored if |
if no response was |
where we normalize m |
the same cache are |
are honored if the |
opening ceremony of the |
no response was received |
same cache are guaranteed |
honored if the requested |
ceremony of the olympics |
response was received after |
cache are guaranteed an |
if the requested messages |
was received after the |
are guaranteed an atomic |
the requested messages are |
requested messages are still |
a smartphone user will |
guaranteed an atomic execution |
received after the maximum |
messages are still in |
are still in the |
smartphone user will need |
after the maximum number |
the top right half |
still in the bounded |
in the bounded buffers |
the maximum number of |
top right half of |
once a message has |
only transactions that access |
user will need a |
right half of the |
maximum number of retransmission |
a message has been |
transactions that access different |
will need a differently |
half of the range |
number of retransmission is |
of retransmission is reached |
that access different caches |
need a differently transcoded |
of the range in |
message has been delivered |
access different caches may |
a differently transcoded version |
it was not certain |
has been delivered to |
was not certain whether |
the range in all |
differently transcoded version than |
been delivered to the |
delivered to the upper |
to the upper levels |
range in all graphs |
transcoded version than the |
and it has been |
not certain whether this |
in all graphs is |
different caches may observe |
version than the people |
it has been expunged |
certain whether this was |
all graphs is not |
graphs is not feasible |
than the people watching |
has been expunged from |
whether this was because |
caches may observe different |
the people watching via |
been expunged from the |
as the sum of |
the sum of m |
people watching via internet |
expunged from the buffers |
from the buffers located |
may observe different orderings |
watching via internet television |
this was because of |
the buffers located at |
buffers located at the |
located at the gossiper |
at the gossiper level |
was because of network |
observe different orderings for |
because of network failure |
requests are simply ignored |
different orderings for independent |
or on their laptops |
orderings for independent update |
the requesting process would |
for independent update transactions |
requesting process would have |
host failure or process |
process would have to |
we use this range |
failure or process failure |
would have to try |
use this range as |
have to try to |
different consumer groups may |
this range as a |
to try to find |
consumer groups may desire |
range as a reference |
try to find the |
to find the missing |
groups may desire different |
as a reference color |
with the new scheme |
find the missing data |
the new scheme it |
the missing data elsewhere |
new scheme it is |
may desire different local |
scheme it is possible |
desire different local ads |
different local ads or |
if data cannot be |
data cannot be recovered |
local ads or sub |
it is possible to |
and we use a |
is possible to distinguish |
every partial execution that |
we signal this to |
signal this to the |
titles to be embedded |
possible to distinguish among |
partial execution that includes |
we use a dashed |
this to the application |
to be embedded into |
to distinguish among these |
execution that includes all |
use a dashed line |
to the application by |
be embedded into their |
distinguish among these different |
that includes all update |
a dashed line to |
the application by delivering |
embedded into their video |
among these different failures |
includes all update transactions |
dashed line to show |
application by delivering an |
by delivering an exception |
all update transactions in |
line to show the |
into their video streams |
additional information the full |
update transactions in and |
information the full report |
delivering an exception upcall |
transactions in and all |
in and all read |
the full report contains |
to show the bound |
full report contains the |
and leave it to |
leave it to the |
only transactions that go |
show the bound between |
report contains the detailed |
avatars in a virtual |
contains the detailed results |
transactions that go through |
the detailed results of |
it to the application |
in a virtual world |
the bound between this |
that go through a |
go through a single |
detailed results of the |
a virtual world can |
results of the trace |
to the application to |
of the trace study |
bound between this value |
virtual world can be |
through a single cache |
the application to decide |
the trace study on |
between this value within |
world can be viewed |
a single cache server |
application to decide how |
to decide how to |
decide how to handle |
how to handle the |
to handle the problem |
this value within the |
can be viewed as |
the size of the |
be viewed as subscribers |
trace study on the |
size of the buffers |
of the buffers is |
value within the feasible |
within the feasible range |
the buffers is configurable |
viewed as subscribers to |
our solution seeks to |
study on the accuracy |
as subscribers to updates |
but this rule implies |
solution seeks to approximate |
on the accuracy and |
subscribers to updates about |
this rule implies that |
seeks to approximate cache |
a shows the optimal |
to updates about objects |
rule implies that certain |
the accuracy and performance |
to approximate cache serializability |
shows the optimal infiltration |
updates about objects in |
implies that certain kinds |
accuracy and performance of |
approximate cache serializability with |
the optimal infiltration rate |
about objects in their |
that certain kinds of |
and performance of the |
performance of the failure |
objects in their vicinity |
certain kinds of failures |
cache serializability with bounded |
of the failure detector |
in the entire feasible |
kinds of failures may |
serializability with bounded caches |
the failure detector in |
the entire feasible range |
and may want more |
of failures may be |
with bounded caches and |
failure detector in the |
detector in the internet |
may want more detailed |
failures may be unrecoverable |
bounded caches and asynchronous |
entire feasible range we |
want more detailed updates |
may be unrecoverable within |
be unrecoverable within the |
unrecoverable within the ssa |
the effectiveness of its |
caches and asynchronous communication |
feasible range we see |
more detailed updates for |
effectiveness of its partition |
and asynchronous communication with |
digests are bounded in |
range we see that |
detailed updates for objects |
of its partition detection |
asynchronous communication with the |
are bounded in the |
bounded in the number |
updates for objects that |
its partition detection mechanism |
communication with the db |
we see that pool |
in the number of |
for objects that are |
the number of messages |
objects that are closer |
number of messages they |
that are closer to |
of messages they advertise |
chooses a strictly positive |
are closer to them |
our idea starts with |
messages they advertise about |
they advertise about in |
closer to them in |
host failure measurements and |
idea starts with an |
starts with an observation |
advertise about in one |
to them in this |
them in this world |
a strictly positive value |
about in one single |
in one single datagram |
one single datagram packet |
failure measurements and measurements |
while this growing heterogeneity |
this growing heterogeneity of |
measurements and measurements of |
strictly positive value for |
and each round only |
growing heterogeneity of device |
objects form clusters with |
and measurements of failure |
positive value for x |
each round only a |
heterogeneity of device types |
form clusters with strong |
measurements of failure detection |
round only a single |
only a single digest |
a single digest is |
single digest is disseminated |
of failure detection for |
clusters with strong locality |
research on cdns has |
failure detection for server |
even if the subset |
if the subset view |
the subset view selected |
detection for server fail |
with strong locality properties |
on cdns has generally |
cdns has generally assumed |
has generally assumed a |
has cardinality greater than |
cardinality greater than one |
generally assumed a homogeneous |
transactions are likely to |
assumed a homogeneous population |
are likely to access |
messages that are potentially |
a homogeneous population of |
likely to access objects |
to access objects that |
that are potentially in |
homogeneous population of end |
the revenue of pool |
access objects that are |
are potentially in transit |
it will be available |
potentially in transit are |
in transit are not |
transit are not retransmitted |
are not retransmitted to |
is depicted in figure |
not retransmitted to requesting |
retransmitted to requesting processes |
will be available later |
be available later this |
close to each other |
for example if a |
available later this year |
b and in the |
example if a process |
later this year through |
for retailers this might |
if a process p |
and in the entire |
this year through the |
retailers this might involve |
a process p makes |
in the entire feasible |
year through the cornell |
this might involve related |
process p makes an |
the entire feasible region |
through the cornell university |
might involve related products |
p makes an explicit |
entire feasible region it |
the cornell university technical |
makes an explicit request |
feasible region it is |
cornell university technical report |
an explicit request for |
for social networks the |
social networks the set |
university technical report server |
explicit request for a |
thus most current systems |
region it is strictly |
most current systems assume |
request for a message |
networks the set of |
the set of friends |
current systems assume multiple |
for a message m |
it is strictly larger |
a message m and |
systems assume multiple video |
is strictly larger than |
message m and the |
for geographical services physical |
assume multiple video streams |
m and the request |
geographical services physical proximity |
multiple video streams to |
and the request lands |
the request lands at |
video streams to be |
request lands at process |
lands at process q |
streams to be sent |
which the pool would |
to be sent from |
and for web albums |
at process q that |
the pool would have |
be sent from the |
for web albums the |
process q that has |
web albums the acl |
sent from the source |
pool would have gotten |
q that has already |
albums the acl objects |
from the source at |
would have gotten without |
that has already sent |
the acl objects and |
the source at different |
have gotten without attacking |
has already sent p |
acl objects and the |
source at different resolutions |
already sent p a |
at different resolutions or |
objects and the pictures |
sent p a copy |
different resolutions or that |
relevant url s the |
resolutions or that a |
and the pictures assigned |
p a copy of |
url s the horus |
or that a single |
s the horus project |
a copy of m |
the pictures assigned to |
that a single highquality |
copy of m in |
a single highquality stream |
pictures assigned to them |
the horus project the |
of m in the |
single highquality stream is |
m in the recent |
highquality stream is transcoded |
horus project the cornell |
in the recent past |
stream is transcoded by |
project the cornell cluster |
the recent past then |
is transcoded by the |
recent past then m |
in some cases applications |
transcoded by the receiver |
past then m will |
then m will not |
m will not be |
will not be retransmitted |
the cornell cluster computing |
some cases applications explicitly |
by the receiver who |
a process creates a |
cases applications explicitly cluster |
cornell cluster computing project |
the receiver who then |
process creates a digest |
applications explicitly cluster their |
c depicts the revenue |
cluster computing project werner |
receiver who then incurs |
creates a digest based |
explicitly cluster their data |
depicts the revenue of |
the revenue of pool |
who then incurs cost |
a digest based upon |
cluster their data accesses |
computing project werner vogels |
then incurs cost for |
digest based upon all |
their data accesses to |
project werner vogels personal |
incurs cost for last |
based upon all the |
data accesses to benefit |
werner vogels personal home |
which is strictly smaller |
upon all the messages |
mile traffic owing to |
traffic owing to unnecessarily |
owing to unnecessarily detailed |
accesses to benefit from |
all the messages received |
vogels personal home page |
is strictly smaller than |
to unnecessarily detailed video |
to benefit from improved |
the messages received by |
personal home page papers |
benefit from improved parallelism |
messages received by means |
received by means of |
by means of any |
means of any communication |
of any communication channels |
in the entire range |
we pose the following |
home page papers on |
pose the following question |
not just the epidemics |
page papers on failure |
papers on failure detection |
on failure detection http |
how can we deliver |
can we deliver live |
we deliver live dynamic |
deliver live dynamic content |
note that the total |
that the total system |
the messages received by |
messages received by fifo |
such as video broadcasts |
received by fifo chained |
by fifo chained channels |
the resulting transactions access |
the total system mining |
resulting transactions access objects |
total system mining power |
or financial stock data |
transactions access objects from |
the message buffers are |
message buffers are bounded |
access objects from a |
objects from a single |
from a single cluster |
and once a message |
over the internet to |
system mining power is |
once a message has |
the internet to large |
mining power is reduced |
although there will also |
internet to large number |
a message has been |
power is reduced when |
is reduced when pool |
to large number of |
message has been delivered |
there will also be |
large number of heterogeneous |
has been delivered by |
will also be some |
number of heterogeneous users |
been delivered by means |
chooses to infiltrate pool |
also be some frequency |
of heterogeneous users simultaneously |
delivered by means of |
be some frequency of |
heterogeneous users simultaneously while |
by means of an |
some frequency of transactions |
users simultaneously while balancing |
means of an upcall |
frequency of transactions that |
simultaneously while balancing bandwidth |
of an upcall it |
of transactions that access |
while balancing bandwidth costs |
an upcall it is |
transactions that access unrelated |
the revenue of third |
upcall it is prone |
that access unrelated objects |
revenue of third parties |
it is prone to |
traffic rates and end |
access unrelated objects in |
is prone to be |
unrelated objects in different |
prone to be replaced |
to be replaced by |
be replaced by the |
replaced by the replacement |
by the replacement policy |
objects in different clusters |
miners not in either |
not in either pool |
the ssa implements several |
ssa implements several replacement |
implements several replacement policies |
live content refers to |
our solution requires minor |
content refers to content |
solution requires minor changes |
refers to content streams |
most advertised message in |
advertised message in digests |
to content streams that |
requires minor changes to |
content streams that must |
minor changes to the |
streams that must be |
changes to the database |
that must be transmitted |
to the database object |
must be transmitted to |
although the ssa should |
the database object representation |
be transmitted to multiple |
the ssa should work |
database object representation format |
transmitted to multiple receivers |
ssa should work well |
to multiple receivers simultaneously |
should work well on |
work well on clusters |
well on clusters with |
on clusters with as |
imposing a small and |
clusters with as many |
a small and constant |
with as many as |
such as a live |
small and constant memory |
as many as thousands |
as a live broadcast |
and constant memory overhead |
many as thousands of |
as thousands of nodes |
companies like google and |
like google and amazon |
google and amazon reportedly |
and amazon reportedly operate |
amazon reportedly operate centers |
ticker updates for financial |
reportedly operate centers with |
independent of the database |
updates for financial stocks |
operate centers with tens |
of the database size |
therefore pays for the |
for financial stocks or |
centers with tens of |
the database size and |
pays for the increased |
financial stocks or object |
with tens of thousands |
database size and the |
for the increased revenue |
stocks or object updates |
tens of thousands of |
size and the transaction |
and the transaction rate |
or object updates in |
of thousands of machines |
thousands of machines in |
of machines in them |
the increased revenue of |
object updates in a |
updates in a virtual |
and are said to |
in a virtual world |
increased revenue of its |
are said to deploy |
revenue of its attacker |
said to deploy some |
this overhead involves tracking |
of its attacker and |
to deploy some popular |
overhead involves tracking and |
its attacker and everyone |
deploy some popular services |
involves tracking and caching |
we are not focused |
attacker and everyone else |
some popular services on |
tracking and caching what |
are not focused on |
and everyone else in |
popular services on huge |
and caching what we |
not focused on streams |
everyone else in the |
services on huge numbers |
on huge numbers of |
huge numbers of nodes |
else in the system |
caching what we refer |
focused on streams with |
what we refer to |
were we to use |
on streams with a |
we refer to as |
we to use the |
streams with a pause |
refer to as dependency |
to use the ssa |
implications to the general |
to as dependency lists |
with a pause or |
use the ssa in |
the ssa in such |
ssa in such settings |
to the general case |
a pause or rewind |
the general case consider |
pause or rewind functions |
our gossip protocol might |
general case consider the |
or rewind functions or |
gossip protocol might need |
case consider the case |
length lists of object |
rewind functions or the |
protocol might need to |
consider the case of |
the case of p |
case of p pools |
might need to be |
lists of object identifiers |
functions or the video |
need to be revisited |
of object identifiers and |
for any choice of |
to be revisited to |
object identifiers and the |
any choice of the |
be revisited to ensure |
identifiers and the associated |
choice of the pools |
revisited to ensure that |
and the associated version |
the associated version numbers |
to ensure that messages |
ensure that messages do |
that messages do not |
messages do not become |
do not become excessively |
not become excessively large |
of the pools sizes |
the pools sizes m |
each representing some recently |
one way to accomplish |
the gradient cdn to |
representing some recently updated |
way to accomplish this |
gradient cdn to make |
some recently updated objects |
to accomplish this might |
cdn to make progress |
recently updated objects upon |
accomplish this might be |
to make progress towards |
updated objects upon which |
this might be to |
make progress towards the |
objects upon which the |
might be to modify |
progress towards the research |
upon which the cached |
be to modify the |
towards the research question |
which the cached object |
to modify the epidemic |
the cached object depends |
modify the epidemic protocol |
the epidemic protocol using |
epidemic protocol using spatial |
transis a communication subsystem |
we propose a novel |
protocol using spatial distributions |
a communication subsystem for |
using spatial distributions to |
propose a novel networked |
at least one pool |
a novel networked content |
spatial distributions to improve |
communication subsystem for high |
least one pool will |
sized list can omit |
distributions to improve the |
to improve the performance |
subsystem for high availability |
one pool will choose |
list can omit dependency |
novel networked content delivery |
pool will choose to |
networked content delivery system |
idigest of papers of |
will choose to perform |
can omit dependency information |
content delivery system called |
choose to perform block |
to perform block withholding |
such an approach would |
delivery system called g |
omit dependency information required |
an approach would let |
system called g radient |
dependency information required to |
approach would let us |
called g radient to |
information required to detect |
would let us restrict |
g radient to address |
required to detect inconsistencies |
let us restrict information |
radient to address the |
us restrict information to |
to address the complex |
in a system with |
restrict information to the |
address the complex caching |
a system with p |
information to the vicinity |
hence it is important |
the complex caching and |
system with p pools |
to the vicinity of |
it is important to |
complex caching and multicasting |
the vicinity of the |
caching and multicasting issues |
is important to use |
vicinity of the nodes |
important to use a |
and multicasting issues associated |
of the nodes where |
multicasting issues associated with |
to use a bound |
the nodes where it |
issues associated with live |
use a bound large |
nodes where it might |
associated with live streaming |
a bound large enough |
where it might be |
it might be needed |
bound large enough to |
with live streaming of |
large enough to capture |
is not an equilibrium |
live streaming of dynamic |
enough to capture most |
in effect adding an |
streaming of dynamic content |
to capture most of |
effect adding an additional |
of dynamic content to |
capture most of the |
adding an additional layer |
dynamic content to a |
most of the relevant |
an additional layer of |
additional layer of hierarchy |
content to a heterogeneous |
of the relevant dependencies |
assume towards negation this |
fast message ordering and |
to a heterogeneous user |
layer of hierarchy to |
of hierarchy to the |
hierarchy to the architecture |
a heterogeneous user population |
towards negation this is |
negation this is not |
this is not the |
is not the case |
we believe the required |
at present we lack |
the systems architecture consists |
believe the required changes |
systems architecture consists of |
present we lack an |
message ordering and membership |
the required changes would |
architecture consists of one |
we lack an automated |
ordering and membership using |
required changes would be |
consists of one or |
lack an automated way |
and membership using a |
changes would be relatively |
of one or more |
an automated way to |
membership using a logical |
would be relatively minor |
one or more content |
automated way to do |
using a logical token |
or more content providers |
is an equilibrium point |
way to do this |
more content providers which |
content providers which together |
providers which together form |
now consider a setting |
which together form a |
consider a setting with |
we require the developer |
together form a cooperative |
a setting with only |
setting with only pools |
form a cooperative network |
epidemic analytical model one |
require the developer to |
a cooperative network of |
analytical model one benefit |
the developer to tune |
cooperative network of g |
model one benefit of |
developer to tune the |
network of g radient |
one benefit of using |
to tune the length |
of g radient cdn |
benefit of using gossip |
tune the length so |
g radient cdn nodes |
and treat the other |
of using gossip in |
the length so that |
treat the other pools |
using gossip in the |
length so that the |
the other pools as |
other pools as independent |
gossip in the ssa |
so that the frequency |
the cdn nodes form |
pools as independent miners |
in the ssa is |
that the frequency of |
cdn nodes form a |
the ssa is that |
the frequency of errors |
nodes form a dynamic |
ssa is that we |
this is the setting |
frequency of errors is |
form a dynamic overlay |
is that we can |
is the setting analyzed |
of errors is reduced |
a dynamic overlay over |
that we can use |
the setting analyzed above |
errors is reduced to |
dynamic overlay over which |
we can use analytical |
setting analyzed above and |
is reduced to an |
overlay over which the |
can use analytical methods |
analyzed above and we |
reduced to an acceptable |
over which the content |
use analytical methods to |
analytical methods to predict |
to an acceptable level |
which the content is |
the content is delivered |
methods to predict the |
to predict the behavior |
predict the behavior of |
the behavior of a |
behavior of a cluster |
above and we have |
reasoning about the trade |
and we have seen |
complementing our experimental work |
and for our initial |
we have seen there |
for our initial prototypes |
a basic result of |
have seen there that |
seen there that pool |
basic result of epidemic |
our initial prototypes we |
result of epidemic theory |
initial prototypes we will |
of epidemic theory states |
prototypes we will look |
reliable communication in the |
epidemic theory states that |
we will look at |
will look at spanning |
look at spanning trees |
communication in the presence |
theory states that simple |
can increase its revenue |
in a manner we |
the concept of cdn |
states that simple epidemics |
increase its revenue by |
in the presence of |
a manner we discuss |
concept of cdn nodes |
that simple epidemics eventually |
its revenue by performing |
the presence of failure |
manner we discuss further |
of cdn nodes is |
simple epidemics eventually infect |
revenue by performing a |
we discuss further below |
cdn nodes is general |
epidemics eventually infect the |
eventually infect the entire |
acm transaction on computer |
by performing a block |
infect the entire population |
transaction on computer systems |
performing a block withholding |
the entire population with |
an architecture in which |
a block withholding attack |
entire population with probability |
dependency lists should be |
architecture in which cdn |
block withholding attack on |
withholding attack on pool |
in which cdn servers |
lists should be roughly |
moreover starting with a |
should be roughly the |
which cdn servers are |
starting with a single |
cdn servers are hosted |
be roughly the same |
with a single infected |
servers are hosted by |
roughly the same size |
a single infected site |
are hosted by isps |
the same size as |
single infected site this |
s infiltration rate by |
infiltration rate by x |
same size as the |
infected site this is |
hosted by isps to |
size as the size |
site this is achieved |
by isps to reduce |
as the size of |
this is achieved in |
isps to reduce redundant |
the size of the |
is achieved in expected |
to reduce redundant incoming |
size of the workload |
achieved in expected time |
reduce redundant incoming bandwidth |
of the workload s |
in expected time proportional |
redundant incoming bandwidth is |
the workload s clusters |
expected time proportional to |
incoming bandwidth is a |
time proportional to the |
bandwidth is a logical |
proportional to the log |
is a logical scenario |
to the log of |
the log of the |
our extensions offer a |
take this values p |
extensions offer a transactional |
this values p m |
log of the population |
of the population size |
offer a transactional interface |
and another example is |
a transactional interface to |
another example is that |
transactional interface to the |
example is that g |
interface to the cache |
is that g radient |
to the cache in |
that g radient nodes |
the cache in addition |
g radient nodes may |
the protocol roughly falls |
cache in addition to |
radient nodes may as |
protocol roughly falls under |
in addition to the |
nodes may as well |
roughly falls under the |
addition to the standard |
may as well be |
falls under the category |
to the standard read |
as well be integrated |
under the category of |
the category of a |
category of a push |
well be integrated into |
be integrated into set |
group membership and viewsynchronous |
membership and viewsynchronous communication |
and viewsynchronous communication in |
and the exact formula |
viewsynchronous communication in partitionable |
the exact formula for |
communication in partitionable asynchronous |
exact formula for it |
in partitionable asynchronous systems |
formula for it can |
our algorithm detects and |
for it can be |
it can be expressed |
can be expressed as |
be expressed as log |
algorithm detects and fixes |
detects and fixes inconsistent |
and fixes inconsistent read |
our approach to the |
approach to the problem |
only transactions at the |
to the problem resembles |
transactions at the cache |
the problem resembles content |
at the cache with |
the cache with constant |
cache with constant complexity |
it does so by |
does so by either |
and in fact the |
so by either aborting |
in fact the expected |
by either aborting the |
fact the expected deployment |
for large values of |
large values of n |
either aborting the transaction |
the expected deployment model |
expected deployment model would |
where n the number |
n the number of |
deployment model would employ |
which can then be |
model would employ a |
the number of sites |
would employ a geographically |
can then be retried |
number of sites participating |
employ a geographically distributed |
of sites participating in |
a geographically distributed set |
sites participating in the |
geographically distributed set of |
participating in the epidemic |
distributed set of isps |
in the epidemic spread |
set of isps or |
of isps or small |
isps or small data |
or invalidating a cached |
let pi be the |
or small data centers |
invalidating a cached object |
pi be the probability |
small data centers of |
a cached object which |
be the probability that |
data centers of the |
cached object which can |
the probability that a |
centers of the kind |
object which can then |
probability that a site |
of the kind operated |
which can then force |
that a site remains |
the kind operated by |
can then force a |
a site remains susceptible |
kind operated by today |
then force a read |
operated by today s |
not touched by the |
touched by the epidemic |
by today s cdn |
force a read from |
today s cdn providers |
after the ith round |
a read from the |
the ith round of |
read from the database |
ith round of the |
round of the protocol |
a site remains susceptible |
site remains susceptible after |
remains susceptible after the |
similar to handling cache |
susceptible after the i |
whereas today s content |
to handling cache misses |
hosting sites cache objects |
th round if it |
unreliable failure detectors for |
round if it was |
failure detectors for reliable |
if it was susceptible |
detectors for reliable distributed |
it was susceptible after |
when the dependency lists |
for reliable distributed systems |
was susceptible after the |
the dependency lists fail |
susceptible after the ith |
dependency lists fail to |
after the ith cycle |
to appear in journal |
lists fail to document |
the ith cycle and |
appear in journal of |
fail to document a |
ith cycle and it |
in journal of the |
to document a necessary |
cycle and it is |
journal of the acm |
document a necessary dependency |
and it is not |
it is not contacted |
is not contacted by |
not contacted by any |
contacted by any infectious |
by any infectious site |
our focus is on |
an application might be |
any infectious site in |
focus is on content |
application might be exposed |
infectious site in the |
is on content that |
might be exposed to |
site in the i |
on content that cannot |
be exposed to stale |
content that cannot be |
exposed to stale values |
that cannot be usefully |
cannot be usefully cached |
because we have in |
we have in mind |
the g radient project |
have in mind client |
g radient project aims |
radient project aims to |
project aims to exploit |
aims to exploit and |
to exploit and develop |
side applications that are |
exploit and develop two |
applications that are unlikely |
and develop two techniques |
that are unlikely to |
develop two techniques that |
are unlikely to validate |
two techniques that improve |
unlikely to validate against |
relation that we obtain |
techniques that improve on |
to validate against the |
impossibility of distributed consensus |
that we obtain is |
that improve on existing |
validate against the back |
of distributed consensus with |
improve on existing cdns |
distributed consensus with one |
consensus with one faulty |
with one faulty process |
journal of the acm |
for many of our |
and algorithms to balance |
many of our intended |
algorithms to balance bandwidth |
of our intended uses |
to balance bandwidth costs |
our intended uses some |
balance bandwidth costs with |
intended uses some level |
bandwidth costs with end |
uses some level of |
some level of undetected |
level of undetected inconsistency |
of undetected inconsistency can |
undetected inconsistency can slip |
inconsistency can slip past |
our design is focused |
design is focused on |
is focused on modularity |
focused on modularity and |
on modularity and incremental |
modularity and incremental deployment |
because the developer would |
the developer would often |
developer would often be |
would often be able |
often be able to |
be able to tune |
since infection starts with |
able to tune the |
infection starts with one |
to tune the mechanism |
starts with one site |
r van and vogels |
for any randomly chosen |
any randomly chosen site |
state operation of large |
randomly chosen site p |
operation of large applications |
dynamic content has substantial |
content has substantial levels |
has substantial levels of |
the rate of unnoticed |
substantial levels of redundancy |
rate of unnoticed inconsistencies |
support for highly reliable |
of unnoticed inconsistencies could |
unnoticed inconsistencies could be |
inconsistencies could be extremely |
even when user interests |
could be extremely low |
when user interests are |
user interests are relatively |
interests are relatively heterogeneous |
with clustered workloads we |
widespread use of streaming |
clustered workloads we will |
use of streaming video |
workloads we will demonstrate |
of streaming video occurs |
we will demonstrate that |
streaming video occurs when |
will demonstrate that it |
video occurs when internet |
acm sigops european workshop |
demonstrate that it is |
occurs when internet users |
that it is sufficient |
as a function of |
when internet users watch |
it is sufficient to |
a function of the |
internet users watch major |
is sufficient to store |
function of the rate |
users watch major events |
sufficient to store a |
of the rate of |
watch major events online |
to store a small |
the rate of gossip |
store a small set |
a small set of |
small set of dependencies |
such as superbowl or |
set of dependencies to |
as superbowl or the |
of dependencies to detect |
superbowl or the olympics |
we can predict the |
dependencies to detect most |
to detect most inconsistencies |
can predict the delay |
predict the delay before |
the delay before a |
and like television users |
we also investigate workloads |
delay before a typical |
also investigate workloads where |
before a typical process |
such clients have little |
investigate workloads where the |
a typical process that |
clients have little tolerance |
workloads where the clustered |
typical process that has |
have little tolerance for |
process that has been |
where the clustered access |
that has been disrupted |
little tolerance for lagged |
has been disrupted by |
the clustered access pattern |
been disrupted by a |
stable state where only |
disrupted by a failure |
tolerance for lagged data |
by a failure will |
clustered access pattern is |
state where only pool |
a failure will learn |
access pattern is less |
reliable multicast for distributed |
failure will learn about |
pattern is less strongly |
multicast for distributed interactive |
will learn about inconsistency |
large numbers of users |
is less strongly evident |
for distributed interactive simulation |
learn about inconsistency introduced |
numbers of users have |
about inconsistency introduced by |
of users have essentially |
proceedings of acm sigcomm |
inconsistency introduced by the |
users have essentially the |
introduced by the failure |
have essentially the same |
our approach is less |
by the failure and |
essentially the same needs |
approach is less effective |
the failure and can |
is less effective even |
failure and can initiate |
less effective even with |
and can initiate repair |
but since they may |
effective even with longer |
since they may access |
even with longer dependency |
they may access the |
with longer dependency list |
may access the streams |
longer dependency list lengths |
access the streams from |
the streams from a |
streams from a variety |
from a variety of |
a variety of devices |
thus our solution is |
if the model predicts |
our solution is not |
the model predicts that |
solution is not a |
with different screen sizes |
model predicts that for |
is not a panacea |
different screen sizes and |
predicts that for a |
screen sizes and resolutions |
that for a given |
for a given gossip |
a given gossip rate |
or different connectivity properties |
two pools where one |
pools where one infiltrates |
where one infiltrates the |
one infiltrates the other |
for applications matched to |
a broken chain should |
applications matched to our |
broken chain should be |
matched to our assumptions |
the current solution is |
optimal infiltration rate x |
chain should be repaired |
current solution is to |
view synchronous communication in |
should be repaired within |
can be highly effective |
solution is to provide |
synchronous communication in large |
is to provide each |
communication in large scale |
to provide each user |
provide each user with |
each user with a |
user with a direct |
with a direct connection |
a direct connection to |
direct connection to a |
connection to a content |
database we assume that |
nd open broadcast workshop |
we assume that the |
assume that the database |
server due to the |
that the database tags |
due to the lack |
the database tags each |
to the lack of |
database tags each object |
as a function of |
tags each object with |
a function of pool |
function of pool sizes |
one can anticipate that |
the lack of robust |
each object with a |
lack of robust multicast |
can anticipate that the |
object with a version |
of robust multicast technologies |
anticipate that the disruption |
with a version number |
that the disruption associated |
a version number specific |
similar issues arise for |
the disruption associated with |
version number specific to |
issues arise for newscasts |
disruption associated with a |
number specific to the |
arise for newscasts of |
associated with a failure |
specific to the transaction |
for newscasts of fast |
with a failure should |
to the transaction that |
a failure should be |
the transaction that most |
failure should be limited |
transaction that most recently |
should be limited to |
increasing reliability of communication |
be limited to the |
and the lines in |
that most recently updated |
transmission of financial data |
reliability of communication in |
limited to the maximum |
most recently updated it |
to the maximum number |
of communication in large |
the maximum number of |
of financial data and |
communication in large scale |
maximum number of updates |
financial data and virtual |
in large scale distributed |
and that there is |
number of updates that |
show the revenue density |
the revenue density of |
data and virtual on |
of updates that would |
that there is a |
updates that would be |
there is a total |
that would be sent |
is a total ordering |
would be sent to |
a total ordering on |
back to the setting |
be sent to a |
our insight is that |
total ordering on version |
ordering on version numbers |
sent to a given |
insight is that a |
is that a data |
to a given subservice |
to the setting at |
the setting at hand |
setting at hand with |
at hand with p |
hand with p pools |
the version of a |
rich channel can be |
a given subservice during |
version of a transaction |
channel can be transformed |
given subservice during a |
the revenue of pool |
of a transaction is |
can be transformed on |
a transaction is chosen |
transaction is chosen to |
is better when x |
is chosen to be |
chosen to be larger |
to be larger than |
be larger than the |
network to create the |
larger than the versions |
to create the dynamic |
than the versions of |
create the dynamic content |
the versions of all |
the dynamic content for |
versions of all objects |
dynamic content for end |
of all objects accessed |
all objects accessed by |
objects accessed by the |
accessed by the transaction |
a generic architecture for |
generic architecture for dependable |
the database stores for |
if we know how |
architecture for dependable distributed |
database stores for each |
we know how large |
for dependable distributed computing |
we could add personalized |
stores for each object |
know how large the |
could add personalized advertisements |
for each object o |
how large the typical |
each object o a |
large the typical update |
object o a list |
the typical update is |
subtitles or encryption keys |
o a list of |
or encryption keys to |
a list of k |
encryption keys to iptv |
list of k dependencies |
keys to iptv broadcasts |
filters or aggregates to |
or aggregates to financial |
and we know the |
aggregates to financial stock |
we know the size |
to financial stock updates |
know the size limit |
the size limit on |
size limit on data |
limit on data sent |
or reduce the update |
on data sent in |
reduce the update rate |
data sent in response |
the update rate for |
can improve its revenue |
update rate for distant |
improve its revenue by |
its revenue by attacking |
revenue by attacking pool |
sent in response to |
rate for distant objects |
in response to explicit |
for distant objects in |
response to explicit requests |
distant objects in the |
objects in the virtual |
in the virtual world |
the virtual world to |
virtual world to which |
world to which the |
attacks is not an |
is not an equilibrium |
not an equilibrium point |
we can predict the |
to which the user |
can predict the amount |
which the user has |
predict the amount of |
the user has subscribed |
a flexible group communications |
the amount of time |
flexible group communications system |
case as a test |
as a test case |
amount of time that |
the same mechanism will |
cornell university technical report |
of time that will |
same mechanism will also |
we take the pool |
take the pool distribution |
the pool distribution in |
pool distribution in january |
mechanism will also allow |
time that will be |
will also allow the |
that will be needed |
also allow the system |
will be needed to |
allow the system to |
be needed to repair |
the system to tailor |
needed to repair the |
system to tailor to |
to repair the resulting |
to tailor to heterogeneous |
repair the resulting data |
tailor to heterogeneous devices |
the resulting data inconsistency |
these capabilities should help |
capabilities should help the |
this is a list |
should help the developer |
transcoding a high definition |
help the developer parameterize |
a high definition broadcast |
the developer parameterize the |
high definition broadcast to |
developer parameterize the cluster |
definition broadcast to adapt |
parameterize the cluster to |
broadcast to adapt its |
the cluster to balance |
to adapt its resolution |
cluster to balance overhead |
adapt its resolution to |
to balance overhead for |
its resolution to serve |
balance overhead for gossip |
resolution to serve a |
overhead for gossip against |
to serve a population |
for gossip against repair |
serve a population of |
gossip against repair times |
a population of heterogeneous |
we analyze the cases |
is a list of |
against repair times desired |
population of heterogeneous devices |
analyze the cases where |
a list of identifiers |
repair times desired by |
of heterogeneous devices from |
the cases where each |
list of identifiers and |
times desired by the |
heterogeneous devices from cell |
cases where each of |
of identifiers and versions |
desired by the application |
devices from cell phones |
where each of the |
identifiers and versions of |
from cell phones to |
each of the pools |
of the pools attacks |
the pools attacks all |
pools attacks all other |
attacks all other open |
all other open pools |
cell phones to tablets |
and versions of other |
phones to tablets to |
versions of other objects |
all of which behave |
of which behave honestly |
of other objects that |
to tablets to iptv |
membership some readers may |
note that attacking all |
tablets to iptv lowering |
other objects that the |
some readers may be |
that attacking all pools |
to iptv lowering overall |
objects that the current |
readers may be curious |
attacking all pools with |
iptv lowering overall bandwidth |
that the current version |
may be curious about |
all pools with force |
lowering overall bandwidth costs |
the current version of |
be curious about what |
pools with force proportional |
overall bandwidth costs without |
current version of o |
curious about what will |
with force proportional to |
bandwidth costs without affecting |
version of o depends |
about what will seem |
force proportional to their |
costs without affecting viewing |
of o depends on |
what will seem to |
proportional to their size |
without affecting viewing experience |
will seem to be |
to their size yields |
seem to be a |
to be a chicken |
their size yields the |
size yields the same |
yields the same results |
the same results as |
only transaction that sees |
same results as attacking |
transaction that sees the |
results as attacking a |
that sees the current |
as attacking a single |
attacking a single pool |
a single pool of |
single pool of their |
pool of their aggregate |
of their aggregate size |
sees the current version |
network transformations will be |
on the one hand |
the current version of |
transformations will be applied |
current version of o |
plugging in the numbers |
will be applied with |
version of o must |
in the numbers into |
we use gossip epidemics |
be applied with pluggable |
of o must not |
the numbers into the |
use gossip epidemics to |
applied with pluggable serverlets |
o must not see |
numbers into the analysis |
gossip epidemics to propagate |
with pluggable serverlets designed |
must not see object |
into the analysis above |
epidemics to propagate information |
pluggable serverlets designed to |
not see object di |
the analysis above shows |
to propagate information about |
serverlets designed to execute |
see object di with |
analysis above shows that |
propagate information about membership |
designed to execute within |
object di with version |
above shows that a |
information about membership changes |
to execute within the |
di with version smaller |
shows that a larger |
execute within the cdn |
with version smaller than |
yet the gossip protocol |
within the cdn nodes |
version smaller than vi |
that a larger pool |
the gossip protocol uses |
the cdn nodes of |
cdn nodes of g |
gossip protocol uses membership |
when a transaction t |
nodes of g radient |
a larger pool needs |
protocol uses membership information |
a transaction t with |
larger pool needs to |
uses membership information to |
transaction t with version |
pool needs to use |
the serverlets encapsulate application |
membership information to select |
a private framework for |
needs to use a |
t with version vt |
information to select gossip |
private framework for distributed |
framework for distributed computation |
speci n ac details |
with version vt touches |
to select gossip peers |
to use a smaller |
for distributed computation edward |
n ac details such |
version vt touches objects |
vt touches objects o |
distributed computation edward tremel |
ac details such as |
use a smaller ratio |
details such as the |
a smaller ratio of |
such as the stream |
our solution starts with |
smaller ratio of its |
as the stream data |
solution starts with approximate |
starts with approximate membership |
and ma rk jelasity |
ratio of its mining |
with approximate membership information |
the stream data format |
ma rk jelasity there |
of its mining power |
it updates both their |
rk jelasity there is |
its mining power for |
extracted from a group |
updates both their versions |
jelasity there is a |
mining power for infiltration |
and the ways to |
both their versions and |
there is a growing |
from a group management |
is a growing class |
the ways to transform |
their versions and their |
power for infiltration and |
a group management service |
a growing class of |
ways to transform a |
versions and their dependency |
and their dependency lists |
group management service component |
growing class of distributed |
to transform a the |
transform a the data |
management service component that |
class of distributed systems |
subsequent accesses to object |
service component that list |
for infiltration and can |
of distributed systems applications |
accesses to object o |
component that list the |
rich objects into more |
objects into more specialized |
distributed systems applications in |
that list the nodes |
systems applications in which |
must see object o |
infiltration and can increase |
list the nodes in |
into more specialized ones |
applications in which data |
and can increase its |
the nodes in the |
with a version not |
in which data stored |
can increase its revenue |
nodes in the cluster |
a version not smaller |
which data stored on |
open issues include understanding |
increase its revenue density |
in the cluster and |
version not smaller than |
data stored on client |
issues include understanding what |
its revenue density more |
the cluster and the |
not smaller than vt |
stored on client platforms |
include understanding what kinds |
revenue density more than |
cluster and the rough |
on client platforms must |
understanding what kinds of |
density more than a |
and the rough mapping |
client platforms must be |
what kinds of content |
more than a small |
than a small pool |
platforms must be aggregated |
it inherits all of |
kinds of content may |
the rough mapping of |
must be aggregated or |
inherits all of the |
of content may be |
rough mapping of services |
be aggregated or analyzed |
all of the l |
content may be subject |
mapping of services to |
achieves its optimum attack |
its optimum attack rate |
optimum attack rate at |
aggregated or analyzed without |
of services to those |
services to those nodes |
may be subject to |
or analyzed without revealing |
of the l dependencies |
be subject to such |
analyzed without revealing private |
the l dependencies of |
and then refines this |
subject to such transformation |
of the pool s |
the pool s mining |
pool s mining power |
without revealing private information |
to such transformation and |
l dependencies of o |
then refines this with |
revealing private information to |
such transformation and which |
increasing its revenue by |
its revenue by almost |
private information to the |
transformation and which dynamic |
refines this with incremental |
information to the operator |
and which dynamic content |
this with incremental updates |
where l is the |
which dynamic content is |
l is the length |
dynamic content is not |
systems such as the |
this amounts to a |
a different concern relates |
is the length of |
such as the smart |
as the smart power |
different concern relates to |
to assess the effect |
amounts to a daily |
the smart power grid |
the length of o |
concern relates to behavior |
assess the effect of |
to a daily revenue |
relates to behavior when |
the effect of the |
control systems for energy |
a daily revenue increase |
daily revenue increase of |
revenue increase of b |
to behavior when membership |
effect of the transformation |
behavior when membership information |
of the transformation on |
when membership information is |
and traffic analysis in |
the transformation on quality |
membership information is perceived |
traffic analysis in large |
so the dependency list |
transformation on quality and |
information is perceived differently |
analysis in large cities |
the dependency list of |
on quality and traffic |
is perceived differently at |
in large cities all |
dependency list of o |
quality and traffic rates |
perceived differently at different |
large cities all depend |
differently at different nodes |
cities all depend on |
all depend on the |
depend on the analysis |
on the analysis of |
usd at the exchange |
at the exchange rate |
although such a condition |
the analysis of data |
how transformation should be |
the exchange rate on |
exchange rate on that |
rate on that date |
transformation should be meaningfully |
such a condition may |
analysis of data supplied |
should be meaningfully expressed |
this represents a considerable |
of data supplied by |
a condition may arise |
be meaningfully expressed and |
represents a considerable increase |
data supplied by measurement |
condition may arise during |
meaningfully expressed and used |
a considerable increase of |
supplied by measurement devices |
may arise during transitional |
expressed and used by |
considerable increase of the |
arise during transitional periods |
yet the clients being |
increase of the pools |
of the pools net |
the pools net revenue |
these quickly resolve as |
the clients being tracked |
and used by content |
quickly resolve as additional |
clients being tracked are |
used by content providers |
resolve as additional rounds |
being tracked are unwilling |
for the smallest pool |
as additional rounds of |
tracked are unwilling to |
additional rounds of gossip |
are unwilling to reveal |
and to learn how |
to learn how computationally |
unwilling to reveal such |
rounds of gossip replace |
learn how computationally intensive |
to reveal such measurement |
the attack is much |
attack is much less |
is much less profitable |
reveal such measurement data |
of gossip replace stale |
how computationally intensive such |
such measurement data directly |
gossip replace stale data |
computationally intensive such transformation |
to reach the optimum |
measurement data directly to |
replace stale data with |
intensive such transformation methods |
reach the optimum it |
data directly to the |
stale data with more |
data with more accurate |
the optimum it needs |
directly to the system |
to the system owner |
optimum it needs almost |
such transformation methods can |
it needs almost a |
who might be curious |
transformation methods can be |
we have never observed |
might be curious about |
be curious about private |
methods can be without |
have never observed a |
needs almost a third |
curious about private client |
about private client information |
never observed a membership |
almost a third of |
can be without overloading |
observed a membership inconsistency |
these systems thus may |
be without overloading the |
without overloading the nodes |
a membership inconsistency that |
systems thus may elicit |
a third of its |
membership inconsistency that persisted |
thus may elicit public |
third of its power |
inconsistency that persisted for |
may elicit public opposition |
balancing bandwidth costs with |
bandwidth costs with end |
that persisted for longer |
elicit public opposition despite |
of its power for |
persisted for longer than |
public opposition despite their |
its power for attacking |
for longer than a |
opposition despite their useful |
power for attacking but |
longer than a few |
despite their useful features |
for attacking but increases |
the g radi ent |
than a few hundred |
their useful features because |
attacking but increases its |
but increases its revenue |
a few hundred milliseconds |
useful features because of |
g radi ent content |
increases its revenue density |
the ssa is quite |
ssa is quite tolerant |
is quite tolerant of |
quite tolerant of short |
radi ent content delivery |
its revenue density by |
revenue density by merely |
ent content delivery system |
features because of a |
content delivery system is |
customuserserviceapp heartbeatmonitor gossiper subserviceprocess |
because of a perceived |
delivery system is currently |
heartbeatmonitor gossiper subserviceprocess chainlink |
of a perceived privacy |
system is currently designed |
gossiper subserviceprocess chainlink subservicecontrol |
a perceived privacy risk |
is currently designed to |
subserviceprocess chainlink subservicecontrol nonblockingtransport |
currently designed to use |
designed to use a |
there are ways to |
to use a spanningtree |
are ways to upload |
use a spanningtree overlay |
ways to upload sensitive |
to upload sensitive data |
upload sensitive data to |
sensitive data to an |
data to an aggregator |
to an aggregator without |
an aggregator without compromising |
similar to most multicast |
aggregator without compromising privacy |
to most multicast network |
most multicast network architectures |
the component stack of |
component stack of one |
stack of one subservice |
but existing options have |
of one subservice process |
existing options have limitations |
with virtual links connecting |
virtual links connecting g |
links connecting g radient |
connecting g radient cdn |
one possibility is to |
g radient cdn nodes |
possibility is to keep |
is to keep the |
failure and recovery process |
to keep the data |
name size discusfish antpool |
size discusfish antpool ghash |
the question is to |
and recovery process failure |
keep the data encrypted |
question is to determine |
recovery process failure detection |
the data encrypted with |
io btchine btcguild eligius |
btchine btcguild eligius others |
process failure detection is |
data encrypted with keys |
is to determine what |
failure detection is accomplished |
encrypted with keys known |
to determine what nodes |
detection is accomplished by |
with keys known only |
determine what nodes the |
is accomplished by means |
keys known only to |
what nodes the in |
accomplished by means of |
known only to the |
by means of two |
only to the clients |
means of two mechanisms |
network processing and connecting |
processing and connecting to |
detecting fifo channels that |
but this requires expensive |
and connecting to the |
fifo channels that break |
this requires expensive homomorphic |
connecting to the diverse |
requires expensive homomorphic encryption |
to the diverse end |
in our case they |
expensive homomorphic encryption if |
our case they are |
homomorphic encryption if the |
case they are tcp |
encryption if the aggregator |
users should be done |
they are tcp channels |
if the aggregator is |
are tcp channels with |
the aggregator is to |
tcp channels with low |
aggregator is to compute |
channels with low value |
is to compute directly |
with low value for |
to compute directly on |
low value for the |
compute directly on it |
value for the so |
we need to optimize |
for the so timeout |
the so timeout property |
need to optimize the |
another is to employ |
to optimize the overlay |
is to employ a |
optimize the overlay to |
to employ a mechanism |
employ a mechanism to |
a mechanism to de |
the overlay to deliver |
based heartbeat detection mechanism |
overlay to deliver the |
correlate client identifiers from |
to deliver the exact |
client identifiers from their |
once a process is |
a process is deceased |
identifiers from their data |
deliver the exact stream |
the exact stream quality |
the information is propagated |
exact stream quality demanded |
as chen et al |
information is propagated within |
stream quality demanded by |
is propagated within the |
quality demanded by users |
propagated within the group |
demanded by users while |
within the group in |
the group in two |
group in two ways |
by users while minimizing |
users while minimizing bandwidth |
while minimizing bandwidth costs |
the process that has |
process that has detected |
that has detected the |
but this imposes restrictions |
has detected the membership |
this imposes restrictions on |
detected the membership change |
imposes restrictions on the |
the membership change feeds |
restrictions on the kind |
we propose to apply |
membership change feeds the |
on the kind of |
propose to apply an |
change feeds the event |
when a transaction is |
the kind of aggregation |
to apply an economics |
feeds the event description |
a transaction is committed |
kind of aggregation that |
apply an economics framework |
the event description into |
of aggregation that can |
event description into the |
aggregation that can be |
description into the chain |
this update is done |
into the chain itself |
that can be done |
considering two primary inputs |
update is done for |
this is delivered in |
two primary inputs in |
is done for all |
is delivered in chain |
primary inputs in determining |
done for all objects |
delivered in chain order |
it would be beneficial |
inputs in determining the |
for all objects in |
in chain order to |
would be beneficial to |
in determining the optimal |
all objects in the |
chain order to every |
order to every non |
determining the optimal network |
objects in the transaction |
be beneficial to execute |
faulty process and where |
process and where necessary |
the optimal network overlay |
beneficial to execute needed |
in the transaction at |
chain repair procedure is |
to execute needed computation |
the transaction at once |
repair procedure is undertaken |
execute needed computation directly |
on the one hand |
needed computation directly on |
computation directly on the |
directly on the client |
on the client platforms |
given a read set |
the same detector process |
we consider the cost |
a read set readset |
same detector process starts |
consider the cost for |
so that the system |
detector process starts up |
the cost for network |
that the system operator |
and a write set |
cost for network edges |
process starts up a |
the system operator or |
a write set writeset |
for network edges to |
starts up a backup |
system operator or analyst |
network edges to carry |
up a backup gossip |
operator or analyst only |
edges to carry traffic |
a backup gossip notification |
or analyst only sees |
containing tuples comprised of |
backup gossip notification stream |
analyst only sees aggregate |
tuples comprised of the |
only sees aggregate results |
this is a fast |
is a fast dying |
a fast dying epidemic |
comprised of the keys |
similar to standard bandwidth |
of the keys accessed |
this approach would provide |
to standard bandwidth pricing |
it spreads rapidly but |
approach would provide a |
spreads rapidly but also |
would provide a better |
rapidly but also dies |
provide a better alternative |
their versions and their |
but also dies out |
a better alternative to |
versions and their dependency |
also dies out rapidly |
we leverage the perceived |
and their dependency lists |
better alternative to central |
leverage the perceived utility |
alternative to central aggregation |
the fifo channels are |
the perceived utility by |
to central aggregation provided |
fifo channels are rebuilt |
perceived utility by end |
central aggregation provided it |
the database aggregates them |
channels are rebuilt appropriately |
aggregation provided it is |
provided it is privacy |
are rebuilt appropriately by |
users for receiving the |
database aggregates them to |
rebuilt appropriately by the |
for receiving the stream |
aggregates them to a |
appropriately by the processes |
receiving the stream at |
them to a single |
by the processes that |
the stream at a |
to a single full |
the processes that identify |
stream at a given |
a single full dependency |
a data aggregation system |
processes that identify themselves |
at a given quality |
single full dependency list |
data aggregation system based |
that identify themselves to |
full dependency list as |
aggregation system based on |
identify themselves to be |
dependency list as follows |
system based on client |
themselves to be affected |
to be affected by |
be affected by the |
affected by the membership |
by the membership change |
the exact solution for |
side computation suggests a |
exact solution for this |
and the group converges |
computation suggests a purely |
suggests a purely peer |
the group converges to |
solution for this optimization |
group converges to a |
converges to a stable |
to a stable configuration |
for this optimization problem |
this optimization problem is |
optimization problem is intractable |
problem is intractable it |
is intractable it is |
intractable it is np |
the six largest open |
update sources can use |
six largest open pool |
sources can use this |
largest open pool sizes |
can use this update |
open pool sizes as |
pool sizes as of |
sizes as of january |
use this update to |
this update to reconnect |
which many systems have |
update to reconnect to |
many systems have used |
to reconnect to a |
systems have used to |
we have developed algorithms |
reconnect to a new |
have used to avoid |
have developed algorithms that |
to a new head |
used to avoid centralized |
a new head of |
developed algorithms that give |
to avoid centralized control |
new head of any |
algorithms that give an |
head of any chain |
that give an approximate |
of any chain that |
give an approximate optimal |
any chain that may |
an approximate optimal solution |
chain that may have |
that may have lost |
may have lost its |
have lost its previous |
lost its previous head |
its previous head as |
previous head as a |
head as a consequence |
as a consequence of |
a consequence of the |
consequence of the crash |
in the case of |
the case of video |
case of video streams |
readset writeset this list |
if a process wants |
a process wants to |
process wants to join |
of video streams whose |
writeset this list is |
their optimal infiltration rates |
video streams whose quality |
this list is pruned |
it starts by sending |
streams whose quality and |
list is pruned to |
of each pool as |
starts by sending a |
whose quality and traffic |
is pruned to match |
each pool as a |
by sending a request |
peer systems have problems |
quality and traffic rates |
pruned to match the |
pool as a fraction |
as a fraction of |
a fraction of its |
fraction of its size |
to match the target |
sending a request to |
systems have problems of |
and traffic rates can |
match the target size |
a request to a |
have problems of their |
problems of their own |
traffic rates can be |
the target size using |
request to a random |
to a random member |
even if we set |
target size using lru |
if it attacked all |
it attacked all others |
attacked all others without |
all others without reciprocation |
rates can be downgraded |
a random member of |
if we set privacy |
can be downgraded by |
and stored with each |
random member of the |
we set privacy concerns |
be downgraded by g |
stored with each write |
member of the group |
and their revenue density |
their revenue density when |
revenue density when attacking |
downgraded by g radient |
set privacy concerns aside |
by g radient cdn |
g radient cdn nodes |
the group member will |
by eschewing centralization entirely |
group member will commence |
t wo p ools |
a list entry can |
member will commence a |
list entry can be |
they can no longer |
entry can be discarded |
wo p ools we |
will commence a membership |
we have derived a |
can no longer take |
have derived a primaldual |
p ools we proceed |
commence a membership change |
can be discarded if |
no longer take advantage |
derived a primaldual approximation |
ools we proceed to |
a membership change protocol |
be discarded if the |
longer take advantage of |
a primaldual approximation algorithm |
we proceed to analyze |
membership change protocol as |
discarded if the same |
take advantage of the |
primaldual approximation algorithm which |
proceed to analyze the |
to analyze the case |
change protocol as described |
if the same entry |
advantage of the powerful |
approximation algorithm which produces |
analyze the case where |
protocol as described above |
the same entry s |
of the powerful management |
algorithm which produces a |
the case where two |
again once all the |
the powerful management tools |
which produces a solution |
same entry s object |
case where two pools |
once all the nodes |
powerful management tools developed |
produces a solution whose |
entry s object appears |
where two pools may |
all the nodes receive |
management tools developed for |
a solution whose total |
s object appears in |
two pools may attack |
the nodes receive the |
tools developed for today |
solution whose total cost |
object appears in another |
pools may attack each |
nodes receive the membership |
developed for today s |
appears in another entry |
may attack each other |
receive the membership event |
for today s cloud |
in another entry with |
the difference between total |
attack each other and |
the membership event and |
today s cloud computing |
another entry with a |
difference between total network |
each other and the |
other and the other |
s cloud computing model |
entry with a larger |
between total network traffic |
membership event and update |
event and update their |
and update their view |
total network traffic costs |
and the other miners |
the other miners mine |
other miners mine solo |
with a larger version |
network traffic costs and |
traffic costs and aggregate |
costs and aggregate end |
clients are isolated network |
again we have pool |
are isolated network hosts |
implementation details the framework |
isolated network hosts rather |
details the framework was |
network hosts rather than |
the framework was implemented |
hosts rather than devices |
were their lengths not |
is within a factor |
rather than devices within |
framework was implemented using |
their lengths not bounded |
within a factor of |
than devices within a |
was implemented using the |
devices within a single |
implemented using the java |
within a single administrative |
dependency lists could quickly |
using the java language |
a single administrative domain |
lists could quickly grow |
controls its infiltration rate |
its infiltration rate x |
could quickly grow to |
the java language and |
and often have difficulty |
quickly grow to include |
java language and its |
often have difficulty maintaining |
grow to include all |
language and its non |
have difficulty maintaining connections |
to include all objects |
difficulty maintaining connections to |
include all objects in |
maintaining connections to each |
all objects in the |
connections to each other |
objects in the database |
of the optimal in |
to each other through |
the optimal in the |
the system design was |
each other through firewalls |
optimal in the worst |
in the worst case |
system design was strongly |
other through firewalls and |
through firewalls and address |
design was strongly influenced |
cache in our scheme |
also controls its infiltration |
controls its infiltration rate |
its infiltration rate x |
firewalls and address translation |
was strongly influenced by |
and address translation barriers |
strongly influenced by prior |
the cache interacts with |
influenced by prior work |
cache interacts with the |
by prior work on |
determining the membership of |
the membership of a |
prior work on highperformance |
interacts with the database |
membership of a peer |
work on highperformance services |
with the database in |
on highperformance services platforms |
the database in essentially |
database in essentially the |
this scenario is illustrated |
scenario is illustrated in |
is illustrated in figure |
peer network is a |
notably welsh s seda |
in essentially the same |
network is a surprisingly |
the total mining power |
total mining power in |
mining power in the |
welsh s seda architecture |
is a surprisingly difficult |
we see that the |
essentially the same manner |
see that the algorithm |
a surprisingly difficult problem |
power in the system |
in the system is |
the system is m |
system is m x |
since there is no |
the same manner as |
that the algorithm has |
there is no one |
same manner as for |
is no one entity |
the algorithm has lower |
manner as for a |
no one entity that |
components are highly autonomous |
algorithm has lower total |
as for a consistency |
has lower total cost |
one entity that knows |
entity that knows the |
lower total cost compared |
that knows the identities |
knows the identities of |
total cost compared to |
the identities of all |
identities of all the |
there are only four |
of all the clients |
cost compared to a |
are only four distinct |
compared to a single |
the direct revenues r |
only four distinct control |
and changes in membership |
to a single stream |
four distinct control threads |
a single stream source |
changes in membership may |
single stream source and |
distinct control threads in |
stream source and a |
of the pools from |
in membership may not |
control threads in the |
source and a minimum |
the pools from mining |
membership may not be |
threads in the component |
and receiving invalidations as |
and a minimum spanning |
pools from mining are |
may not be detected |
in the component stack |
receiving invalidations as the |
a minimum spanning tree |
from mining are their |
not be detected and |
the component stack of |
invalidations as the database |
minimum spanning tree streaming |
mining are their effective |
are their effective mining |
their effective mining rates |
as the database updates |
spanning tree streaming protocol |
be detected and propagated |
component stack of a |
the database updates objects |
tree streaming protocol in |
detected and propagated in |
stack of a process |
streaming protocol in a |
and propagated in a |
protocol in a simulation |
propagated in a timely |
in a simulation based |
in a timely fashion |
a simulation based on |
simulation based on a |
based on a collection |
on a collection of |
a collection of as |
the caches read from |
namely one for the |
caches read from the |
one for the non |
read from the database |
for the non blocking |
from the database not |
the non blocking transport |
the database not only |
without a centralized service |
database not only the |
a centralized service to |
not only the object |
centralized service to assign |
only the object s |
service to assign and |
the tcp chain and |
the object s value |
to assign and manage |
tcp chain and for |
assign and manage node |
chain and for the |
and manage node identities |
gradient mst naive broadcast |
and for the heartbeat |
for the heartbeat component |
mst naive broadcast total |
but also its version |
naive broadcast total cost |
also its version and |
its version and the |
version and the dependency |
and the dependency list |
the ssa is roughly |
the extended cache exports |
extended cache exports a |
cache exports a transactional |
peer system is extremely |
exports a transactional read |
two attacking pools system |
system is extremely vulnerable |
is extremely vulnerable to |
extremely vulnerable to a |
vulnerable to a few |
to a few malicious |
a few malicious peers |
few malicious peers becoming |
malicious peers becoming a |
peers becoming a majority |
client read requests are |
becoming a majority of |
read requests are extended |
a majority of the |
requests are extended with |
majority of the apparent |
are extended with a |
of the apparent nodes |
extended with a transaction |
the apparent nodes in |
with a transaction identifier |
apparent nodes in the |
nodes in the system |
a transaction identifier and |
transaction identifier and a |
identifier and a last |
even choosing peers fairly |
choosing peers fairly becomes |
peers fairly becomes difficult |
because peers usually do |
peers usually do not |
usually do not store |
do not store the |
not store the entire |
store the entire membership |
the entire membership list |
entire membership list locally |
as a function of |
a function of pool |
function of pool sizes |
and it is fairly |
it is fairly easy |
is fairly easy for |
fairly easy for malicious |
easy for malicious peers |
for malicious peers to |
malicious peers to poison |
peers to poison local |
to poison local mem |
poison local mem cornell |
the transaction identifier txnid |
local mem cornell bership |
transaction identifier txnid allows |
mem cornell bership views |
identifier txnid allows the |
cornell bership views so |
txnid allows the cache |
bership views so that |
allows the cache to |
views so that they |
the cache to recognize |
so that they will |
cache to recognize reads |
that they will be |
to recognize reads belonging |
they will be preferred |
recognize reads belonging to |
will be preferred as |
reads belonging to the |
be preferred as neighbors |
belonging to the same |
preferred as neighbors by |
to the same transaction |
as neighbors by honest |
neighbors by honest nodes |
the cache responds with |
cache responds with either |
responds with either the |
with either the value |
either the value of |
the value of the |
value of the requested |
of the requested object |
or with an abort |
with an abort if |
an abort if it |
abort if it detects |
if it detects an |
it detects an inconsistency |
since neither completely centralized |
detects an inconsistency between |
neither completely centralized aggregation |
an inconsistency between this |
completely centralized aggregation nor |
inconsistency between this read |
centralized aggregation nor a |
between this read and |
aggregation nor a completely |
nor a completely peer |
this read and any |
read and any of |
and any of the |
any of the previous |
of the previous reads |
the previous reads with |
previous reads with the |
peer system is adequate |
reads with the same |
system is adequate for |
with the same transaction |
is adequate for our |
the same transaction id |
adequate for our purposes |
we explore a new |
explore a new approach |
we do not guarantee |
a new approach that |
do not guarantee that |
new approach that combines |
not guarantee that inconsistencies |
approach that combines the |
guarantee that inconsistencies will |
that combines the features |
that inconsistencies will be |
combines the features of |
inconsistencies will be detected |
the features of these |
features of these two |
of these two extremes |
the lastop allows the |
although the idea of |
lastop allows the cache |
the idea of a |
allows the cache to |
idea of a communication |
the cache to garbage |
of a communication system |
a communication system that |
communication system that combines |
system that combines some |
that combines some centralized |
combines some centralized control |
some centralized control with |
collect its transaction record |
centralized control with a |
control with a peer |
its transaction record after |
transaction record after responding |
record after responding to |
after responding to the |
responding to the last |
to the last read |
peer overlay is not |
overlay is not new |
the last read operation |
last read operation of |
two pools infiltrating each |
pools infiltrating each other |
we are the first |
read operation of the |
are the first to |
operation of the transaction |
the first to use |
first to use such |
to use such a |
use such a system |
divided by the total |
by the total mining |
the total mining rate |
such a system to |
the cache will treat |
a system to preserve |
cache will treat subsequent |
system to preserve privacy |
will treat subsequent accesses |
to preserve privacy while |
treat subsequent accesses with |
preserve privacy while computing |
subsequent accesses with the |
privacy while computing on |
accesses with the same |
while computing on sensitive |
computing on sensitive data |
with the same transaction |
the same transaction id |
same transaction id as |
transaction id as new |
id as new transactions |
this combination is a |
combination is a sensible |
is a sensible tradeoff |
a sensible tradeoff for |
sensible tradeoff for the |
tradeoff for the kinds |
for the kinds of |
to implement this interface |
the kinds of systems |
the g radient optimization |
kinds of systems we |
g radient optimization is |
of systems we target |
radient optimization is effective |
the cache maintains a |
optimization is effective compared |
cache maintains a record |
in which there is |
is effective compared to |
maintains a record of |
which there is an |
effective compared to a |
a record of each |
there is an owner |
compared to a centralized |
record of each transaction |
is an owner or |
to a centralized source |
of each transaction with |
an owner or operator |
a centralized source and |
each transaction with its |
owner or operator who |
centralized source and a |
transaction with its read |
or operator who can |
source and a minimum |
with its read values |
operator who can be |
and a minimum spanning |
who can be trusted |
a minimum spanning tree |
can be trusted to |
be trusted to provide |
trusted to provide basic |
to provide basic services |
provide basic services such |
basic services such as |
and their dependency lists |
services such as node |
such as node identification |
as node identification and |
node identification and membership |
identification and membership tracking |
protocol even as system |
and membership tracking but |
on a read of |
even as system sizes |
membership tracking but not |
a read of keycurr |
as system sizes scale |
tracking but not to |
but not to see |
not to see non |
system sizes scale up |
the cache first obtains |
cache first obtains the |
aggregated raw client data |
first obtains the requested |
obtains the requested entry |
the requested entry from |
error bars represent one |
the total revenue of |
total revenue of each |
revenue of each pool |
we treat the system |
bars represent one standard |
of each pool is |
each pool is its |
pool is its direct |
is its direct mining |
its direct mining revenue |
treat the system operator |
represent one standard deviation |
requested entry from memory |
the system operator as |
one standard deviation over |
system operator as an |
operator as an honest |
and the infiltration revenue |
the infiltration revenue from |
infiltration revenue from the |
revenue from the previous |
from the previous round |
which is the attacked |
is the attacked pool |
the attacked pool s |
attacked pool s total |
pool s total revenue |
s total revenue multiplied |
who will keep the |
total revenue multiplied by |
revenue multiplied by its |
multiplied by its infiltration |
by its infiltration rate |
will keep the system |
keep the system running |
the system running correctly |
the pool s total |
system running correctly but |
pool s total revenue |
running correctly but cannot |
s total revenue is |
the details are deferred |
the entry includes the |
correctly but cannot be |
total revenue is divided |
details are deferred to |
entry includes the value |
but cannot be allowed |
revenue is divided among |
is divided among its |
cannot be allowed to |
are deferred to a |
version vercurr and dependency |
be allowed to see |
divided among its loyal |
among its loyal miners |
its loyal miners and |
loyal miners and miners |
miners and miners that |
and miners that infiltrated |
miners that infiltrated it |
deferred to a full |
vercurr and dependency list |
allowed to see more |
to a full report |
and dependency list deplistcurr |
at stable state this |
stable state this is |
state this is r |
to see more information |
a full report on |
see more information than |
the cache checks the |
full report on g |
more information than he |
cache checks the currently |
report on g radient |
information than he or |
checks the currently read |
than he or she |
the currently read object |
he or she needs |
or she needs to |
she needs to know |
currently read object against |
read object against each |
object against each of |
against each of the |
each of the previously |
of the previously read |
the previously read objects |
conclusion a number of |
a number of interesting |
number of interesting open |
if a previously read |
of interesting open questions |
a previously read version |
interesting open questions remain |
previously read version v |
open questions remain the |
we introduce a method |
read version v is |
questions remain the focus |
introduce a method for |
version v is older |
remain the focus of |
a method for constructing |
v is older than |
the focus of our |
method for constructing a |
is older than expected |
focus of our continued |
for constructing a communication |
older than expected by |
of our continued investigation |
constructing a communication overlay |
than expected by the |
a communication overlay among |
expected by the current |
communication overlay among the |
by the current read |
how diverse are the |
overlay among the client |
the current read s |
diverse are the classes |
among the client nodes |
current read s dependencies |
are the classes of |
the client nodes that |
read s dependencies v |
the classes of content |
client nodes that can |
s dependencies v k |
classes of content that |
nodes that can safely |
of content that are |
that can safely be |
content that are amenable |
can safely be used |
that are amenable to |
safely be used to |
are amenable to our |
be used to perform |
amenable to our in |
used to perform aggregation |
to perform aggregation and |
perform aggregation and computation |
aggregation and computation on |
and computation on private |
computation on private data |
we obtain the following |
obtain the following closed |
the following closed expressions |
following closed expressions for |
closed expressions for each |
how do we best |
although this overlay is |
do we best assess |
this overlay is set |
we express the revenues |
express the revenues as |
the revenues as functions |
revenues as functions of |
as functions of x |
we best assess the |
overlay is set up |
best assess the effect |
is set up and |
assess the effect of |
set up and operated |
the effect of such |
up and operated by |
effect of such transformations |
and operated by the |
of such transformations on |
operated by the system |
by the system owner |
such transformations on stream |
transformations on stream quality |
it provides minimal opportunity |
provides minimal opportunity for |
minimal opportunity for the |
how should these transformations |
opportunity for the owner |
should these transformations be |
for the owner to |
these transformations be expressed |
the owner to learn |
owner to learn any |
to learn any information |
learn any information about |
any information about the |
information about the data |
about the data being |
the data being aggregated |
data being aggregated other |
being aggregated other than |
or the current read |
aggregated other than the |
and utilized by the |
the current read vcurr |
other than the final |
utilized by the originating |
current read vcurr is |
than the final result |
by the originating content |
read vcurr is older |
the final result of |
the originating content providers |
vcurr is older than |
final result of the |
result of the computation |
is older than expected |
originating content providers to |
older than expected by |
content providers to best |
when combined with differential |
than expected by the |
providers to best balance |
combined with differential privacy |
expected by the dependencies |
to best balance content |
with differential privacy techniques |
by the dependencies of |
the dependencies of a |
dependencies of a previous |
of a previous read |
to protect the aggregation |
domain specificity with ease |
a previous read v |
protect the aggregation results |
the aggregation results themselves |
previous read v v |
specificity with ease of |
with ease of development |
it can be used |
can be used to |
be used to ensure |
used to ensure that |
to ensure that no |
ensure that no query |
how can our overlay |
that no query made |
can our overlay respond |
no query made to |
our overlay respond to |
query made to the |
overlay respond to churn |
made to the system |
respond to churn among |
to the system reveals |
as seen by the |
seen by the entire |
by the entire chain |
to churn among g |
the system reveals the |
churn among g radient |
system reveals the contribution |
among g radient nodes |
reveals the contribution of |
g radient nodes realistically |
the contribution of any |
radient nodes realistically low |
contribution of any particular |
nodes realistically low in |
of any particular node |
realistically low in many |
low in many common |
in many common cases |
many common cases such |
our overlay network looks |
common cases such as |
overlay network looks a |
cases such as video |
an inconsistency is detected |
network looks a bit |
such as video streaming |
looks a bit like |
a bit like a |
bit like a gossip |
like a gossip infrastructure |
otherwise the cache returns |
the cache returns the |
cache returns the read |
but higher in alternative |
returns the read value |
the read value to |
read value to the |
value to the client |
higher in alternative deployment |
in alternative deployment scenarios |
upon detecting an inconsistency |
and can be used |
can be used to |
be used to run |
used to run gossip |
the cache can take |
cache can take one |
can take one of |
take one of three |
one of three paths |
with the key difference |
the key difference that |
how do we ensure |
key difference that the |
do we ensure that |
difference that the random |
we ensure that the |
that the random peer |
ensure that the computational |
the random peer selection |
that the computational intensity |
random peer selection of |
the computational intensity of |
peer selection of gossip |
computational intensity of our |
experimental results and validation |
intensity of our transformations |
abort the current transaction |
selection of gossip is |
of our transformations do |
of gossip is replaced |
our transformations do not |
gossip is replaced with |
compared to the other |
to the other approaches |
is replaced with a |
transformations do not place |
replaced with a completely |
with a completely deterministic |
a completely deterministic function |
do not place too |
this has the benefit |
not place too much |
has the benefit of |
place too much load |
nodes are assigned virtual |
the benefit of affecting |
too much load on |
are assigned virtual ids |
benefit of affecting only |
much load on our |
assigned virtual ids that |
of affecting only the |
load on our g |
virtual ids that are |
affecting only the running |
on our g radient |
the tests reported here |
ids that are either |
only the running transaction |
our g radient overlay |
tests reported here employ |
that are either integers |
the running transaction and |
g radient overlay nodes |
reported here employ a |
here employ a hard |
running transaction and limiting |
are either integers or |
transaction and limiting collateral |
either integers or finite |
and limiting collateral damage |
integers or finite field |
or finite field elements |
g radient contributes a |
the ssa is a |
and each node uses |
radient contributes a novel |
ssa is a work |
contributes a novel platform |
each node uses a |
is a work in |
node uses a function |
a work in progress |
uses a function based |
a novel platform for |
a function based on |
novel platform for continued |
function based on either |
abort the current transaction |
platform for continued study |
fledged system will use |
for continued study and |
the current transaction and |
based on either modular |
each pool controls only |
pool controls only its |
controls only its own |
only its own infiltration |
its own infiltration rate |
system will use a |
continued study and progress |
current transaction and evict |
on either modular arithmetic |
will use a software |
study and progress to |
in each round of |
each round of the |
round of the pool |
of the pool game |
and progress to ever |
transaction and evict the |
either modular arithmetic or |
use a software partitioning |
progress to ever more |
and evict the violating |
each pool will optimize |
modular arithmetic or finite |
a software partitioning mechanism |
to ever more effective |
pool will optimize its |
arithmetic or finite fields |
software partitioning mechanism based |
ever more effective delivery |
more effective delivery mechanisms |
or finite fields to |
partitioning mechanism based on |
will optimize its infiltration |
object from the cache |
mechanism based on the |
based on the web |
finite fields to compute |
optimize its infiltration rate |
its infiltration rate of |
infiltration rate of the |
rate of the other |
fields to compute the |
this approach guesses that |
on the web services |
to compute the order |
approach guesses that future |
the web services request |
compute the order in |
guesses that future transactions |
acts at step t |
web services request invocation |
the order in which |
that future transactions are |
services request invocation model |
order in which it |
it optimizes its revenue |
optimizes its revenue with |
its revenue with x |
future transactions are likely |
in which it should |
although extracting the partitioning |
transactions are likely to |
which it should communicate |
extracting the partitioning key |
are likely to abort |
it should communicate with |
the partitioning key from |
likely to abort because |
should communicate with the |
partitioning key from incoming |
to abort because of |
communicate with the other |
key from incoming requests |
abort because of this |
with the other nodes |
from incoming requests will |
because of this object |
incoming requests will impose |
requests will impose some |
will impose some overhead |
we construct this function |
construct this function to |
this function to ensure |
we do not expect |
function to ensure that |
do not expect performance |
to ensure that the |
not expect performance of |
ensure that the network |
expect performance of the |
that the network is |
performance of the full |
the network is optimally |
network is optimally robust |
is optimally robust and |
optimally robust and efficient |
fledged system to deviate |
check which is the |
system to deviate significantly |
which is the violating |
converging in logarithmic time |
to deviate significantly from |
is the violating object |
in logarithmic time and |
deviate significantly from what |
logarithmic time and tolerating |
significantly from what is |
time and tolerating message |
from what is reported |
what is reported below |
if it is the |
and tolerating message failures |
it is the currently |
tolerating message failures with |
is the currently accessed |
message failures with minimal |
the currently accessed object |
failures with minimal delay |
bandwidth multicast in cooperative |
multicast in cooperative environments |
key cryptography to encrypt |
cryptography to encrypt messages |
ensuring that the the |
acts at step t |
that the the system |
the the system operator |
treat this access as |
it optimizes its revenue |
optimizes its revenue with |
its revenue with x |
the system operator cannot |
this access as a |
system operator cannot infer |
access as a miss |
operator cannot infer anything |
as a miss and |
cannot infer anything about |
a miss and respond |
infer anything about the |
miss and respond to |
anything about the data |
and respond to it |
about the data being |
respond to it with |
the data being aggregated |
to it with a |
data being aggregated by |
it with a value |
being aggregated by observing |
with a value read |
aggregated by observing network |
a value read from |
by observing network traffic |
value read from the |
read from the database |
even the communication pattern |
the communication pattern is |
communication pattern is completely |
pattern is completely predictable |
is completely predictable and |
if the violating object |
completely predictable and hence |
predictable and hence reveals |
and hence reveals nothing |
the violating object was |
violating object was returned |
object was returned to |
was returned to the |
returned to the user |
to the user as |
the user as the |
malicious nodes cannot significantly |
user as the result |
nodes cannot significantly deviate |
as the result of |
cannot significantly deviate from |
the result of a |
significantly deviate from correct |
update injection time against |
result of a read |
deviate from correct behavior |
injection time against delivery |
time against delivery time |
from correct behavior without |
of a read earlier |
against delivery time at |
delivery time at node |
correct behavior without being |
behavior without being detected |
approaching the zettabyte era |
a read earlier in |
read earlier in the |
earlier in the transaction |
so the network encourages |
the network encourages the |
network encourages the operator |
encourages the operator to |
cisco visual networking index |
an equilibrium exists where |
equilibrium exists where neither |
exists where neither pool |
the operator to behave |
operator to behave correctly |
and it even tolerates |
can improve its revenue |
it even tolerates byzantine |
improve its revenue by |
its revenue by changing |
revenue by changing its |
by changing its infiltration |
changing its infiltration rate |
evict the stale object |
even tolerates byzantine failure |
the stale object and |
tolerates byzantine failure by |
stale object and abort |
byzantine failure by a |
object and abort the |
and abort the transaction |
failure by a small |
by a small minority |
a small minority of |
small minority of clients |
any pair of values |
pair of values x |
this ensures that important |
ensures that important queries |
consistency with unbounded resources |
that important queries will |
important queries will not |
queries will not be |
will not be corrupted |
not be corrupted or |
be corrupted or blocked |
corrupted or blocked by |
or blocked by compromised |
blocked by compromised devices |
cache detects all inconsistencies |
such that arg maxx |
and that an adversary |
that an adversary cannot |
an adversary cannot compromise |
as stated in the |
adversary cannot compromise the |
stated in the following |
cannot compromise the privacy |
in the following theorem |
compromise the privacy of |
the privacy of client |
privacy of client data |
of client data by |
global mobile data traffic |
client data by gaining |
mobile data traffic forecast |
data by gaining control |
data traffic forecast update |
by gaining control of |
gaining control of a |
control of a few |
of a few devices |
a few devices in |
few devices in the |
devices in the system |
cache with unbounded cache |
with unbounded cache size |
unbounded cache size and |
cache size and unbounded |
size and unbounded dependency |
and unbounded dependency lists |
unbounded dependency lists implements |
dependency lists implements cache |
ro bert orma ndi |
deferred to appendix a |
istva n hegedu s |
is by constructing a |
by constructing a serialization |
and ma rk jelasity |
constructing a serialization of |
a serialization of the |
serialization of the transactions |
of the transactions in |
gossip learning with linear |
the transactions in the |
learning with linear models |
transactions in the database |
with linear models on |
in the database and |
linear models on fully |
the database and in |
models on fully distributed |
database and in one |
on fully distributed this |
and in one cache |
fully distributed this work |
update delay as seen |
distributed this work was |
delay as seen by |
this work was supported |
as seen by individual |
based on the fact |
seen by individual processes |
on the fact that |
the fact that the |
fact that the transactions |
that the transactions in |
by a grant from |
the transactions in the |
a grant from the |
transactions in the database |
grant from the nsf |
in the database are |
from the nsf data |
the database are serializable |
database are serializable by |
are serializable by definition |
practice and exsmart grids |
the implications of theorem |
and exsmart grids program |
will be seen in |
be seen in section |
seen in section v |
cache converges to perfect |
converges to perfect detection |
to perfect detection when |
perfect detection when stable |
detection when stable clusters |
when stable clusters are |
multicast routing in datagram |
stable clusters are as |
routing in datagram internetworks |
clusters are as large |
in datagram internetworks and |
are as large as |
datagram internetworks and extended |
as large as its |
internetworks and extended lans |
large as its dependency |
as its dependency lists |
our experiments were conducted |
experiments were conducted using |
were conducted using the |
conducted using the ssa |
acm transactions on computer |
in such a scenario |
using the ssa framework |
transactions on computer systems |
the ssa framework deployed |
ssa framework deployed on |
framework deployed on a |
deployed on a tightly |
the dependency lists are |
on a tightly coupled |
dependency lists are large |
a tightly coupled homogeneous |
lists are large enough |
tightly coupled homogeneous cluster |
are large enough to |
coupled homogeneous cluster of |
large enough to describe |
enough to describe all |
to describe all relevant |
describe all relevant dependencies |
the nodes are connected |
the feasible region for |
feasible region for the |
e xperimental s etup |
antony rowstron and peter |
nodes are connected by |
region for the pool |
for the pool sizes |
the pool sizes is |
pool sizes is m |
xperimental s etup to |
rowstron and peter druschel |
are connected by two |
s etup to evaluate |
connected by two separate |
etup to evaluate the |
by two separate high |
to evaluate the effectiveness |
two separate high speed |
evaluate the effectiveness of |
separate high speed ethernet |
the effectiveness of our |
high speed ethernet backbone |
effectiveness of our scheme |
speed ethernet backbone planes |
and routing for large |
we experimented with several |
experimented with several configurations |
we implemented a prototype |
some placed the control |
placed the control traffic |
to study the properties |
the control traffic on |
study the properties of |
control traffic on a |
the properties of the |
traffic on a different |
properties of the cache |
on a different switched |
a different switched ethernet |
different switched ethernet segment |
switched ethernet segment while |
the revenue function for |
ethernet segment while others |
we only need a |
revenue function for ri |
segment while others aggregated |
only need a single |
need a single column |
while others aggregated both |
function for ri is |
others aggregated both the |
for ri is concave |
aggregated both the control |
ri is concave in |
both the control traffic |
is concave in xi |
the control traffic and |
concave in xi for |
control traffic and the |
in xi for all |
traffic and the data |
namely a single cache |
xi for all feasible |
for all feasible values |
all feasible values of |
feasible values of the |
values of the variables |
and the data traffic |
a single cache backed |
the data traffic on |
single cache backed by |
data traffic on the |
cache backed by a |
traffic on the same |
on the same segment |
backed by a single |
by a single database |
a single database server |
no significant differences were |
significant differences were observed |
but this may be |
this may be because |
may be because our |
be because our control |
because our control traffic |
illustrates the structure of |
our control traffic consisted |
the structure of our |
control traffic consisted mainly |
structure of our experimental |
traffic consisted mainly of |
consisted mainly of fast |
of our experimental setup |
therefore the solutions for |
the solutions for equations |
a single database implements |
which put little stress |
single database implements a |
put little stress on |
database implements a transactional |
little stress on the |
implements a transactional key |
stress on the communication |
on the communication channels |
are unique and are |
unique and are either |
and are either at |
in the future we |
are either at the |
correctness of a gossip |
the future we hope |
either at the borders |
of a gossip based |
future we hope to |
at the borders of |
a gossip based membership |
we hope to explore |
the borders of the |
gossip based membership protocol |
hope to explore scenarios |
borders of the feasible |
to explore scenarios that |
update clients access database |
of the feasible region |
the feasible region or |
feasible region or where |
region or where ri |
in proceedings of the |
which sends invalidations to |
explore scenarios that generate |
proceedings of the twenty |
sends invalidations to the |
scenarios that generate exceptionally |
invalidations to the cache |
that generate exceptionally heavy |
fourth annual acm sympo |
generate exceptionally heavy control |
exceptionally heavy control traffic |
which would allow us |
would allow us to |
from section v we |
section v we know |
v we know that |
we know that no |
allow us to explore |
only clients access cache |
and analysis of a |
attack is not an |
is not an equilibrium |
not an equilibrium point |
us to explore the |
analysis of a peer |
to explore the benefits |
explore the benefits of |
since each pool can |
the benefits of isolation |
each pool can increase |
benefits of isolation of |
of isolation of that |
pool can increase its |
and ma rk jelasity |
isolation of that traffic |
of that traffic with |
that traffic with respect |
traffic with respect to |
receives all transactions and |
can increase its revenue |
with respect to data |
respect to data traffic |
all transactions and rigorously |
increase its revenue by |
a private framework for |
transactions and rigorously detects |
its revenue by choosing |
private framework for distributed |
in the interest of |
and rigorously detects inconsistencies |
revenue by choosing a |
by choosing a strictly |
choosing a strictly positive |
a strictly positive infiltration |
strictly positive infiltration rate |
the interest of brevity |
rigorously detects inconsistencies for |
framework for distributed comsium |
interest of brevity we |
detects inconsistencies for statistics |
for distributed comsium on |
of brevity we did |
distributed comsium on principles |
brevity we did not |
comsium on principles of |
we did not perform |
on principles of distributed |
did not perform any |
principles of distributed computing |
not perform any experiments |
perform any experiments to |
any experiments to evaluate |
experiments to evaluate the |
to evaluate the load |
evaluate the load balancing |
the load balancing component |
a set of cache |
load balancing component but |
set of cache clients |
balancing component but we |
of cache clients perform |
component but we plan |
cache clients perform readonly |
but we plan to |
clients perform readonly transactions |
we plan to do |
perform readonly transactions through |
plan to do so |
readonly transactions through a |
to do so in |
do so in the |
so in the future |
transactions through a single |
through a single cache |
a single cache server |
is not a solution |
not a solution to |
a solution to equations |
all the experiments involved |
the experiments involved a |
experiments involved a single |
the cache serves the |
involved a single partitioned |
cache serves the requests |
a single partitioned and |
serves the requests from |
single partitioned and replicated |
the requests from its |
partitioned and replicated service |
requests from its local |
from its local storage |
its local storage if |
local storage if possible |
for ease of exposition |
nash equilibrium therefore exists |
equilibrium therefore exists with |
therefore exists with x |
this service implements a |
or reads from the |
service implements a simple |
implements a simple wall |
reads from the database |
from the database otherwise |
the service itself maintains |
service itself maintains the |
itself maintains the time |
the cache registers an |
with updates coming from |
cache registers an upcall |
updates coming from client |
registers an upcall that |
coming from client applications |
an upcall that can |
from client applications that |
upcall that can be |
client applications that read |
that can be used |
applications that read a |
can be used by |
that read a high |
be used by the |
used by the database |
by the database to |
the database to report |
quality clock and send |
database to report invalidations |
clock and send the |
and send the current |
send the current value |
scaling virtual worlds with |
virtual worlds with a |
worlds with a physical |
with a physical metaphor |
after each update transaction |
as processes forward updates |
processes forward updates along |
each update transaction the |
forward updates along the |
updates along the chain |
update transaction the database |
transaction the database asynchronously |
the database asynchronously sends |
they will track the |
database asynchronously sends invalidations |
will track the clock |
asynchronously sends invalidations to |
track the clock themselves |
sends invalidations to the |
invalidations to the cache |
to the cache for |
the cache for all |
all of our partitioning |
cache for all objects |
of our partitioning scenarios |
for all objects that |
our partitioning scenarios included |
all objects that were |
partitioning scenarios included at |
objects that were modified |
scenarios included at least |
included at least four |
at least four subservices |
uniform node sampling service |
node sampling service robust |
sampling service robust against |
service robust against collusions |
and each subservice included |
robust against collusions of |
each subservice included between |
against collusions of malicious |
collusions of malicious nodes |
we expect these to |
chosen uniformly at random |
expect these to be |
these to be typical |
to be typical cases |
be typical cases for |
typical cases for real |
cases for real deployments |
for real deployments of |
are dropped by the |
real deployments of the |
deployments of the ssa |
dropped by the experiment |
it should be noted |
should be noted that |
be noted that small |
noted that small subservice |
that small subservice sizes |
ifip international conference on |
this is extreme and |
international conference on dependable |
is extreme and would |
conference on dependable systems |
extreme and would only |
on dependable systems and |
dependable systems and networks |
and would only be |
would only be seen |
can result in degenerate |
only be seen in |
result in degenerate behavior |
be seen in the |
in degenerate behavior and |
seen in the real |
degenerate behavior and are |
in the real world |
behavior and are not |
the real world under |
and are not appropriate |
real world under conditions |
are not appropriate configurations |
not appropriate configurations for |
world under conditions of |
using symbolic computation tools |
appropriate configurations for the |
configurations for the ssa |
for the ssa architecture |
under conditions of overload |
we see that there |
see that there is |
that there is a |
conditions of overload or |
there is a single |
is a single pair |
of overload or when |
a single pair of |
single pair of values |
pair of values for |
of values for which |
values for which equation |
overload or when the |
mapping between service processes |
or when the system |
between service processes and |
when the system configuration |
service processes and physical |
the system configuration is |
processes and physical nodes |
holds for any feasible |
for any feasible choice |
any feasible choice of |
feasible choice of m |
system configuration is changed |
in order to avoid |
order to avoid os |
to avoid os resource |
avoid os resource contention |
both the database and |
the database and the |
we experimented with groups |
database and the cache |
experimented with groups of |
and the cache report |
the cache report all |
cache report all completed |
report all completed transactions |
all completed transactions to |
completed transactions to a |
transactions to a consistency |
to a consistency monitor |
numerical analysis a numerical |
analysis a numerical analysis |
a numerical analysis confirms |
numerical analysis confirms these |
analysis confirms these observations |
created in order to |
in order to gather |
order to gather statistics |
to gather statistics for |
byzantine resilient random membership |
gather statistics for our |
statistics for our evaluation |
resilient random membership sampling |
we simulate the pool |
simulate the pool game |
the pool game for |
pool game for a |
this server collects both |
game for a range |
for a range of |
a range of pool |
range of pool sizes |
server collects both committed |
in proceedings of the |
proceedings of the twenty |
collects both committed and |
for each choice of |
each choice of pool |
choice of pool sizes |
seventh acm symposium on |
both committed and aborted |
acm symposium on principles |
committed and aborted transactions |
we start the simulation |
symposium on principles of |
by convention the head |
and aborted transactions and |
start the simulation when |
on principles of distributed |
convention the head of |
aborted transactions and it |
the simulation when both |
principles of distributed computing |
the head of the |
transactions and it maintains |
simulation when both pools |
when both pools do |
both pools do not |
pools do not infiltrate |
do not infiltrate each |
not infiltrate each other |
and it maintains the |
head of the chain |
it maintains the full |
of the chain for |
maintains the full dependency |
the chain for each |
the full dependency graph |
chain for each group |
for each group was |
each group was called |
group was called node |
it performs full serialization |
performs full serialization graph |
full serialization graph testing |
and all update requests |
all update requests for |
update requests for a |
requests for a partition |
for a partition were |
a partition were routed |
partition were routed towards |
were routed towards this |
routed towards this node |
and calculates the rate |
calculates the rate of |
since delivery delays in |
the rate of inconsistent |
delivery delays in the |
rate of inconsistent transactions |
delays in the chain |
of inconsistent transactions that |
in the chain were |
inconsistent transactions that committed |
the chain were measured |
transactions that committed and |
chain were measured relative |
were measured relative to |
measured relative to node |
and the revenue densities |
the revenue densities are |
revenue densities are r |
that committed and the |
committed and the rate |
and the rate of |
the rate of consistent |
rate of consistent transactions |
all the statistics pertaining |
of consistent transactions that |
the statistics pertaining to |
consistent transactions that were |
statistics pertaining to the |
transactions that were unnecessarily |
pertaining to the group |
that were unnecessarily aborted |
to the group disregarded |
the group disregarded node |
a platform for distributed |
platform for distributed service |
for distributed service deployment |
our prototype does not |
distributed service deployment in |
prototype does not address |
service deployment in end |
at each round one |
does not address the |
we simulated two classes |
deployment in end user |
in end user homes |
not address the issue |
simulated two classes of |
two classes of failures |
address the issue of |
each round one pool |
the issue of cache |
round one pool chooses |
issue of cache eviction |
of cache eviction when |
at some time t |
cache eviction when running |
eviction when running out |
some time t one |
time t one process |
when running out of |
running out of memory |
in proceedings of the |
proceedings of the acm |
of the acm sigcomm |
one pool chooses its |
pool chooses its optimal |
chooses its optimal infiltration |
its optimal infiltration rate |
optimal infiltration rate based |
all objects in the |
objects in the workload |
in the workload fit |
the workload fit in |
workload fit in the |
the system must detect |
fit in the cache |
infiltration rate based on |
system must detect the |
must detect the failure |
rate based on the |
and eviction is only |
based on the pool |
repair the broken fifo |
the broken fifo channel |
on the pool sizes |
eviction is only done |
is only done if |
only done if there |
done if there is |
if there is a |
there is a direct |
is a direct reason |
the pool sizes and |
pool sizes and the |
sizes and the rate |
and the rate with |
the rate with which |
rate with which it |
with which it is |
which it is infiltrated |
the failed process recovers |
failed process recovers and |
had we modeled them |
process recovers and rejoins |
and we calculate the |
recovers and rejoins the |
and rejoins the chain |
we calculate the revenue |
evictions would reduce the |
would reduce the cache |
reduce the cache hit |
the cache hit rate |
calculate the revenue after |
the join protocol would |
join protocol would run |
the revenue after convergence |
revenue after convergence with |
after convergence with equation |
but could not cause |
and the previously failed |
could not cause new |
the previously failed node |
not cause new inconsistencies |
previously failed node would |
failed node would become |
node would become the |
would become the new |
become the new tail |
the new tail of |
new tail of the |
tail of the chain |
recall the players in |
we evaluate the effectiveness |
the players in the |
evaluate the effectiveness of |
players in the pool |
the scenario is intended |
the effectiveness of our |
in the pool game |
scenario is intended to |
effectiveness of our transactional |
the pool game are |
is intended to model |
of our transactional cache |
pool game are chosen |
game are chosen with |
are chosen with the |
chosen with the round |
with the round robin |
the round robin policy |
our transactional cache using |
intended to model a |
transactional cache using various |
to model a common |
cache using various workloads |
so the pools take |
the pools take turns |
using various workloads and |
model a common case |
various workloads and varying |
a common case in |
and we let the |
we let the game |
let the game run |
the game run until |
game run until convergence |
workloads and varying the |
common case in which |
and varying the size |
the results are illustrated |
results are illustrated in |
are illustrated in figure |
case in which the |
varying the size of |
in which the failure |
the size of the |
which the failure detection |
size of the dependency |
the failure detection mechanism |
epidemic algorithms for replicated |
algorithms for replicated database |
of the dependency lists |
failure detection mechanism senses |
detection mechanism senses a |
for replicated database maintenance |
the dependency lists maintained |
each run with some |
run with some m |
dependency lists maintained by |
mechanism senses a transient |
in proceedings of the |
lists maintained by the |
senses a transient problem |
proceedings of the sixth |
maintained by the cache |
of the sixth annual |
by the cache and |
the sixth annual acm |
values results in a |
the cache and the |
cache and the database |
results in a single |
sixth annual acm symposium |
a node that has |
in a single point |
a single point in |
single point in each |
point in each graph |
in each graph in |
each graph in figure |
for the cases considered |
annual acm symposium on |
node that has become |
acm symposium on principles |
that has become overloaded |
symposium on principles of |
short dependency lists suffice |
we depict the infiltration |
on principles of distributed |
has become overloaded or |
become overloaded or is |
principles of distributed computing |
depict the infiltration rates |
the infiltration rates of |
infiltration rates of both |
rates of both pools |
of both pools x |
overloaded or is unresponsive |
or is unresponsive for |
is unresponsive for some |
unresponsive for some other |
for some other reason |
an open question for |
open question for further |
question for further study |
such as garbage collection |
for further study is |
further study is whether |
study is whether there |
is whether there are |
whether there are workloads |
there are workloads that |
are workloads that might |
workloads that might require |
that might require limited |
and does not respond |
might require limited but |
does not respond to |
require limited but larger |
not respond to the |
limited but larger values |
respond to the heartbeat |
to the heartbeat within |
the heartbeat within the |
heartbeat within the accepted |
within the accepted window |
note that dependencies arise |
that dependencies arise from |
dependencies arise from the |
arise from the topology |
from the topology of |
the topology of the |
topology of the object |
of the object graph |
b and the pools |
and the pools revenue |
the pools revenue densities |
pools revenue densities r |
by reconfiguring the chain |
and not from the |
not from the size |
from the size of |
the size of the |
size of the transactions |
the load on node |
load on node drops |
of the transactions read |
the transactions read and |
transactions read and write |
read and write sets |
and the problem will |
the problem will eventually |
problem will eventually resolve |
as a baseline for |
a baseline for comparison |
it then requests a |
then requests a rejoin |
we also implemented a |
also implemented a timeout |
a node crash that |
node crash that results |
for each choice of |
each choice of m |
crash that results in |
that results in a |
results in a reboot |
in a reboot would |
it reduces the probability |
a reboot would result |
reduces the probability of |
reboot would result in |
the probability of inconsistency |
would result in similar |
probability of inconsistency by |
the values of x |
result in similar behavior |
of inconsistency by limiting |
inconsistency by limiting the |
by limiting the life |
limiting the life span |
the life span of |
life span of cache |
span of cache entries |
we compare this method |
compare this method against |
all the nodes in |
this method against our |
the nodes in the |
method against our transactional |
nodes in the subservice |
against our transactional cache |
in the subservice remain |
our transactional cache by |
the subservice remain operational |
transactional cache by measuring |
cache by measuring its |
by measuring its effectiveness |
measuring its effectiveness with |
its effectiveness with a |
but one of them |
effectiveness with a varying |
in lecture notes in |
one of them becomes |
with a varying time |
lecture notes in computer |
of them becomes overloaded |
are the points in |
notes in computer science |
the points in each |
points in each of |
causing the tcp link |
in each of the |
the tcp link to |
each of the graphs |
of the graphs with |
the graphs with the |
graphs with the respective |
with the respective coordinates |
tcp link to the |
link to the upstream |
to the upstream node |
the upstream node to |
upstream node to become |
node to become congested |
to become congested and |
both read and update |
become congested and starving |
read and update transactions |
and update transactions access |
congested and starving downstream |
and starving downstream nodes |
j graphs we draw |
graphs we draw a |
our experiment satisfies all |
experiment satisfies all read |
which begin to miss |
begin to miss updates |
we draw a border |
draw a border around |
a border around the |
border around the region |
this scenario models a |
only transactions from the |
transactions from the cache |
scenario models a behavior |
around the region where |
the region where there |
region where there is |
where there is no |
while passing all update |
models a behavior common |
passing all update transactions |
a behavior common in |
all update transactions directly |
update transactions directly to |
behavior common in experiments |
common in experiments on |
transactions directly to the |
directly to the backend |
to the backend database |
attack by i in |
by i in equilibrium |
in experiments on our |
experiments on our cluster |
each cache server is |
cache server is unaware |
for the ri graphs |
when a node becomes |
server is unaware of |
the ri graphs we |
a node becomes very |
is unaware of the |
ri graphs we draw |
node becomes very busy |
unaware of the other |
graphs we draw a |
becomes very busy or |
rx for data center |
of the other servers |
we draw a line |
very busy or the |
for data center communication |
the other servers it |
draw a line around |
busy or the communication |
data center communication scalability |
other servers it has |
a line around the |
or the communication subsystem |
servers it has its |
line around the region |
the communication subsystem becomes |
it has its own |
around the region where |
communication subsystem becomes heavily |
has its own clients |
the region where the |
subsystem becomes heavily loaded |
its own clients and |
region where the revenue |
own clients and communicates |
where the revenue is |
clients and communicates directly |
tcp at the node |
the revenue is the |
revenue is the same |
is the same as |
the same as in |
same as in the |
as in the no |
at the node upstream |
and communicates directly with |
the node upstream from |
communicates directly with the |
node upstream from it |
directly with the backend |
upstream from it will |
with the backend database |
from it will sense |
it will sense congestion |
will sense congestion and |
sense congestion and reduce |
congestion and reduce its |
and reduce its window |
the percentage of read |
reduce its window size |
we first observe that |
first observe that only |
observe that only in |
only transactions can be |
if the impacted node |
the impacted node is |
transactions can be arbitrarily |
that only in extreme |
impacted node is in |
can be arbitrarily high |
be arbitrarily high or |
node is in the |
is in the middle |
in the middle of |
arbitrarily high or low |
high or low in |
or low in this |
low in this situation |
the middle of the |
middle of the chain |
only in extreme cases |
in extreme cases a |
extreme cases a pool |
cases a pool does |
a pool does not |
pool does not attack |
does not attack its |
not attack its counterpart |
we can push the |
it ceases to relay |
can push the percentage |
ceases to relay updates |
push the percentage up |
at equilibrium a pool |
or does so after |
does so after long |
so after long delays |
equilibrium a pool will |
our simulation focuses on |
a pool will refrain |
simulation focuses on just |
hence downstream nodes fall |
downstream nodes fall behind |
focuses on just a |
pool will refrain from |
on just a single |
will refrain from attacking |
just a single cache |
refrain from attacking only |
a single cache it |
the chain replication scheme |
chain replication scheme slows |
single cache it would |
from attacking only if |
replication scheme slows to |
cache it would behave |
attacking only if the |
only if the other |
if the other pool |
the other pool is |
other pool is larger |
pool is larger than |
based fast overlay topology |
fast overlay topology construction |
is larger than about |
it would behave the |
scheme slows to a |
slows to a crawl |
would behave the same |
behave the same had |
the same had there |
same had there been |
had there been many |
there been many cache |
been many cache servers |
live streaming with utilities |
of the total mining |
the total mining power |
the ssa benefits from |
ssa benefits from its |
benefits from its gossip |
from its gossip repair |
its gossip repair mechanisms |
cache can be used |
which route missing updates |
can be used with |
route missing updates around |
we observe that a |
be used with any |
missing updates around the |
updates around the slow |
around the slow node |
observe that a pool |
used with any transactional |
with any transactional backend |
that a pool improves |
any transactional backend and |
transactional backend and any |
backend and any transactional |
route them to that |
them to that node |
and any transactional workload |
a pool improves its |
pool improves its revenue |
improves its revenue compared |
its revenue compared to |
revenue compared to the |
compared to the no |
when it recovers and |
it recovers and needs |
recovers and needs to |
only transactions will be |
and needs to repair |
transactions will be similar |
needs to repair its |
to repair its state |
will be similar to |
be similar to non |
attacks scenario only when |
scenario only when it |
only when it controls |
when it controls a |
it controls a strict |
controls a strict majority |
a strict majority of |
strict majority of the |
majority of the total |
of the total mining |
the total mining power |
knowing that gossip will |
the underlying database is |
that gossip will kick |
underlying database is only |
gossip will kick in |
these are the small |
are the small triangular |
the small triangular regions |
small triangular regions in |
an upstream node can |
triangular regions in figures |
database is only accessed |
upstream node can deliberately |
is only accessed on |
node can deliberately drop |
only accessed on cache |
can deliberately drop updates |
accessed on cache misses |
deliberately drop updates on |
drop updates on congested |
updates on congested tcp |
on congested tcp connections |
in the rest of |
the rest of the |
rest of the space |
we used our wall |
inconsistencies may be observed |
and maarten van steen |
the trapezoids in the |
trapezoids in the figures |
clock service to evaluate |
an architecture for scalable |
service to evaluate the |
architecture for scalable and |
for scalable and fault |
the revenue of the |
to evaluate the behavior |
we will use synthetic |
revenue of the pool |
evaluate the behavior of |
will use synthetic workloads |
of the pool is |
the pool is inferior |
pool is inferior compared |
is inferior compared to |
inferior compared to the |
compared to the no |
use synthetic workloads so |
the behavior of the |
synthetic workloads so we |
behavior of the overall |
workloads so we can |
of the overall system |
so we can evaluate |
the overall system in |
we can evaluate how |
overall system in various |
can evaluate how much |
the prisoner s dilemma |
system in various scenarios |
evaluate how much inconsistency |
prisoner s dilemma in |
s dilemma in a |
dilemma in a healthy |
in a healthy bitcoin |
a healthy bitcoin environment |
in various scenarios and |
how much inconsistency can |
various scenarios and with |
much inconsistency can be |
scenarios and with different |
and with different parameters |
inconsistency can be observed |
where neither pool controls |
can be observed as |
neither pool controls a |
a stream of updates |
be observed as a |
pool controls a strict |
controls a strict majority |
a strict majority of |
strict majority of the |
majority of the mining |
of the mining power |
observed as a function |
stream of updates of |
as a function of |
of updates of various |
both pools will earn |
a function of the |
updates of various rates |
pools will earn less |
function of the amount |
of various rates is |
will earn less at |
of the amount of |
various rates is injected |
earn less at equilibrium |
the amount of clustering |
rates is injected into |
is injected into the |
amount of clustering in |
of clustering in the |
clustering in the workload |
less at equilibrium than |
injected into the head |
into the head of |
the head of the |
head of the chain |
this also allows us |
at equilibrium than if |
equilibrium than if both |
than if both pools |
if both pools ran |
both pools ran without |
pools ran without attacking |
also allows us to |
allows us to look |
us to look at |
to look at the |
look at the dynamic |
for groups of nodes |
we can analyze in |
at the dynamic behavior |
the dynamic behavior of |
dynamic behavior of the |
behavior of the system |
can analyze in this |
analyze in this case |
established point in time |
in this case a |
when the amount of |
this case a game |
the amount of clustering |
a victim node receives |
case a game where |
amount of clustering and |
victim node receives a |
a game where each |
of clustering and the |
node receives a command |
receives a command that |
clustering and the clustering |
and the clustering formation |
a command that forces |
command that forces it |
the clustering formation change |
clustering formation change over |
that forces it to |
forces it to halt |
formation change over time |
game where each pool |
where each pool chooses |
each pool chooses either |
the node continues to |
pool chooses either to |
node continues to listen |
continues to listen for |
chooses either to attack |
either to attack and |
to attack and optimize |
attack and optimize its |
and optimize its revenue |
to listen for commands |
we will look at |
listen for commands that |
will look at workloads |
for commands that would |
commands that would restart |
that would restart it |
or to refrain from |
to refrain from attacking |
look at workloads based |
at workloads based on |
workloads based on amazon |
based on amazon s |
on amazon s product |
amazon s product co |
this is accomplished by |
is accomplished by having |
accomplished by having node |
without loss of generality |
purchasing and orkut s |
and orkut s social |
send a crash command |
as we have seen |
we have seen in |
have seen in section |
seen in section v |
a crash command to |
orkut s social network |
crash command to the |
s social network to |
command to the victim |
social network to see |
to the victim node |
network to see how |
the victim node once |
to see how much |
victim node once a |
see how much inconsistency |
node once a certain |
ordering transactions with prediction |
how much inconsistency t |
once a certain number |
a certain number of |
transactions with prediction in |
with prediction in distributed |
certain number of updates |
can increase its revenue |
increase its revenue above |
prediction in distributed object |
number of updates were |
cache can detect as |
in distributed object stores |
of updates were injected |
can detect as a |
distributed object stores ittay |
updates were injected into |
detect as a function |
object stores ittay eyal |
were injected into the |
as a function of |
a function of dependency |
injected into the chain |
does attack but pool |
function of dependency list |
of dependency list length |
and compare this with |
compare this with a |
we denote the revenue |
denote the revenue of |
the revenue of pool |
the victim node will |
this with a ttl |
victim node will stop |
node will stop participating |
will stop participating in |
stop participating in the |
participating in the normal |
in the normal protocol |
the normal protocol and |
normal protocol and will |
we are also interested |
protocol and will handle |
and will handle only |
are also interested in |
also interested in overhead |
will handle only wakeup |
handle only wakeup commands |
only wakeup commands from |
department of computer science |
wakeup commands from this |
particularly the additional load |
commands from this moment |
from this moment onwards |
the additional load on |
the exact value of |
exact value of r |
additional load on the |
the chain detects the |
load on the backend |
chain detects the failure |
depends on the values |
on the values of |
the values of m |
on the backend database |
repairs and announces the |
the backend database that |
department of electrical engineering |
and announces the membership |
swift institute swift institute |
backend database that could |
announces the membership change |
institute swift institute working |
database that could form |
but it is always |
it is always smaller |
is always smaller than |
always smaller than one |
after a number of |
that could form if |
swift institute working paper |
israel abstract numbers of |
a number of updates |
could form if the |
institute working paper no |
as we have seen |
we have seen above |
number of updates have |
form if the the |
abstract numbers of storage |
numbers of storage nodes |
if the the rate |
of updates have been |
the the rate of |
updates have been injected |
when client transactions access |
the rate of cache |
rate of cache misses |
of cache misses increases |
client transactions access data |
transactions access data on |
have been injected since |
been injected since the |
access data on multiple |
data on multiple shards |
injected since the crash |
since the crash command |
b presented three strategies |
the crash command was |
the issue of consistency |
presented three strategies for |
crash command was issued |
does choose to attack |
issue of consistency arises |
three strategies for responding |
strategies for responding to |
for responding to inconsistency |
responding to inconsistency detection |
sends a wakeup command |
a wakeup command to |
wakeup command to the |
command to the victim |
to the victim node |
but does not surpass |
does not surpass one |
for both the synthetic |
we would use a |
both the synthetic and |
would use a system |
the synthetic and realistic |
synthetic and realistic workloads |
use a system with |
a system with acid |
s dilemma ittay eyal |
dilemma ittay eyal publication |
system with acid transactions |
the game is summarized |
game is summarized in |
is summarized in figure |
the victim node rejoins |
ittay eyal publication date |
we compare the efficacy |
victim node rejoins the |
compare the efficacy of |
node rejoins the group |
the efficacy of the |
efficacy of the three |
of the three strategies |
it has to catch |
this is the classical |
is the classical prisoner |
the classical prisoner s |
classical prisoner s dilemma |
has to catch up |
synthetic workloads synthetic workloads |
to catch up by |
workloads synthetic workloads allow |
catch up by obtaining |
attack is the dominant |
is the dominant strategy |
up by obtaining copies |
synthetic workloads allow us |
by obtaining copies of |
workloads allow us to |
obtaining copies of updates |
because this model facilitates |
allow us to understand |
chooses to attack or |
to attack or not |
copies of updates that |
electronic copy available at |
this model facilitates reasoning |
us to understand the |
of updates that it |
updates that it has |
that it has missed |
to understand the efficacy |
understand the efficacy of |
the efficacy of t |
the revenue of pool |
we experimentally determined that |
model facilitates reasoning about |
cache as a function |
facilitates reasoning about system |
as a function of |
a function of clustering |
reasoning about system properties |
repetitions of each experiment |
is larger when attacking |
larger when attacking than |
for the experiments described |
about system properties and |
when attacking than when |
attacking than when refraining |
than when refraining from |
when refraining from attack |
of each experiment were |
the experiments described here |
system properties and makes |
and the same for |
the same for pool |
properties and makes possible |
each experiment were enough |
and makes possible a |
experiment were enough to |
makes possible a variety |
were enough to yield |
cache with a maximum |
with a maximum of |
enough to yield accurate |
to yield accurate measurements |
possible a variety of |
a variety of highassurance |
elements per dependency list |
at equilibrium of this |
equilibrium of this attack |
variety of highassurance guarantees |
yield accurate measurements with |
accurate measurements with low |
measurements with low variance |
the miner s dilemma |
miner s dilemma ittay |
s dilemma ittay eyal |
describes synthetic workload generation |
when both pools attack |
dilemma ittay eyal cornell |
the acid model is |
ittay eyal cornell university |
shows the update delivery |
eyal cornell university abstract |
the revenue of each |
acid model is often |
the update delivery delay |
cornell university abstract an |
revenue of each pool |
model is often avoided |
update delivery delay for |
university abstract an open |
measures how many inconsistencies |
of each pool is |
is often avoided in |
delivery delay for a |
abstract an open distributed |
how many inconsistencies we |
each pool is smaller |
often avoided in today |
delay for a set |
an open distributed system |
many inconsistencies we can |
pool is smaller than |
is smaller than its |
smaller than its revenue |
than its revenue if |
its revenue if neither |
revenue if neither pool |
if neither pool attacked |
open distributed system can |
inconsistencies we can detect |
avoided in today s |
for a set of |
distributed system can be |
we can detect as |
in today s large |
a set of four |
system can be secured |
can detect as a |
detect as a function |
set of four consecutive |
can be secured by |
the game is not |
game is not played |
is not played once |
of four consecutive nodes |
be secured by requiring |
scale systems due to |
as a function of |
four consecutive nodes in |
secured by requiring participants |
systems due to efficiency |
a function of clustering |
consecutive nodes in a |
by requiring participants to |
due to efficiency concerns |
function of clustering and |
nodes in a chain |
requiring participants to present |
of clustering and section |
participants to present proof |
clustering and section v |
where each pool can |
to present proof of |
starting with the victim |
with the victim node |
present proof of work |
proof of work and |
of work and rewarding |
work and rewarding them |
considers clustering changes over |
and rewarding them for |
rewarding them for participation |
clustering changes over time |
existing approaches typically run |
each pool can change |
approaches typically run transactions |
pool can change its |
the chain length is |
the bitcoin digital currency |
typically run transactions speculatively |
can change its strategy |
change its strategy between |
its strategy between attack |
strategy between attack and |
between attack and no |
compares the efficacy of |
bitcoin digital currency introduced |
run transactions speculatively and |
and we report on |
the efficacy of various |
digital currency introduced this |
transactions speculatively and perform |
we report on a |
efficacy of various approaches |
currency introduced this mechanism |
the pools can agree |
speculatively and perform certification |
report on a gossip |
of various approaches to |
which is adopted by |
on a gossip rate |
and perform certification after |
various approaches to dealing |
to refrain from attacking |
a gossip rate of |
perform certification after they |
is adopted by almost |
approaches to dealing with |
certification after they complete |
adopted by almost all |
and in each round |
in each round xxx |
each round xxx xxx |
round xxx xxx pool |
to dealing with detected |
after they complete to |
by almost all contemporary |
dealing with detected inconsistencies |
they complete to preserve |
almost all contemporary digital |
all contemporary digital currencies |
milliseconds at a steady |
complete to preserve consistency |
no attack xxx pool |
contemporary digital currencies and |
at a steady update |
digital currencies and related |
a steady update injection |
either committing or aborting |
currencies and related services |
steady update injection rate |
committing or aborting each |
update injection rate of |
or aborting each transaction |
our basic synthetic workload |
aborting each transaction depending |
a natural process leads |
basic synthetic workload is |
each transaction depending on |
natural process leads participants |
synthetic workload is constructed |
transaction depending on conflicts |
process leads participants of |
workload is constructed as |
leads participants of such |
is constructed as follows |
participants of such systems |
of such systems to |
there are three anomalies |
such systems to form |
are three anomalies that |
systems to form pools |
three anomalies that can |
anomalies that can be |
rain an architecture for |
that can be seen |
an architecture for acid |
can be seen on |
where members aggregate their |
architecture for acid transactions |
be seen on the |
members aggregate their power |
for acid transactions in |
seen on the graphs |
aggregate their power and |
acid transactions in a |
their power and share |
transactions in a resilient |
power and share the |
in a resilient archive |
the first one is |
and share the rewards |
a resilient archive with |
first one is experienced |
resilient archive with independent |
archive with independent nodes |
one is experienced by |
experience with bitcoin shows |
is experienced by the |
with bitcoin shows that |
experienced by the victim |
bitcoin shows that the |
the system orders transactions |
by the victim node |
shows that the largest |
system orders transactions before |
the victim node for |
that the largest pools |
the objects are divided |
orders transactions before they |
victim node for updates |
the largest pools are |
objects are divided into |
transactions before they begin |
node for updates injected |
largest pools are often |
are divided into clusters |
before they begin by |
for updates injected between |
pools are often open |
divided into clusters of |
they begin by employing |
into clusters of size |
begin by employing predictors |
allowing anyone to join |
by employing predictors that |
employing predictors that estimate |
predictors that estimate the |
that estimate the set |
estimate the set of |
the set of objects |
it has long been |
set of objects each |
has long been known |
of objects each transaction |
seconds after the start |
long been known that |
objects each transaction will |
after the start of |
been known that a |
each transaction will access |
the start of the |
known that a member |
start of the experiment |
that a member can |
a member can sabotage |
such predictors can be |
member can sabotage an |
the second is experienced |
predictors can be implemented |
can sabotage an open |
second is experienced by |
can be implemented with |
sabotage an open pool |
is experienced by all |
be implemented with machine |
an open pool by |
experienced by all the |
implemented with machine learning |
open pool by seemingly |
by all the other |
with machine learning tools |
pool by seemingly joining |
all the other nodes |
by seemingly joining it |
the other nodes for |
seemingly joining it but |
other nodes for update |
joining it but never |
nodes for update messages |
it but never sharing |
for update messages injected |
but never sharing its |
update messages injected at |
never sharing its proofs |
messages injected at around |
sharing its proofs of |
its proofs of work |
the pool shares its |
and there are two |
seconds after the start |
pool shares its revenue |
a transaction reserves a |
there are two types |
after the start of |
shares its revenue with |
transaction reserves a version |
are two types of |
the start of the |
its revenue with the |
reserves a version of |
two types of workloads |
start of the experiment |
revenue with the attacker |
a version of each |
version of each object |
of each object it |
each object it will |
while the third one |
object it will use |
and so each of |
clustering is perfect and |
the third one is |
so each of its |
when later accessing the |
later accessing the objects |
third one is a |
each of its participants |
of its participants earns |
its participants earns less |
one is a smaller |
prisoner s dilemma for |
s dilemma for two |
dilemma for two pools |
is a smaller mixed |
is perfect and each |
it will see these |
we define and analyze |
the revenue density of |
perfect and each transaction |
will see these reserved |
a smaller mixed burst |
define and analyze a |
revenue density of each |
and each transaction chooses |
see these reserved versions |
smaller mixed burst for |
and analyze a game |
density of each pool |
each transaction chooses a |
mixed burst for updates |
analyze a game where |
of each pool is |
transaction chooses a single |
burst for updates injected |
a game where pools |
each pool is determined |
chooses a single cluster |
for updates injected at |
leases for future object |
game where pools use |
pool is determined by |
a single cluster and |
for future object versions |
where pools use some |
is determined by the |
single cluster and chooses |
pools use some of |
seconds into the experiment |
determined by the decision |
leases are issued for |
times with repetitions within |
by the decision of |
use some of their |
are issued for a |
with repetitions within this |
note that the y |
the decision of both |
decision of both pools |
of both pools whether |
both pools whether to |
axes have different scales |
issued for a predefined |
repetitions within this cluster |
within this cluster to |
some of their participants |
have different scales to |
for a predefined time |
a predefined time period |
this cluster to establish |
of their participants to |
different scales to observe |
pools whether to attack |
whether to attack or |
to attack or not |
scales to observe how |
not the lease holder |
their participants to infiltrate |
cluster to establish its |
the dominant strategy of |
dominant strategy of each |
may unilaterally decide to |
to establish its access |
establish its access set |
strategy of each player |
of each player is |
each player is to |
player is to attack |
to observe how the |
unilaterally decide to ignore |
participants to infiltrate other |
in the second type |
however the payoff of |
decide to ignore a |
to ignore a reservation |
observe how the system |
the second type of |
the payoff of both |
to infiltrate other pools |
how the system handles |
second type of workloads |
payoff of both would |
to run effectively at |
infiltrate other pools and |
the system handles the |
type of workloads access |
of workloads access is |
run effectively at large |
other pools and perform |
system handles the transient |
handles the transient failure |
the transient failure better |
effectively at large scale |
pools and perform such |
and perform such an |
perform such an attack |
of both would be |
workloads access is not |
therefore the third anomaly |
both would be larger |
with any number of |
any number of pools |
rain must tolerate performance |
access is not fully |
is not fully contained |
the third anomaly appears |
must tolerate performance hiccups |
would be larger if |
not fully contained within |
third anomaly appears to |
be larger if they |
larger if they both |
attacks is not a |
fully contained within each |
contained within each cluster |
anomaly appears to grow |
is not a nash |
not a nash equilibrium |
appears to grow with |
if they both refrain |
they both refrain from |
both refrain from attacking |
when a transaction starts |
all of which are |
to grow with the |
we study the special |
of which are common |
grow with the chain |
with the chain distance |
it chooses a cluster |
study the special cases |
which are common in |
are common in such |
common in such settings |
chooses a cluster uniformly |
the special cases where |
a pool can detect |
the chain distance from |
a cluster uniformly at |
special cases where either |
pool can detect whether |
chain distance from the |
cluster uniformly at random |
progress should never depend |
cases where either two |
can detect whether it |
distance from the victim |
from the victim node |
where either two pools |
detect whether it is |
should never depend on |
either two pools or |
whether it is being |
the growth is not |
growth is not significant |
two pools or any |
each object is chosen |
it is being attacked |
never depend on the |
since the cause of |
object is chosen using |
is being attacked and |
pools or any number |
depend on the responsiveness |
the cause of this |
is chosen using a |
being attacked and deduce |
or any number of |
on the responsiveness of |
cause of this anomaly |
chosen using a bounded |
attacked and deduce that |
any number of identical |
the responsiveness of any |
of this anomaly is |
using a bounded pareto |
and deduce that the |
number of identical pools |
responsiveness of any single |
this anomaly is an |
a bounded pareto distribution |
deduce that the other |
that the other pool |
the other pool is |
other pool is violating |
pool is violating the |
is violating the agreement |
of any single machine |
anomaly is an artifact |
bounded pareto distribution starting |
of identical pools play |
is an artifact of |
pareto distribution starting at |
identical pools play the |
an artifact of java |
distribution starting at detected |
starting at detected inconsistencies |
pools play the game |
artifact of java s |
rain requires reliable entities |
play the game and |
cooperation where neither pool |
of java s garbage |
requires reliable entities in |
the game and the |
where neither pool attacks |
java s garbage collection |
reliable entities in cloud |
game and the rest |
and the rest of |
s garbage collection mechanism |
update clients access the |
the rest of the |
garbage collection mechanism kicking |
collection mechanism kicking in |
clients access the database |
rest of the participants |
of the participants are |
it is common to |
as can be noted |
neither pool attacks is |
pool attacks is a |
attacks is a possible |
is a possible stable |
a possible stable state |
is common to shard |
performed recovery for the |
the participants are uninvolved |
access the database at |
recovery for the updates |
the database at a |
for the updates it |
database at a rate |
the updates it has |
data across large numbers |
at a rate of |
in both of these |
updates it has missed |
both of these cases |
across large numbers of |
of these cases there |
it has missed during |
these cases there exists |
large numbers of nodes |
cases there exists an |
has missed during the |
there exists an equilibrium |
missed during the period |
exists an equilibrium that |
atomic transactions are typically |
an equilibrium that constitutes |
during the period it |
equilibrium that constitutes a |
transactions are typically implemented |
the period it was |
that constitutes a tragedy |
despite the fact that |
constitutes a tragedy of |
are typically implemented by |
only clients access the |
clients access the cache |
period it was down |
a tragedy of the |
typically implemented by running |
implemented by running transactions |
access the cache at |
because the chain delivers |
the fact that the |
by running transactions speculatively |
tragedy of the commons |
the cache at a |
the chain delivers new |
fact that the single |
of the commons where |
cache at a rate |
at a rate of |
and then certifying them |
that the single nash |
the commons where the |
chain delivers new updates |
aborting ones that cause |
ones that cause conflicts |
delivers new updates at |
the single nash equilibrium |
single nash equilibrium in |
nash equilibrium in every |
equilibrium in every round |
in every round is |
every round is to |
round is to attack |
new updates at the |
commons where the participating |
in high contention scenarios |
updates at the moment |
where the participating pools |
at the moment of |
the participating pools attack |
this approach has drawbacks |
the moment of rejoin |
participating pools attack one |
case as an example |
pools attack one another |
as an example we |
rather than achieving any |
all past updates were |
attack one another and |
an example we take |
than achieving any substantial |
past updates were solely |
one another and earn |
example we take again |
achieving any substantial level |
updates were solely recovered |
another and earn less |
we take again the |
take again the pool |
again the pool sizes |
the pool sizes shown |
pool sizes shown in |
sizes shown in figure |
and earn less than |
any substantial level of |
were solely recovered by |
earn less than they |
substantial level of concurrency |
solely recovered by means |
less than they would |
recovered by means of |
by means of epidemics |
than they would have |
they would have if |
it prevents concurrency by |
and study the case |
study the case where |
the case where the |
case where the two |
where the two largest |
the two largest pools |
would have if none |
the second anomaly that |
prevents concurrency by aborting |
have if none had |
second anomaly that shows |
concurrency by aborting all |
if none had attacked |
anomaly that shows up |
by aborting all but |
that shows up in |
aborting all but one |
all but one of |
shows up in the |
the optimal infiltration rates |
but one of the |
up in the update |
one of the contending |
in the update delivery |
of the contending transactions |
the decision whether or |
out of the total |
of the total system |
the total system mining |
our work explores a |
decision whether or not |
total system mining power |
the update delivery delay |
work explores a new |
whether or not to |
update delivery delay for |
explores a new option |
or not to attack |
delivery delay for the |
not to attack is |
delay for the nodes |
to attack is the |
for the nodes downstream |
attack is the miner |
the nodes downstream from |
ordering transactions in advance |
is the miner s |
nodes downstream from the |
transactions in advance based |
the miner s dilemma |
downstream from the victim |
in advance based on |
from the victim node |
advance based on the |
the victim node reflects |
an instance of the |
based on the objects |
victim node reflects the |
instance of the iterative |
on the objects they |
the objects they are |
node reflects the period |
of the iterative prisoner |
the iterative prisoner s |
objects they are likely |
reflects the period when |
the period when the |
iterative prisoner s dilemma |
they are likely to |
are likely to access |
period when the chain |
when the chain is |
the chain is broken |
the game is played |
providing acid transactions in |
and the pools would |
the pools would lose |
acid transactions in a |
game is played daily |
during the time it |
transactions in a resilient |
is played daily by |
the time it took |
ratio of inconsistencies as |
in a resilient archive |
played daily by the |
time it took for |
of inconsistencies as a |
a resilient archive with |
daily by the active |
it took for the |
inconsistencies as a function |
resilient archive with independent |
by the active bitcoin |
took for the failure |
as a function of |
for the failure detection |
the active bitcoin pools |
compared to the no |
the failure detection mechanism |
archive with independent nodes |
the head of its |
failure detection mechanism to |
which apparently choose not |
head of its cluster |
detection mechanism to declare |
apparently choose not to |
of its cluster i |
mechanism to declare the |
choose not to attack |
q i dentical p |
to declare the node |
i dentical p ools |
if this balance breaks |
declare the node deceased |
this preliminary ordering decreases |
preliminary ordering decreases abort |
ordering decreases abort rate |
dentical p ools let |
the revenue of open |
revenue of open pools |
to start up the |
if the pareto variable |
start up the membership |
of open pools might |
and eliminates aborts in |
eliminates aborts in error |
the pareto variable plus |
up the membership change |
open pools might diminish |
p ools let there |
pareto variable plus the |
the membership change protocol |
making them unattractive to |
variable plus the offset |
ools let there be |
to allow fast recovery |
them unattractive to participants |
plus the offset results |
the offset results in |
and for the membership |
allow fast recovery from |
let there be q |
offset results in a |
for the membership information |
fast recovery from failures |
there be q pools |
results in a number |
the membership information to |
recovery from failures our |
be q pools of |
in a number outside |
membership information to propagate |
from failures our scheme |
failures our scheme does |
a number outside the |
number outside the range |
the chain is interrupted |
our scheme does not |
scheme does not introduce |
is a digital currency |
chain is interrupted between |
is interrupted between node |
does not introduce any |
a digital currency that |
digital currency that is |
not introduce any locks |
q pools of identical |
currency that is gaining |
that is gaining acceptance |
pools of identical size |
the system consistency and |
of identical size that |
system consistency and durability |
and hence the updates |
identical size that engage |
consistency and durability rely |
hence the updates circumvent |
size that engage in |
and durability rely on |
the updates circumvent the |
that engage in block |
durability rely on a |
updates circumvent the gap |
circumvent the gap by |
rely on a single |
on a single scalable |
the gap by means |
gap by means of |
by means of gossip |
the count wraps back |
count wraps back to |
a single scalable tier |
single scalable tier of |
scalable tier of highly |
updates can bypass nodes |
with an estimated market |
an estimated market capitalization |
can bypass nodes in |
engage in block withholding |
in block withholding against |
block withholding against one |
simulations using the transactional |
bypass nodes in the |
withholding against one another |
estimated market capitalization of |
nodes in the chain |
ycsb workloads show the |
market capitalization of over |
inconsistency detection as a |
workloads show the scalability |
in the chain using |
other miners neither attack |
miners neither attack nor |
neither attack nor are |
attack nor are being |
nor are being attacked |
show the scalability and |
the chain using the |
detection as a function |
the scalability and benefits |
chain using the gossip |
as a function of |
in this case there |
this case there exists |
case there exists a |
there exists a symmetric |
exists a symmetric equilibrium |
scalability and benefits of |
using the gossip as |
we start by exploring |
and benefits of acidrain |
the gossip as it |
start by exploring the |
gossip as it can |
by exploring the importance |
without loss of generality |
as it can be |
exploring the importance of |
it can be seen |
the importance of the |
can be seen in |
a step of pool |
importance of the cluster |
be seen in the |
of the cluster structure |
seen in the figure |
center computing systems often |
the cluster structure by |
computing systems often maintain |
it controls its attack |
cluster structure by varying |
but this phenomenon is |
bitcoin s security stems |
s security stems from |
systems often maintain massive |
structure by varying the |
this phenomenon is less |
controls its attack rates |
its attack rates each |
attack rates each of |
rates each of the |
each of the other |
of the other pools |
often maintain massive data |
by varying the parameter |
phenomenon is less likely |
security stems from a |
maintain massive data sets |
varying the parameter of |
the parameter of the |
is less likely as |
stems from a robust |
sharded over large this |
parameter of the pareto |
less likely as the |
from a robust incentive |
a robust incentive system |
over large this work |
of the pareto distribution |
likely as the node |
and due to symmetry |
due to symmetry they |
to symmetry they are |
we vary the pareto |
large this work was |
as the node receiving |
symmetry they are all |
they are all the |
are all the same |
this work was funded |
the node receiving the |
participants are required to |
vary the pareto parameter |
node receiving the update |
are required to provide |
the pareto parameter from |
receiving the update is |
by grants from darpa |
required to provide expensive |
the update is farther |
to provide expensive proofs |
update is farther away |
provide expensive proofs of |
expensive proofs of work |
the attack rate of |
and the elkin research |
the elkin research fund |
is farther away downstream |
and they are rewarded |
they are rewarded according |
farther away downstream from |
away downstream from the |
are rewarded according to |
rewarded according to their |
according to their efforts |
attack rate of pool |
downstream from the victim |
only at a single |
from the victim node |
in this experiment we |
this architecture has proved |
at a single tier |
against any other pool |
this experiment we are |
architecture has proved both |
a single tier of |
experiment we are only |
has proved both stable |
contains an aggregated view |
single tier of the |
each of the other |
we are only interested |
proved both stable and |
both stable and scalable |
tier of the system |
of the other pools |
are only interested in |
an aggregated view of |
of the system a |
the other pools can |
other pools can attack |
pools can attack its |
can attack its peers |
attack its peers as |
its peers as well |
and it is used |
aggregated view of the |
the system a set |
only interested in detection |
it is used by |
view of the data |
system a set of |
is used by most |
of the data in |
a set of independent |
set of independent highly |
so we choose the |
used by most contemporary |
the data in figure |
all attack rates by |
attack rates by all |
rates by all attackers |
by all attackers are |
for the entire chain |
we choose the abort |
by most contemporary digital |
most contemporary digital currencies |
of independent highly available |
choose the abort strategy |
all attackers are identical |
contemporary digital currencies and |
at gossip rates of |
independent highly available logs |
digital currencies and related |
currencies and related services |
shows the ratio of |
the ratio of inconsistencies |
used in a novel |
in a novel manner |
ratio of inconsistencies detected |
the attack rate of |
attack rate of any |
rate of any pool |
of any pool other |
any pool other than |
all other entities may |
of inconsistencies detected by |
other entities may fail |
inconsistencies detected by t |
against any other pool |
entities may fail and |
may fail and can |
fail and can be |
cache compared to the |
and can be replaced |
milliseconds showing that the |
compared to the total |
can be replaced instantly |
showing that the behavior |
to the total number |
be replaced instantly on |
that the behavior of |
the total number of |
replaced instantly on failure |
the behavior of the |
total number of potential |
behavior of the scheme |
number of potential inconsistencies |
of the scheme is |
the architecture maintains consistency |
the scheme is not |
architecture maintains consistency even |
scheme is not a |
is not a fluke |
maintains consistency even in |
consistency even in the |
even in the event |
in the event of |
the event of false |
event of false suspicion |
the direct revenue of |
direct revenue of each |
revenue of each of |
of each of the |
each of the other |
of the other pools |
note that the delay |
that the delay of |
reservations serve as suggestions |
similarly denote by r |
the delay of the |
serve as suggestions a |
delay of the updates |
as suggestions a reservation |
of the updates delivered |
suggestions a reservation that |
the distribution is almost |
the revenue densities of |
revenue densities of pool |
the updates delivered at |
distribution is almost uniform |
a reservation that is |
updates delivered at the |
is almost uniform across |
reservation that is not |
delivered at the victim |
almost uniform across the |
that is not used |
at the victim node |
uniform across the object |
is not used because |
the victim node is |
across the object set |
not used because of |
victim node is significantly |
used because of a |
node is significantly larger |
are instantiated to mi |
because of a sluggish |
is significantly larger than |
and the inconsistency detection |
our results apply to |
of a sluggish or |
significantly larger than that |
the inconsistency detection ratio |
results apply to all |
a sluggish or dead |
larger than that of |
inconsistency detection ratio is |
apply to all such |
sluggish or dead owner |
than that of the |
detection ratio is low |
to all such incentive |
or dead owner is |
dead owner is ignored |
ratio is low the |
all such incentive systems |
that of the nodes |
is low the dependency |
of the nodes downstream |
the independence of system |
low the dependency lists |
the nodes downstream of |
independence of system elements |
but we use bitcoin |
the dependency lists are |
nodes downstream of it |
of system elements allows |
we use bitcoin terminology |
dependency lists are too |
downstream of it in |
system elements allows for |
use bitcoin terminology and |
lists are too small |
of it in the |
it in the chain |
bitcoin terminology and examples |
are too small to |
elements allows for good |
terminology and examples since |
too small to hold |
allows for good scalability |
and examples since it |
we observed that even |
small to hold all |
examples since it serves |
observed that even with |
to hold all relevant |
since it serves as |
that even with sufficiently |
hold all relevant information |
it serves as an |
due to the interdependence |
even with sufficiently high |
serves as an active |
at the other extreme |
with sufficiently high gossip |
to the interdependence of |
as an active and |
sufficiently high gossip rate |
the interdependence of the |
an active and archetypal |
interdependence of the log |
active and archetypal example |
the only node to |
of the log contents |
only node to experience |
node to experience any |
to experience any significant |
bitcoin implements its incentive |
experience any significant inconsistency |
implements its incentive systems |
any significant inconsistency window |
the distribution is so |
its incentive systems with |
significant inconsistency window is |
distribution is so spiked |
incentive systems with a |
inconsistency window is the |
has to be carefully |
is so spiked that |
systems with a data |
window is the node |
to be carefully coordinated |
so spiked that almost |
with a data structure |
is the node that |
be carefully coordinated to |
spiked that almost all |
a data structure called |
the node that failed |
carefully coordinated to maintain |
that almost all accesses |
data structure called the |
coordinated to maintain consistency |
note that when the |
structure called the blockchain |
almost all accesses of |
that when the failed |
we evaluate our architecture |
all accesses of a |
when the failed node |
the failed node rejoins |
accesses of a transaction |
the blockchain is a |
evaluate our architecture by |
of a transaction are |
blockchain is a serialization |
our architecture by simulation |
a transaction are within |
queries are performed against |
is a serialization of |
architecture by simulation with |
transaction are within a |
are performed against its |
a serialization of all |
by simulation with the |
are within a cluster |
performed against its data |
serialization of all bitcoin |
simulation with the transactional |
against its data before |
of all bitcoin transactions |
allowing for perfect inconsistency |
its data before it |
for perfect inconsistency detection |
data before it has |
before it has time |
it is a single |
it has time to |
is a single global |
has time to fully |
time to fully recover |
we note that the |
a single global ledger |
note that the rate |
single global ledger maintained |
that the rate of |
once the chain is |
the rate of detected |
global ledger maintained by |
the chain is restored |
rate of detected inconsistencies |
ledger maintained by an |
of detected inconsistencies is |
maintained by an open |
all new updates are |
new updates are received |
detected inconsistencies is so |
by an open distributed |
we contrast the effectiveness |
inconsistencies is so high |
an open distributed system |
contrast the effectiveness of |
symmetric case we have |
case we have r |
there were rare cases |
the effectiveness of employing |
is so high at |
since anyone can join |
were rare cases when |
effectiveness of employing prediction |
so high at this |
anyone can join the |
rare cases when gossip |
of employing prediction and |
high at this point |
can join the open |
cases when gossip circumvented |
employing prediction and the |
at this point that |
join the open system |
when gossip circumvented the |
the expression is shown |
expression is shown in |
is shown in equation |
the open system and |
gossip circumvented the chain |
prediction and the scalability |
this point that much |
open system and participate |
circumvented the chain replication |
and the scalability of |
point that much of |
system and participate in |
the chain replication even |
the scalability of acid |
that much of the |
and participate in maintaining |
chain replication even though |
much of the load |
rain with other approaches |
replication even though the |
participate in maintaining the |
of the load goes |
even though the chain |
in maintaining the blockchain |
the load goes to |
though the chain was |
load goes to the |
the chain was not |
chain was not broken |
goes to the backend |
to the backend database |
bitcoin uses a proof |
uses a proof of |
tm m om i |
the backend database and |
backend database and saturates |
database and saturates it |
but this happened only |
given any value of |
any value of q |
value of q and |
of q and mi |
reducing the overall throughput |
a proof of work |
this happened only for |
proof of work mechanism |
happened only for gossip |
of work mechanism to |
only for gossip rates |
work mechanism to deter |
om n om i |
for gossip rates close |
mechanism to deter attacks |
gossip rates close to |
rates close to the |
close to the update |
to the update injection |
the update injection rate |
participation requires exerting significant |
so far we have |
requires exerting significant computational |
far we have considered |
we have considered behavior |
have considered behavior with |
exerting significant computational resources |
later in this section |
log i log n |
considered behavior with static |
behavior with static clusters |
in this section we |
a participant who proves |
the feasible range of |
feasible range of the |
range of the infiltration |
of the infiltration rates |
the infiltration rates is |
this section we will |
participant who proves she |
i log n figure |
over the entire run |
section we will show |
who proves she has |
the entire run of |
we will show that |
proves she has exerted |
entire run of each |
will show that even |
she has exerted enough |
run of each experiment |
within this range ri |
this range ri is |
range ri is continuous |
schematic structure of acid |
of each experiment accesses |
show that even with |
has exerted enough resources |
each experiment accesses are |
that even with these |
and concave in x |
experiment accesses are confined |
exerted enough resources with |
even with these rapid |
tms access multiple objects |
accesses are confined to |
enough resources with a |
with these rapid repairs |
access multiple objects per |
are confined to the |
resources with a proof |
multiple objects per transaction |
the gossip overhead is |
with a proof of |
confined to the same |
gossip overhead is actually |
objects are managed by |
are managed by oms |
overhead is actually low |
the optimal point for |
optimal point for pool |
a proof of work |
proof of work is |
of work is allowed |
work is allowed to |
is allowed to take |
allowed to take a |
to take a step |
take a step in |
a step in the |
in a real system |
step in the protocol |
is falsely suspected to |
in the protocol by |
falsely suspected to have |
the protocol by generating |
suspected to have failed |
protocol by generating a |
by generating a block |
and so if t |
and replaced by omi |
of the messages were |
the messages were delivered |
participants are compensated for |
cache converges to maintain |
messages were delivered by |
are compensated for their |
converges to maintain the |
were delivered by gossip |
compensated for their efforts |
to maintain the correct |
since the function is |
delivered by gossip ahead |
for their efforts with |
maintain the correct dependency |
the correct dependency lists |
by gossip ahead of |
their efforts with newly |
causing them to concurrently |
them to concurrently serve |
correct dependency lists as |
gossip ahead of the |
efforts with newly minted |
with newly minted bitcoins |
to concurrently serve the |
dependency lists as clusters |
ahead of the chain |
the function is concave |
concurrently serve the same |
serve the same objects |
of the chain for |
the process of creating |
process of creating a |
lists as clusters change |
oms are backed by |
are backed by highlyavailable |
backed by highlyavailable logs |
the chain for gossip |
our setup serves as |
of creating a block |
creating a block is |
chain for gossip rate |
setup serves as a |
serves as a valid |
as a valid quasi |
where they store tentative |
for gossip rate identical |
function is concave the |
a block is called |
they store tentative transaction |
gossip rate identical to |
rate identical to the |
block is called mining |
store tentative transaction entries |
tentative transaction entries for |
identical to the update |
to the update injection |
the update injection rate |
we investigate the convergence |
investigate the convergence of |
the convergence of t |
and the participants miners |
is concave the equation |
transaction entries for serialization |
cache when clusters change |
when clusters change over |
clusters change over time |
in order to win |
order to win the |
to win the reward |
concave the equation yields |
the equation yields a |
equation yields a single |
yields a single feasible |
a single feasible solution |
since the dependency lists |
contains a plot of |
many miners try to |
the dependency lists of |
a plot of update |
miners try to generate |
try to generate blocks |
dependency lists of the |
plot of update injection |
which is a function |
lists of the objects |
of update injection time |
is a function of |
the system automatically adjusts |
of the objects are |
update injection time against |
a function of the |
function of the attack |
of the attack rates |
the attack rates of |
attack rates of the |
rates of the other |
of the other pools |
the objects are updated |
injection time against update |
system structure the structure |
system automatically adjusts the |
objects are updated using |
time against update delivery |
structure the structure of |
automatically adjusts the difficulty |
are updated using lru |
against update delivery time |
the structure of the |
adjusts the difficulty of |
update delivery time for |
structure of the system |
the difficulty of block |
delivery time for the |
the dependency list of |
of the system is |
difficulty of block generation |
time for the victim |
dependency list of an |
the system is illustrated |
for the victim node |
list of an object |
system is illustrated in |
such that one block |
that one block is |
of an object o |
is illustrated in figure |
to find a symmetric |
find a symmetric equilibrium |
ideally this is a |
an object o tends |
one block is added |
this is a straight |
object o tends to |
at the base of |
the base of acid |
block is added every |
o tends to include |
tends to include those |
is a straight line |
to include those objects |
rain are a set |
a straight line because |
include those objects that |
are a set of |
straight line because of |
those objects that are |
a set of independent |
minutes to the blockchain |
line because of chain |
objects that are frequently |
set of independent highly |
because of chain replication |
that are frequently accessed |
this means that each |
are frequently accessed together |
available logs that together |
means that each miner |
frequently accessed together with |
logs that together describe |
note that once the |
that each miner seldom |
accessed together with o |
that together describe the |
that once the victim |
each miner seldom generates |
together describe the state |
describe the state of |
once the victim node |
miner seldom generates a |
seldom generates a block |
dependencies in a new |
the state of the |
the victim node recovers |
and obtain a single |
obtain a single feasible |
a single feasible solution |
although its revenue may |
in a new cluster |
state of the entire |
of the entire system |
the equilibrium infiltration rate |
a new cluster automatically |
it gracefully catches up |
its revenue may be |
revenue may be positive |
new cluster automatically push |
gracefully catches up and |
each log is accessed |
log is accessed through |
may be positive in |
cluster automatically push out |
catches up and does |
equilibrium infiltration rate and |
is accessed through an |
be positive in expectation |
automatically push out dependencies |
up and does so |
infiltration rate and the |
accessed through an object |
push out dependencies that |
and does so quickly |
rate and the matching |
through an object manager |
a miner may have |
out dependencies that are |
does so quickly for |
and the matching revenues |
the matching revenues are |
matching revenues are shown |
revenues are shown in |
are shown in equation |
dependencies that are now |
so quickly for both |
miner may have to |
that caches the data |
quickly for both gossip |
that are now outside |
may have to wait |
caches the data and |
for both gossip rates |
are now outside the |
have to wait for |
the data and provides |
both gossip rates identical |
now outside the cluster |
to wait for an |
data and provides the |
gossip rates identical and |
wait for an extended |
and provides the data |
rates identical and half |
for an extended period |
provides the data structure |
identical and half the |
an extended period to |
the data structure abstraction |
we perform an experiment |
extended period to create |
as in the two |
and half the update |
data structure abstraction exporting |
perform an experiment where |
period to create a |
half the update injection |
structure abstraction exporting read |
an experiment where accesses |
to create a block |
the update injection rate |
abstraction exporting read and |
exporting read and write |
experiment where accesses suddenly |
create a block and |
a block and earn |
read and write operations |
where accesses suddenly become |
accesses suddenly become clustered |
block and earn the |
now consider the link |
consider the link congestion |
and earn the actual |
initially accesses are uniformly |
the link congestion case |
earn the actual bitcoins |
the revenue at the |
accesses are uniformly at |
which are managed by |
are managed by transaction |
are uniformly at random |
uniformly at random from |
managed by transaction managers |
revenue at the symmetric |
at the symmetric equilibrium |
the symmetric equilibrium is |
at random from the |
miners form mining pools |
symmetric equilibrium is inferior |
equilibrium is inferior to |
is inferior to the |
inferior to the no |
random from the entire |
tms provide the atomic |
where all members mine |
from the entire set |
provide the atomic transaction |
all members mine concurrently |
the atomic transaction abstraction |
members mine concurrently and |
mine concurrently and they |
concurrently and they share |
and they share their |
they share their revenue |
they receive instructions from |
share their revenue whenever |
receive instructions from clients |
their revenue whenever one |
instructions from clients to |
revenue whenever one of |
from clients to start |
whenever one of them |
clients to start and |
one of them creates |
to start and end |
of them creates a |
up our analysis addresses |
then at a single |
them creates a block |
start and end a |
and end a transaction |
at a single moment |
our analysis addresses the |
a single moment they |
analysis addresses the eventual |
addresses the eventual revenue |
the eventual revenue of |
eventual revenue of the |
revenue of the pools |
single moment they become |
and operations to perform |
pools are typically implemented |
moment they become perfectly |
operations to perform on |
are typically implemented as |
assuming the mining difficulty |
they become perfectly clustered |
to perform on individual |
typically implemented as a |
the mining difficulty is |
become perfectly clustered into |
perform on individual objects |
implemented as a pool |
as a pool manager |
perfectly clustered into clusters |
on individual objects within |
individual objects within the |
a pool manager and |
clustered into clusters of |
into clusters of size |
objects within the transaction |
pool manager and a |
manager and a cohort |
and a cohort of |
a cohort of miners |
mining difficulty is set |
difficulty is set based |
is set based on |
transactions are aborted on |
the tms predict which |
are aborted on detecting |
the pool manager joins |
set based on the |
based on the effective |
on the effective mining |
the effective mining power |
tms predict which objects |
aborted on detecting an |
pool manager joins the |
predict which objects it |
on detecting an inconsistency |
manager joins the bitcoin |
which objects it is |
objects it is likely |
joins the bitcoin system |
we use a transaction |
it is likely to |
the bitcoin system as |
bitcoin system as a |
use a transaction rate |
is likely to access |
not including mining power |
including mining power used |
mining power used for |
power used for withholding |
and reserve these object |
system as a single |
a transaction rate of |
reserve these object versions |
as a single miner |
transaction rate of approximately |
difficulty is updated only |
is updated only periodically |
updated only periodically every |
instead of generating proof |
of generating proof of |
generating proof of work |
they speculatively perform each |
speculatively perform each operation |
perform each operation with |
each operation with the |
it outsources the work |
operation with the help |
outsources the work to |
with the help of |
the work to the |
the help of the |
work to the miners |
help of the appropriate |
of the appropriate oms |
the appropriate oms and |
when mining power in |
in order to evaluate |
appropriate oms and according |
mining power in the |
power in the system |
in the system is |
the system is regularly |
system is regularly increasing |
order to evaluate the |
oms and according to |
to evaluate the miners |
and according to the |
evaluate the miners efforts |
shows the percentage of |
according to the order |
to the order set |
the percentage of transactions |
which has been true |
the pool manager accepts |
percentage of transactions that |
pool manager accepts partial |
has been true for |
the order set by |
of transactions that commit |
manager accepts partial proof |
been true for the |
order set by the |
transactions that commit and |
accepts partial proof of |
true for the majority |
for the majority of |
the majority of bitcoin |
majority of bitcoin s |
of bitcoin s history |
that commit and are |
partial proof of work |
set by the reservations |
commit and are consistent |
proof of work and |
of work and estimates |
work and estimates each |
and estimates each miner |
estimates each miner s |
each miner s power |
they certify the transaction |
miner s power according |
certify the transaction by |
s power according to |
the percentage of transactions |
power according to the |
no adjustment may be |
adjustment may be necessary |
percentage of transactions that |
according to the rate |
the transaction by checking |
of transactions that commit |
to the rate with |
transaction by checking for |
if an attacker purchases |
the rate with which |
transactions that commit but |
by checking for conflicts |
checking for conflicts in |
rate with which it |
that commit but are |
commit but are inconsistent |
for conflicts in each |
with which it submits |
an attacker purchases new |
conflicts in each log |
which it submits such |
it submits such partial |
submits such partial proof |
such partial proof of |
partial proof of work |
attacker purchases new mining |
and the percentage of |
the percentage of transactions |
percentage of transactions that |
when a miner generates |
of transactions that abort |
purchases new mining hardware |
membership monitors are in |
a miner generates a |
miner generates a full |
monitors are in charge |
new mining hardware and |
generates a full proof |
are in charge of |
mining hardware and employs |
a full proof of |
in charge of deciding |
hardware and employs it |
and employs it directly |
employs it directly for |
it directly for block |
directly for block withholding |
full proof of work |
charge of deciding and |
of deciding and publishing |
deciding and publishing which |
and publishing which machines |
publishing which machines perform |
which machines perform which |
machines perform which roles |
this mining power is |
it sends it to |
mining power is never |
sends it to the |
power is never included |
namely which machines run |
it to the pool |
is never included in |
which machines run the |
to the pool manager |
never included in the |
machines run the log |
the pool manager which |
included in the difficulty |
run the log and |
pool manager which publishes |
in the difficulty calculation |
the log and model |
manager which publishes this |
the difficulty calculation the |
log and model and |
which publishes this proof |
difficulty calculation the system |
and model and goal |
publishes this proof of |
this proof of work |
model and goal we |
calculation the system is |
the system is never |
system is never aware |
is never aware of |
never aware of it |
proof of work to |
and goal we assume |
of work to the |
goal we assume unreliable |
the difficulty is therefore |
work to the bitcoin |
to the bitcoin system |
difficulty is therefore already |
we assume unreliable servers |
assume unreliable servers that |
unreliable servers that may |
servers that may crash |
that may crash or |
may crash or hang |
is therefore already correctly |
the pool manager thus |
therefore already correctly calculated |
pool manager thus receives |
already correctly calculated and |
manager thus receives the |
correctly calculated and the |
calculated and the attack |
and the attack is |
the attack is profitable |
attack is profitable immediately |
thus receives the full |
to accommodate reliable storage |
abort evict retry behavior |
receives the full revenue |
evict retry behavior on |
the full revenue of |
retry behavior on inconsistency |
full revenue of the |
if the mining power |
the mining power is |
mining power is static |
behavior on inconsistency detection |
revenue of the block |
of the block and |
the block and distributes |
the attack becomes profitable |
block and distributes it |
as explained in section |
attack becomes profitable only |
and distributes it fairly |
becomes profitable only after |
distributes it fairly according |
it fairly according to |
fairly according to its |
according to its members |
to its members power |
profitable only after the |
only after the bitcoin |
after the bitcoin system |
the bitcoin system has |
bitcoin system has normalized |
system has normalized the |
the system exposes a |
has normalized the revenues |
normalized the revenues by |
the revenues by adjusting |
revenues by adjusting difficulty |
system exposes a transactional |
many of the pools |
exposes a transactional data |
of the pools are |
a transactional data store |
the pools are open |
transactional data store supporting |
pools are open they |
data store supporting serializable |
store supporting serializable transactions |
are open they allow |
the revenue of an |
open they allow any |
a client invokes a |
client invokes a begin |
they allow any miner |
revenue of an attacking |
allow any miner to |
of an attacking pool |
any miner to join |
an attacking pool is |
miner to join them |
attacking pool is reduced |
to join them using |
pool is reduced due |
join them using a |
them using a public |
using a public internet |
a public internet interface |
is reduced due to |
reduced due to the |
due to the reduction |
to the reduction in |
a field from a |
field from a table |
such open pools are |
the reduction in block |
open pools are susceptible |
reduction in block generation |
pools are susceptible to |
in block generation of |
block generation of both |
generation of both the |
of both the attacking |
both the attacking and |
the attacking and attacked |
attacking and attacked pools |
are susceptible to the |
susceptible to the classical |
to the classical block |
the classical block withholding |
classical block withholding attack |
setting the value of |
the value of a |
value of a field |
pool knowledge and r |
of a field in |
a field in a |
field in a table |
finally the client invokes |
the client invokes the |
client invokes the endtransaction |
invokes the endtransaction command |
where a miner sends |
and the system responds |
s accesses are uniformly |
a miner sends only |
the system responds with |
accesses are uniformly at |
miner sends only partial |
system responds with either |
are uniformly at random |
sends only partial proof |
responds with either a |
only partial proof of |
with either a commit |
partial proof of work |
either a commit or |
proof of work to |
a commit or an |
of work to the |
commit or an abort |
work to the pool |
to the pool manager |
the pool manager and |
pool manager and discards |
manager and discards full |
committed transactions form a |
and discards full proof |
transactions form a serializable |
discards full proof of |
form a serializable execution |
full proof of work |
the efficacy of t |
tms are equipped with |
due to the partial |
cache as a function |
are equipped with predictors |
to the partial proof |
as a function of |
equipped with predictors that |
the partial proof of |
a function of the |
with predictors that foresee |
partial proof of work |
function of the strategy |
predictors that foresee which |
proof of work it |
of the strategy taken |
that foresee which objects |
of work it sends |
the strategy taken for |
foresee which objects a |
work it sends to |
strategy taken for handling |
which objects a transaction |
it sends to the |
taken for handling detected |
objects a transaction is |
sends to the pool |
for handling detected inconsistencies |
a transaction is likely |
transaction is likely to |
is likely to access |
likely to access on |
to access on its |
the miner is considered |
access on its initiation |
miner is considered a |
is considered a regular |
considered a regular pool |
a regular pool member |
regular pool member and |
pool member and the |
member and the pool |
and the pool can |
the pool can estimate |
pool can estimate its |
can estimate its power |
of the uncommitable tranasctions |
and evict and retry |
evict and retry reduce |
in an implementation of |
and retry reduce the |
the attacker shares the |
an implementation of the |
retry reduce the rate |
attacker shares the revenue |
implementation of the system |
reduce the rate of |
shares the revenue obtained |
of the system one |
the rate of uncommitable |
the revenue obtained by |
the system one may |
rate of uncommitable transactions |
revenue obtained by the |
system one may use |
of uncommitable transactions to |
obtained by the other |
one may use multiple |
uncommitable transactions to about |
by the other pool |
may use multiple oms |
the other pool members |
use multiple oms per |
multiple oms per log |
but does not contribute |
dividing the log s |
the log s object |
log s object set |
it reduces the revenue |
reduces the revenue of |
or the other way |
the other way around |
the revenue of the |
the middle portion is |
revenue of the other |
middle portion is committed |
of the other members |
have multiple logs report |
portion is committed transactions |
multiple logs report to |
is committed transactions that |
logs report to a |
but also its own |
committed transactions that are |
report to a single |
to a single om |
transactions that are inconsistent |
we provide necessary background |
provide necessary background on |
necessary background on the |
background on the bitcoin |
and the top portion |
the choice depends on |
on the bitcoin protocol |
the top portion is |
choice depends on the |
top portion is aborted |
depends on the throughput |
portion is aborted transactions |
on the throughput of |
pools and the classical |
the throughput of the |
and the classical block |
throughput of the specific |
the classical block withholding |
of the specific implementations |
the specific implementations chosen |
classical block withholding attack |
block withholding attack in |
specific implementations chosen for |
implementations chosen for each |
chosen for each service |
and solving we obtain |
withholding attack in section |
attack in section ii |
in this paper we |
this paper we use |
paper we use a |
solving we obtain a |
we obtain a single |
obtain a single expression |
a single expression for |
single expression for any |
expression for any ri |
and specify our model |
specify our model in |
our model in section |
model in section iii |
since in the in |
in the in order |
mapping for simplicity of |
for simplicity of presentation |
for a broader view |
the in order to |
in order to choose |
order to choose its |
we now describe the |
now describe the operation |
a broader view of |
to choose its optimal |
choose its optimal infiltration |
its optimal infiltration rate |
describe the operation of |
broader view of the |
the operation of acid |
view of the protocol |
a pool has to |
pool has to know |
of the protocol and |
has to know the |
to know the rate |
know the rate at |
we start with an |
the rate at which |
rate at which it |
at which it is |
which it is attacked |
start with an overview |
the protocol and ecosystem |
with an overview of |
protocol and ecosystem the |
an overview of the |
overview of the system |
and ecosystem the reader |
and the revenue density |
the revenue density of |
revenue density of potential |
density of potential victim |
of potential victim pools |
of the system s |
ecosystem the reader may |
the system s structure |
the reader may refer |
a pool can estimate |
system s structure in |
s structure in section |
pool can estimate the |
reader may refer to |
can estimate the rate |
may refer to the |
refer to the survey |
to the survey by |
the survey by bonneau |
survey by bonneau et |
by bonneau et al |
update delay as seen |
estimate the rate with |
delay as seen by |
the rate with which |
as seen by individual |
and proceed to describe |
proceed to describe the |
seen by individual processes |
rate with which it |
to describe the algorithm |
by individual processes during |
with which it is |
describe the algorithm in |
individual processes during persistent |
processes during persistent link |
the algorithm in section |
which it is attacked |
during persistent link congestion |
persistent link congestion node |
it is attacked by |
in this work we |
is attacked by comparing |
this work we analyze |
work we analyze block |
we analyze block withholding |
analyze block withholding attacks |
block withholding attacks among |
withholding attacks among pools |
attacked by comparing the |
om for each shard |
updates on upstream and |
on upstream and downstream |
a pool that employs |
upstream and downstream fifo |
and downstream fifo channels |
pool that employs the |
and which tms are |
which tms are available |
that employs the pool |
by comparing the rates |
employs the pool block |
comparing the rates of |
any client can access |
the pool block withholding |
the rates of partial |
client can access any |
pool block withholding attack |
rates of partial and |
can access any tm |
block withholding attack registers |
of partial and full |
access any tm for |
withholding attack registers with |
partial and full proofs |
any tm for any |
attack registers with the |
and full proofs of |
tm for any given |
for any given transaction |
full proofs of work |
proofs of work it |
of work it receives |
work it receives from |
other than the logs |
it receives from its |
receives from its miners |
registers with the victim |
with the victim pool |
server role assignment may |
the victim pool as |
as explained in section |
explained in section ii |
victim pool as a |
role assignment may be |
pool as a regular |
assignment may be inconsistent |
as a regular miner |
in order to estimate |
order to estimate the |
to estimate the revenue |
estimate the revenue densities |
the revenue densities of |
revenue densities of the |
densities of the other |
of the other pools |
it receives tasks from |
receives tasks from the |
is supposed to be |
tasks from the victim |
supposed to be managed |
to be managed by |
from the victim pool |
a pool can use |
pool can use one |
can use one of |
use one of two |
one of two methods |
the victim pool and |
perfectly clustered synthetic workload |
be managed by a |
victim pool and transfers |
clustered synthetic workload where |
managed by a single |
pool and transfers them |
synthetic workload where the |
by a single om |
and transfers them to |
workload where the clusters |
transfers them to some |
where the clusters shift |
them to some of |
the clusters shift by |
to some of its |
some of its own |
of its own miners |
we call these infiltrating |
call these infiltrating miners |
at a given time |
marked by vertical lines |
and the mining power |
but this may change |
the mining power spent |
this may change due |
mining power spent by |
may change due to |
power spent by a |
change due to an |
spent by a pool |
due to an unjustified |
by a pool the |
to an unjustified crash |
a pool the infiltration |
an unjustified crash suspicion |
pool the infiltration rate |
s access is unclustered |
unjustified crash suspicion whereupon |
crash suspicion whereupon an |
suspicion whereupon an object |
when electronic copy available |
and as a result |
electronic copy available at |
as a result the |
a result the dependency |
result the dependency lists |
the dependency lists are |
dependency lists are useless |
only few inconsistencies are |
few inconsistencies are detected |
may temporarily be managed |
temporarily be managed by |
be managed by two |
managed by two oms |
of the transactions that |
the transactions that commit |
that do not know |
transactions that commit have |
do not know of |
that commit have witnessed |
not know of one |
commit have witnessed inconsistent |
know of one another |
have witnessed inconsistent data |
rain uses log servers |
uses log servers for |
log servers for reliable |
servers for reliable storage |
inconsistency window against gossip |
window against gossip rate |
against gossip rate at |
gossip rate at the |
rate at the failed |
each log server provides |
at the failed node |
log server provides a |
the attacking pool s |
accesses become perfectly clustered |
server provides a sequentially |
attacking pool s infiltrating |
provides a sequentially consistent |
pool s infiltrating miners |
a sequentially consistent log |
s infiltrating miners deliver |
sequentially consistent log object |
infiltrating miners deliver partial |
we see fast improvement |
miners deliver partial proofs |
see fast improvement of |
deliver partial proofs of |
fast improvement of inconsistency |
partial proofs of work |
improvement of inconsistency detection |
the attacker transfers them |
the inconsistency rate drops |
attacker transfers them to |
inconsistency rate drops as |
update operations are linearizable |
transfers them to the |
rate drops as the |
them to the victim |
drops as the abort |
to the victim pool |
but reads may return |
as the abort rate |
reads may return outdated |
the abort rate rises |
may return outdated results |
abort rate rises this |
letting the attacked pool |
rate rises this is |
the attacked pool estimate |
rises this is desired |
attacked pool estimate their |
this is desired as |
multiple machines may append |
is desired as well |
pool estimate their power |
machines may append entries |
may append entries to |
append entries to a |
entries to a log |
time between node failure |
the overall rate of |
between node failure and |
when the infiltrating miners |
overall rate of consistent |
node failure and rejoin |
machines may register to |
the infiltrating miners deliver |
rate of consistent committed |
failure and rejoin as |
may register to the |
register to the log |
of consistent committed transactions |
and rejoin as number |
infiltrating miners deliver a |
consistent committed transactions drops |
the log then sends |
committed transactions drops because |
rejoin as number of |
miners deliver a full |
log then sends to |
transactions drops because the |
as number of consecutive |
deliver a full proof |
then sends to each |
drops because the probability |
number of consecutive updates |
a full proof of |
sends to each all |
because the probability of |
of consecutive updates missed |
full proof of work |
to each all entries |
the probability of conflicts |
consecutive updates missed by |
probability of conflicts in |
updates missed by the |
the attacking pool discards |
of conflicts in the |
from the first one |
missed by the victim |
by the victim node |
conflicts in the clustered |
the first one in |
attacking pool discards it |
in the clustered scenario |
first one in the |
one in the log |
the clustered scenario is |
clustered scenario is higher |
this attack affects the |
attack affects the revenues |
affects the revenues of |
the revenues of the |
revenues of the pools |
of the pools in |
the pools in several |
and then new entries |
pools in several ways |
then new entries as |
to illustrate more realistic |
new entries as they |
illustrate more realistic behavior |
entries as they arrive |
the victim pool s |
victim pool s effective |
pool s effective mining |
we use clustered accesses |
s effective mining rate |
use clustered accesses that |
an om may instruct |
effective mining rate is |
clustered accesses that slowly |
om may instruct the |
mining rate is unchanged |
accesses that slowly drift |
may instruct the log |
instruct the log to |
the log to truncate |
log to truncate its |
to truncate its prefix |
but its total revenue |
transactions are perfectly clustered |
expression for ri in |
for ri in a |
ri in a system |
in a system with |
a system with pools |
system with pools of |
with pools of equal |
pools of equal size |
its total revenue is |
as in the previous |
in the previous experiment |
total revenue is divided |
revenue is divided among |
algorithm we now describe |
is divided among more |
we now describe the |
divided among more miners |
now describe the acid |
minutes the cluster structure |
the cluster structure shifts |
cluster structure shifts by |
the attacker s mining |
attacker s mining power |
s mining power is |
mining power is reduced |
we explain the reservation |
explain the reservation and |
the reservation and certification |
reservation and certification protocol |
since some of its |
some of its miners |
of its miners are |
its miners are used |
miners are used for |
are used for block |
used for block withholding |
but it earns additional |
it earns additional revenue |
earns additional revenue through |
additional revenue through its |
then discuss prediction errors |
revenue through its infiltration |
through its infiltration of |
its infiltration of the |
infiltration of the other |
of the other pool |
a transaction begins with |
transaction begins with the |
begins with the tm |
the total effective mining |
with the tm receiving |
total effective mining power |
the tm receiving a |
effective mining power in |
tm receiving a begin |
mining power in the |
power in the system |
in the system is |
the system is reduced |
transaction instruction from the |
instruction from the client |
causing the bitcoin protocol |
the tm assigns it |
the bitcoin protocol to |
tm assigns it a |
bitcoin protocol to reduce |
assigns it a unique |
it a unique txnid |
protocol to reduce the |
to reduce the difficulty |
q mi q mi |
and predicts which objects |
predicts which objects the |
which objects the transaction |
taking all these factors |
objects the transaction will |
all these factors into |
the transaction will access |
these factors into account |
it interrogates the oms |
interrogates the oms about |
the oms about all |
we observe that a |
oms about all these |
about all these objects |
observe that a pool |
that a pool might |
a pool might be |
pool might be able |
might be able to |
and they respond with |
be able to increase |
they respond with the |
and wrapping back to |
able to increase its |
respond with the latest |
wrapping back to zero |
to increase its revenue |
with the latest unreserved |
back to zero after |
increase its revenue by |
the latest unreserved timestamp |
its revenue by attacking |
latest unreserved timestamp of |
revenue by attacking other |
unreserved timestamp of each |
by attacking other pools |
timestamp of each object |
the tm chooses a |
each pool therefore makes |
tm chooses a timestamp |
pool therefore makes a |
chooses a timestamp larger |
therefore makes a choice |
a timestamp larger than |
makes a choice of |
timestamp larger than maximum |
a choice of whether |
larger than maximum among |
choice of whether to |
than maximum among the |
of whether to attack |
maximum among the responses |
whether to attack each |
to attack each of |
attack each of the |
each of the other |
of the other pools |
and asks the oms |
the objects dependency lists |
the other pools in |
asks the oms to |
objects dependency lists are |
other pools in the |
the oms to reserve |
dependency lists are outdated |
pools in the system |
oms to reserve the |
to reserve the objects |
reserve the objects with |
the objects with this |
this leads to a |
objects with this timestamp |
leads to a sudden |
and with what infiltration |
with this timestamp to |
to a sudden increased |
with what infiltration rate |
this timestamp to txnid |
a sudden increased inconsistency |
sudden increased inconsistency rate |
increased inconsistency rate that |
inconsistency rate that converges |
this gives rise to |
rate that converges back |
the oms confirm the |
gives rise to the |
that converges back to |
oms confirm the reservation |
rise to the pool |
converges back to zero |
confirm the reservation if |
to the pool game |
the reservation if no |
reservation if no concurrent |
until this convergence is |
if no concurrent tm |
this convergence is interrupted |
we specify this game |
no concurrent tm has |
convergence is interrupted by |
specify this game and |
concurrent tm has reserved |
is interrupted by the |
this game and provide |
tm has reserved a |
interrupted by the next |
by the next shift |
has reserved a larger |
game and provide initial |
reserved a larger timestamp |
and provide initial analysis |
a larger timestamp in |
larger timestamp in the |
timestamp in the meantime |
q symmetric equilibrium values |
provide initial analysis in |
initial analysis in section |
analysis in section iv |
symmetric equilibrium values for |
the tm then proceeds |
equilibrium values for a |
tm then proceeds to |
values for a system |
for a system of |
a system of q |
system of q pools |
b presented three possible |
in section v we |
of q pools of |
q pools of equal |
pools of equal sizes |
section v we analyze |
gossip chain inconsistency window |
then proceeds to serve |
presented three possible strategies |
v we analyze the |
proceeds to serve transaction |
often publish this data |
three possible strategies for |
we analyze the scenario |
to serve transaction operations |
publish this data to |
possible strategies for the |
analyze the scenario where |
serve transaction operations by |
this data to demonstrate |
strategies for the cache |
the scenario where exactly |
transaction operations by routing |
operations by routing them |
for the cache to |
scenario where exactly two |
data to demonstrate their |
by routing them to |
the cache to deal |
where exactly two of |
to demonstrate their honesty |
routing them to the |
cache to deal with |
exactly two of the |
demonstrate their honesty to |
their honesty to their |
honesty to their miners |
two of the pools |
them to the appropriate |
to deal with inconsistency |
of the pools take |
to the appropriate oms |
deal with inconsistency detection |
the pools take part |
pools take part in |
take part in the |
part in the game |
each operation is sent |
in the game and |
operation is sent to |
the game and only |
is sent to the |
game and only one |
sent to the om |
and only one can |
to the om in |
only one can attack |
the om in charge |
one can attack the |
om in charge of |
can attack the other |
in charge of the |
charge of the object |
the attacker can always |
attacker can always increase |
can always increase its |
always increase its revenue |
an example flow of |
increase its revenue by |
example flow of the |
flow of the algorithm |
its revenue by attacking |
aborting and evicting value |
along with the txnid |
we conclude that in |
conclude that in the |
that in the general |
in the general case |
the oms order accesses |
oms order accesses based |
order accesses based on |
accesses based on timestamp |
based on timestamp reservations |
with any number of |
a pool can infiltrate |
any number of pools |
and respond only when |
respond only when the |
pool can infiltrate each |
only when the correct |
when the correct version |
the correct version is |
correct version is available |
can infiltrate each of |
infiltrate each of the |
each of the other |
each committed transaction is |
attacks is not a |
is not a nash |
not a nash equilibrium |
committed transaction is assigned |
transaction is assigned a |
through when possible as |
when possible as in |
is assigned a timestamp |
of the other pools |
possible as in cache |
as in cache miss |
section vi deals with |
when reading an object |
the other pools with |
vi deals with the |
deals with the case |
with the case of |
the case of two |
case of two pools |
other pools with some |
the timestamp of the |
pools with some nominal |
timestamp of the latest |
where each can attack |
we will now compare |
of the latest transaction |
inconsistency window against gossip |
each can attack the |
can attack the other |
will now compare their |
the latest transaction that |
window against gossip rate |
against gossip rate for |
now compare their efficacies |
latest transaction that wrote |
transaction that wrote this |
gossip rate for the |
rate for the whole |
we use the approximate |
use the approximate clusters |
analysis becomes more complicated |
that wrote this object |
for the whole chain |
with some nominal probing |
the approximate clusters workload |
becomes more complicated in |
wrote this object is |
this object is returned |
approximate clusters workload with |
more complicated in two |
complicated in two ways |
object is returned to |
is returned to the |
returned to the tm |
some nominal probing mining |
nominal probing mining power |
probing mining power and |
mining power and measure |
the transaction s timestamp |
power and measure the |
transaction s timestamp is |
the revenue of each |
and measure the revenue |
s timestamp is chosen |
revenue of each pool |
measure the revenue density |
a window size of |
timestamp is chosen to |
of each pool affects |
the revenue density directly |
is chosen to be |
each pool affects the |
revenue density directly by |
a pareto parameter of |
pool affects the revenue |
chosen to be larger |
density directly by monitoring |
directly by monitoring the |
to be larger than |
affects the revenue of |
by monitoring the probe |
be larger than the |
the revenue of the |
monitoring the probe s |
the probe s rewards |
probe s rewards from |
s rewards from the |
rewards from the pool |
larger than the largest |
revenue of the other |
and the maximum dependency |
time between node failure |
than the largest timestamp |
of the other through |
the maximum dependency list |
between node failure and |
the largest timestamp returned |
the other through the |
maximum dependency list size |
dependency list size is |
node failure and rejoin |
largest timestamp returned by |
other through the infiltrating |
through the infiltrating miners |
list size is set |
size is set to |
timestamp returned by its |
returned by its operations |
failure and rejoin as |
block withholding recycling we |
and rejoin as number |
and not larger than |
not larger than its |
we prove that for |
rejoin as number of |
withholding recycling we assume |
larger than its reserved |
prove that for a |
as number of consecutive |
number of consecutive updates |
than its reserved timestamp |
that for a static |
recycling we assume that |
of consecutive updates missed |
for a static choice |
the lower portion of |
we assume that the |
consecutive updates missed by |
a static choice of |
lower portion of the |
once a tm receives |
portion of the graph |
static choice of infiltration |
assume that the infiltrating |
that the infiltrating miners |
updates missed by the |
of the graph is |
choice of infiltration rates |
a tm receives an |
tm receives an end |
missed by the victim |
by the victim node |
of infiltration rates the |
infiltration rates the pool |
transaction instruction from a |
instruction from a client |
the graph is the |
rates the pool revenues |
the pool revenues converge |
graph is the ratio |
it notifies the transaction |
notifies the transaction s |
the transaction s oms |
the infiltrating miners are |
infiltrating miners are loyal |
miners are loyal to |
are loyal to the |
detailing the transaction s |
once one pool changes |
is the ratio of |
the transaction s timestamp |
transaction s timestamp and |
s timestamp and log |
the ratio of committed |
loyal to the attacker |
one pool changes its |
ratio of committed transactions |
pool changes its infiltration |
of committed transactions that |
changes its infiltration rate |
the logs in charge |
committed transactions that the |
its infiltration rate of |
logs in charge of |
transactions that the abort |
some of the pool |
of the pool s |
the pool s members |
pool s members may |
s members may be |
members may be disloyal |
may be disloyal infiltrators |
infiltration rate of the |
in charge of the |
that the abort strategy |
rate of the other |
charge of the shards |
the abort strategy provides |
when sending disloyal miners |
of the shards it |
the latter may prefer |
sending disloyal miners to |
abort strategy provides a |
the shards it touched |
latter may prefer to |
disloyal miners to perform |
strategy provides a significant |
may prefer to change |
miners to perform block |
to perform block withholding |
perform block withholding at |
block withholding at other |
withholding at other pools |
when it receives such |
inconsistency window against the |
prefer to change its |
provides a significant improvement |
it receives such a |
window against the ratio |
to change its infiltration |
change its infiltration rate |
a significant improvement over |
receives such a notification |
against the ratio between |
an attacker takes a |
attacker takes a significant |
takes a significant risk |
an om appends to |
its infiltration rate of |
significant improvement over a |
the ratio between injection |
om appends to its |
infiltration rate of the |
improvement over a normal |
ratio between injection rate |
appends to its log |
rate of the former |
between injection rate and |
to its log an |
its log an entry |
injection rate and gossip |
rate and gossip rate |
therefore the game itself |
can use a loyal |
use a loyal miner |
a loyal miner w |
loyal miner w to |
miner w to infiltrate |
w to infiltrate pool |
as the strategy detects |
the game itself takes |
log an entry consisting |
the strategy detects and |
game itself takes multiple |
an entry consisting of |
strategy detects and aborts |
itself takes multiple rounds |
entry consisting of the |
detects and aborts over |
takes multiple rounds to |
consisting of the txnid |
multiple rounds to converge |
thinking the miner is |
the miner is loyal |
miner is loyal to |
is loyal to it |
different update injection delay |
we show analytically that |
show analytically that the |
of all inconsistent transactions |
analytically that the game |
might use it to |
use it to attack |
it to attack pool |
all inconsistent transactions that |
that the game has |
such logs may be |
s overload by dropping |
the game has a |
inconsistent transactions that would |
logs may be implemented |
overload by dropping updates |
game has a single |
transactions that would have |
may be implemented with |
be implemented with various |
implemented with various techniques |
has a single nash |
that would have been |
would have been committed |
by dropping updates on |
from smr to log |
dropping updates on its |
a single nash equilibrium |
smr to log chains |
the miner m can |
miner m can perform |
m can perform honest |
can perform honest mining |
perform honest mining for |
honest mining for pool |
single nash equilibrium and |
updates on its inbound |
but the other strategies |
nash equilibrium and numerically |
on its inbound and |
the other strategies make |
equilibrium and numerically study |
its inbound and outbound |
other strategies make further |
strategies make further improvements |
and numerically study the |
inbound and outbound fifo |
rather than withhold its |
than withhold its blocks |
and outbound fifo channels |
numerically study the equilibrium |
evict reduces uncommittable transactions |
and not return any |
not return any revenue |
return any revenue to |
we abstract this write |
study the equilibrium points |
outbound fifo channels according |
any revenue to pool |
reduces uncommittable transactions to |
the equilibrium points for |
fifo channels according to |
equilibrium points for different |
channels according to a |
points for different pool |
according to a random |
for different pool sizes |
set with the read |
to a random distribution |
it will take its |
will take its share |
of its value with |
for pools smaller than |
take its share of |
its share of pool |
its value with abort |
with the read timestamps |
a random distribution throughout |
random distribution throughout the |
distribution throughout the first |
this indicates that violating |
throughout the first three |
the first three quarters |
first three quarters of |
three quarters of the |
quarters of the experiment |
and assume highly available |
assume highly available logs |
which thinks the miner |
thinks the miner is |
the miner is loyal |
miner is loyal to |
is loyal to it |
cache entries are likely |
at the equilibrium point |
entries are likely to |
set with written values |
the equilibrium point both |
are likely to be |
likely to be repeat |
to be repeat offenders |
and deliver it back |
deliver it back to |
it back to pool |
equilibrium point both pools |
they are too old |
point both pools earn |
are too old for |
both pools earn less |
too old for objects |
pools earn less than |
old for objects that |
to avoid such a |
avoid such a risk |
for objects that are |
earn less than they |
action should be either |
objects that are likely |
less than they would |
should be either committed |
a pool needs a |
that are likely to |
than they would have |
be either committed or |
either committed or aborted |
are likely to be |
they would have in |
would have in the |
committed or aborted in |
likely to be accessed |
to be accessed together |
have in the nonequilibrium |
or aborted in all |
aborted in all its |
in all its logs |
in the nonequilibrium no |
pool needs a sufficient |
be accessed together with |
accessed together with them |
together with them in |
and therefore cannot be |
needs a sufficient number |
a sufficient number of |
therefore cannot be removed |
with them in future |
them in future transactions |
cannot be removed from |
sufficient number of verified |
be removed from any |
since pools can decide |
and so it is |
removed from any of |
number of verified miners |
pools can decide to |
and we report on |
so it is better |
from any of them |
any of them before |
can decide to start |
it is better to |
is better to evict |
better to evict them |
decide to start or |
of verified miners miners |
of them before the |
to start or stop |
start or stop attacking |
them before the result |
retry reduces uncommittable transactions |
reduces uncommittable transactions further |
or stop attacking at |
before the result is |
the result is published |
uncommittable transactions further to |
stop attacking at any |
attacking at any point |
updates that were initially |
transactions further to about |
verified miners miners that |
miners miners that it |
miners that it knows |
the committing tm appends |
this can be modeled |
that it knows to |
it knows to be |
knows to be loyal |
can be modeled as |
that were initially dropped |
of its value with |
were initially dropped and |
committing tm appends a |
initially dropped and eventually |
tm appends a gc |
dropped and eventually made |
appends a gc entry |
and eventually made their |
a gc entry to |
eventually made their way |
the optimal infiltration rate |
be modeled as the |
its value with abort |
gc entry to all |
made their way through |
optimal infiltration rate may |
infiltration rate may be |
rate may be as |
may be as high |
be as high as |
entry to all the |
their way through gossip |
modeled as the miner |
realistic workloads we now |
to all the transaction |
way through gossip could |
as the miner s |
workloads we now evaluate |
all the transaction s |
through gossip could later |
the miner s dilemma |
we now evaluate the |
the transaction s logs |
of the pool size |
gossip could later be |
miner s dilemma an |
now evaluate the efficacy |
transaction s logs after |
could later be sent |
s dilemma an instance |
evaluate the efficacy of |
the efficacy of t |
s logs after receiving |
later be sent via |
dilemma an instance of |
but this is only |
cache with workloads based |
be sent via fifo |
an instance of the |
logs after receiving an |
this is only in |
with workloads based on |
sent via fifo channels |
instance of the iterative |
after receiving an acknowledgement |
is only in extreme |
workloads based on two |
via fifo channels as |
of the iterative prisoner |
receiving an acknowledgement that |
only in extreme cases |
in extreme cases when |
extreme cases when pools |
cases when pools are |
when pools are large |
based on two sampled |
fifo channels as shown |
the iterative prisoner s |
an acknowledgement that they |
on two sampled topologies |
channels as shown by |
iterative prisoner s dilemma |
for practical pool sizes |
acknowledgement that they all |
two sampled topologies from |
as shown by the |
that they all registered |
sampled topologies from the |
shown by the increasingly |
attacking is the dominant |
a pool may need |
pool may need up |
may need up to |
they all registered the |
is the dominant strategy |
topologies from the online |
by the increasingly large |
all registered the transaction |
the dominant strategy in |
from the online retailer |
the increasingly large density |
registered the transaction s |
dominant strategy in each |
the online retailer amazon |
increasingly large density of |
the transaction s result |
of its mining power |
its mining power for |
mining power for infiltration |
large density of dark |
strategy in each iteration |
online retailer amazon and |
an om can invoke |
retailer amazon and the |
om can invoke log |
amazon and the social |
but if the pools |
can invoke log prefix |
and the social network |
plots closer to the |
if the pools can |
invoke log prefix truncation |
the social network orkut |
pools typically have loyal |
closer to the tail |
the pools can agree |
log prefix truncation if |
prefix truncation if the |
to the tail of |
pools can agree not |
can agree not to |
truncation if the prefix |
the tail of the |
tail of the chain |
agree not to attack |
if the prefix was |
describes how we generated |
how we generated these |
we generated these workloads |
typically have loyal mining |
the prefix was summarized |
as before note that |
both benefit in the |
benefit in the long |
in the long run |
have loyal mining power |
before note that the |
measures the efficacy of |
the efficacy of t |
and all its transactions |
note that the yaxes |
loyal mining power either |
all its transactions have |
that the yaxes have |
cache on these workloads |
we address in section |
mining power either run |
its transactions have corresponding |
the yaxes have different |
on these workloads as |
address in section vii |
power either run directly |
transactions have corresponding gc |
have corresponding gc entries |
these workloads as a |
in section vii the |
either run directly by |
yaxes have different scales |
workloads as a function |
section vii the case |
run directly by the |
then waits for the |
have different scales to |
as a function of |
vii the case where |
directly by the pool |
waits for the entry |
different scales to observe |
a function of maximum |
the case where the |
by the pool owners |
for the entry to |
scales to observe the |
function of maximum dependency |
case where the participants |
the pool owners or |
the entry to appear |
to observe the delays |
of maximum dependency list |
where the participants are |
pool owners or sold |
entry to appear in |
observe the delays better |
maximum dependency list size |
the participants are an |
owners or sold as |
to appear in the |
appear in the log |
and compares this to |
the figures show that |
or sold as a |
participants are an arbitrary |
compares this to a |
figures show that even |
sold as a service |
a transaction is committed |
are an arbitrary number |
this to a strategy |
show that even for |
as a service but |
transaction is committed if |
an arbitrary number of |
to a strategy based |
that even for a |
a service but run |
is committed if and |
arbitrary number of identical |
a strategy based on |
even for a gossip |
for a gossip rate |
committed if and only |
number of identical pools |
strategy based on ttls |
service but run on |
but run on the |
run on the pool |
on the pool owners |
the pool owners hardware |
there exists a symmetric |
if and only if |
a gossip rate half |
exists a symmetric equilibrium |
and only if it |
gossip rate half the |
a symmetric equilibrium in |
only if it is |
compares the efficacy of |
rate half the injection |
symmetric equilibrium in which |
if it is written |
the efficacy of the |
half the injection rate |
equilibrium in which each |
it is written to |
efficacy of the three |
in which each participating |
recall that this is |
of the three strategies |
is written to all |
written to all logs |
that this is the |
the three strategies of |
which each participating pool |
this is the rate |
and it does not |
each participating pool attacks |
three strategies of dealing |
is the rate at |
it does not conflict |
participating pool attacks each |
strategies of dealing with |
the rate at which |
however the size of |
pool attacks each of |
of dealing with detected |
does not conflict with |
rate at which digests |
the size of this |
attacks each of the |
dealing with detected inconsistencies |
not conflict with previous |
conflict with previous transactions |
each of the other |
of the other participating |
with previous transactions on |
previous transactions on any |
are exchanged between two |
the other participating pools |
size of this mining |
transactions on any of |
on any of them |
of this mining power |
as in the minority |
in the minority two |
this mining power is |
exchanged between two or |
we generated two workloads |
conflicts are violations of |
are violations of read |
between two or more |
generated two workloads based |
two workloads based on |
here too at equilibrium |
mining power is considered |
two or more processes |
workloads based on real |
based on real data |
power is considered a |
is considered a trade |
the epidemics could deliver |
read or writewrite order |
considered a trade secret |
a trade secret and |
trade secret and is |
secret and is not |
and is not published |
too at equilibrium all |
epidemics could deliver messages |
each om checks for |
at equilibrium all pools |
could deliver messages with |
om checks for local |
we started from a |
equilibrium all pools earn |
deliver messages with a |
messages with a delay |
checks for local conflicts |
started from a snapshot |
all pools earn less |
pools earn less than |
with a delay of |
for local conflicts by |
from a snapshot of |
a snapshot of amazon |
earn less than with |
a delay of about |
local conflicts by checking |
countermeasures as in the |
snapshot of amazon s |
less than with the |
than with the no |
as in the case |
of amazon s product |
amazon s product co |
in the case of |
conflicts by checking timestamps |
the case of classical |
purchasing graph taken early |
by checking timestamps in |
case of classical block |
checking timestamps in the |
of classical block withholding |
classical block withholding explained |
block withholding explained in |
withholding explained in section |
explained in section ii |
our results imply that |
timestamps in the prefix |
results imply that block |
in the prefix of |
s for the rest |
imply that block withholding |
the prefix of the |
for the rest of |
that block withholding by |
a pool might detect |
pool might detect that |
might detect that it |
detect that it is |
that it is being |
it is being attacked |
block withholding by pools |
prefix of the log |
the rest of the |
withholding by pools leads |
of the log up |
rest of the chain |
of the chain during |
each product sold by |
the log up to |
log up to the |
up to the transaction |
the chain during a |
product sold by the |
but cannot detect which |
cannot detect which of |
to the transaction entry |
chain during a congestion |
sold by the online |
by pools leads to |
pools leads to an |
during a congestion that |
by the online retailer |
detect which of its |
which of its miners |
of its miners is |
its miners is the |
miners is the attacker |
leads to an unfavorable |
and sends its local |
a congestion that took |
the online retailer is |
to an unfavorable equilibrium |
sends its local result |
therefore a pool cannot |
a pool cannot block |
pool cannot block or |
cannot block or punish |
block or punish withholding |
or punish withholding miners |
online retailer is a |
retailer is a node |
due to the anonymity |
the plot also shows |
is a node and |
to the anonymity of |
plot also shows that |
various techniques can be |
techniques can be used |
the anonymity of miners |
to the calling tm |
also shows that delays |
a node and each |
can be used to |
a single pool might |
node and each pair |
shows that delays increased |
that delays increased with |
if all return success |
single pool might be |
and each pair of |
be used to encourage |
delays increased with time |
pool might be tempted |
each pair of products |
used to encourage miners |
to encourage miners to |
encourage miners to submit |
miners to submit full |
to submit full blocks |
therefore if congestion may |
might be tempted to |
pair of products purchased |
then the transaction has |
the transaction has committed |
be tempted to attack |
of products purchased in |
a pool can pay |
if congestion may span |
products purchased in a |
leading the other pools |
otherwise it has aborted |
congestion may span large |
purchased in a single |
in a single user |
the other pools to |
may span large periods |
span large periods of |
large periods of time |
other pools to attack |
the tm notifies the |
pool can pay a |
a single user session |
pools to attack as |
tm notifies the client |
can pay a bonus |
pay a bonus for |
a bonus for submitting |
bonus for submitting a |
for submitting a full |
submitting a full proof |
a full proof of |
full proof of work |
notifies the client of |
the gossip rate must |
single user session is |
to attack as well |
this would increase the |
gossip rate must be |
user session is an |
session is an edge |
would increase the revenue |
rate must be carefully |
the client of the |
the implications might be |
the original graph contains |
must be carefully tuned |
client of the transaction |
implications might be devastating |
might be devastating for |
original graph contains more |
be carefully tuned to |
of the transaction result |
increase the revenue of |
be devastating for open |
graph contains more than |
carefully tuned to compensate |
the transaction result and |
the revenue of the |
devastating for open pools |
tuned to compensate for |
transaction result and instructs |
revenue of the miner |
to compensate for the |
result and instructs the |
of the miner that |
if their revenues are |
compensate for the losses |
and instructs the oms |
the miner that found |
their revenues are reduced |
for the losses induced |
instructs the oms to |
the oms to place |
the losses induced by |
losses induced by the |
miners will prefer to |
miner that found a |
oms to place this |
induced by the congested |
will prefer to form |
that found a block |
to place this result |
by the congested tcp |
prefer to form closed |
found a block while |
place this result in |
the congested tcp channels |
to form closed pools |
a block while reducing |
this result in the |
result in the logs |
block while reducing the |
form closed pools that |
we used a snapshot |
the second round of |
the oms notify the |
closed pools that cannot |
used a snapshot of |
while reducing the revenue |
second round of experiments |
oms notify the tm |
pools that cannot be |
a snapshot of the |
reducing the revenue of |
round of experiments quantified |
notify the tm once |
that cannot be attacked |
snapshot of the friendship |
the revenue of the |
revenue of the other |
of the other miners |
the other miners from |
other miners from this |
miners from this block |
the tm once the |
cannot be attacked in |
of the friendship relations |
of experiments quantified the |
tm once the results |
be attacked in this |
the friendship relations graph |
friendship relations graph in |
experiments quantified the average |
once the results are |
attacked in this manner |
while the average revenue |
the average revenue of |
quantified the average and |
the results are logged |
though this may be |
average revenue of each |
revenue of each miner |
of each miner would |
robustness in case of |
the average and maximum |
relations graph in the |
graph in the orkut |
this may be conceived |
in case of a |
average and maximum inconsistency |
and maximum inconsistency window |
in the orkut social |
may be conceived as |
case of a tm |
of a tm or |
a tm or om |
tm or om crash |
be conceived as bad |
each miner would stay |
miner would stay the |
would stay the same |
or a missing result |
maximum inconsistency window for |
the orkut social network |
conceived as bad news |
a missing result or |
inconsistency window for a |
window for a service |
as bad news for |
missing result or gc |
result or gc entry |
bad news for public |
news for public mining |
for public mining pools |
small miners will suffer |
miners will suffer from |
will suffer from higher |
suffer from higher variance |
from higher variance in |
on the whole it |
higher variance in revenue |
due to message loss |
the whole it may |
whole it may be |
under various update injection |
another approach is to |
it may be good |
various update injection rates |
approach is to introduce |
may be good news |
update injection rates and |
another tm may read |
is to introduce a |
be good news to |
injection rates and gossip |
tm may read the |
may read the transaction |
good news to the |
rates and gossip rates |
and gossip rates respectively |
each user is a |
news to the bitcoin |
to the bitcoin system |
read the transaction entry |
user is a node |
we define the inconsistency |
the transaction entry in |
transaction entry in one |
which prefers small pools |
define the inconsistency window |
to introduce a joining |
is a node and |
entry in one of |
the inconsistency window as |
introduce a joining fee |
a node and each |
in one of the |
we examine the practicality |
inconsistency window as the |
a joining fee by |
node and each pair |
one of the logs |
examine the practicality of |
window as the time |
joining fee by paying |
and each pair of |
the practicality of the |
as the time interval |
fee by paying new |
each pair of users |
practicality of the attack |
the time interval during |
by paying new miners |
pair of users with |
and continue the certification |
of the attack in |
time interval during which |
paying new miners less |
of users with a |
continue the certification and |
the attack in section |
interval during which queries |
new miners less for |
users with a friend |
the certification and gc |
attack in section viii |
during which queries against |
miners less for their |
with a friend relationship |
certification and gc process |
in section viii and |
which queries against the |
queries against the service |
a friend relationship is |
section viii and discuss |
less for their work |
against the service return |
friend relationship is an |
if a tm places |
viii and discuss implications |
for their work until |
the service return a |
relationship is an edge |
a tm places a |
and discuss implications and |
their work until they |
service return a stale |
return a stale value |
discuss implications and model |
implications and model extensions |
the original graph contains |
tm places a transaction |
work until they have |
until they have established |
they have established a |
have established a reputation |
established a reputation with |
a reputation with the |
shows that the inconsistency |
and model extensions in |
that the inconsistency window |
reputation with the pool |
the inconsistency window grows |
original graph contains more |
inconsistency window grows slowly |
places a transaction entry |
miners that seek flexibility |
graph contains more than |
window grows slowly as |
model extensions in section |
grows slowly as the |
that seek flexibility may |
a transaction entry in |
extensions in section ix |
slowly as the gap |
seek flexibility may not |
transaction entry in a |
as the gap between |
flexibility may not accept |
entry in a strict |
the gap between the |
our contributions are the |
gap between the update |
may not accept this |
not accept this policy |
accept this policy and |
this policy and choose |
policy and choose another |
and choose another pool |
between the update injection |
in a strict subset |
contributions are the following |
the update injection rate |
a strict subset of |
update injection rate and |
strict subset of the |
injection rate and the |
rate and the gossip |
because the sampled topologies |
the pool can use |
subset of the transaction |
and the gossip rate |
the sampled topologies are |
pool can use a |
of the transaction s |
the gossip rate widens |
sampled topologies are large |
definition of the pool |
topologies are large and |
the transaction s log |
of the pool game |
the pool game where |
are large and we |
transaction s log set |
large and we only |
can use a honeypot |
pool game where pools |
the graph s x |
and we only need |
use a honeypot trap |
game where pools in |
when another tm is |
graph s x axis |
we only need to |
a honeypot trap by |
where pools in a |
another tm is instructed |
s x axis represents |
only need to simulate |
honeypot trap by sending |
pools in a proof |
tm is instructed to |
x axis represents the |
need to simulate a |
trap by sending the |
is instructed to fix |
axis represents the ratio |
to simulate a single |
by sending the miners |
ofwork secured system attack |
instructed to fix this |
represents the ratio between |
simulate a single column |
sending the miners tasks |
secured system attack one |
the ratio between the |
a single column of |
the miners tasks which |
it cannot tell whether |
system attack one another |
attack one another with |
ratio between the update |
single column of the |
miners tasks which it |
cannot tell whether the |
one another with a |
between the update injection |
column of the system |
tasks which it knows |
tell whether the original |
another with a pool |
the update injection rate |
of the system for |
which it knows will |
whether the original tm |
with a pool block |
update injection rate and |
the system for our |
it knows will result |
the original tm is |
a pool block withholding |
injection rate and gossip |
system for our purposes |
rate and gossip rate |
original tm is crashed |
pool block withholding attack |
knows will result in |
will result in a |
result in a full |
in a full proof |
a full proof of |
full proof of work |
tm is crashed or |
is crashed or slow |
this confirms that epidemics |
for our purposes one |
confirms that epidemics are |
our purposes one database |
that epidemics are a |
purposes one database server |
in the general case |
epidemics are a robust |
one database server and |
we introduce poison entries |
are a robust tunable |
database server and one |
a robust tunable mechanism |
server and one cache |
robust tunable mechanism providing |
if a miner fails |
the fixing tm places |
and one cache server |
tunable mechanism providing graceful |
mechanism providing graceful degradation |
fixing tm places a |
attacks is not an |
one cache server we |
cache server we down |
tm places a poison |
is not an equilibrium |
a miner fails to |
places a poison entry |
sample both graphs to |
the inconsistency window shifts |
inconsistency window shifts in |
a poison entry in |
miner fails to submit |
window shifts in accordance |
poison entry in the |
fails to submit the |
shifts in accordance with |
entry in the logs |
with two minority pools |
two minority pools participating |
in accordance with the |
in the logs that |
the logs that miss |
accordance with the update |
with the update injection |
the only nash equilibrium |
logs that miss the |
that miss the original |
miss the original entry |
we use a technique |
only nash equilibrium is |
to submit the full |
the update injection rate |
use a technique based |
nash equilibrium is when |
submit the full proof |
a poison is interpreted |
a technique based on |
equilibrium is when the |
is when the pools |
poison is interpreted as |
technique based on random |
the full proof of |
full proof of work |
proof of work it |
of work it is |
work it is tagged |
it is tagged as |
is tagged as an |
tagged as an attacker |
based on random walks |
notice that the difference |
when the pools attack |
is interpreted as a |
to prevent the attacker |
prevent the attacker from |
the attacker from learning |
attacker from learning them |
interpreted as a transaction |
that the difference between |
the pools attack one |
on random walks that |
the honeypot tasks have |
the difference between the |
pools attack one another |
as a transaction entry |
random walks that maintains |
honeypot tasks have to |
tasks have to be |
and both earn less |
walks that maintains important |
difference between the maximum |
have to be regularly |
to be regularly refreshed |
both earn less than |
that maintains important properties |
between the maximum inconsistency |
a transaction entry with |
earn less than if |
maintains important properties of |
the maximum inconsistency window |
transaction entry with a |
entry with a conflict |
important properties of the |
maximum inconsistency window and |
pools can also incorporate |
the original entry may |
properties of the original |
inconsistency window and the |
less than if none |
than if none had |
original entry may either |
of the original graph |
window and the average |
can also incorporate out |
if none had attacked |
entry may either arrive |
and the average inconsistency |
also incorporate out of |
may either arrive eventually |
the average inconsistency window |
incorporate out of band |
out of band mechanisms |
of band mechanisms to |
band mechanisms to deter |
mechanisms to deter attacks |
either arrive eventually or |
average inconsistency window is |
miners therefore face the |
specifically clustering which is |
such as verifying the |
arrive eventually or not |
therefore face the miner |
inconsistency window is two |
clustering which is central |
which is central to |
face the miner s |
window is two orders |
is two orders of |
two orders of magnitude |
the miner s dilemma |
as verifying the identity |
is central to our |
central to our experiment |
this reflects the degree |
and the following are |
an instance of the |
instance of the iterative |
we start by choosing |
the following are ignored |
verifying the identity of |
reflects the degree to |
of the iterative prisoner |
start by choosing a |
the identity of miners |
the degree to which |
the iterative prisoner s |
any tm can therefore |
by choosing a node |
identity of miners or |
degree to which the |
iterative prisoner s dilemma |
tm can therefore observe |
choosing a node uniformly |
of miners or using |
to which the victim |
can therefore observe the |
a node uniformly and |
miners or using trusted |
or using trusted computing |
using trusted computing technologies |
therefore observe the log |
node uniformly and random |
repeatedly choosing between attack |
which the victim node |
observe the log and |
uniformly and random and |
choosing between attack and |
the victim node lags |
the log and consistently |
and random and start |
between attack and no |
victim node lags the |
log and consistently determine |
random and start a |
node lags the other |
that assure no block |
and start a random |
and consistently determine the |
lags the other nodes |
assure no block withholding |
no block withholding is |
block withholding is taking |
withholding is taking place |
start a random walk |
consistently determine the state |
the other nodes during |
a random walk from |
this would require miners |
other nodes during the |
with multiple pools of |
determine the state of |
random walk from that |
walk from that location |
nodes during the period |
multiple pools of equal |
the state of the |
state of the transaction |
during the period before |
pools of equal size |
would require miners to |
the period before it |
of equal size there |
require miners to use |
without a race hazard |
period before it has |
equal size there is |
size there is a |
before it has fully |
prediction errors if there |
there is a symmetric |
it has fully caught |
has fully caught up |
errors if there are |
is a symmetric nash |
a symmetric nash equilibrium |
if there are no |
the walk reverts back |
next we evaluated the |
there are no prediction |
are no prediction errors |
where all pools earn |
we evaluated the inconsistency |
miners to use specialized |
walk reverts back to |
all pools earn less |
evaluated the inconsistency window |
to use specialized hardware |
use specialized hardware and |
specialized hardware and software |
the inconsistency window of |
reverts back to the |
pools earn less than |
there are no aborts |
inconsistency window of a |
back to the first |
earn less than if |
less than if none |
if the transaction accesses |
to the first node |
the first node and |
window of a service |
than if none had |
the transaction accesses an |
transaction accesses an object |
first node and start |
node and start again |
if none had attacked |
an overhead miners may |
overhead miners may not |
miners may not accept |
this is repeated until |
of a service running |
accesses an object that |
is repeated until the |
a service running at |
an object that was |
repeated until the target |
service running at a |
object that was not |
until the target number |
the target number of |
running at a particular |
that was not predicted |
there is no known |
is no known silver |
no known silver bullet |
inefficient equilibria for open |
target number of nodes |
at a particular update |
this object has no |
all these techniques reduce |
number of nodes have |
a particular update rate |
equilibria for open pools |
object has no reserved |
has no reserved version |
of nodes have been |
for open pools may |
these techniques reduce the |
no reserved version for |
reserved version for it |
and for three different |
open pools may serve |
techniques reduce the pool |
nodes have been visited |
for three different intervals |
pools may serve the |
reduce the pool s |
accessing it can therefore |
three different intervals in |
may serve the system |
the pool s attractiveness |
pool s attractiveness and |
s attractiveness and deter |
attractiveness and deter miners |
it can therefore result |
different intervals in which |
serve the system by |
can therefore result in |
intervals in which the |
the system by reducing |
therefore result in a |
in which the victim |
block withholding in practice |
show a further down |
system by reducing their |
which the victim node |
result in a conflict |
in a conflict of |
by reducing their attraction |
the victim node is |
victim node is halted |
a conflict of the |
reducing their attraction and |
withholding in practice long |
conflict of the transaction |
their attraction and pushing |
in practice long term |
of the transaction or |
attraction and pushing miners |
nodes to provide some |
to provide some perception |
the transaction or of |
and pushing miners towards |
pushing miners towards smaller |
show average and maximum |
provide some perception of |
transaction or of the |
or of the following |
of the following ones |
average and maximum inconsistency |
some perception of the |
perception of the topologies |
miners towards smaller closed |
and maximum inconsistency windows |
practice long term block |
towards smaller closed pools |
maximum inconsistency windows for |
the graphs are visibly |
graphs are visibly clustered |
no conflict would occur |
long term block withholding |
inconsistency windows for both |
term block withholding attacks |
the classical block withholding |
the amazon topology more |
windows for both the |
but if one does |
block withholding attacks are |
withholding attacks are difficult |
attacks are difficult to |
are difficult to hide |
if one does it |
classical block withholding attack |
amazon topology more so |
for both the victim |
since miners using an |
block withholding attack is |
topology more so than |
one does it will |
both the victim and |
miners using an attacked |
withholding attack is as |
more so than the |
does it will be |
the victim and for |
victim and for the |
attack is as old |
so than the orkut |
it will be detected |
will be detected at |
and for the other |
is as old as |
than the orkut one |
using an attacked pool |
be detected at certification |
detected at certification time |
as old as pools |
old as pools themselves |
for the other processes |
the other processes of |
and result in an |
other processes of one |
processes of one subservice |
but its use by |
an attacked pool would |
result in an abort |
its use by pools |
attacked pool would notice |
in an abort of |
the more messages the |
use by pools has |
pool would notice the |
would notice the reduced |
notice the reduced revenue |
the reduced revenue density |
by pools has not |
its topology has a |
an abort of a |
abort of a transaction |
pools has not been |
topology has a more |
more messages the victim |
has not been suggested |
performance may be slightly |
messages the victim node |
the victim node needs |
has a more clustered |
not been suggested until |
may be slightly reduced |
such attacks are rarely |
attacks are rarely reported |
a more clustered structure |
but consistency is maintained |
victim node needs to |
been suggested until recently |
and we can therefore |
and so the dependency |
node needs to recover |
we can therefore conclude |
we overview related attacks |
so the dependency lists |
if a transaction does |
a transaction does not |
overview related attacks and |
the dependency lists hold |
the larger the inconsistency |
larger the inconsistency window |
transaction does not access |
related attacks and prior |
dependency lists hold more |
lists hold more relevant |
does not access an |
attacks and prior work |
and prior work in |
hold more relevant information |
again the difference between |
not access an object |
can therefore conclude that |
therefore conclude that they |
treating nodes of the |
access an object that |
prior work in section |
work in section x |
the difference between the |
nodes of the graphs |
an object that was |
object that was predicted |
difference between the average |
and conclude with final |
conclude that they are |
that they are indeed |
they are indeed rare |
the tm must still |
of the graphs as |
between the average and |
conclude with final remarks |
tm must still release |
the graphs as database |
the average and maximum |
average and maximum in |
with final remarks in |
must still release the |
graphs as database objects |
a recent exception is |
final remarks in section |
still release the reservation |
recent exception is an |
remarks in section xi |
release the reservation when |
transactions are likely to |
exception is an attack |
the reservation when the |
are likely to access |
is an attack on |
reservation when the transaction |
likely to access objects |
an attack on the |
when the transaction ends |
to access objects that |
p reliminaries b itcoin |
attack on the eligius |
on the eligius pool |
the eligius pool performed |
eligius pool performed in |
pool performed in may |
performed in may and |
in may and june |
reliminaries b itcoin and |
this reservation might slow |
access objects that are |
b itcoin and p |
reservation might slow the |
objects that are topologically |
itcoin and p ooled |
might slow the processing |
that are topologically close |
and p ooled m |
slow the processing of |
are topologically close to |
p ooled m ining |
the processing of other |
topologically close to one |
ooled m ining bitcoin |
processing of other transactions |
close to one another |
m ining bitcoin is |
of other transactions that |
ining bitcoin is a |
other transactions that wait |
bitcoin is a distributed |
for the online retailer |
transactions that wait for |
that wait for its |
wait for its release |
it is likely that |
is likely that objects |
likely that objects bought |
that objects bought together |
but would not break |
objects bought together are |
would not break consistency |
bought together are also |
together are also viewed |
are also viewed and |
also viewed and updated |
viewed and updated together |
bitcoin before detecting the |
before detecting the attack |
if a tm is |
a tm is suspected |
tm is suspected as |
is suspected as failed |
at which point payouts |
which point payouts to |
point payouts to the |
payouts to the attackers |
to the attackers were |
the attackers were blocked |
its reservations are revoked |
the attackers continued the |
attackers continued the attack |
viewing and buying a |
this may harm performance |
and buying a toy |
buying a toy train |
a toy train and |
toy train and matching |
train and matching rails |
but cannot break consistency |
more bitcoin before realizing |
bitcoin before realizing they |
before realizing they were |
for the social network |
realizing they were not |
they were not receiving |
were not receiving their |
not receiving their payout |
evaluation we evaluate acid |
it is likely that |
is likely that data |
likely that data of |
the reasons the attack |
that data of befriended |
reasons the attack was |
data of befriended users |
rain by comparing its |
the attack was so |
of befriended users are |
by comparing its performance |
attack was so easily |
befriended users are viewed |
comparing its performance to |
was so easily subverted |
users are viewed and |
its performance to the |
so easily subverted is |
are viewed and updated |
performance to the classical |
easily subverted is the |
viewed and updated together |
to the classical approach |
subverted is the limited |
the classical approach that |
is the limited efforts |
classical approach that does |
the limited efforts of |
approach that does not |
limited efforts of the |
efforts of the attackers |
of the attackers to |
the attackers to hide |
attackers to hide themselves |
that does not use |
clients use the system |
tagging a person in |
a person in a |
person in a picture |
does not use prediction |
they have only used |
use the system by |
not use prediction and |
have only used two |
the system by issuing |
commenting on a post |
use prediction and compare |
only used two payout |
used two payout addresses |
two payout addresses to |
payout addresses to collect |
addresses to collect their |
to collect their payouts |
prediction and compare its |
system by issuing transactions |
on a post by |
and compare its certification |
a post by a |
post by a friend |
compare its certification protocol |
and so it was |
and the system s |
its certification protocol with |
the system s only |
so it was possible |
certification protocol with other |
system s only task |
it was possible for |
protocol with other certification |
s only task is |
was possible for the |
with other certification schemes |
by a friend s |
only task is to |
possible for the alert |
a friend s friend |
task is to serialize |
for the alert pool |
we use a custom |
is to serialize transactions |
the alert pool manager |
or viewing one s |
viewing one s neighborhood |
alert pool manager to |
to serialize transactions in |
pool manager to cluster |
serialize transactions in a |
manager to cluster the |
transactions in a single |
simulating each of the |
we run a set |
run a set of |
in a single ledger |
each of the agents |
to cluster the attacking |
a set of experiments |
a single ledger and |
of the agents in |
cluster the attacking miners |
set of experiments similar |
single ledger and reject |
the agents in the |
agents in the system |
of experiments similar to |
ledger and reject transactions |
the attacking miners and |
in the system clients |
experiments similar to the |
and reject transactions that |
attacking miners and obtain |
similar to the t |
reject transactions that cannot |
miners and obtain a |
and obtain a statistically |
transactions that cannot be |
obtain a statistically significant |
a statistically significant proof |
statistically significant proof of |
varying cache entry ttl |
significant proof of their |
proof of their wrongdoing |
our workloads are an |
cache entry ttl to |
that cannot be serialized |
workloads are an adaptation |
entry ttl to evaluate |
cannot be serialized due |
it is unknown whether |
are an adaptation of |
ttl to evaluate the |
be serialized due to |
is unknown whether this |
an adaptation of the |
to evaluate the efficacy |
serialized due to conflicts |
due to conflicts with |
adaptation of the transactional |
evaluate the efficacy of |
unknown whether this was |
whether this was a |
this was a classical |
was a classical block |
a classical block withholding |
classical block withholding attack |
the efficacy of this |
to conflicts with previous |
of the transactional ycsb |
with the goal of |
the goal of sabotage |
efficacy of this method |
the transactional ycsb specification |
conflicts with previous transactions |
or a more elaborate |
a more elaborate scheme |
of this method in |
this method in reducing |
method in reducing inconsistencies |
in reducing inconsistencies and |
to verify the effectiveness |
verify the effectiveness of |
the effectiveness of block |
effectiveness of block withholding |
of block withholding for |
block withholding for profit |
reducing inconsistencies and the |
bitcoin transactions are protected |
inconsistencies and the corresponding |
transactions are protected with |
and the corresponding overhead |
are protected with cryptographic |
protected with cryptographic techniques |
with cryptographic techniques that |
cryptographic techniques that ensure |
techniques that ensure that |
that ensure that only |
based on the original |
ensure that only the |
that only the rightful |
only the rightful owner |
limiting ttl has detrimental |
the rightful owner of |
ttl has detrimental effects |
rightful owner of a |
implemented an experimental bitcoin |
has detrimental effects on |
owner of a bitcoin |
of a bitcoin can |
detrimental effects on cache |
effects on cache hit |
on cache hit ratio |
an experimental bitcoin test |
a bitcoin can transfer |
bitcoin can transfer it |
delivery distribution for a |
distribution for a chain |
quickly increasing the database |
increasing the database workload |
experimental bitcoin test network |
bitcoin test network and |
the transaction ledger is |
test network and demonstrated |
network and demonstrated the |
and demonstrated the practicality |
demonstrated the practicality of |
the practicality of the |
practicality of the attack |
transaction ledger is stored |
by increasing database access |
each transaction has a |
ledger is stored by |
increasing database access rate |
transaction has a set |
is stored by a |
database access rate to |
has a set of |
gossip rate left figure |
stored by a network |
access rate to more |
a set of read |
bitcoin s health large |
by a network of |
rate to more than |
s health large pools |
a network of miners |
to more than twice |
update operations spread along |
operations spread along its |
network of miners in |
more than twice its |
health large pools hinder |
spread along its execution |
of miners in a |
than twice its original |
large pools hinder bitcoin |
miners in a data |
object accesses follow one |
pools hinder bitcoin s |
twice its original load |
in a data structure |
accesses follow one of |
hinder bitcoin s distributed |
its original load we |
a data structure caller |
follow one of two |
bitcoin s distributed nature |
original load we only |
data structure caller the |
one of two different |
s distributed nature as |
load we only observe |
structure caller the blockchain |
of two different random |
two different random distributions |
we only observe a |
distributed nature as they |
only observe a reduction |
nature as they put |
observe a reduction of |
a reduction of inconsistencies |
reduction of inconsistencies of |
revenue for proof of |
of inconsistencies of about |
as they put a |
for proof of work |
they put a lot |
proof of work the |
put a lot of |
of work the blockchain |
on each graph left |
where each object is |
a lot of mining |
work the blockchain records |
each graph left bars |
each object is chosen |
this is more than |
the blockchain records the |
graph left bars denote |
object is chosen uniformly |
is chosen uniformly at |
is more than twice |
blockchain records the transactions |
left bars denote transient |
bars denote transient failure |
chosen uniformly at random |
more than twice the |
records the transactions in |
lot of mining power |
than twice the rate |
the transactions in units |
transactions in units of |
twice the rate of |
the rate of inconsistencies |
in units of blocks |
of mining power in |
rate of inconsistencies achieved |
of inconsistencies achieved by |
right bars denote a |
mining power in the |
inconsistencies achieved by t |
bars denote a transient |
power in the hands |
in the hands of |
the hands of a |
hands of a few |
of a few pool |
a few pool managers |
gc logs are truncated |
cache for the retailer |
dubbed the genesis block |
denote a transient failure |
this has been mostly |
for the retailer workload |
logs are truncated to |
a transient failure corroborated |
has been mostly addressed |
the retailer workload and |
are truncated to conserve |
is defined as part |
transient failure corroborated with |
been mostly addressed by |
retailer workload and only |
truncated to conserve resources |
defined as part of |
failure corroborated with a |
mostly addressed by community |
workload and only slightly |
to conserve resources and |
as part of the |
corroborated with a link |
addressed by community pressure |
and only slightly better |
conserve resources and to |
part of the protocol |
with a link congestion |
by community pressure on |
only slightly better than |
resources and to reduce |
a link congestion phenomenon |
link congestion phenomenon modeled |
slightly better than the |
and to reduce log |
a valid block contains |
community pressure on miners |
congestion phenomenon modeled by |
better than the rate |
to reduce log replay |
valid block contains the |
pressure on miners to |
on miners to avoid |
miners to avoid forming |
to avoid forming large |
avoid forming large pools |
reduce log replay time |
block contains the hash |
than the rate of |
log replay time on |
message drop on the |
the rate of inconsistencies |
contains the hash of |
replay time on om |
drop on the adjacent |
rate of inconsistencies achieved |
the hash of the |
time on om recovery |
on the adjacent fifo |
of inconsistencies achieved by |
hash of the previous |
the adjacent fifo channels |
inconsistencies achieved by t |
each om occasionally summarizes |
of the previous block |
adjacent fifo channels of |
fifo channels of node |
om occasionally summarizes the |
occasionally summarizes the log |
cache for the social |
the hash of the |
summarizes the log prefix |
for the social network |
the social network workload |
hash of the transactions |
however such recommendations had |
such recommendations had only |
recommendations had only had |
and with twice the |
of the transactions in |
the transactions in the |
and places this summary |
with twice the additional |
twice the additional load |
transactions in the current |
places this summary in |
this summary in the |
the additional load on |
in the current block |
had only had limited |
only had limited success |
additional load on the |
summary in the log |
load on the database |
and a bitcoin address |
and mining is still |
a bitcoin address which |
mining is still dominated |
bitcoin address which is |
is still dominated by |
still dominated by a |
the presence of a |
address which is to |
dominated by a small |
by a small number |
a small number of |
small number of large |
number of large pools |
presence of a summary |
which is to be |
we generate a transactional |
of a summary of |
is to be credited |
generate a transactional workload |
as a characteristic example |
a summary of the |
to be credited with |
a transactional workload that |
summary of the log |
be credited with a |
transactional workload that accesses |
in the period of |
the period of november |
credited with a reward |
workload that accesses products |
of the log up |
with a reward for |
that accesses products that |
the log up to |
a reward for generating |
accesses products that are |
log up to a |
reward for generating the |
products that are topologically |
up to a certain |
for generating the block |
that are topologically close |
to a certain entry |
a certain entry is |
certain entry is not |
entry is not sufficient |
is not sufficient to |
any miner may add |
not sufficient to allow |
miner may add a |
sufficient to allow truncation |
we use random walks |
may add a valid |
to allow truncation at |
add a valid block |
a valid block to |
each transaction starts by |
three pools generated over |
allow truncation at that |
valid block to the |
transaction starts by picking |
truncation at that entry |
block to the chain |
starts by picking a |
to the chain by |
by picking a node |
picking a node uniformly |
this reason is that |
a node uniformly at |
node uniformly at random |
reason is that truncation |
of the proofs of |
the proofs of work |
is that truncation must |
uniformly at random and |
that truncation must not |
at random and takes |
proving that it has |
truncation must not break |
that it has spent |
must not break transaction |
steps of a random |
of a random walk |
not break transaction certification |
it has spent a |
has spent a certain |
the nodes visited by |
spent a certain amount |
nodes visited by the |
a certain amount of |
visited by the random |
certain amount of work |
the fact that block |
by the random walk |
amount of work and |
prediction our first test |
fact that block withholding |
the random walk are |
of work and publishing |
our first test scenario |
that block withholding attacks |
random walk are the |
work and publishing the |
first test scenario imposes |
block withholding attacks are |
walk are the objects |
and publishing the block |
test scenario imposes a |
withholding attacks are rarely |
are the objects the |
publishing the block with |
scenario imposes a load |
attacks are rarely observed |
the objects the transaction |
the block with the |
imposes a load substantially |
are rarely observed may |
objects the transaction accesses |
block with the proof |
a load substantially below |
rarely observed may indicate |
with the proof over |
load substantially below the |
update transactions first read |
observed may indicate that |
the proof over an |
substantially below the system |
transactions first read all |
first read all objects |
proof over an overlay |
below the system s |
the system s capacity |
read all objects from |
over an overlay network |
may indicate that the |
system s capacity with |
all objects from the |
an overlay network to |
overlay network to all |
objects from the database |
indicate that the active |
network to all other |
to all other miners |
that the active pools |
and then update all |
then update all objects |
update all objects at |
all objects at the |
each transaction reads and |
objects at the database |
the active pools have |
transaction reads and writes |
when a miner creates |
a miner creates a |
miner creates a block |
read transactions read the |
active pools have reached |
transactions read the objects |
read the objects directly |
the objects directly from |
it is compensated for |
objects directly from the |
directly from the cache |
is compensated for its |
pools have reached an |
compensated for its efforts |
the simulation is faithful |
simulation is faithful to |
for its efforts with |
its efforts with bitcoins |
is faithful to the |
faithful to the algorithm |
have reached an implicit |
reached an implicit or |
an implicit or explicit |
implicit or explicit agreement |
or explicit agreement not |
in this section we |
this compensation includes a |
compensation includes a per |
with the exception of |
this section we evaluate |
section we evaluate t |
the exception of a |
explicit agreement not to |
agreement not to attack |
not to attack one |
cache using the workloads |
transaction fee paid by |
to attack one another |
exception of a small |
using the workloads described |
fee paid by the |
of a small shortcut |
the workloads described above |
paid by the users |
a small shortcut oms |
by the users electronic |
small shortcut oms grant |
an attacked pool cannot |
the users electronic copy |
we found that the |
shortcut oms grant reservations |
attacked pool cannot detect |
users electronic copy available |
found that the abort |
oms grant reservations by |
pool cannot detect which |
electronic copy available at |
that the abort rate |
grant reservations by arrival |
cannot detect which of |
the abort rate is |
reservations by arrival time |
detect which of its |
abort rate is negligible |
by arrival time rather |
which of its miners |
of its miners are |
its miners are attacking |
miners are attacking it |
arrival time rather than |
rate is negligible in |
time rather than by |
is negligible in all |
negligible in all runs |
let alone which pool |
alone which pool controls |
which pool controls the |
pool controls the miners |
rather than by timestamp |
efficacy is therefore defined |
we found that less |
is therefore defined to |
at some point a |
found that less than |
therefore defined to be |
some point a pool |
this results in deadlocks |
defined to be the |
point a pool might |
results in deadlocks in |
to be the ratio |
a pool might miscalculate |
in deadlocks in high |
be the ratio of |
pool might miscalculate and |
deadlocks in high contention |
the ratio of inconsistent |
might miscalculate and decide |
in high contention scenarios |
ratio of inconsistent transactions |
miscalculate and decide to |
and decide to try |
decide to try and |
to try and increase |
try and increase its |
and increase its revenue |
and these are resolved |
of the messages were |
of inconsistent transactions out |
these are resolved with |
the messages were delivered |
inconsistent transactions out of |
transactions out of all |
whose transactions are included |
messages were delivered by |
one pool might be |
out of all commits |
are resolved with timeouts |
were delivered by gossip |
pool might be enough |
might be enough to |
be enough to break |
enough to break the |
to break the agreement |
delivered by gossip for |
and an amount of |
first we vary prediction |
the overhead of the |
by gossip for the |
an amount of minted |
we vary prediction accuracy |
possibly leading to a |
overhead of the system |
gossip for the nodes |
amount of minted bitcoins |
leading to a constant |
of the system is |
the system is twofold |
of minted bitcoins that |
to a constant rate |
for the nodes to |
minted bitcoins that are |
a constant rate of |
the nodes to the |
bitcoins that are thus |
constant rate of attacks |
nodes to the left |
dependency list maintenance implies |
the average ratio of |
that are thus introduced |
rate of attacks among |
of attacks among pools |
attacks among pools and |
among pools and a |
pools and a reduced |
and a reduced revenue |
list maintenance implies storage |
average ratio of objects |
are thus introduced into |
to the left of |
if open pools reach |
ratio of objects the |
thus introduced into the |
maintenance implies storage and |
the left of the |
left of the victim |
of objects the predictor |
introduced into the system |
implies storage and bandwidth |
open pools reach a |
objects the predictor guesses |
storage and bandwidth overhead |
pools reach a state |
the work which a |
the predictor guesses out |
and bandwidth overhead at |
this confirms that gossip |
reach a state where |
work which a miner |
predictor guesses out of |
bandwidth overhead at both |
confirms that gossip rarely |
a state where their |
which a miner is |
guesses out of the |
overhead at both the |
that gossip rarely is |
state where their revenue |
a miner is required |
out of the set |
at both the database |
gossip rarely is used |
where their revenue density |
their revenue density is |
revenue density is reduced |
density is reduced due |
is reduced due to |
reduced due to attacks |
of the set the |
both the database and |
rarely is used to |
miner is required to |
the set the transaction |
the database and the |
is used to circumvent |
used to circumvent chain |
is required to do |
set the transaction eventually |
database and the cache |
miners will leave them |
to circumvent chain replication |
required to do is |
the transaction eventually accesses |
will leave them in |
circumvent chain replication in |
to do is to |
as well as compute |
leave them in favor |
chain replication in the |
do is to repeatedly |
well as compute overhead |
them in favor of |
is equivalent to no |
is to repeatedly calculate |
as compute overhead for |
in favor of other |
favor of other available |
of other available options |
to repeatedly calculate a |
compute overhead for dependency |
replication in the normal |
in the normal case |
repeatedly calculate a a |
overhead for dependency list |
equivalent to no prediction |
miners of sufficient size |
of sufficient size can |
sufficient size can mine |
size can mine solo |
a peculiar effect is |
calculate a a hash |
for dependency list merging |
to no prediction and |
peculiar effect is noticeable |
a a hash function |
dependency list merging at |
smaller miners can form |
no prediction and no |
effect is noticeable in |
is noticeable in figure |
list merging at the |
miners can form private |
prediction and no reservation |
a hash function specifically |
merging at the server |
can form private pools |
form private pools with |
private pools with closed |
pools with closed access |
at the server and |
hash function specifically the |
in that more messages |
the server and consistency |
function specifically the sha |
limited to trusted participants |
that more messages are |
server and consistency checks |
and an accuracy of |
more messages are delivered |
and consistency checks at |
consistency checks at the |
messages are delivered via |
are delivered via gossip |
checks at the cache |
such a change may |
a change may be |
change may be in |
even in the prefix |
in the prefix part |
the prefix part of |
prefix part of the |
part of the chain |
may be in favor |
be in favor of |
in favor of bitcoin |
favor of bitcoin as |
although the effect is |
means predicting all accesses |
of bitcoin as a |
bitcoin as a whole |
the effect is also |
the storage required is |
effect is also evident |
storage required is only |
is also evident in |
also evident in the |
evident in the suffix |
since they require such |
they require such intimate |
require such intimate trust |
it is more significant |
required is only for |
is more significant on |
is only for object |
private pools are likely |
pools are likely to |
are likely to be |
likely to be smaller |
of a block header |
more significant on the |
only for object ids |
significant on the left |
and form a fine |
for object ids and |
on the left hand |
the left hand side |
left hand side figure |
to indicate that he |
form a fine grained |
object ids and versions |
where the gossip rate |
the gossip rate is |
gossip rate is higher |
a fine grained distribution |
increasing contention by decreasing |
indicate that he has |
fine grained distribution of |
contention by decreasing the |
because we observed this |
that he has performed |
he has performed this |
and both updates and |
by decreasing the number |
we observed this phenomenon |
observed this phenomenon only |
has performed this work |
both updates and checks |
decreasing the number of |
the number of objects |
this phenomenon only with |
updates and checks are |
and checks are o |
phenomenon only with update |
the miner provides a |
grained distribution of mining |
only with update rates |
with update rates of |
distribution of mining power |
miner provides a probabilistic |
of mining power with |
provides a probabilistic proof |
a probabilistic proof as |
in the number of |
probabilistic proof as follows |
load with a hot |
the number of objects |
mining power with many |
number of objects in |
we suspect that the |
the generated block has |
of objects in the |
objects in the system |
suspect that the network |
generated block has a |
block has a nonce |
in the system and |
that the network stack |
power with many small |
with many small pools |
many small pools and |
small pools and solo |
pools and solo miners |
the system and o |
the network stack is |
has a nonce field |
network stack is more |
stack is more efficient |
is more efficient in |
more efficient in dealing |
efficient in dealing with |
in dealing with udp |
which can contain any |
dealing with udp packets |
can contain any value |
with udp packets then |
udp packets then with |
packets then with tcp |
in the size of |
then with tcp ones |
the size of the |
with tcp ones under |
the miner places different |
size of the dependency |
tcp ones under heavy |
ones under heavy load |
of the dependency lists |
a pool may engage |
miner places different values |
pool may engage in |
which is limited to |
places different values in |
may engage in an |
different values in this |
engage in an attack |
values in this field |
in an attack against |
increasing contention by increasing |
in this field and |
an attack against another |
the second and potentially |
contention by increasing the |
this field and calculates |
attack against another pool |
second and potentially more |
by increasing the hot |
field and calculates the |
against another pool not |
and potentially more significant |
and calculates the hash |
calculates the hash for |
potentially more significant overhead |
another pool not to |
pool not to increase |
not to increase its |
to increase its absolute |
increase its absolute revenue |
the hash for each |
commit rate drops as |
more significant overhead is |
hash for each value |
rate drops as contention |
drops as contention rises |
significant overhead is the |
but rather to attract |
delivery distribution for a |
if the result of |
rather to attract miners |
distribution for a chain |
overhead is the effect |
the result of the |
accurate prediction reduces or |
to attract miners by |
is the effect on |
result of the hash |
prediction reduces or even |
reduces or even eliminates |
the effect on cache |
of the hash is |
attract miners by temporarily |
or even eliminates this |
effect on cache hit |
the hash is smaller |
miners by temporarily increasing |
even eliminates this drop |
on cache hit ratio |
hash is smaller than |
is smaller than a |
cache hit ratio due |
by temporarily increasing its |
smaller than a target |
hit ratio due to |
temporarily increasing its revenue |
than a target value |
ratio due to evictions |
in highest contention scenarios |
increasing its revenue relative |
its revenue relative to |
revenue relative to a |
the nonce is considered |
nonce is considered a |
due to evictions and |
to evictions and hence |
even with moderate prediction |
is considered a solution |
relative to a competing |
to a competing pool |
with moderate prediction accuracy |
evictions and hence the |
and hence the database |
hence the database load |
recent work has investigated |
and the block is |
the block is valid |
we obtain significant improvement |
work has investigated the |
obtain significant improvement over |
since cache load is |
has investigated the motivation |
significant improvement over the |
cache load is significantly |
the number of attempts |
investigated the motivation of |
improvement over the classical |
load is significantly larger |
number of attempts to |
the motivation of pools |
over the classical approach |
consistency windows is slightly |
is significantly larger than |
of attempts to find |
motivation of pools to |
windows is slightly more |
significantly larger than database |
attempts to find a |
of pools to utilize |
is slightly more than |
larger than database load |
to find a single |
pools to utilize part |
slightly more than an |
find a single hash |
to utilize part of |
more than an order |
a single hash is |
orders of magnitude for |
than an order of |
an order of magnitude |
we define slack to |
of magnitude for facebook |
utilize part of their |
single hash is therefore |
define slack to be |
part of their resources |
and this is attributable |
hash is therefore random |
slack to be the |
of their resources towards |
this is attributable to |
is therefore random with |
to be the average |
their resources towards sabotage |
is attributable to the |
therefore random with a |
be the average ratio |
resources towards sabotage attacks |
attributable to the victim |
random with a geometric |
the average ratio between |
towards sabotage attacks against |
sabotage attacks against each |
attacks against each other |
with a geometric distribution |
average ratio between the |
even a minor deterioration |
to the victim node |
ratio between the number |
a minor deterioration in |
the victim node observe |
between the number of |
minor deterioration in hit |
as each attempt is |
victim node observe that |
the number of accesses |
deterioration in hit ratio |
each attempt is a |
node observe that the |
number of accesses predicted |
in hit ratio can |
attempt is a bernoulli |
observe that the two |
of accesses predicted and |
hit ratio can yield |
is a bernoulli trial |
that the two graphs |
accesses predicted and the |
ratio can yield a |
a bernoulli trial with |
the two graphs denoting |
predicted and the number |
can yield a prohibitive |
bernoulli trial with a |
two graphs denoting the |
and the number of |
yield a prohibitive load |
trial with a success |
graphs denoting the maximum |
the number of objects |
the model of those |
a prohibitive load on |
with a success probability |
denoting the maximum inconsistency |
number of objects accessed |
of objects accessed by |
prohibitive load on the |
a success probability determined |
the maximum inconsistency windows |
model of those works |
objects accessed by the |
load on the backend |
success probability determined by |
maximum inconsistency windows for |
of those works is |
accessed by the transaction |
on the backend database |
probability determined by the |
inconsistency windows for the |
those works is different |
determined by the target |
windows for the victim |
for the victim node |
by the target value |
if a transaction accesses |
works is different from |
the victim node and |
c shows the experiment |
shows the experiment results |
at the existing huge |
is different from the |
victim node and for |
the existing huge hashing |
each data point is |
node and for the |
and for the entire |
existing huge hashing rates |
then with a slack |
with a slack of |
for the entire chain |
data point is the |
huge hashing rates and |
hashing rates and small |
the entire chain are |
entire chain are identical |
different from the pool |
rates and small target |
point is the result |
is the result of |
which means that clients |
from the pool game |
and small target values |
the result of a |
means that clients perceiving |
the pool game model |
result of a single |
that clients perceiving significant |
it would reserve another |
pool game model in |
of a single run |
the time to find |
clients perceiving significant inconsistency |
game model in two |
time to find a |
perceiving significant inconsistency are |
model in two major |
to find a single |
we vary the dependency |
significant inconsistency are the |
vary the dependency list |
find a single hash |
in two major ways |
inconsistency are the ones |
the dependency list size |
a single hash can |
two major ways a |
now with uniform random |
are the ones that |
dependency list size and |
single hash can be |
major ways a sabotage |
with uniform random load |
the ones that are |
list size and for |
hash can be approximated |
ways a sabotage attack |
uniform random load and |
ones that are querying |
size and for each |
can be approximated by |
a sabotage attack does |
random load and a |
that are querying the |
and for each value |
be approximated by an |
sabotage attack does not |
load and a variable |
are querying the victim |
for each value run |
approximated by an exponential |
by an exponential distribution |
and a variable number |
querying the victim node |
each value run the |
attack does not transfer |
a variable number of |
the victim node while |
value run the experiment |
does not transfer revenue |
not transfer revenue from |
transfer revenue from victim |
revenue from victim to |
from victim to attacker |
variable number of objects |
the average time for |
victim node while it |
average time for a |
run the experiment for |
node while it is |
the effect of using |
time for a miner |
the experiment for the |
while it is still |
it is still recovering |
is still recovering state |
for a miner to |
experiment for the two |
and migrating miners switch |
migrating miners switch to |
miners switch to less |
finally we performed a |
effect of using a |
a miner to find |
switch to less attacked |
to less attacked pools |
we performed a set |
of using a perfect |
miner to find a |
for the two workloads |
performed a set of |
using a perfect predictor |
to find a solution |
changing pool sizes and |
the two workloads and |
a set of experiments |
find a solution is |
pool sizes and hence |
sizes and hence revenues |
and hence revenues until |
hence revenues until convergence |
two workloads and measure |
set of experiments to |
a solution is therefore |
workloads and measure the |
the model is parametrized |
solution is therefore proportional |
of experiments to determine |
and measure the average |
model is parametrized by |
is therefore proportional to |
with predictors that overpredict |
experiments to determine the |
measure the average values |
the average values of |
therefore proportional to its |
predictors that overpredict by |
to determine the distribution |
is parametrized by the |
average values of these |
proportional to its hashing |
that overpredict by factors |
determine the distribution of |
parametrized by the cost |
values of these metrics |
to its hashing rate |
overpredict by factors of |
the distribution of messages |
by the cost of |
its hashing rate or |
distribution of messages delivered |
the cost of the |
hashing rate or mining |
cache is able to |
is able to reduce |
of messages delivered by |
rate or mining power |
cost of the attack |
able to reduce inconsistencies |
messages delivered by the |
delivered by the chain |
the impact of overprediction |
to reduce inconsistencies significantly |
of the attack and |
by the chain vs |
impact of overprediction is |
to maintain a constant |
the attack and by |
the chain vs delivered |
chain vs delivered by |
vs delivered by gossip |
maintain a constant rate |
attack and by the |
of overprediction is surprisingly |
for the retailer workload |
a constant rate of |
constant rate of bitcoin |
overprediction is surprisingly minor |
and by the mobility |
one transient failure affects |
transient failure affects the |
failure affects the wall |
by the mobility of |
the mobility of the |
mobility of the miners |
a finding that should |
rate of bitcoin generation |
a single dependency reduces |
the runs are eight |
single dependency reduces inconsistencies |
and the analysis demonstrates |
runs are eight times |
dependency reduces inconsistencies to |
the analysis demonstrates that |
are eight times longer |
and as part of |
analysis demonstrates that when |
eight times longer than |
as part of its |
demonstrates that when considering |
times longer than the |
part of its defense |
that when considering only |
longer than the runs |
than the runs before |
of their original value |
when considering only sabotage |
of its defense against |
considering only sabotage attacks |
two dependencies reduce inconsistencies |
its defense against denial |
both in total experiment |
in total experiment time |
dependencies reduce inconsistencies to |
defense against denial of |
ordering transactions in advance |
total experiment time and |
only sabotage attacks there |
sabotage attacks there are |
attacks there are regions |
there are regions where |
are regions where no |
transactions in advance reduces |
experiment time and time |
against denial of service |
in advance reduces conflicts |
time and time the |
of their original value |
denial of service and |
of service and other |
advance reduces conflicts and |
and time the victim |
time the victim node |
the victim node is |
victim node is halted |
and three to less |
three to less than |
service and other attacks |
reduces conflicts and increases |
conflicts and increases commit |
and increases commit ratio |
attack is the best |
is the best strategy |
the system normalizes the |
system normalizes the rate |
normalizes the rate of |
high conflict rates occur |
the rate of block |
rate of block generation |
conflict rates occur without |
the miner s dilemma |
for the social network |
rates occur without with |
show the number of |
the social network workload |
miner s dilemma is |
occur without with uniform |
the number of messages |
the protocol deterministically defines |
without with uniform access |
s dilemma is therefore |
number of messages delivered |
protocol deterministically defines the |
with uniform access to |
dilemma is therefore not |
of messages delivered by |
deterministically defines the target |
uniform access to a |
of the inconsistencies remain |
messages delivered by the |
defines the target value |
is therefore not manifested |
access to a small |
in both workloads there |
the target value for |
therefore not manifested in |
not manifested in that |
manifested in that model |
both workloads there is |
target value for each |
delivered by the chain |
to a small number |
workloads there is no |
value for each block |
pool competition for miners |
by the chain replication |
a small number of |
there is no visible |
for each block according |
competition for miners is |
the chain replication mechanism |
small number of objects |
is no visible effect |
each block according to |
for miners is an |
chain replication mechanism and |
no visible effect on |
block according to the |
miners is an incentive |
replication mechanism and the |
visible effect on cache |
according to the time |
is an incentive in |
mechanism and the ones |
effect on cache hit |
to the time required |
and high probability of |
an incentive in and |
incentive in and of |
on cache hit ratio |
the time required to |
high probability of accessing |
and the ones delivered |
in and of its |
and of its own |
of its own for |
its own for mutual |
own for mutual attacks |
time required to generate |
probability of accessing a |
the ones delivered by |
and hence no increased |
and a pool may |
of accessing a hotzone |
ones delivered by the |
required to generate recent |
hence no increased access |
no increased access rate |
delivered by the epidemics |
to generate recent blocks |
a pool may therefore |
increased access rate at |
access rate at the |
rate at the database |
pool may therefore choose |
even inaccurate prediction is |
for each of the |
each of the nodes |
the reduction in inconsistency |
may therefore choose to |
inaccurate prediction is significant |
of the nodes in |
reduction in inconsistency ratio |
therefore choose to perform |
prediction is significant in |
the nodes in a |
is updated once every |
in inconsistency ratio is |
choose to perform block |
is significant in high |
nodes in a chain |
inconsistency ratio is significantly |
to perform block withholding |
significant in high contention |
ratio is significantly better |
perform block withholding even |
is significantly better for |
block withholding even if |
again we omitted the |
significantly better for the |
compared to the the |
to the the classical |
we omitted the head |
better for the next |
withholding even if its |
blocks such that the |
the the classical approach |
omitted the head of |
for the next we |
even if its revenue |
such that the average |
the head of the |
the next we compared |
if its revenue would |
that the average time |
head of the chain |
next we compared our |
its revenue would increase |
the average time for |
of the chain node |
we compared our technique |
revenue would increase only |
average time for each |
the chain node because |
compared our technique with |
would increase only after |
increase only after the |
only after the next |
after the next difficult |
the next difficult adjustment |
time for each block |
commit ratio is affected |
chain node because its |
our technique with a |
for each block to |
ratio is affected if |
node because its behavior |
the two models are |
two models are therefore |
models are therefore complimentary |
is affected if the |
because its behavior is |
technique with a simple |
each block to be |
affected if the predictor |
its behavior is not |
with a simple approach |
the analysis of their |
block to be found |
if the predictor reserves |
behavior is not representative |
a simple approach in |
analysis of their combination |
of their combination is |
the predictor reserves unnecessary |
simple approach in which |
to be found is |
their combination is left |
combination is left for |
is left for future |
left for future work |
predictor reserves unnecessary objects |
approach in which we |
and in this experiment |
reserves unnecessary objects by |
in which we limited |
in this experiment we |
unnecessary objects by a |
which we limited the |
this experiment we have |
objects by a factor |
we limited the life |
experiment we have chains |
by a factor of |
we assumed in our |
limited the life span |
we have chains of |
note that the exponential |
a factor of slack |
assumed in our analysis |
have chains of length |
that the exponential distribution |
in our analysis that |
the exponential distribution is |
our analysis that pools |
exponential distribution is memoryless |
analysis that pools do |
that pools do not |
pools do not charge |
do not charge fees |
not charge fees from |
charge fees from their |
here inconsistencies are not |
if all miners mine |
fees from their members |
inconsistencies are not detected |
note that when all |
all miners mine for |
from their members since |
that when all accesses |
miners mine for block |
but their probability of |
when all accesses are |
their probability of being |
mine for block number |
for block number b |
all accesses are to |
probability of being witnessed |
their members since such |
accesses are to the |
of being witnessed is |
once the block is |
being witnessed is reduced |
members since such fees |
since such fees are |
such fees are typically |
fees are typically nominal |
are to the hot |
the block is found |
witnessed is reduced by |
to the hot zone |
is reduced by having |
delivered updates by means |
block is found at |
reduced by having the |
updates by means of |
is found at time |
by having the cache |
by means of the |
found at time t |
having the cache evict |
of a pool s |
a pool s revenue |
the cache evict entries |
means of the gossip |
all miners switch to |
cache evict entries after |
of the gossip repair |
miners switch to mine |
evict entries after a |
the gossip repair mechanism |
switch to mine for |
entries after a certain |
to mine for the |
after a certain period |
mine for the subsequent |
commit rates are lower |
a certain period even |
for the subsequent block |
rates are lower with |
as the nodes get |
certain period even if |
the subsequent block b |
are lower with imperfect |
the model can be |
model can be extended |
can be extended to |
be extended to include |
extended to include pools |
to include pools fees |
lower with imperfect prediction |
the nodes get further |
period even if the |
with imperfect prediction than |
nodes get further away |
even if the database |
fees would add a |
at t without changing |
imperfect prediction than in |
get further away from |
if the database did |
would add a friction |
t without changing their |
prediction than in the |
further away from the |
the database did not |
add a friction element |
without changing their probability |
than in the uniform |
away from the victim |
database did not indicate |
a friction element to |
changing their probability distribution |
in the uniform random |
from the victim node |
did not indicate they |
not indicate they are |
their probability distribution of |
the uniform random case |
uniform random case with |
indicate they are invalid |
probability distribution of finding |
friction element to the |
more of the messages |
distribution of finding a |
of finding a block |
of the messages were |
element to the flow |
finding a block after |
the messages were delivered |
to the flow of |
a block after t |
messages were delivered by |
the flow of revenue |
flow of revenue among |
of revenue among infiltrated |
revenue among infiltrated and |
among infiltrated and infiltrating |
infiltrated and infiltrating pools |
were delivered by means |
delivered by means of |
by means of the |
means of the chain |
the probability that a |
compares the efficacy of |
the efficacy of the |
probability that a miner |
efficacy of the abort |
that a miner i |
a miner i with |
would change to take |
because the repair mechanism |
evict and retry policies |
change to take into |
miner i with mining |
the repair mechanism relinked |
and retry policies with |
to take into account |
i with mining power |
repair mechanism relinked the |
this is because all |
retry policies with the |
take into account a |
into account a pool |
account a pool fee |
a pool fee of |
pool fee of f |
fee of f pp |
of f pp ri |
is because all accesses |
policies with the amazon |
with mining power mi |
mechanism relinked the chain |
because all accesses to |
with the amazon and |
mining power mi finds |
relinked the chain and |
all accesses to the |
the amazon and orkut |
power mi finds the |
the chain and chain |
accesses to the hot |
amazon and orkut workloads |
mi finds the next |
chain and chain replication |
finds the next block |
zone go through a |
and chain replication began |
the next block is |
go through a single |
in these experiments we |
chain replication began to |
next block is its |
through a single om |
these experiments we use |
replication began to function |
block is its ratio |
a single om that |
experiments we use dependency |
began to function normally |
is its ratio out |
single om that becomes |
we use dependency lists |
its ratio out of |
om that becomes a |
use dependency lists of |
ratio out of the |
the speed with which |
dependency lists of length |
that becomes a bottleneck |
out of the total |
speed with which the |
of the total mining |
with which the chain |
the total mining power |
on the bright side |
which the chain is |
just as with the |
the chain is restored |
total mining power m |
as with the synthetic |
chain is restored depends |
mining power m in |
since object access conflicts |
with the synthetic workload |
is restored depends on |
power m in the |
object access conflicts occur |
restored depends on the |
m in the system |
access conflicts occur only |
depends on the rate |
evicting conflicting transactions is |
conflicts occur only at |
on the rate of |
conflicting transactions is an |
occur only at a |
the rate of the |
transactions is an effective |
only at a single |
at a single shard |
is an effective way |
rate of the fast |
miner miner miner pool |
an effective way of |
effective way of invalidating |
the reservations prevent deadlocks |
way of invalidating stale |
reservations prevent deadlocks and |
miner miner miner pool |
of invalidating stale objects |
prevent deadlocks and result |
a pool with a |
invalidating stale objects that |
deadlocks and result in |
pool with a fee |
stale objects that might |
and on the responsiveness |
and result in perfect |
with a fee of |
objects that might cause |
on the responsiveness of |
result in perfect commit |
in perfect commit ratio |
that might cause problems |
the responsiveness of the |
a fee of f |
perfect commit ratio with |
might cause problems for |
responsiveness of the failure |
fee of f is |
commit ratio with perfect |
cause problems for future |
of the failure detection |
of f is a |
ratio with perfect prediction |
problems for future transactions |
the failure detection mechanism |
f is a less |
is a less attractive |
a less attractive target |
less attractive target for |
attractive target for block |
target for block withholding |
the effects are more |
effects are more pronounced |
are more pronounced for |
future development the current |
since the attacker s |
development the current ssa |
more pronounced for the |
pronounced for the well |
where some of the |
the current ssa implementation |
the attacker s revenue |
some of the objects |
current ssa implementation uses |
attacker s revenue is |
s revenue is reduced |
revenue is reduced by |
with the amazon workload |
ssa implementation uses gossip |
is reduced by f |
of the objects belong |
implementation uses gossip in |
the objects belong to |
abort is able to |
and one miner mines |
objects belong to a |
belong to a so |
uses gossip in situations |
is able to detect |
one miner mines solo |
however it is also |
to a so called |
a so called hot |
it is also less |
is also less attractive |
also less attractive for |
less attractive for miners |
attractive for miners in |
for miners in general |
gossip in situations where |
pools datacenters are built |
and each access is |
in situations where faster |
of the inconsistent transactions |
datacenters are built around |
are built around the |
each access is either |
situations where faster notifications |
whereas with the less |
built around the world |
access is either to |
is either to the |
either to the hot |
trading off the two |
where faster notifications might |
clustered orkut workload it |
orkut workload it only |
faster notifications might be |
off the two for |
workload it only detects |
notifications might be helpful |
the two for best |
or outside of it |
two for best protection |
for best protection is |
best protection is left |
protection is left for |
is left for future |
left for future work |
chosen uniformly within each |
uniformly within each zone |
as part of the |
part of the treatment |
of the treatment of |
the treatment of the |
treatment of the miner |
we believe that when |
mining is only profitable |
in both cases evict |
believe that when a |
is only profitable using |
both cases evict reduces |
that when a node |
we set an average |
only profitable using dedicated |
cases evict reduces uncommittable |
when a node fails |
set an average transaction |
an average transaction per |
profitable using dedicated hardware |
evict reduces uncommittable transactions |
a node fails or |
node fails or joins |
average transaction per unit |
using dedicated hardware in |
reduces uncommittable transactions considerably |
r elated w ork |
elated w ork a |
dedicated hardware in cutting |
hardware in cutting edge |
it would be useful |
in cutting edge mining |
relative to their value |
to their value with |
would be useful to |
cutting edge mining rigs |
the block withholding attack |
their value with abort |
be useful to spread |
block withholding attack the |
and transactions arrivals are |
useful to spread the |
otherwise the energy costs |
withholding attack the danger |
transactions arrivals are governed |
to spread the news |
the energy costs exceed |
attack the danger of |
arrivals are governed by |
spread the news as |
energy costs exceed the |
the danger of a |
with the amazon workload |
are governed by a |
the news as quickly |
costs exceed the expected |
exceed the expected revenue |
the amazon workload and |
governed by a poisson |
news as quickly as |
danger of a block |
by a poisson process |
as quickly as possible |
of a block withholding |
a poisson process with |
although expected revenue from |
a block withholding attack |
poisson process with the |
expected revenue from mining |
we realize that for |
process with the required |
with the required tput |
revenue from mining is |
realize that for some |
block withholding attack is |
in the amazon workload |
from mining is proportional |
that for some particular |
withholding attack is as |
attack is as old |
is as old as |
as old as bitcoin |
old as bitcoin pools |
for some particular tasks |
mining is proportional to |
retry further reduces this |
the attack was described |
attack was described by |
was described by rosenfeld |
we are unaware of |
further reduces this value |
some particular tasks gossip |
is proportional to the |
are unaware of work |
reduces this value to |
particular tasks gossip could |
proportional to the power |
unaware of work that |
tasks gossip could be |
to the power of |
of work that uses |
gossip could be done |
the power of the |
work that uses prediction |
could be done more |
power of the mining |
that uses prediction to |
be done more efficiently |
of its value with |
of the mining rigs |
uses prediction to order |
its value with abort |
the mining rigs used |
prediction to order distributed |
to order distributed transactions |
order distributed transactions before |
distributed transactions before certification |
as pools were becoming |
we are therefore exploring |
a single home miner |
pools were becoming a |
r elated w ork |
are therefore exploring the |
single home miner using |
were becoming a dominant |
elated w ork a |
therefore exploring the use |
home miner using a |
exploring the use of |
becoming a dominant player |
the use of ip |
a dominant player in |
use of ip multicast |
uses static analysis to |
recent years have seen |
dominant player in the |
player in the bitcoin |
in the bitcoin world |
static analysis to allow |
years have seen a |
of ip multicast for |
miner using a small |
ip multicast for dissemination |
have seen a surge |
multicast for dissemination of |
analysis to allow separate |
for dissemination of urgent |
the paper described the |
paper described the standard |
described the standard attack |
to allow separate workers |
dissemination of urgent information |
seen a surge of |
of urgent information as |
used by a miner |
urgent information as long |
a surge of progress |
allow separate workers to |
using a small rig |
by a miner to |
information as long as |
surge of progress in |
separate workers to process |
a small rig is |
a miner to sabotage |
as long as the |
of progress in the |
workers to process independent |
small rig is unlikely |
miner to sabotage a |
long as the physical |
progress in the development |
to process independent transactions |
rig is unlikely to |
to sabotage a pool |
as the physical nodes |
in the development of |
process independent transactions without |
is unlikely to mine |
sabotage a pool at |
the physical nodes are |
the development of scalable |
independent transactions without synchronization |
unlikely to mine a |
a pool at the |
physical nodes are not |
development of scalable object |
to mine a block |
pool at the cost |
nodes are not on |
of scalable object stores |
rain s suggestive prediction |
at the cost of |
the cost of reducing |
cost of reducing its |
of reducing its own |
reducing its own revenue |
mine a block for |
scalable object stores that |
are not on a |
gargamel determines the final |
a block for years |
object stores that support |
not on a public |
on a public network |
a public network segment |
stores that support transactions |
a more general view |
determines the final transaction |
the final transaction order |
more general view of |
general view of fairness |
some systems such as |
view of fairness in |
we plan to include |
and does not tolerate |
of fairness in proof |
plan to include support |
does not tolerate false |
fairness in proof of |
to include support for |
not tolerate false positive |
in proof of work |
include support for the |
tolerate false positive prediction |
false positive prediction errors |
support for the partitioning |
proof of work schemes |
for the partitioning of |
of work schemes was |
work schemes was discussed |
schemes was discussed in |
the partitioning of the |
miners often organize themselves |
partitioning of the services |
it targets a different |
often organize themselves into |
of the services by |
targets a different setting |
organize themselves into mining |
the services by means |
a different setting than |
themselves into mining pools |
services by means of |
different setting than acid |
by means of registering |
means of registering partition |
of registering partition function |
registering partition function handlers |
partition function handlers with |
function handlers with a |
handlers with a global |
with a global data |
it is a fully |
a pool is a |
is a fully replicated |
pool is a group |
a fully replicated data |
is a group of |
fully replicated data store |
a group of miners |
in the context of |
the context of the |
context of the hashcash |
we have implemented only |
of the hashcash system |
group of miners that |
have implemented only the |
of miners that share |
with a centralized scheduler |
implemented only the server |
miners that share their |
only the server side |
that share their revenues |
the server side load |
share their revenues when |
server side load balancing |
their revenues when one |
side load balancing scheme |
revenues when one of |
when one of them |
one of them successfully |
for an increasing number |
of them successfully mines |
an increasing number of |
we are considering ways |
them successfully mines a |
successfully mines a block |
increasing number of shards |
are considering ways to |
early work did not |
export novel consistency definitions |
considering ways to extend |
for each block found |
novel consistency definitions that |
we run multiple simulations |
work did not address |
ways to extend our |
consistency definitions that allow |
the revenue is distributed |
did not address the |
to extend our approach |
run multiple simulations to |
definitions that allow for |
revenue is distributed among |
not address the possibility |
extend our approach for |
multiple simulations to find |
that allow for effective |
is distributed among the |
address the possibility of |
our approach for use |
simulations to find the |
allow for effective optimizations |
distributed among the pool |
the possibility of pools |
approach for use in |
to find the maximal |
among the pool members |
several recent systems implement |
for use in settings |
recent systems implement full |
possibility of pools infiltrating |
the pool members in |
find the maximal tput |
use in settings where |
systems implement full fledged |
of pools infiltrating other |
pools infiltrating other pools |
the maximal tput the |
in settings where partitioning |
implement full fledged atomicity |
pool members in proportion |
infiltrating other pools for |
other pools for block |
pools for block withholding |
full fledged atomicity while |
members in proportion to |
maximal tput the system |
settings where partitioning is |
fledged atomicity while preserving |
where partitioning is done |
atomicity while preserving the |
in proportion to their |
tput the system can |
partitioning is done on |
while preserving the system |
proportion to their mining |
the system can handle |
is done on the |
preserving the system s |
to their mining power |
done on the client |
on the client side |
the system s scalability |
a global log forms |
system s scalability with |
global log forms a |
s scalability with a |
log forms a bottleneck |
scalability with a wide |
with a wide variety |
experimentally demonstrate that block |
a wide variety of |
wide variety of workloads |
side access to subservice |
access to subservice membership |
the expected revenue of |
pc with smr tms |
to subservice membership information |
subservice membership information is |
expected revenue of a |
with smr tms is |
demonstrate that block withholding |
membership information is needed |
google s spanner utilizes |
revenue of a pool |
smr tms is blocked |
that block withholding can |
s spanner utilizes accurate |
of a pool member |
tms is blocked by |
we are also developing |
block withholding can increase |
spanner utilizes accurate clock |
a pool member is |
is blocked by contention |
are also developing a |
withholding can increase the |
can increase the attacker |
increase the attacker s |
the attacker s revenue |
also developing a gui |
utilizes accurate clock synchronization |
pool member is therefore |
blocked by contention much |
developing a gui assisted |
a gui assisted automated |
member is therefore the |
by contention much earlier |
contention much earlier than |
gui assisted automated web |
is therefore the same |
they do not address |
much earlier than acid |
assisted automated web service |
therefore the same as |
do not address the |
automated web service deployment |
web service deployment tool |
not address the question |
by balakrishnan et al |
rain due to its |
focused on web service |
on web service applications |
address the question of |
the question of mutual |
question of mutual attacks |
due to its longer |
the same as its |
is constructed on top |
developers could simply drop |
to its longer certification |
same as its revenue |
constructed on top of |
could simply drop a |
its longer certification time |
as its revenue had |
on top of the |
simply drop a wsdl |
drop a wsdl service |
a wsdl service description |
its revenue had it |
top of the scalable |
we briefly review here |
revenue had it mined |
of the scalable corfu |
briefly review here work |
had it mined solo |
have recently noted that |
review here work related |
and the system will |
recently noted that a |
here work related to |
the system will generate |
noted that a pool |
work related to acidrain |
system will generate a |
that a pool can |
due to the large |
will generate a xml |
related to acidrain s |
to acidrain s certification |
to the large power |
generate a xml description |
a pool can increase |
acidrain s certification protocol |
the large power of |
a xml description that |
pool can increase its |
large power of the |
one approach for certification |
can increase its overall |
xml description that can |
power of the pool |
approach for certification is |
increase its overall revenue |
description that can be |
for certification is to |
certification is to use |
that can be used |
its overall revenue with |
is to use a |
it finds blocks at |
can be used later |
utilize a large set |
overall revenue with block |
to use a single |
finds blocks at a |
be used later on |
a large set of |
large set of independent |
use a single highly |
blocks at a much |
used later on to |
later on to actually |
set of independent logs |
at a much higher |
available service that orders |
on to actually deploy |
a much higher rate |
revenue with block withholding |
service that orders all |
to actually deploy the |
actually deploy the service |
deploy the service automatically |
with block withholding if |
that orders all transactions |
and so the frequency |
the service will be |
service will be partitioned |
block withholding if all |
so the frequency of |
orders all transactions in |
all transactions in the |
the frequency of revenue |
withholding if all other |
transactions in the system |
and deployed on the |
frequency of revenue collection |
if all other mining |
deployed on the fly |
of revenue collection is |
all other mining is |
on the fly on |
revenue collection is higher |
other mining is performed |
the fly on top |
fly on top of |
on top of the |
top of the processing |
of the processing nodes |
mining is performed by |
allowing for a stable |
is performed by honest |
performed by honest pools |
for a stable daily |
a stable daily or |
stable daily or weekly |
daily or weekly income |
we consider the general |
consider the general case |
scaling up to turn |
up to turn the |
to turn the ssa |
turn the ssa into |
use lock chains and |
the ssa into a |
ssa into a full |
into a full scale |
a full scale platform |
lock chains and assume |
most pools are controlled |
the general case where |
chains and assume transactions |
pools are controlled by |
one of the immediate |
general case where not |
a transaction commits if |
and assume transactions are |
are controlled by a |
of the immediate future |
case where not all |
transaction commits if and |
assume transactions are known |
controlled by a centralized |
the immediate future challenges |
where not all mining |
commits if and only |
transactions are known in |
by a centralized pool |
immediate future challenges is |
not all mining is |
if and only if |
are known in advance |
a centralized pool manager |
future challenges is the |
all mining is performed |
and only if it |
challenges is the necessity |
these methods all scale |
only if it has |
mining is performed through |
is the necessity of |
methods all scale well |
if it has no |
miners register with the |
the necessity of evaluating |
all scale well and |
is performed through public |
performed through public pools |
register with the pool |
necessity of evaluating a |
scale well and in |
it has no conflicts |
with the pool manager |
of evaluating a full |
well and in many |
has no conflicts with |
and analyze situations where |
the pool manager and |
evaluating a full raps |
and in many cases |
no conflicts with previous |
conflicts with previous committed |
pool manager and mine |
a full raps of |
in many cases allow |
analyze situations where pools |
with previous committed transactions |
manager and mine on |
full raps of racs |
raps of racs deployment |
situations where pools can |
where pools can attack |
pools can attack one |
can attack one another |
many cases allow databases |
and mine on its |
multiple partitioned and cloned |
cases allow databases to |
mine on its behalf |
partitioned and cloned services |
the discrepancy between the |
discrepancy between the calculations |
between the calculations of |
allow databases to accept |
and cloned services running |
transaction rate is high |
databases to accept loads |
cloned services running on |
the pool manager generates |
to accept loads similar |
pool manager generates tasks |
services running on our |
manager generates tasks and |
such a global service |
accept loads similar to |
running on our tightly |
generates tasks and the |
a global service becomes |
and our results for |
on our tightly coupled |
tasks and the miners |
loads similar to those |
global service becomes a |
service becomes a bottleneck |
our tightly coupled cluster |
and the miners search |
similar to those handled |
to those handled by |
tightly coupled cluster would |
the miners search for |
our results for the |
those handled by non |
our system has no |
miners search for solutions |
results for the special |
coupled cluster would lead |
system has no such |
search for solutions based |
for the special case |
cluster would lead to |
has no such bottleneck |
for solutions based on |
the special case analyzed |
would lead to a |
solutions based on these |
special case analyzed there |
lead to a series |
they are not expected |
based on these tasks |
case analyzed there can |
to a series of |
are not expected to |
on these tasks that |
analyzed there can be |
a series of other |
not expected to disrupt |
these tasks that can |
there can be explained |
series of other issues |
expected to disrupt the |
tasks that can serve |
serialized all transactions when |
can be explained by |
of other issues that |
to disrupt the prevailing |
that can serve as |
all transactions when they |
be explained by the |
other issues that should |
disrupt the prevailing two |
can serve as proof |
transactions when they enter |
explained by the strong |
issues that should be |
that should be investigated |
when they enter the |
by the strong approximations |
the strong approximations in |
strong approximations in that |
approximations in that work |
they enter the system |
serve as proof of |
enter the system to |
placement given a set |
given a set of |
a set of services |
the system to achieve |
as proof of work |
note that we are |
we calculate exactly how |
how to place the |
system to achieve a |
that we are addressing |
calculate exactly how infiltrating |
to place the clones |
to achieve a deterministic |
we are addressing the |
once they find a |
they find a solution |
place the clones on |
achieve a deterministic order |
are addressing the problem |
addressing the problem of |
the clones on physical |
exactly how infiltrating miners |
the problem of read |
clones on physical nodes |
they send it to |
how infiltrating miners reduce |
despite nondeterministic operations the |
on physical nodes in |
send it to the |
it to the pool |
nondeterministic operations the transactions |
only incoherent caches that |
physical nodes in order |
nodes in order to |
to the pool manager |
operations the transactions take |
incoherent caches that respond |
infiltrating miners reduce the |
in order to satisfy |
caches that respond to |
miners reduce the revenue |
order to satisfy certain |
that respond to queries |
the pool manager behaves |
reduce the revenue density |
to satisfy certain constraints |
respond to queries without |
they consider only stored |
pool manager behaves as |
the revenue density of |
revenue density of the |
density of the infiltrated |
of the infiltrated pool |
to queries without access |
consider only stored procedures |
manager behaves as a |
queries without access to |
behaves as a single |
without access to the |
which enable this approach |
as a single miner |
access to the backend |
caching placement deciding if |
temporary block withholding in |
a single miner in |
to the backend database |
placement deciding if some |
whereas we address long |
block withholding in the |
single miner in the |
deciding if some services |
we address long running |
withholding in the block |
miner in the bitcoin |
previous work on coherent |
if some services would |
address long running transactions |
in the block withholding |
in the bitcoin system |
work on coherent caches |
some services would benefit |
long running transactions and |
the block withholding attack |
services would benefit if |
running transactions and use |
once it obtains a |
would benefit if they |
benefit if they are |
transactions and use prediction |
it obtains a legitimate |
block withholding attack discussed |
if they are fitted |
and use prediction to |
obtains a legitimate block |
withholding attack discussed in |
they are fitted with |
use prediction to infer |
a legitimate block from |
attack discussed in this |
are fitted with response |
prediction to infer an |
legitimate block from one |
discussed in this work |
fitted with response caches |
to infer an order |
block from one of |
in this work the |
from one of its |
one of its miners |
and ultimately placing the |
ultimately placing the cache |
placing the cache components |
the cache components in |
cache components in a |
components in a smart |
in a smart way |
this work the withheld |
work the withheld blocks |
the withheld blocks are |
withheld blocks are never |
blocks are never published |
the block transfers the |
location placing multiple service |
block transfers the revenue |
placing multiple service clones |
transfers the revenue to |
transactions are also serialized |
multiple service clones on |
the revenue to the |
blocks can be withheld |
can be withheld temporarily |
service clones on the |
revenue to the control |
are also serialized by |
clones on the same |
not following the bitcoin |
following the bitcoin protocol |
to the control of |
on the same physical |
also serialized by a |
the control of the |
the same physical node |
serialized by a central |
by a central service |
control of the pool |
same physical node to |
to improve an attacker |
improve an attacker s |
an attacker s revenue |
and then scheduled according |
physical node to exploit |
of the pool manager |
then scheduled according to |
node to exploit fast |
a miner or a |
scheduled according to this |
to exploit fast ipc |
supports transactions using locks |
according to this global |
to this global order |
exploit fast ipc communication |
the pool manager then |
transactions using locks or |
miner or a pool |
fast ipc communication as |
pool manager then distributes |
using locks or communication |
or a pool can |
ipc communication as opposed |
manager then distributes the |
locks or communication with |
rain avoids a central |
avoids a central service |
communication as opposed to |
then distributes the revenue |
or communication with the |
a pool can perform |
pool can perform a |
can perform a selfish |
perform a selfish mining |
as opposed to network |
distributes the revenue among |
communication with the database |
with the database on |
opposed to network messages |
the revenue among the |
a selfish mining attack |
the database on each |
to network messages if |
revenue among the miners |
database on each transaction |
network messages if the |
among the miners according |
targets a different problem |
messages if the benefits |
the miners according to |
if the benefits overweigh |
these techniques are not |
miners according to their |
the benefits overweigh the |
techniques are not applicable |
according to their mining |
benefits overweigh the cost |
are not applicable in |
to their mining power |
overweigh the cost incurred |
where it embraces non |
not applicable in our |
the cost incurred by |
cost incurred by resource |
applicable in our scenario |
with selfish mining the |
incurred by resource contention |
the architecture is illustrated |
determinism and separates execution |
and separates execution from |
by resource contention on |
architecture is illustrated in |
is illustrated in figure |
separates execution from verification |
resource contention on the |
contention on the shared |
on the shared host |
selfish mining the attacker |
mining the attacker increases |
the result is somewhat |
the attacker increases its |
management tools developing tools |
in order to estimate |
result is somewhat analogous |
attacker increases its revenue |
tools developing tools that |
order to estimate the |
is somewhat analogous to |
increases its revenue by |
developing tools that monitor |
to estimate the mining |
somewhat analogous to our |
its revenue by temporarily |
tools that monitor service |
estimate the mining power |
analogous to our separation |
revenue by temporarily withholding |
that monitor service properties |
the mining power of |
to our separation of |
by temporarily withholding its |
monitor service properties such |
mining power of a |
our separation of optimistic |
temporarily withholding its blocks |
service properties such as |
properties such as response |
separation of optimistic ordering |
withholding its blocks and |
power of a miner |
such as response time |
of optimistic ordering and |
its blocks and publishing |
optimistic ordering and conservative |
ordering and conservative certification |
the pool manager sets |
blocks and publishing them |
pool manager sets a |
and publishing them in |
manager sets a partial |
make it easier to |
publishing them in response |
sets a partial target |
by restarting new clones |
it easier to create |
easier to create a |
a partial target for |
them in response to |
to create a practical |
partial target for each |
target for each member |
create a practical predictor |
in response to block |
using vmm tricks virtual |
response to block publication |
vmm tricks virtual machines |
to block publication by |
certification scalability to evaluate |
tricks virtual machines can |
block publication by other |
scalability to evaluate the |
virtual machines can be |
publication by other pools |
by other pools and |
other pools and miners |
to evaluate the scalability |
machines can be used |
evaluate the scalability of |
can be used to |
the scalability of acid |
be used to migrate |
this attack is independent |
used to migrate transparently |
attack is independent of |
to migrate transparently a |
rain s certification mechanism |
is independent of the |
migrate transparently a collection |
than the target of |
independent of the block |
transparently a collection of |
the target of the |
we avoid prediction and |
of the block withholding |
a collection of services |
target of the bitcoin |
avoid prediction and measure |
the block withholding attack |
collection of services on |
of the bitcoin system |
prediction and measure the |
block withholding attack we |
of services on a |
and measure the maximal |
withholding attack we discuss |
services on a different |
measure the maximal commit |
attack we discuss here |
each miner is required |
on a different physical |
miner is required to |
we discuss here and |
a different physical processor |
the maximal commit rate |
is required to send |
discuss here and the |
maximal commit rate it |
required to send the |
here and the two |
or provide isolation guarantees |
commit rate it can |
to send the pool |
and the two can |
provide isolation guarantees between |
isolation guarantees between co |
send the pool manager |
the two can be |
rate it can accommodate |
the pool manager blocks |
two can be performed |
can be performed in |
be performed in concert |
it can accommodate with |
pool manager blocks that |
can accommodate with an |
the ssa can be |
manager blocks that are |
an attacker can also |
ssa can be seen |
accommodate with an increasing |
blocks that are correct |
attacker can also perform |
can be seen as |
with an increasing number |
that are correct according |
can also perform a |
be seen as a |
an increasing number of |
are correct according to |
also perform a double |
seen as a platform |
increasing number of shards |
correct according to the |
perform a double spending |
as a platform that |
according to the partial |
to the partial target |
a platform that leverages |
platform that leverages tradeoffs |
that leverages tradeoffs between |
leverages tradeoffs between weaker |
tradeoffs between weaker consistency |
a double spending attack |
double spending attack as |
spending attack as follows |
with a compensating gossip |
the partial target is |
a compensating gossip repair |
writes of objects chosen |
partial target is chosen |
compensating gossip repair mechanism |
of objects chosen uniformly |
target is chosen to |
objects chosen uniformly at |
is chosen to be |
for higher availability and |
chosen uniformly at random |
chosen to be large |
higher availability and simplicity |
uniformly at random from |
at random from a |
random from a small |
from a small set |
this is an old |
a small set of |
is an old idea |
such that partial solutions |
an old idea first |
that partial solutions arrive |
old idea first explored |
partial solutions arrive frequently |
idea first explored in |
solutions arrive frequently enough |
first explored in the |
explored in the grapevine |
arrive frequently enough for |
he intentionally generates two |
intentionally generates two conflicting |
generates two conflicting transactions |
frequently enough for the |
enough for the manager |
for the manager to |
the manager to accurately |
places one in a |
one in a block |
and later in systems |
later in systems like |
in systems like bayou |
in a block it |
a block it withholds |
manager to accurately estimate |
to accurately estimate the |
acidrain against two approaches |
accurately estimate the power |
and publishes the other |
publishes the other transaction |
estimate the power of |
which offer a broad |
more details in section |
the power of the |
offer a broad operational |
a broad operational spectrum |
power of the miner |
after the recipient sees |
broad operational spectrum between |
operational spectrum between strong |
the recipient sees the |
recipient sees the published |
sees the published transaction |
acid in the distributed |
smr tms is two |
in the distributed database |
the distributed database cases |
the attacker publishes the |
attacker publishes the withheld |
publishes the withheld block |
phase commit with reliable |
commit with reliable coordinators |
the withheld block to |
withheld block to revoke |
block to revoke the |
to revoke the former |
revoke the former transaction |
several database and distributed |
to reduce management overhead |
database and distributed systems |
and distributed systems take |
distributed systems take advantage |
systems take advantage of |
take advantage of the |
advantage of the same |
of the same tradeoff |
this attack is performed |
global log is an |
as the value of |
attack is performed by |
log is an architecture |
the value of bitcoin |
for example allowing multiple |
is performed by miners |
is an architecture where |
value of bitcoin rose |
example allowing multiple updates |
performed by miners or |
an architecture where tms |
allowing multiple updates to |
by miners or pools |
architecture where tms submit |
multiple updates to occur |
miners or pools against |
bitcoin mining has become |
where tms submit all |
updates to occur simultaneously |
or pools against service |
pools against service providers |
tms submit all transactions |
to occur simultaneously at |
mining has become a |
against service providers that |
service providers that accept |
providers that accept bitcoin |
has become a rapidly |
submit all transactions to |
occur simultaneously at distinct |
become a rapidly advancing |
all transactions to a |
simultaneously at distinct replicas |
and it not directly |
it not directly related |
not directly related to |
at distinct replicas by |
a rapidly advancing industry |
transactions to a single |
directly related to this |
related to this work |
to a single global |
distinct replicas by specifying |
a single global log |
replicas by specifying a |
technological advancements lead to |
single global log and |
by specifying a maximum |
advancements lead to ever |
global log and check |
specifying a maximum accepted |
block withholding defense most |
withholding defense most crypto |
log and check conflicts |
a maximum accepted deviation |
lead to ever more |
and check conflicts on |
maximum accepted deviation from |
to ever more efficient |
currencies use a proof |
check conflicts on that |
db access rate normed |
ever more efficient hashing |
accepted deviation from strong |
conflicts on that single |
on that single log |
deviation from strong consistency |
more efficient hashing asics |
work architecture similar to |
architecture similar to bitcoin |
where finding proof of |
finding proof of work |
proof of work is |
db access rate normed |
of work is the |
work is the result |
is the result of |
the result of solution |
result of solution guessing |
of solution guessing and |
solution guessing and checking |
all of the algorithms |
of the algorithms we |
the algorithms we are |
has lower latency for |
algorithms we are aware |
lower latency for a |
latency for a given |
for a given throughput |
we are aware of |
are aware of are |
aware of are susceptible |
of are susceptible to |
are susceptible to the |
susceptible to the block |
to the block withholding |
the block withholding attack |
tolerating a bounded number |
pc since its faster |
this is a simplification |
a bounded number of |
since its faster certification |
is a simplification that |
bounded number of consistency |
as in all of |
its faster certification reduces |
a simplification that is |
number of consistency violations |
in all of them |
faster certification reduces contention |
simplification that is sufficient |
of consistency violations to |
all of them the |
that is sufficient for |
consistency violations to increase |
of them the miner |
it has no bottleneck |
is sufficient for our |
violations to increase concurrency |
to increase concurrency of |
has no bottleneck as |
sufficient for our analysis |
them the miner can |
increase concurrency of transactions |
no bottleneck as with |
the miner can check |
bottleneck as with a |
as with a global |
with a global log |
the intricacies of reward |
miner can check whether |
intricacies of reward systems |
can check whether she |
of reward systems are |
that has less overhead |
or replication according to |
hit ratio hit ratio |
has less overhead in |
less overhead in small |
replication according to the |
according to the need |
check whether she found |
overhead in small scale |
reward systems are explained |
whether she found a |
systems are explained in |
she found a full |
found a full or |
a full or a |
full or a partial |
or a partial proof |
a partial proof of |
partial proof of work |
while the parameters we |
the parameters we choose |
parameters we choose are |
we choose are arbitrary |
prominent examples are litecoin |
our work on the |
the trends are robust |
work on the ssa |
on the ssa is |
the ssa is the |
ssa is the first |
is the first to |
the first to apply |
choosing other parameters would |
first to apply such |
other parameters would provide |
to apply such thinking |
parameters would provide similar |
apply such thinking to |
would provide similar trends |
such thinking to a |
thinking to a cluster |
to a cluster computing |
a cluster computing environment |
a notable exception is |
notable exception is p |
product a nity social |
a nity social network |
platform was designed to |
was designed to provide |
designed to provide a |
to provide a cluster |
provide a cluster based |
a cluster based environment |
cluster based environment for |
based environment for scalable |
environment for scalable internet |
for scalable internet services |
scalable internet services of |
internet services of the |
services of the sort |
of the sort used |
the sort used in |
sort used in web |
used in web servers |
it is possible to |
is possible to use |
possible to use an |
caching proxies and transformation |
proxies and transformation proxies |
to use an alternative |
use an alternative proof |
an alternative proof of |
which we discuss in |
service components are controlled |
alternative proof of work |
we discuss in section |
components are controlled by |
proof of work mechanism |
discuss in section ix |
are controlled by a |
of work mechanism in |
controlled by a front |
work mechanism in which |
by a front end |
mechanism in which miners |
a front end machine |
phase commit for transaction |
forks block propagation in |
in which miners would |
front end machine that |
commit for transaction certification |
block propagation in the |
which miners would not |
end machine that acts |
propagation in the overlay |
the downside of these |
machine that acts as |
miners would not be |
in the overlay network |
downside of these approaches |
that acts as a |
would not be able |
the overlay network takes |
of these approaches compared |
acts as a request |
not be able to |
overlay network takes seconds |
these approaches compared to |
as a request dispatcher |
be able to distinguish |
approaches compared to acid |
a request dispatcher and |
able to distinguish partial |
therefore it is possible |
request dispatcher and incorporates |
to distinguish partial from |
distinguish partial from full |
dispatcher and incorporates the |
rain is that they |
it is possible for |
partial from full proofs |
from full proofs of |
full proofs of work |
is possible for two |
and incorporates the load |
is that they require |
possible for two distant |
incorporates the load balancing |
that they require a |
for two distant miners |
the load balancing and |
they require a coordinator |
two distant miners to |
load balancing and restart |
require a coordinator that |
distant miners to generate |
balancing and restart logics |
a coordinator that performs |
miners to generate competing |
coordinator that performs transactions |
to generate competing blocks |
that performs transactions to |
performs transactions to be |
transactions to be highly |
to be highly available |
end processes are detected |
processes are detected to |
are detected to have |
detected to have failed |
both of which name |
of which name the |
this requires another consensus |
which name the same |
new processes are forked |
name the same block |
processes are forked to |
the same block as |
are forked to take |
in addition to the |
same block as their |
forked to take over |
addition to the one |
block as their predecessor |
to take over the |
to the one at |
take over the load |
the one at the |
one at the shard |
at the shard itself |
tacc workers can be |
workers can be composed |
can be composed to |
be composed to address |
composed to address more |
to address more complex |
address more complex tasks |
such a solution could |
tacc stands for transformation |
a solution could reduce |
are rare since the |
solution could reduce or |
rare since the average |
could reduce or remove |
since the average mining |
related work our transaction |
reduce or remove the |
or remove the danger |
work our transaction ordering |
the average mining interval |
ssa can be seen |
our transaction ordering protocol |
remove the danger of |
the danger of block |
danger of block withholding |
transaction ordering protocol is |
average mining interval is |
can be seen as |
ordering protocol is inspired |
be seen as revisiting |
protocol is inspired by |
seen as revisiting these |
is inspired by a |
making such a change |
as revisiting these architectural |
inspired by a state |
such a change may |
revisiting these architectural ideas |
a change may not |
these architectural ideas in |
machine ordering mechanism suggested |
change may not be |
architectural ideas in conjunction |
ordering mechanism suggested by |
mechanism suggested by lamport |
product a nity social |
ideas in conjunction with |
and they occur on |
may not be in |
a nity social network |
in conjunction with chain |
conjunction with chain replication |
not be in the |
be in the interest |
in the interest of |
the interest of the |
interest of the community |
they occur on average |
occur on average once |
on average once every |
or even its potential |
but we have generalized |
have long supported clustered |
we have generalized the |
long supported clustered architectures |
could lead to a |
lead to a reduction |
to a reduction of |
a reduction of pool |
reduction of pool sizes |
have generalized the protocol |
and were the first |
generalized the protocol to |
were the first systems |
as explained in section |
explained in section ix |
the first systems to |
the protocol to work |
first systems to exploit |
protocol to work with |
systems to exploit the |
to work with arbitrary |
to exploit the style |
work with arbitrary overlapping |
exploit the style of |
with arbitrary overlapping par |
the style of partitioning |
style of partitioning that |
of partitioning that leads |
decentralized pools although most |
partitioning that leads to |
pools although most pools |
references the approaches of |
that leads to a |
leads to a raps |
the approaches of mdcc |
although most pools use |
most pools use a |
to a raps of |
the system has a |
pools use a centralized |
use a centralized manager |
system has a mechanism |
a raps of racs |
raps of racs solution |
has a mechanism to |
a prominent exception is |
prominent exception is p |
a mechanism to solve |
mechanism to solve forks |
to solve forks when |
solve forks when they |
pool a distributed pool |
forks when they do |
most database systems adhere |
when they do occur |
a distributed pool architecture |
database systems adhere closely |
systems adhere closely to |
adhere closely to the |
closely to the acid |
to the acid model |
distributed pool architecture with |
causing one of the |
pool architecture with no |
architecture with no central |
at potentially high cost |
one of the blocks |
with no central manager |
potentially high cost in |
are close to acid |
of the blocks to |
high cost in terms |
the blocks to be |
cost in terms of |
rain s certification mechanism |
blocks to be discarded |
in terms of reduced |
terms of reduced availability |
of reduced availability during |
reduced availability during faults |
we ignore bifurcations for |
ignore bifurcations for the |
bifurcations for the sake |
for the sake of |
the sake of simplicity |
but the question of |
discuss this problem in |
the question of whether |
rain separates the om |
question of whether a |
separates the om abstraction |
since the choice of |
of whether a pool |
the om abstraction from |
the choice of the |
whether a pool is |
om abstraction from the |
choice of the discarded |
a pool is run |
abstraction from the highly |
of the discarded block |
ultimately arguing for precisely |
pool is run by |
the discarded block on |
arguing for precisely the |
is run by a |
discarded block on bifurcation |
for precisely the weak |
run by a centralized |
block on bifurcation is |
precisely the weak update |
by a centralized manager |
on bifurcation is random |
leasing mechanism and fast |
the weak update model |
weak update model that |
mechanism and fast recovery |
a centralized manager or |
update model that we |
model that we adopted |
that we adopted here |
one may incorporate this |
centralized manager or with |
we also address garbage |
may incorporate this event |
manager or with a |
application servers like the |
servers like the j |
incorporate this event into |
or with a decentralized |
also address garbage collection |
this event into the |
with a decentralized architecture |
event into the probability |
a decentralized architecture is |
into the probability of |
which cannot be done |
limited cache entry ttl |
cache entry ttl fig |
the probability of finding |
cannot be done independently |
decentralized architecture is almost |
probability of finding a |
be done independently at |
offer persistent state support |
architecture is almost immaterial |
of finding a block |
done independently at the |
persistent state support by |
experiments with workloads based |
independently at the logs |
is almost immaterial for |
state support by wrapping |
with workloads based on |
almost immaterial for the |
support by wrapping soft |
and consider instead the |
workloads based on a |
immaterial for the attack |
for the attack we |
the attack we describe |
based on a web |
by wrapping soft state |
consider instead the probability |
on a web retailer |
wrapping soft state business |
instead the probability of |
a web retailer product |
soft state business logic |
the probability of finding |
web retailer product affinity |
pool group can be |
state business logic components |
probability of finding a |
retailer product affinity topology |
group can be infiltrated |
can be infiltrated and |
be infiltrated and attacked |
product affinity topology and |
business logic components on |
of finding a block |
affinity topology and a |
logic components on top |
finding a block that |
topology and a social |
components on top of |
a block that is |
and a social network |
pool code can be |
on top of a |
block that is not |
a social network topology |
code can be changed |
top of a relational |
of a relational or |
social network topology illustrated |
network topology illustrated in |
that is not discarded |
a relational or object |
can be changed to |
be changed to support |
changed to support attacks |
to support attacks against |
support attacks against other |
attacks against other pools |
pools often charge a |
often charge a small |
they also target large |
charge a small percentage |
on the other hand |
a small percentage of |
scale highly available services |
small percentage of the |
percentage of the revenue |
of the revenue as |
the revenue as fee |
and hence we believe |
hence we believe they |
we believe they could |
believe they could benefit |
they could benefit from |
could benefit from ssa |
pool can be used |
we discuss in section |
compared against the alternative |
a new paradigm for |
can be used by |
discuss in section ix |
against the alternative of |
new paradigm for building |
be used by groups |
in a similar vein |
in section ix the |
the alternative of reducing |
paradigm for building scalable |
used by groups of |
by groups of miners |
alternative of reducing cache |
for building scalable distributed |
section ix the implications |
groups of miners to |
of reducing cache entry |
building scalable distributed systems |
ix the implications of |
of miners to easily |
miners to easily form |
to easily form closed |
easily form closed pools |
reducing cache entry time |
framework makes it easy |
the implications of such |
makes it easy to |
these do not accept |
do not accept untrusted |
not accept untrusted miners |
implications of such fees |
it easy to create |
of such fees to |
easy to create robust |
such fees to our |
fees to our analysis |
to create robust scalable |
create robust scalable services |
and are therefore protected |
are therefore protected against |
therefore protected against block |
protected against block withholding |
ninja is arguably more |
data points are medians |
many pools are open |
is arguably more flexible |
points are medians and |
pools are open and |
arguably more flexible than |
are medians and error |
are open and accept |
c onclusion we explored |
more flexible than application |
medians and error bars |
open and accept any |
onclusion we explored a |
flexible than application servers |
and error bars bound |
and accept any interested |
accept any interested miner |
uses an architecture similar |
error bars bound the |
we explored a block |
than application servers in |
an architecture similar to |
architecture similar to our |
a pool interface is |
explored a block withholding |
application servers in that |
similar to our certification |
pool interface is typically |
a block withholding attack |
servers in that it |
to our certification mechanism |
interface is typically comprised |
block withholding attack among |
in that it performs |
is typically comprised of |
withholding attack among bitcoin |
that it performs connection |
but addresses minitransactions that |
this could work well |
attack among bitcoin mining |
it performs connection management |
typically comprised of a |
addresses minitransactions that are |
could work well if |
among bitcoin mining pools |
performs connection management and |
comprised of a web |
minitransactions that are submitted |
work well if a |
bitcoin mining pools an |
connection management and automatically |
of a web interface |
that are submitted as |
well if a system |
if a system has |
management and automatically partitions |
a web interface for |
are submitted as a |
submitted as a whole |
a system has multiple |
and automatically partitions and |
web interface for registration |
mining pools an attack |
with no attempt to |
automatically partitions and replicates |
interface for registration and |
system has multiple classes |
has multiple classes of |
multiple classes of objects |
partitions and replicates persistent |
and replicates persistent state |
pools an attack that |
no attempt to order |
for registration and a |
all clustered but with |
but the framework takes |
attempt to order potentially |
registration and a miner |
clustered but with different |
but with different associated |
the framework takes a |
to order potentially conflicting |
and a miner interface |
a miner interface for |
with different associated clustering |
framework takes a different |
order potentially conflicting transactions |
an attack that is |
miner interface for the |
different associated clustering properties |
takes a different tiered |
attack that is possible |
interface for the mining |
we address full transactions |
a different tiered approach |
that is possible in |
for the mining software |
different tiered approach to |
tiered approach to services |
where the clients sequentially |
is possible in any |
approach to services based |
the clients sequentially access |
possible in any similar |
to services based on |
services based on bases |
clients sequentially access objects |
in any similar system |
in order to mine |
active proxies and units |
any similar system that |
sequentially access objects before |
order to mine for |
and represents shared state |
access objects before ending |
to mine for a |
mine for a pool |
represents shared state by |
objects before ending a |
before ending a transaction |
shared state by means |
state by means of |
by means of distributed |
means of distributed data |
consistent inconsistent aborted ab |
and use prediction to |
a miner registers with |
of distributed data structures |
similar system that rewards |
inconsistent aborted ab ev |
use prediction to order |
miner registers with the |
registers with the web |
aborted ab ev re |
prediction to order them |
conclusion our paper presents |
with the web interface |
ab ev re ab |
to order them in |
order them in advance |
our paper presents the |
ev re ab ev |
system that rewards for |
paper presents the scalable |
re ab ev re |
we believe our techniques |
supplies a bitcoin address |
presents the scalable services |
the scalable services architecture |
ab ev re i |
believe our techniques could |
a bitcoin address to |
that rewards for proof |
rewards for proof of |
for proof of work |
bitcoin address to receive |
a new platform for |
ev re i i |
our techniques could be |
address to receive its |
such systems are gaining |
re i i tr |
techniques could be used |
new platform for porting |
to receive its future |
systems are gaining popularity |
i i tr tr |
could be used to |
platform for porting a |
receive its future shares |
i tr tr o |
be used to reduce |
for porting a large |
its future shares of |
future shares of the |
tr tr o o |
used to reduce abort |
porting a large class |
a large class of |
large class of service |
tr o o rt |
to reduce abort rates |
running most digital currencies |
most digital currencies and |
o o rt ct |
reduce abort rates of |
shares of the revenue |
oriented applications onto clusters |
digital currencies and related |
o rt ct rt |
abort rates of systems |
currencies and related services |
rt ct rt ct |
rates of systems using |
the ssa was designed |
and receives from the |
we observe that no |
of systems using sinfonia |
ssa was designed to |
ct rt ct y |
receives from the pool |
systems using sinfonia or |
was designed to be |
rt ct y y |
ct y y amazon |
y y amazon orkut |
attacks is not a |
is not a nash |
not a nash equilibrium |
y amazon orkut fig |
designed to be as |
from the pool credentials |
using sinfonia or a |
if none of the |
none of the other |
of the other pools |
the other pools attack |
the pool credentials for |
to be as simple |
the efficacy of t |
pool credentials for mining |
a pool can increase |
be as simple as |
cache as a function |
pool can increase its |
as simple as possible |
sinfonia or a similar |
then he feeds his |
can increase its revenue |
as a function of |
or a similar certification |
he feeds his credentials |
increase its revenue by |
and at the core |
a function of the |
a similar certification mechanism |
feeds his credentials and |
its revenue by attacking |
at the core uses |
function of the inconsistency |
his credentials and the |
revenue by attacking the |
by attacking the others |
of the inconsistency handling |
credentials and the pool |
the core uses just |
the inconsistency handling strategy |
and the pool s |
core uses just two |
inconsistency handling strategy for |
handling strategy for realistic |
strategy for realistic workloads |
uses just two primitive |
just two primitive mechanisms |
the pool s address |
when two pools can |
two pools can attack |
pools can attack each |
can attack each other |
tcp chains that support |
pool s address to |
chains that support a |
much work has been |
s address to its |
that support a variant |
work has been done |
they face a version |
face a version of |
support a variant of |
has been done on |
address to its mining |
to its mining rig |
a variant of chain |
been done on creating |
done on creating consistent |
variant of chain replication |
a version of the |
on creating consistent caches |
creating consistent caches for |
consistent caches for web |
caches for web servers |
and gossip epidemics which |
version of the prisoner |
of the prisoner s |
the prisoner s dilemma |
gossip epidemics which are |
the mining rig obtains |
epidemics which are used |
mining rig obtains its |
if one pool chooses |
rig obtains its tasks |
one pool chooses to |
pool chooses to attack |
obtains its tasks from |
which are used to |
its tasks from the |
are used to manage |
the victim s revenue |
victim s revenue is |
s revenue is reduced |
tasks from the pool |
used to manage configuration |
from the pool and |
to manage configuration data |
and it can retaliate |
the pool and sends |
manage configuration data and |
configuration data and initiate |
pool and sends partial |
it can retaliate by |
data and initiate repair |
and sends partial and |
can retaliate by attacking |
and initiate repair after |
initiate repair after failures |
retaliate by attacking and |
by attacking and increase |
attacking and increase its |
and increase its revenue |
sends partial and full |
with appropriate parameter settings |
partial and full proof |
and full proof of |
full proof of work |
given a gossip rate |
a gossip rate that |
typically with the stratum |
gossip rate that is |
at nash equilibrium both |
with the stratum protocol |
rate that is sufficiently |
nash equilibrium both earn |
that is sufficiently fast |
equilibrium both earn less |
is sufficiently fast relative |
both earn less than |
sufficiently fast relative to |
earn less than they |
fast relative to the |
less than they would |
than they would have |
relative to the update |
to the update rates |
the update rates seen |
update rates seen in |
rates seen in the |
seen in the cluster |
they would have if |
would have if neither |
have if neither attacked |
with multiple pools of |
we find that the |
as it finds blocks |
multiple pools of equal |
highly available storage for |
find that the ssa |
pools of equal size |
available storage for interactive |
the pool manager credits |
of equal size a |
that the ssa can |
storage for interactive services |
the ssa can rapidly |
equal size a similar |
pool manager credits the |
ssa can rapidly and |
size a similar situation |
manager credits the miner |
can rapidly and automatically |
a similar situation arises |
credits the miner s |
rapidly and automatically reconfigure |
similar situation arises with |
situation arises with a |
and automatically reconfigure itself |
the miner s account |
arises with a symmetric |
with a symmetric equilibrium |
miner s account according |
automatically reconfigure itself after |
s account according to |
reconfigure itself after a |
account according to its |
the fact that block |
itself after a failure |
according to its share |
to its share of |
after a failure and |
fact that block withholding |
its share of the |
and higher level objects |
that block withholding is |
a failure and can |
share of the work |
block withholding is not |
failure and can rapidly |
withholding is not common |
and can rapidly repair |
is not common may |
and transfers these funds |
can rapidly repair data |
not common may be |
transfers these funds either |
rapidly repair data inconsistencies |
common may be explained |
these funds either on |
repair data inconsistencies that |
may be explained by |
funds either on request |
data inconsistencies that arise |
be explained by modeling |
either on request or |
inconsistencies that arise during |
explained by modeling the |
on request or automatically |
such systems consider only |
that arise during the |
by modeling the attack |
request or automatically to |
systems consider only one |
arise during the period |
modeling the attack decisions |
or automatically to the |
consider only one object |
during the period when |
the attack decisions as |
automatically to the aforementioned |
only one object at |
the period when the |
attack decisions as an |
to the aforementioned bitcoin |
one object at a |
object at a time |
period when the cluster |
when the cluster configuration |
the aforementioned bitcoin address |
a transactional record manager |
transactional record manager for |
the cluster configuration was |
and only individual read |
only individual read and |
record manager for shared |
cluster configuration was still |
too big pools despite |
individual read and write |
read and write operations |
configuration was still disrupted |
decisions as an iterative |
as an iterative prisoner |
as they do not |
big pools despite their |
manager for shared flash |
an iterative prisoner s |
they do not support |
do not support a |
our goal is to |
iterative prisoner s dilemma |
pools despite their important |
not support a transactional |
goal is to make |
despite their important role |
support a transactional interface |
is to make the |
their important role of |
to make the software |
important role of enabling |
we argue that the |
make the software available |
there are few if |
are few if any |
few if any multi |
the software available to |
role of enabling small |
argue that the situation |
software available to a |
that the situation is |
available to a general |
to a general user |
a general user community |
general user community in |
these systems generally try |
the situation is unstable |
systems generally try to |
situation is unstable since |
generally try to avoid |
pools can constitute a |
is unstable since the |
try to avoid staleness |
can constitute a threat |
unstable since the attack |
to avoid staleness through |
constitute a threat to |
since the attack can |
avoid staleness through techniques |
a threat to the |
the attack can be |
attack can be done |
can be done anonymously |
staleness through techniques such |
through techniques such as |
techniques such as time |
threat to the bitcoin |
acknowledgments the authors are |
to the bitcoin system |
the authors are grateful |
the bitcoin system if |
authors are grateful to |
bitcoin system if their |
one pool may decide |
are grateful to the |
system if their size |
pool may decide to |
grateful to the research |
if their size is |
their size is too |
to the research team |
the research team at |
size is too large |
may decide to increase |
research team at afrl |
our work considers multi |
decide to increase its |
a middleware for highperformance |
if one pool controls |
to increase its revenue |
team at afrl in |
at afrl in rome |
object transactional consistency of |
transactional consistency of cache |
consistency of cache access |
middleware for highperformance transaction |
one pool controls the |
increase its revenue and |
for highperformance transaction processing |
pool controls the majority |
its revenue and drag |
for their help in |
controls the majority of |
the majority of mining |
early work on scalable |
their help in understanding |
revenue and drag the |
majority of mining power |
work on scalable database |
help in understanding the |
and drag the others |
on scalable database caching |
in understanding the challenges |
drag the others to |
scalable database caching mostly |
understanding the challenges of |
the system becomes unstable |
the others to attack |
others to attack as |
to attack as well |
database caching mostly ignored |
the challenges of using |
caching mostly ignored transactional |
challenges of using service |
mostly ignored transactional consistency |
of using service oriented |
ending with a reduced |
with a reduced revenue |
a reduced revenue for |
reduced revenue for all |
using service oriented architectures |
service oriented architectures in |
oriented architectures in large |
architectures in large scale |
in large scale settings |
the inferior revenue would |
inferior revenue would push |
revenue would push miners |
would push miners to |
push miners to join |
miners to join private |
to join private pools |
and to the researchers |
to the researchers at |
work has been done |
the researchers at amazon |
has been done on |
which can verify that |
been done on creating |
done on creating consistent |
on creating consistent caches |
creating consistent caches for |
consistent caches for databases |
can verify that their |
verify that their registered |
that their registered miners |
their registered miners do |
registered miners do not |
miners do not withhold |
do not withhold blocks |
for helping us understand |
helping us understand the |
us understand the architectures |
understand the architectures employed |
the architectures employed in |
this would lead to |
would lead to smaller |
lead to smaller pools |
architectures employed in very |
extends a centralized database |
employed in very large |
a centralized database with |
in very large data |
and so ultimately to |
centralized database with support |
very large data centers |
boosting dbms performance by |
so ultimately to a |
database with support for |
dbms performance by parallelising |
performance by parallelising write |
with support for caches |
support for caches that |
by parallelising write transactions |
ultimately to a better |
for caches that provide |
caches that provide snapshot |
that provide snapshot isolation |
provide snapshot isolation semantics |
to a better environment |
a better environment for |
better environment for bitcoin |
albeit the snapshots seen |
the snapshots seen may |
snapshots seen may be |
seen may be stale |
warns that the system |
environment for bitcoin as |
for bitcoin as a |
bitcoin as a whole |
to improve the commit |
improve the commit rate |
the commit rate for |
commit rate for read |
that the system is |
the system is unstable |
system is unstable with |
is unstable with even |
unstable with even smaller |
for their valuable advice |
with even smaller pools |
where the cache holds |
the author is grateful |
author is grateful to |
is grateful to ken |
grateful to ken birman |
the cache holds several |
prediction of transaction behavior |
cache holds several versions |
of transaction behavior has |
holds several versions of |
transaction behavior has the |
several versions of an |
behavior has the potential |
versions of an object |
has the potential to |
in realistic scenarios of |
of an object and |
realistic scenarios of the |
the potential to significantly |
emin gu n sirer |
an object and enables |
an exercise in distributed |
exercise in distributed computing |
potential to significantly decrease |
object and enables the |
scenarios of the bitcoin |
to significantly decrease abort |
and enables the cache |
of the bitcoin system |
communications of the acm |
significantly decrease abort rates |
enables the cache to |
and the paper shepherd |
the paper shepherd joseph |
paper shepherd joseph bonneau |
the cache to choose |
the bitcoin system no |
decrease abort rates in |
cache to choose a |
bitcoin system no pool |
abort rates in large |
to choose a version |
system no pool controls |
rates in large scale |
choose a version that |
no pool controls a |
in large scale transactional |
a version that allows |
pool controls a majority |
large scale transactional systems |
version that allows a |
controls a majority of |
scale transactional systems with |
that allows a transaction |
allows a transaction to |
a transaction to commit |
a majority of the |
transactional systems with high |
majority of the mining |
systems with high contention |
of the mining power |
this technique could also |
technique could also be |
could also be used |
also be used with |
be used with our |
used with our solution |
rain we employ prediction |
peer electronic cash system |
we employ prediction to |
for one day in |
employ prediction to obtain |
one day in june |
prediction to obtain soft |
to obtain soft reservations |
obtain soft reservations and |
soft reservations and implement |
reservations and implement atomic |
and implement atomic transactions |
implement atomic transactions while |
atomic transactions while requiring |
transactions while requiring high |
while requiring high availability |
requiring high availability only |
high availability only in |
availability only in a |
only in a single |
in a single tier |
a single tier of |
single tier of independent |
tier of independent logs |
a single pool called |
also support snapshot isolation |
this allows for low |
single pool called ghash |
but can be used |
can be used with |
be used with any |
used with any backend |
with any backend database |
including ones that are |
ones that are sharded |
that are sharded and |
rain s operations never |
s operations never depend |
operations never depend on |
ebay s paypal unit |
never depend on a |
s paypal unit to |
paypal unit to start |
depend on a single |
of the blocks in |
unit to start accepting |
on a single machine |
the blocks in the |
provides a transactionally consistent |
a transactionally consistent cache |
a single machine by |
blocks in the bitcoin |
to start accepting bitcoin |
start accepting bitcoin payments |
single machine by allowing |
in the bitcoin main |
transactionally consistent cache for |
machine by allowing fast |
the bitcoin main chain |
an architecture to support |
consistent cache for the |
by allowing fast recovery |
architecture to support scalable |
cache for the jboss |
for the jboss middleware |
to support scalable online |
the bitcoin community backlashed |
allowing fast recovery from |
support scalable online personalization |
bitcoin community backlashed at |
fast recovery from failures |
scalable online personalization in |
community backlashed at the |
recovery from failures and |
online personalization in the |
backlashed at the pool |
from failures and performance |
personalization in the web |
failures and performance hiccups |
support transactions on cached |
transactions on cached enterprise |
on cached enterprise javabeans |
which has done nothing |
the international journal on |
has done nothing worse |
international journal on very |
done nothing worse than |
journal on very large |
nothing worse than being |
on very large data |
very large data bases |
worse than being extremely |
than being extremely successful |
allows update transactions to |
update transactions to read |
transactions to read stale |
to read stale data |
read stale data out |
stale data out of |
data out of caches |
out of caches and |
google adds bitcoin currency |
of caches and provide |
caches and provide bounds |
and provide bounds on |
provide bounds on how |
bounds on how much |
on how much staleness |
how much staleness is |
much staleness is allowed |
adds bitcoin currency conversion |
bitcoin currency conversion to |
currency conversion to search |
these techniques require fast |
io reduced its relative |
techniques require fast communication |
reduced its relative mining |
require fast communication between |
its relative mining power |
fast communication between the |
relative mining power and |
communication between the cache |
mining power and publicly |
between the cache and |
power and publicly committed |
the cache and the |
and publicly committed to |
cache and the database |
publicly committed to stay |
and the database for |
committed to stay away |
the database for good |
to stay away from |
database for good performance |
stay away from the |
benchmarking cloud serving systems |
cloud serving systems with |
in our work caches |
serving systems with ycsb |
our work caches are |
work caches are asynchronously |
caches are asynchronously updated |
which is how caches |
is how caches currently |
how caches currently work |
caches currently work in |
currently work in large |
work in large multi |
block withholding and its |
withholding and its detection |
and its detection classical |
its detection classical block |
f uture d irections |
detection classical block withholding |
uture d irections the |
d irections the dependency |
irections the dependency list |
the dependency list sizes |
dependency list sizes for |
list sizes for all |
sizes for all objects |
for all objects in |
all objects in t |
cache are currently all |
are currently all of |
currently all of the |
all of the same |
of the same maximum |
the same maximum length |
this may not be |
may not be optimal |
is an attack performed |
if the workload accesses |
an attack performed by |
the workload accesses objects |
attack performed by a |
workload accesses objects in |
performed by a pool |
accesses objects in clusters |
by a pool member |
objects in clusters of |
in clusters of different |
a pool member against |
clusters of different sizes |
pool member against the |
member against the other |
against the other pool |
objects of larger clusters |
the other pool members |
of larger clusters call |
larger clusters call for |
clusters call for longer |
call for longer dependency |
for longer dependency lists |
once appropriate real workloads |
the attacking miner registers |
appropriate real workloads are |
real workloads are available |
attacking miner registers with |
miner registers with the |
it may be possible |
registers with the pool |
may be possible to |
with the pool and |
be possible to improve |
the pool and apparently |
possible to improve performance |
pool and apparently starts |
to improve performance by |
and apparently starts mining |
we plan to build |
improve performance by dynamically |
apparently starts mining honestly |
plan to build on |
performance by dynamically changing |
starts mining honestly it |
to build on our |
by dynamically changing per |
epidemic algorithms for replicated |
algorithms for replicated database |
build on our simulation |
object dependency list sizes |
for replicated database maintenance |
mining honestly it regularly |
on our simulation results |
balancing between objects to |
our simulation results by |
honestly it regularly sends |
between objects to maintain |
simulation results by implementing |
in proceedings of the |
it regularly sends the |
objects to maintain the |
results by implementing acid |
proceedings of the sixth |
regularly sends the pool |
to maintain the same |
of the sixth annual |
sends the pool partial |
maintain the same overall |
rain and exploring the |
the sixth annual acm |
the pool partial proof |
the same overall space |
and exploring the different |
sixth annual acm symposium |
pool partial proof of |
same overall space overhead |
exploring the different aspects |
annual acm symposium on |
partial proof of work |
the different aspects of |
acm symposium on principles |
another option is to |
different aspects of its |
symposium on principles of |
option is to explore |
aspects of its performance |
on principles of distributed |
is to explore an |
of its performance in |
principles of distributed computing |
to explore an approach |
its performance in realistic |
the attacking miner sends |
explore an approach in |
performance in realistic settings |
attacking miner sends only |
an approach in which |
miner sends only partial |
approach in which each |
sends only partial proof |
of particular interest are |
in which each type |
only partial proof of |
which each type of |
partial proof of work |
each type of object |
type of object would |
of object would have |
object would have its |
would have its own |
have its own dependency |
its own dependency list |
own dependency list bound |
different network topologies with |
if it finds a |
network topologies with a |
it finds a full |
topologies with a single |
finds a full solution |
with a single datacenter |
a full solution that |
a single datacenter and |
full solution that constitutes |
single datacenter and with |
agnostic and treats all |
solution that constitutes a |
datacenter and with multiple |
and treats all objects |
that constitutes a full |
and with multiple datacenters |
treats all objects and |
constitutes a full proof |
all objects and object |
a full proof of |
objects and object relations |
and object relations as |
object relations as equal |
full proof of work |
proof of work it |
of work it discards |
work it discards the |
using an lru policy |
it discards the solution |
an lru policy to |
behavior in face of |
lru policy to trim |
in face of high |
policy to trim the |
face of high contention |
to trim the list |
trim the list of |
the list of dependencies |
reducing the pool s |
the pool s total |
pool s total revenue |
rain should prove efficient |
there may be cases |
may be cases in |
be cases in which |
cases in which the |
in which the application |
which the application could |
the application could explicitly |
application could explicitly inform |
where its overhead may |
could explicitly inform the |
its overhead may be |
explicitly inform the cache |
this attack is illustrated |
overhead may be wasteful |
inform the cache of |
attack is illustrated in |
the cache of relevant |
is illustrated in figure |
cache of relevant object |
of relevant object dependencies |
and those could then |
those could then be |
could then be treated |
then be treated as |
be treated as more |
treated as more important |
as more important and |
more important and retained |
the attacker does not |
while other less important |
behavior in error prone |
in error prone scenarios |
other less important ones |
attacker does not change |
less important ones are |
does not change the |
important ones are managed |
not change the pool |
ones are managed by |
change the pool s |
are managed by some |
support for data sharing |
the pool s effective |
managed by some other |
for data sharing among |
pool s effective mining |
by some other policy |
performance with predictors of |
data sharing among mobile |
s effective mining power |
some other policy such |
with predictors of different |
sharing among mobile users |
other policy such as |
policy such as lru |
predictors of different qualities |
and does not affect |
in ieee workshop on |
does not affect directly |
ieee workshop on mobile |
not affect directly the |
workshop on mobile computing |
in a web album |
affect directly the revenue |
on mobile computing systems |
a web album the |
directly the revenue of |
web album the set |
the revenue of other |
album the set of |
revenue of other pools |
the set of pictures |
set of pictures and |
of pictures and their |
pictures and their acl |
and their acl is |
repurposing bitcoin work for |
their acl is an |
bitcoin work for data |
work for data preservation |
acl is an important |
is an important dependency |
the attacked pool shares |
an important dependency whereas |
attacked pool shares its |
in proceedings of the |
important dependency whereas occasional |
pool shares its revenue |
proceedings of the ieee |
dependency whereas occasional tagging |
shares its revenue with |
its revenue with the |
whereas occasional tagging operations |
occasional tagging operations that |
revenue with the attacker |
of the ieee symposium |
tagging operations that relate |
operations that relate pictures |
the ieee symposium on |
ieee symposium on security |
symposium on security and |
on security and privacy |
that relate pictures to |
therefore each miner earns |
lightweight elasticity in shared |
relate pictures to users |
each miner earns less |
elasticity in shared storage |
pictures to users may |
in shared storage databases |
to users may be |
shared storage databases for |
users may be less |
may be less important |
storage databases for the |
as the same revenue |
databases for the cloud |
the same revenue is |
for the cloud using |
it may be straightforward |
same revenue is distributed |
the cloud using live |
may be straightforward to |
revenue is distributed among |
cloud using live data |
be straightforward to extend |
is distributed among more |
using live data migration |
straightforward to extend the |
distributed among more miners |
to extend the cache |
extend the cache api |
the cache api to |
cache api to allow |
api to allow the |
to allow the application |
allow the application to |
the application to specify |
application to specify such |
recall that the proof |
to specify such dependencies |
specify such dependencies and |
that the proof of |
such dependencies and to |
dependencies and to modify |
and to modify t |
the proof of work |
proof of work is |
of work is only |
cache to respect them |
work is only valid |
namecoin dns dotbit project |
is only valid for |
only valid for a |
c onclusion existing large |
valid for a specific |
for a specific block |
scale computing frameworks make |
computing frameworks make heavy |
frameworks make heavy use |
make heavy use of |
heavy use of edge |
use of edge caches |
of edge caches to |
edge caches to reduce |
as it is the |
caches to reduce client |
it is the nonce |
to reduce client latency |
is the nonce with |
the nonce with which |
nonce with which the |
but this form of |
with which the block |
this form of caching |
which the block s |
form of caching has |
the block s hash |
of caching has not |
block s hash is |
caching has not been |
s hash is smaller |
has not been available |
hash is smaller than |
not been available for |
is smaller than its |
been available for transactional |
smaller than its target |
available for transactional applications |
we believe this is |
believe this is one |
the attacking miner cannot |
this is one reason |
attacking miner cannot use |
is one reason that |
miner cannot use it |
one reason that transactions |
reason that transactions are |
that transactions are generally |
transactions are generally not |
are generally not considered |
generally not considered to |
not considered to be |
considered to be a |
to be a viable |
live migration in shared |
be a viable option |
migration in shared nothing |
a viable option in |
in shared nothing databases |
viable option in extremely |
option in extremely large |
shared nothing databases for |
although the term block |
in extremely large systems |
nothing databases for elastic |
the term block withholding |
databases for elastic cloud |
term block withholding has |
for elastic cloud platforms |
block withholding has become |
withholding has become canonical |
a variant of serializability |
variant of serializability that |
of serializability that is |
serializability that is suitable |
that is suitable for |
is suitable for incoherent |
suitable for incoherent caches |
note that the block |
a next generation smart |
next generation smart contract |
that the block is |
which cannot communicate with |
the block is discarded |
cannot communicate with the |
block is discarded and |
communicate with the backend |
is discarded and never |
with the backend database |
discarded and never introduced |
the backend database on |
and never introduced into |
backend database on every |
database on every read |
on every read access |
never introduced into the |
introduced into the system |
into the system as |
the system as the |
we then presented t |
system as the name |
as the name block |
the name block withholding |
name block withholding implies |
an architecture for controlling |
architecture for controlling transaction |
for controlling transaction consistency |
controlling transaction consistency with |
transaction consistency with caches |
the system extends the |
system extends the edge |
extends the edge cache |
the edge cache by |
miners miners miners pool |
edge cache by allowing |
based scalable network services |
cache by allowing it |
by allowing it to |
allowing it to offer |
it to offer a |
to offer a transactional |
offer a transactional interface |
we believe that t |
cache is the first |
is the first transaction |
classical block withholding attack |
proceedings of the sixteenth |
aware caching architecture in |
of the sixteenth acm |
caching architecture in which |
the sixteenth acm symposium |
architecture in which caches |
sixteenth acm symposium on |
a group of miners |
in which caches are |
acm symposium on operating |
group of miners attack |
which caches are updated |
symposium on operating systems |
of miners attack pool |
caches are updated asynchronously |
on operating systems principles |
with a block withholding |
a block withholding attack |
a lookup request only |
lookup request only requires |
request only requires a |
only requires a round |
denoted by a dashed |
trip to the database |
by a dashed red |
to the database in |
a dashed red arrow |
the database in case |
database in case there |
in case there is |
case there is a |
there is a cache |
analysis of bitcoin pooled |
of bitcoin pooled mining |
bitcoin pooled mining reward |
pooled mining reward systems |
is a cache miss |
a cache miss there |
cache miss there is |
this attack reduces the |
miss there is no |
attack reduces the attacker |
there is no additional |
reduces the attacker s |
is no additional traffic |
the attacker s revenue |
no additional traffic and |
attacker s revenue compared |
additional traffic and delays |
s revenue compared to |
traffic and delays to |
revenue compared to solo |
and delays to ensure |
delays to ensure cache |
compared to solo mining |
to solo mining or |
to ensure cache coherence |
solo mining or honest |
mining or honest pool |
or honest pool participation |
cache associates dependency information |
associates dependency information with |
dependency information with cached |
information with cached database |
with cached database objects |
it suffers from the |
suffers from the reduced |
from the reduced revenue |
while leaving the interaction |
the reduced revenue like |
leaving the interaction between |
reduced revenue like the |
the interaction between the |
revenue like the other |
interaction between the backend |
like the other pool |
the dangers of replication |
between the backend systems |
the other pool participants |
dangers of replication and |
the backend systems and |
of replication and d |
backend systems and the |
systems and the cache |
and the cache otherwise |
the cache otherwise unchanged |
and its revenue is |
its revenue is less |
this information includes version |
revenue is less than |
information includes version identifiers |
is less than its |
fast distributed transactions a |
includes version identifiers and |
version identifiers and bounded |
distributed transactions a solution |
less than its share |
than its share of |
its share of the |
share of the total |
of the total mining |
with this modest amount |
this modest amount of |
the total mining power |
modest amount of additional |
total mining power in |
amount of additional information |
mining power in the |
power in the system |
we show that inconsistency |
show that inconsistency can |
that inconsistency can be |
inconsistency can be greatly |
can be greatly reduced |
be greatly reduced or |
greatly reduced or even |
the classical block withholding |
reduced or even completely |
classical block withholding attack |
or even completely eliminated |
block withholding attack can |
even completely eliminated in |
completely eliminated in some |
eliminated in some cases |
for partitioned database systems |
withholding attack can therefore |
attack can therefore only |
can therefore only be |
therefore only be used |
only be used for |
cache is intended for |
is intended for clustered |
intended for clustered workloads |
be used for sabotage |
and those arise naturally |
those arise naturally in |
arise naturally in social |
naturally in social networks |
at a cost to |
a cost to the |
cost to the attacker |
research perspectives on bitcoin |
perspectives on bitcoin and |
on bitcoin and secondgeneration |
bitcoin and secondgeneration cryptocurrencies |
mobile applications with spatial |
applications with spatial locality |
in ieee symposium on |
ieee symposium on security |
symposium on security and |
on security and privacy |
our experiments demonstrate t |
even if a pool |
if a pool detects |
a pool detects that |
cache to be effective |
pool detects that it |
to be effective in |
detects that it is |
be effective in realistic |
that it is under |
effective in realistic workloads |
it is under a |
in realistic workloads based |
is under a block |
realistic workloads based on |
under a block withholding |
workloads based on datasets |
a block withholding attack |
based on datasets from |
on datasets from amazon |
datasets from amazon and |
from amazon and orkut |
it might not be |
using dependency lists of |
might not be able |
dependency lists of size |
not be able to |
be able to detect |
able to detect which |
to detect which of |
detect which of its |
which of its registered |
of its registered miners |
its registered miners are |
distributed main memory transaction |
registered miners are the |
main memory transaction processing |
memory transaction processing system |
miners are the perpetrators |
the ninja architecture for |
ninja architecture for robust |
architecture for robust internet |
a pool can estimate |
scale systems and services |
pool can estimate its |
can estimate its expected |
estimate its expected mining |
its expected mining power |
expected mining power and |
mining power and its |
and was also able |
power and its actual |
was also able to |
and its actual mining |
also able to increase |
its actual mining power |
able to increase consistent |
actual mining power by |
to increase consistent transaction |
mining power by the |
increase consistent transaction rate |
power by the rates |
consistent transaction rate by |
by the rates of |
the rates of partial |
rates of partial proofs |
of partial proofs of |
partial proofs of work |
proofs of work and |
of work and full |
work and full proofs |
and full proofs of |
full proofs of work |
with only nominal overhead |
only nominal overhead on |
nominal overhead on the |
supplied by its miners |
overhead on the database |
our experiments with synthetic |
a difference above a |
experiments with synthetic workloads |
difference above a set |
with synthetic workloads showed |
above a set confidence |
synthetic workloads showed that |
a set confidence interval |
workloads showed that t |
set confidence interval indicates |
confidence interval indicates an |
interval indicates an attack |
cache s efficacy depends |
s efficacy depends on |
efficacy depends on the |
depends on the clustering |
on the clustering level |
the clustering level of |
to detect whether a |
clustering level of the |
level of the workload |
detect whether a single |
whether a single miner |
a single miner is |
single miner is attacking |
miner is attacking it |
cache adapts to dynamically |
adapts to dynamically changing |
to dynamically changing workloads |
the pool must use |
dynamically changing workloads where |
pool must use a |
changing workloads where clusters |
must use a similar |
workloads where clusters change |
use a similar technique |
where clusters change over |
clusters change over time |
due to resource limitations |
to resource limitations t |
verify replication for multi |
comparing the estimated mining |
the estimated mining power |
cache maintains only a |
estimated mining power of |
maintains only a short |
spatial gossip and resource |
mining power of the |
only a short dependency |
power of the attacker |
gossip and resource location |
of the attacker based |
a short dependency list |
and resource location protocols |
the attacker based on |
attacker based on its |
based on its partial |
in proceedings of the |
proceedings of the thirty |
on its partial proof |
which is naturally imperfect |
its partial proof of |
is naturally imperfect and |
third annual acm symposium |
partial proof of work |
naturally imperfect and does |
annual acm symposium on |
proof of work with |
imperfect and does not |
acm symposium on theory |
of work with the |
and does not include |
symposium on theory of |
work with the fact |
does not include all |
on theory of computing |
with the fact it |
not include all dependencies |
the fact it never |
fact it never supplies |
it never supplies a |
never supplies a full |
we proved that when |
supplies a full proof |
proved that when resources |
a full proof of |
that when resources are |
when resources are unbounded |
full proof of work |
if the attacker has |
cache s algorithm implements |
s algorithm implements cache |
the attacker has a |
attacker has a small |
has a small mining |
a small mining power |
it will send frequent |
will send frequent partial |
send frequent partial proofs |
frequent partial proofs of |
partial proofs of work |
but the pool will |
the pool will only |
pool will only expect |
will only expect to |
only expect to see |
expect to see a |
to see a full |
see a full proof |
a full proof of |
full proof of work |
proof of work at |
of work at very |
work at very low |
at very low frequency |
it cannot obtain statistically |
cannot obtain statistically significant |
obtain statistically significant results |
statistically significant results that |
significant results that would |
results that would indicate |
that would indicate an |
would indicate an attack |
a technique for increasing |
technique for increasing concurrency |
for increasing concurrency in |
an attacker can use |
increasing concurrency in a |
attacker can use multiple |
concurrency in a replicated |
in a replicated system |
can use multiple small |
information propagation in the |
propagation in the bitcoin |
in the bitcoin network |
acm transactions on database |
use multiple small block |
transactions on database systems |
multiple small block withholding |
small block withholding miners |
block withholding miners and |
withholding miners and replace |
miners and replace them |
and replace them frequently |
th ieee international conference |
ieee international conference on |
international conference on peer |
a small miner is |
using time instead of |
time instead of timeout |
instead of timeout for |
of timeout for fault |
a miner whose expected |
miner whose expected full |
whose expected full proof |
expected full proof of |
full proof of work |
proof of work frequency |
of work frequency is |
work frequency is yearly |
such a miner will |
a miner will see |
miner will see a |
will see a non |
negligible average daily revenue |
bitcoin and the age |
and the age of |
the age of bespoke |
age of bespoke silicon |
in proceedings of the |
adaptive distributed data management |
distributed data management with |
data management with weak |
management with weak consistent |
with weak consistent replicated |
weak consistent replicated data |
international conference on compilers |
in proceedings of the |
architectures and synthesis for |
and synthesis for embedded |
synthesis for embedded systems |
if the attacker replaces |
the attacker replaces such |
attacker replaces such a |
replaces such a small |
such a small miner |
a small miner every |
small miner every month |
acm symposium on applied |
symposium on applied computing |
he will collect about |
will collect about b |
at the end of |
the end of each |
end of each month |
the pool must decide |
pool must decide within |
must decide within this |
decide within this month |
within this month whether |
this month whether the |
month whether the miner |
whether the miner is |
the miner is an |
miner is an attacker |
and revoke its earnings |
into the bitcoin mines |
or just an unlucky |
just an unlucky honest |
an unlucky honest miner |
from paxos to corfu |
since an honest miner |
an honest miner of |
honest miner of this |
miner of this power |
of this power is |
this power is unlikely |
power is unlikely to |
is unlikely to find |
unlikely to find a |
to find a full |
find a full proof |
a full proof of |
full proof of work |
proof of work within |
of work within a |
work within a month |
aware adaptable web services |
according to the exponential |
to the exponential distribution |
in proceedings of the |
a pool that rejects |
pool that rejects miners |
that rejects miners based |
th international world wide |
rejects miners based on |
international world wide web |
miners based on this |
world wide web conference |
based on this criterion |
wide web conference on |
on this criterion would |
web conference on alternate |
this criterion would reject |
conference on alternate track |
criterion would reject the |
on alternate track papers |
would reject the majority |
alternate track papers and |
reject the majority of |
track papers and posters |
the majority of its |
majority of its honest |
of its honest miners |
the alternative of rejecting |
alternative of rejecting small |
of rejecting small miners |
rejecting small miners in |
small miners in general |
miners in general or |
in general or distributing |
general or distributing revenue |
or distributing revenue on |
distributing revenue on a |
revenue on a yearly |
on a yearly basis |
a yearly basis contradicts |
yearly basis contradicts the |
basis contradicts the goal |
contradicts the goal of |
the goal of pooled |
goal of pooled mining |
concurrency control and availability |
control and availability in |
and availability in multi |
m odel and s |
odel and s tandard |
and s tandard o |
s tandard o peration |
tandard o peration we |
o peration we specify |
peration we specify the |
we specify the basic |
specify the basic model |
the basic model in |
basic model in which |
model in which participants |
in which participants operate |
which participants operate in |
participants operate in section |
operate in section iii |
proceed to describe how |
to describe how honest |
describe how honest miners |
how honest miners operate |
honest miners operate in |
miners operate in this |
operate in this environment |
in this environment in |
this environment in sections |
environment in sections iii |
and how the classical |
how the classical block |
google s globally distributed |
the classical block withholding |
s globally distributed database |
classical block withholding attack |
block withholding attack is |
withholding attack is implemented |
attack is implemented with |
acm transactions on computer |
is implemented with our |
transactions on computer systems |
implemented with our model |
on predictive modeling for |
with our model in |
predictive modeling for optimizing |
our model in section |
modeling for optimizing transaction |
model in section iii |
for optimizing transaction execution |
optimizing transaction execution in |
transaction execution in parallel |
execution in parallel oltp |
in parallel oltp systems |
model the system is |
the system is comprised |
system is comprised of |
is comprised of the |
comprised of the bitcoin |
of the bitcoin network |
the bitcoin network and |
bitcoin network and nodes |
network and nodes with |
and nodes with unique |
nodes with unique ids |
and progresses in steps |
a node i generates |
node i generates tasks |
profiles for the situated |
for the situated web |
i generates tasks which |
generates tasks which are |
tasks which are associated |
which are associated with |
in proceedings of the |
are associated with its |
proceedings of the eleventh |
associated with its id |
of the eleventh international |
with its id i |
the eleventh international conference |
eleventh international conference on |
international conference on world |
conference on world wide |
on world wide web |
a node can work |
node can work on |
can work on a |
work on a task |
on a task for |
a task for the |
task for the duration |
for the duration of |
the duration of a |
duration of a step |
scalable deferred update replication |
the result of this |
result of this work |
of this work is |
this work is a |
work is a set |
is a set of |
a set of partial |
set of partial proofs |
of partial proofs of |
partial proofs of work |
proofs of work and |
of work and a |
work and a set |
and a set of |
a set of full |
set of full proofs |
of full proofs of |
full proofs of work |
the number of proofs |
number of proofs in |
of proofs in each |
proofs in each set |
in each set has |
each set has a |
set has a poisson |
has a poisson distribution |
partial proofs with a |
proofs with a large |
with a large mean |
a large mean and |
large mean and full |
mean and full proofs |
and full proofs with |
full proofs with a |
proofs with a small |
with a small mean |
nodes that work on |
that work on tasks |
van renesse and f |
work on tasks are |
distributed data structures over |
on tasks are called |
data structures over a |
tasks are called a |
structures over a shared |
the case for determinism |
are called a miners |
over a shared log |
case for determinism in |
chain replication for supporting |
for determinism in database |
replication for supporting high |
determinism in database systems |
in proceedings of the |
for supporting high throughput |
miners have identical power |
supporting high throughput and |
high throughput and availability |
th acm symposium on |
and hence identical probabilities |
acm symposium on operating |
in sixth symposium on |
hence identical probabilities to |
symposium on operating systems |
sixth symposium on operating |
identical probabilities to generate |
on operating systems principles |
symposium on operating systems |
probabilities to generate proofs |
on operating systems design |
to generate proofs of |
operating systems design and |
generate proofs of work |
systems design and implementation |
the bitcoin network pays |
bitcoin network pays for |
network pays for full |
pays for full proofs |
for full proofs of |
full proofs of work |
to acquire this payoff |
acquire this payoff an |
this payoff an entity |
payoff an entity publishes |
an entity publishes a |
entity publishes a task |
publishes a task task |
a task task and |
task task and its |
task and its corresponding |
and its corresponding proof |
its corresponding proof of |
corresponding proof of work |
proof of work to |
of work to the |
work to the network |
the payoff goes to |
payoff goes to the |
goes to the id |
to the id associated |
the id associated with |
id associated with task |
the bitcoin protocol normalizes |
bitcoin protocol normalizes revenue |
protocol normalizes revenue such |
normalizes revenue such that |
revenue such that the |
such that the average |
that the average total |
the average total revenue |
average total revenue distributed |
total revenue distributed in |
revenue distributed in each |
distributed in each step |
in each step is |
each step is a |
step is a constant |
is a constant throughout |
ordering transactions with prediction |
a constant throughout the |
transactions with prediction in |
constant throughout the execution |
with prediction in distributed |
throughout the execution of |
prediction in distributed object |
in distributed object stores |
the execution of the |
execution of the system |
any node can transact |
node can transact bitcoins |
can transact bitcoins to |
th workshop on large |
enabling scalable online personalization |
transact bitcoins to another |
scalable online personalization on |
bitcoins to another node |
online personalization on the |
scale distributed systems and |
to another node by |
personalization on the web |
distributed systems and middleware |
another node by issuing |
node by issuing a |
by issuing a bitcoin |
in proceedings of the |
issuing a bitcoin transaction |
nd acm conference on |
acm conference on electronic |
conference on electronic commerce |
nodes that generate tasks |
that generate tasks but |
generate tasks but outsource |
tasks but outsource the |
but outsource the work |
outsource the work are |
the work are called |
work are called pools |
pools send tasks to |
send tasks to miners |
tasks to miners over |
to miners over the |
miners over the network |
the miners receive the |
miners receive the tasks |
and send the partial |
send the partial and |
how a mining monopoly |
a mining monopoly can |
mining monopoly can attack |
monopoly can attack bitcoin |
the partial and full |
partial and full proofs |
and full proofs of |
full proofs of work |
proofs of work to |
of work to the |
work to the pool |
apart from working on |
from working on tasks |
key transactions for key |
and receipt are instantaneous |
we assume that the |
assume that the number |
that the number of |
the number of miners |
number of miners is |
of miners is large |
miners is large enough |
is large enough such |
large enough such that |
an architecture for well |
enough such that mining |
such that mining power |
that mining power can |
mining power can be |
power can be split |
can be split arbitrarily |
be split arbitrarily without |
split arbitrarily without resolution |
arbitrarily without resolution constraints |
in symposium on operating |
symposium on operating systems |
on operating systems principles |
denote the number of |
the number of pools |
number of pools with |
of pools with p |
the total number of |
total number of mining |
number of mining power |
of mining power in |
mining power in the |
power in the system |
in the system with |
the system with m |
system with m and |
with m and the |
m and the miners |
and the miners participating |
the miners participating in |
miners participating in pool |
participating in pool i |
we use a quasistatic |
use a quasistatic analysis |
a quasistatic analysis where |
quasistatic analysis where miner |
transparent error correction for |
error correction for lambda |
analysis where miner participation |
majority is not enough |
correction for lambda networks |
for lambda networks mahesh |
lambda networks mahesh balakrishnan |
bitcoin mining is vulnerable |
where miner participation in |
miner participation in a |
participation in a pool |
in financial cryptography and |
the costs and limits |
costs and limits of |
in a pool does |
financial cryptography and data |
cryptography and data security |
a pool does not |
and limits of availability |
pool does not change |
limits of availability for |
does not change over |
of availability for replicated |
not change over time |
availability for replicated services |
in proceedings of the |
proceedings of the eighteenth |
of the eighteenth acm |
the eighteenth acm symposium |
eighteenth acm symposium on |
acm symposium on operating |
symposium on operating systems |
solo mining a solo |
on operating systems principles |
mining a solo miner |
a solo miner is |
solo miner is a |
miner is a node |
is a node that |
facebook s distributed data |
a node that generates |
s distributed data store |
node that generates its |
distributed data store for |
that generates its own |
data store for the |
generates its own tasks |
store for the social |
for the social graph |
in every step it |
in usenix annual technical |
usenix annual technical conference |
every step it generates |
step it generates a |
it generates a task |
works on it for |
on it for the |
it for the duration |
for the duration of |
the duration of the |
duration of the step |
of the step and |
the step and if |
step and if it |
and if it finds |
if it finds a |
it finds a full |
finds a full proof |
a full proof of |
full proof of work |
abstract the global network |
the global network of |
it publishes this proof |
global network of datacenters |
publishes this proof of |
network of datacenters is |
this proof of work |
of datacenters is emerging |
proof of work to |
datacenters is emerging as |
of work to earn |
is emerging as an |
work to earn the |
emerging as an important |
to earn the payoff |
as an important distributed |
an important distributed systems |
important distributed systems paradigm |
distributed systems paradigm commodity |
systems paradigm commodity clusters |
paradigm commodity clusters running |
commodity clusters running high |
pools a pool is |
a pool is a |
pool is a node |
design and evaluation of |
is a node that |
and evaluation of a |
a node that serves |
evaluation of a conitbased |
node that serves as |
of a conitbased continuous |
speed lambda networks across |
that serves as a |
a conitbased continuous consistency |
lambda networks across hundreds |
serves as a coordinator |
conitbased continuous consistency model |
networks across hundreds of |
as a coordinator and |
continuous consistency model for |
cooperative equilibrium for supergames |
a coordinator and multiple |
across hundreds of milliseconds |
consistency model for replicated |
coordinator and multiple miners |
hundreds of milliseconds of |
model for replicated services |
the review of economic |
review of economic studies |
of milliseconds of network |
and multiple miners can |
milliseconds of network latency |
acm transactions on computer |
multiple miners can register |
transactions on computer systems |
miners can register to |
packet loss on long |
can register to a |
register to a pool |
to a pool and |
a pool and work |
pool and work for |
and work for it |
haul networks can cripple |
networks can cripple application |
can cripple application performance |
cripple application performance a |
application performance a loss |
in every step it |
performance a loss rate |
every step it generates |
a loss rate of |
step it generates a |
it generates a task |
generates a task for |
a task for each |
task for each registered |
for each registered miner |
each registered miner and |
registered miner and sends |
miner and sends it |
and sends it over |
sends it over the |
it over the network |
each miner receives its |
is sufficient to reduce |
miner receives its task |
sufficient to reduce tcp |
receives its task and |
its task and works |
task and works on |
and works on it |
works on it for |
ip throughput by an |
on it for the |
throughput by an order |
it for the duration |
by an order of |
for the duration of |
an order of magnitude |
the duration of the |
order of magnitude on |
duration of the step |
of magnitude on a |
at the end of |
the end of the |
end of the step |
scaling memcache at facebook |
the miner sends the |
miner sends the pool |
sends the pool the |
the pool the full |
pool the full and |
the full and the |
full and the partial |
term competition a game |
and the partial proofs |
maelstrom is an edge |
th usenix symposium on |
the partial proofs of |
is an edge appliance |
usenix symposium on networked |
partial proofs of work |
an edge appliance that |
symposium on networked systems |
proofs of work it |
edge appliance that masks |
on networked systems design |
of work it has |
appliance that masks packet |
networked systems design and |
work it has found |
that masks packet loss |
systems design and implementation |
masks packet loss transparently |
packet loss transparently and |
loss transparently and quickly |
the pool receives the |
transparently and quickly from |
pool receives the proofs |
and quickly from inter |
receives the proofs of |
the proofs of work |
proofs of work of |
of work of all |
work of all its |
of all its miners |
aggregating traffic for high |
registers the partial proofs |
the partial proofs of |
partial proofs of work |
proofs of work and |
of work and publishes |
speed encoding and using |
work and publishes the |
encoding and using a |
and publishes the full |
and using a new |
publishes the full proofs |
using a new forward |
a new forward error |
new forward error correction |
forward error correction scheme |
error correction scheme to |
it calculates its overall |
correction scheme to handle |
calculates its overall revenue |
scheme to handle bursty |
to handle bursty loss |
and proceeds to distribute |
proceeds to distribute it |
to distribute it among |
distribute it among its |
it among its miners |
introduction the emergence of |
the emergence of commodity |
each miner receives revenue |
emergence of commodity clusters |
miner receives revenue proportional |
of commodity clusters and |
receives revenue proportional to |
commodity clusters and datacenters |
revenue proportional to its |
clusters and datacenters has |
proportional to its success |
and datacenters has enabled |
to its success in |
datacenters has enabled a |
its success in the |
has enabled a new |
success in the current |
enabled a new class |
in the current step |
a new class of |
new class of globally |
class of globally distributed |
of globally distributed highperformance |
globally distributed highperformance applications |
namely the ratio of |
distributed highperformance applications that |
the ratio of its |
highperformance applications that coordinate |
ratio of its partial |
applications that coordinate over |
of its partial proofs |
that coordinate over vast |
its partial proofs of |
coordinate over vast geographical |
partial proofs of work |
over vast geographical distances |
proofs of work out |
of work out of |
work out of all |
out of all partial |
of all partial proofs |
all partial proofs of |
partial proofs of work |
proofs of work the |
of work the pool |
work the pool received |
a financial firm s |
financial firm s new |
firm s new york |
s new york city |
new york city datacenter |
we assume that pools |
york city datacenter may |
assume that pools do |
city datacenter may receive |
that pools do not |
datacenter may receive real |
pools do not collect |
do not collect fees |
not collect fees of |
collect fees of the |
fees of the revenue |
time updates from a |
updates from a stock |
from a stock exchange |
a stock exchange in |
stock exchange in switzerland |
pool fees and their |
fees and their implications |
and their implications on |
their implications on our |
implications on our analysis |
conduct financial transactions with |
on our analysis are |
financial transactions with banks |
our analysis are discussed |
transactions with banks in |
analysis are discussed in |
with banks in asia |
are discussed in section |
discussed in section ix |
cache data in london |
data in london for |
in london for locality |
london for locality and |
for locality and mirror |
locality and mirror it |
and mirror it to |
block withholding miner a |
mirror it to kansas |
withholding miner a miner |
it to kansas for |
miner a miner registered |
to kansas for disaster |
a miner registered at |
miner registered at a |
registered at a pool |
at a pool can |
a pool can perform |
pool can perform the |
can perform the classical |
perform the classical block |
the classical block withholding |
classical block withholding attack |
to interconnect these bandwidth |
an attacker miner operates |
hungry datacenters across the |
attacker miner operates as |
datacenters across the globe |
miner operates as if |
operates as if it |
as if it worked |
if it worked for |
it worked for the |
worked for the pool |
organizations are increasingly deploying |
are increasingly deploying private |
increasingly deploying private lambda |
deploying private lambda networks |
it receives its tasks |
receives its tasks and |
its tasks and works |
tasks and works on |
and works on them |
only at the end |
at the end of |
the end of each |
io bitcoin mining pool |
end of each round |
of each round it |
each round it sends |
transactional consistency and automatic |
round it sends only |
consistency and automatic management |
it sends only its |
and automatic management in |
sends only its partial |
automatic management in an |
only its partial proofs |
management in an application |
its partial proofs of |
in an application data |
partial proofs of work |
an application data cache |
raw bandwidth is ubiquitous |
bandwidth is ubiquitous and |
and omits full proofs |
is ubiquitous and cheaply |
omits full proofs of |
ubiquitous and cheaply available |
full proofs of work |
and cheaply available in |
th usenix symposium on |
proofs of work if |
cheaply available in the |
usenix symposium on operating |
of work if it |
available in the form |
symposium on operating systems |
work if it had |
in the form of |
on operating systems design |
if it had found |
it had found any |
operating systems design and |
the form of existing |
systems design and implementation |
form of existing dark |
of existing dark fiber |
the pool registers the |
pool registers the miner |
registers the miner s |
the miner s partial |
miner s partial proofs |
running and maintaining high |
but cannot distinguish between |
cannot distinguish between miners |
distinguish between miners running |
between miners running honestly |
miners running honestly and |
running honestly and block |
honestly and block withholding |
and block withholding miners |
free networks over this |
networks over this fiber |
over this fiber is |
this fiber is difficult |
the implications are that |
fiber is difficult and |
implications are that a |
is difficult and expensive |
are that a miner |
that a miner that |
a miner that engages |
miner that engages in |
that engages in block |
engages in block withholding |
in block withholding does |
block withholding does not |
capacity optical links are |
withholding does not contribute |
optical links are almost |
does not contribute to |
links are almost never |
not contribute to the |
are almost never congested |
contribute to the pool |
to the pool s |
the pool s overall |
pool s overall mining |
s overall mining power |
they drop packets for |
drop packets for numerous |
packets for numerous reasons |
for numerous reasons dirty |
but still shares the |
still shares the pool |
shares the pool s |
the pool s revenue |
pool s revenue according |
s revenue according to |
revenue according to its |
according to its sent |
to its sent partial |
its sent partial proofs |
sent partial proofs of |
partial proofs of work |
to reason about a |
reason about a pool |
about a pool s |
a pool s efficiency |
pool s efficiency we |
s efficiency we define |
efficiency we define its |
we define its per |
miner revenue as follows |
the revenue density of |
revenue density of a |
density of a pool |
of a pool is |
a pool is the |
pool is the ratio |
is the ratio between |
fast iterative graph computation |
the ratio between the |
iterative graph computation with |
ratio between the average |
graph computation with block |
between the average revenue |
computation with block updates |
the average revenue a |
average revenue a pool |
revenue a pool member |
a pool member earns |
pool member earns and |
member earns and the |
earns and the average |
and the average revenue |
of the vldb endowment |
the average revenue it |
average revenue it would |
for example and in |
revenue it would have |
example and in different |
it would have earned |
and in different patterns |
would have earned as |
have earned as a |
earned as a solo |
as a solo miner |
ranging from singleton drops |
from singleton drops to |
singleton drops to extended |
drops to extended bursts |
the revenue density of |
revenue density of a |
density of a solo |
of a solo miner |
and that of a |
that of a miner |
of a miner working |
a miner working with |
miner working with an |
working with an unattacked |
with an unattacked pool |
an unattacked pool are |
unattacked pool are one |
if a pool is |
a pool is attacked |
pool is attacked with |
is attacked with block |
attacked with block withholding |
its revenue density decreases |
congestion loss has been |
loss has been observed |
has been observed on |
been observed on long |
continuous analysis because our |
analysis because our analysis |
because our analysis will |
our analysis will be |
analysis will be of |
haul networks as well |
will be of the |
be of the average |
of the average revenue |
we will consider proofs |
will consider proofs of |
consider proofs of work |
both full and partial |
as continuous deterministic sizes |
according to their probability |
work on a task |
on a task therefore |
a task therefore results |
task therefore results in |
therefore results in a |
concurrency control and recovery |
results in a deterministic |
control and recovery in |
in a deterministic fraction |
and recovery in database |
recovery in database systems |
a deterministic fraction of |
deterministic fraction of proof |
fraction of proof of |
of proof of work |
kncminer bitcoin mining cloud |
bitcoin mining cloud mining |
t he p ool |
he p ool g |
p ool g ame |
ool g ame a |
the pool block withholding |
pool block withholding attack |
block withholding attack just |
withholding attack just as |
attack just as a |
just as a miner |
as a miner can |
a miner can perform |
miner can perform block |
can perform block withholding |
perform block withholding on |
block withholding on a |
withholding on a pool |
on a pool j |
a pool i can |
pool i can use |
i can use some |
can use some of |
use some of its |
some of its mining |
of its mining power |
its mining power to |
mining power to infiltrate |
power to infiltrate a |
to infiltrate a pool |
infiltrate a pool j |
a pool j and |
pool j and perform |
j and perform a |
and perform a block |
perform a block withholding |
a block withholding attack |
block withholding attack on |
withholding attack on j |
denote the amount of |
the amount of such |
amount of such infiltrating |
of such infiltrating mining |
ms w n s |
w n s e |
n s e figure |
such infiltrating mining power |
infiltrating mining power at |
mining power at step |
power at step t |
at step t by |
step t by xi |
example lambda network tional |
lambda network tional lambdarail |
the dynamics of viral |
dynamics of viral marketing |
miners working for pool |
acm transactions on the |
transactions on the web |
working for pool i |
either mining honestly or |
mining honestly or used |
honestly or used for |
or used for infiltrating |
used for infiltrating pool |
for infiltrating pool j |
are loyal to pool |
loyal to pool i |
at the end of |
the end of a |
end of a round |
pool i aggregates its |
i aggregates its revenue |
aggregates its revenue from |
its revenue from mining |
revenue from mining in |
from mining in the |
mining in the current |
in the current round |
the current round and |
current round and from |
round and from its |
and from its infiltration |
as has its crippling |
from its infiltration in |
has its crippling effect |
its infiltration in the |
its crippling effect on |
infiltration in the previous |
crippling effect on commodity |
effect on commodity protocols |
in the previous round |
motivating research into loss |
it distributes the revenue |
distributes the revenue evenly |
an authorization architecture for |
authorization architecture for trustworthy |
architecture for trustworthy computing |
the revenue evenly among |
resistant data transfer protocols |
revenue evenly among all |
evenly among all its |
in proceedings of the |
proceedings of the twenty |
among all its loyal |
all its loyal miners |
its loyal miners according |
loyal miners according to |
miners according to their |
according to their partial |
to their partial proofs |
their partial proofs of |
partial proofs of work |
third acm symposium on |
acm symposium on operating |
symposium on operating systems |
on operating systems principles |
the pool s miners |
pool s miners are |
s miners are oblivious |
miners are oblivious to |
are oblivious to their |
oblivious to their role |
to their role and |
their role and they |
role and they operate |
and they operate as |
they operate as regular |
operate as regular honest |
as regular honest miners |
measurement and analysis of |
and analysis of online |
analysis of online social |
of online social networks |
in proceedings of the |
revenue convergence note that |
th acm sigcomm conference |
convergence note that pool |
acm sigcomm conference on |
note that pool j |
sigcomm conference on internet |
that pool j sends |
conference on internet measurement |
pool j sends its |
j sends its revenue |
sends its revenue to |
its revenue to infiltrators |
conservative flow control mechanisms |
revenue to infiltrators from |
flow control mechanisms designed |
to infiltrators from pool |
control mechanisms designed to |
infiltrators from pool i |
mechanisms designed to deal |
from pool i at |
designed to deal with |
pool i at the |
to deal with the |
i at the end |
deal with the systematic |
at the end of |
with the systematic congestion |
the end of the |
the systematic congestion of |
end of the step |
systematic congestion of the |
congestion of the commodity |
of the commodity internet |
the commodity internet react |
commodity internet react too |
and this revenue is |
internet react too sharply |
this revenue is calculated |
react too sharply to |
revenue is calculated in |
too sharply to ephemeral |
is calculated in pool |
sharply to ephemeral loss |
calculated in pool i |
to ephemeral loss on |
in pool i at |
ephemeral loss on over |
pool i at the |
i at the beginning |
at the beginning of |
the beginning of the |
beginning of the subsequent |
of the subsequent step |
provisioned links a single |
links a single packet |
a single packet loss |
single packet loss in |
packet loss in ten |
if there is a |
loss in ten thousand |
there is a chain |
in ten thousand is |
is a chain of |
a chain of pools |
ten thousand is enough |
thousand is enough to |
is enough to reduce |
enough to reduce tcp |
ip throughput to a |
throughput to a third |
to a third over |
a third over a |
sampling from large graphs |
in proceedings of the |
and one in a |
one in a thousand |
in a thousand drops |
a thousand drops it |
thousand drops it by |
drops it by an |
it by an order |
by an order of |
an order of magnitude |
th acm sigkdd international |
acm sigkdd international conference |
sigkdd international conference on |
where each pool infiltrates |
international conference on knowledge |
each pool infiltrates the |
conference on knowledge discovery |
pool infiltrates the previous |
on knowledge discovery and |
time applications are impacted |
infiltrates the previous one |
knowledge discovery and data |
applications are impacted by |
discovery and data mining |
are impacted by the |
the pool revenue will |
impacted by the reliance |
pool revenue will not |
by the reliance of |
revenue will not be |
the reliance of reliability |
will not be static |
reliance of reliability mechanisms |
of reliability mechanisms on |
reliability mechanisms on acknowledgments |
mechanisms on acknowledgments and |
on acknowledgments and retransmissions |
since the revenue from |
the revenue from infiltration |
revenue from infiltration takes |
from infiltration takes one |
infiltration takes one step |
takes one step to |
limiting the latency of |
one step to take |
the latency of packet |
step to take each |
to take each hop |
latency of packet recovery |
of packet recovery to |
packet recovery to at |
recovery to at least |
to at least the |
from the first step |
at least the round |
least the round trip |
the round trip time |
the revenue of pool |
since it is only |
it is only infiltrated |
if delivery is sequenced |
is only infiltrated and |
only infiltrated and loses |
infiltrated and loses some |
and loses some of |
loses some of its |
each lost packet acts |
some of its revenue |
of its revenue for |
lost packet acts as |
its revenue for pool |
packet acts as a |
acts as a virtual |
as a virtual road |
block in the fifo |
in the fifo channel |
starting from the second |
from the second step |
the fifo channel until |
fifo channel until it |
channel until it is |
until it is recovered |
the revenue of pool |
resistant protocols is not |
protocols is not an |
comprised of its own |
is not an alternative |
of its own mining |
not an alternative in |
its own mining and |
an alternative in corporate |
own mining and its |
alternative in corporate datacenters |
mining and its revenue |
and its revenue from |
its revenue from the |
revenue from the infiltration |
from the infiltration of |
where standardization is the |
the infiltration of pool |
on power splitting games |
power splitting games in |
don t settle for |
t settle for eventual |
standardization is the key |
is the key to |
the key to low |
key to low and |
to low and predictable |
with some revenue lost |
scalable causal consistency for |
causal consistency for wide |
low and predictable maintenance |
some revenue lost due |
revenue lost due to |
and predictable maintenance costs |
splitting games in distributed |
games in distributed computation |
area storage with cops |
lost due to its |
nei this work was |
due to its infiltration |
this work was supported |
work was supported in |
to its infiltration by |
its infiltration by pool |
was supported in part |
supported in part by |
in part by grants |
part by grants from |
by grants from afosr |
the case of bitcoin |
case of bitcoin pooled |
of bitcoin pooled mining |
rd acm symposium on |
starting from the third |
from the third step |
acm symposium on operating |
symposium on operating systems |
on operating systems principles |
ther is eliminating loss |
is eliminating loss events |
the revenue of pool |
eliminating loss events on |
loss events on a |
events on a network |
on a network that |
a network that could |
network that could nsf |
that could nsf and |
could nsf and intel |
nsf and intel corporation |
span thousands of miles |
max is the longest |
is the longest chain |
the longest chain in |
longest chain in the |
chain in the system |
there is a need |
is a need to |
a need to link |
need to link loss |
the revenue stabilizes after |
if there are loops |
there are loops in |
are loops in the |
loops in the infiltration |
in the infiltration graph |
side appliance locations of |
appliance locations of packet |
locations of packet loss |
of packet loss receive |
the system will converge |
system will converge to |
will converge to a |
converge to a certain |
to a certain revenue |
side appliance receiver buffer |
appliance receiver buffer overflow |
as stated in the |
stated in the following |
in the following lemma |
local recovery receiving end |
kernel code no dropped |
code no dropped packets |
transactional storage for geo |
if infiltration rates are |
no dropped packets figure |
infiltration rates are constant |
the pool revenues converge |
pool revenues converge to |
revenues converge to a |
converge to a limit |
maelstrom communication path mask |
to a limit as |
communication path mask loss |
a limit as time |
path mask loss on |
limit as time progresses |
mask loss on the |
loss on the link |
rd acm symposium on |
acm symposium on operating |
symposium on operating systems |
on operating systems principles |
denote the revenue density |
the revenue density of |
revenue density of pool |
density of pool i |
of pool i at |
pool i at the |
because recovery delays for |
i at the end |
recovery delays for lost |
at the end of |
delays for lost packets |
the end of step |
end of step t |
for lost packets translate |
weekly bitcoin network statistics |
of step t by |
lost packets translate into |
step t by ri |
packets translate into dramatic |
translate into dramatic reductions |
into dramatic reductions in |
dramatic reductions in application |
and define the revenue |
define the revenue density |
the revenue density vector |
revenue density vector r |
because applications and os |
applications and os networking |
and os networking stacks |
os networking stacks in |
networking stacks in commodity |
stacks in commodity datacenters |
in commodity datacenters cannot |
commodity datacenters cannot be |
datacenters cannot be rewritten |
cannot be rewritten from |
be rewritten from scratch |
is a promising solution |
a promising solution for |
promising solution for reliability |
solution for reliability over |
for reliability over long |
packet recovery latency is |
recovery latency is independent |
latency is independent of |
is independent of the |
independent of the rtt |
of the rtt of |
the rtt of the |
rtt of the link |
replicated systems fast as |
systems fast as possible |
while fec codes have |
fec codes have been |
codes have been used |
have been used for |
been used for decades |
used for decades within |
for decades within link |
the revenues at all |
th usenix symposium on |
revenues at all pools |
faster commodity processors have |
usenix symposium on operating |
at all pools converge |
commodity processors have enabled |
symposium on operating systems |
all pools converge as |
processors have enabled packet |
on operating systems design |
pools converge as follows |
operating systems design and |
systems design and implementation |
level fec at end |
end fec is very |
fec is very attractive |
is very attractive for |
very attractive for inter |
theoretic analysis of ddos |
easy to deploy and |
to deploy and customize |
analysis of ddos attacks |
of ddos attacks against |
ddos attacks against bitcoin |
attacks against bitcoin mining |
against bitcoin mining pools |
and does not require |
does not require specialized |
not require specialized equipment |
require specialized equipment in |
in workshop on bitcoin |
workshop on bitcoin research |
specialized equipment in the |
equipment in the network |
p in every round |
in the network linking |
the network linking the |
network linking the datacenters |
pool i uses its |
i uses its mining |
uses its mining power |
its mining power of |
mining power of m |
host fec has two |
fec has two major |
combining acid and base |
has two major issues |
acid and base in |
j used for direct |
two major issues first |
and base in a |
used for direct mining |
base in a distributed |
for direct mining p |
in a distributed database |
it s not transparent |
requiring modification of the |
modification of the end |
and shares it among |
th usenix symposium on |
shares it among its |
usenix symposium on operating |
it among its m |
symposium on operating systems |
on operating systems design |
operating systems design and |
systems design and implementation |
it s not necessarily |
s not necessarily rapid |
when bitcoin mining pools |
bitcoin mining pools run |
mining pools run dry |
fec works best over |
works best over high |
in workshop on bitcoin |
workshop on bitcoin research |
stable traffic rates and |
traffic rates and performs |
rates and performs poorly |
and performs poorly if |
performs poorly if the |
poorly if the data |
all sums are over |
if the data rate |
sums are over the |
the data rate in |
are over the range |
data rate in the |
rate in the channel |
in the channel is |
the channel is low |
channel is low and |
is low and sporadic |
as in a single |
in a single end |
comparison of mining pools |
denote the direct mining |
the direct mining revenue |
direct mining revenue density |
mining revenue density of |
revenue density of each |
density of each pool |
we present the maelstrom |
present the maelstrom error |
the maelstrom error correction |
maelstrom error correction appliance |
error correction appliance a |
which is a constant |
correction appliance a rack |
is a constant factor |
appliance a rack of |
a rack of proxies |
rack of proxies residing |
of proxies residing between |
proxies residing between a |
residing between a datacenter |
between a datacenter and |
a datacenter and its |
datacenter and its wan |
and its wan link |
comparison of mining pools |
a shared log design |
shared log design for |
log design for flash |
design for flash clusters |
th usenix symposium on |
maelstrom encodes fec packets |
usenix symposium on networked |
encodes fec packets over |
symposium on networked systems |
fec packets over traffic |
on networked systems design |
packets over traffic flowing |
networked systems design and |
over traffic flowing through |
systems design and implementation |
traffic flowing through it |
flowing through it and |
through it and routes |
it and routes them |
and routes them to |
routes them to a |
them to a corresponding |
to a corresponding appliance |
a corresponding appliance at |
corresponding appliance at the |
appliance at the destination |
at the destination datacenter |
which decodes them and |
decodes them and recovers |
them and recovers lost |
and recovers lost data |
hashcash amortizable publicly auditable |
amortizable publicly auditable cost |
maelstrom is completely transparent |
is completely transparent it |
completely transparent it does |
transparent it does not |
it does not require |
does not require modification |
not require modification of |
require modification of end |
host software and is |
software and is agnostic |
and is agnostic to |
is agnostic to the |
agnostic to the network |
to the network connecting |
the network connecting the |
the pool game in |
network connecting the datacenter |
pool game in the |
game in the pool |
in the pool game |
the pool game pools |
pool game pools try |
game pools try to |
pools try to optimize |
try to optimize their |
it eliminates the dependence |
to optimize their infiltration |
eliminates the dependence of |
optimize their infiltration rates |
the dependence of fec |
their infiltration rates of |
dependence of fec recovery |
infiltration rates of other |
of fec recovery latency |
rates of other pools |
fec recovery latency on |
of other pools to |
recovery latency on the |
other pools to maximize |
latency on the data |
pools to maximize their |
on the data rate |
to maximize their revenue |
the data rate in |
data rate in any |
rate in any single |
in any single node |
the overall number of |
overall number of miners |
number of miners and |
of miners and the |
miners and the number |
and the number of |
node channel by encoding |
the number of miners |
channel by encoding over |
number of miners loyal |
by encoding over the |
of miners loyal to |
encoding over the aggregated |
miners loyal to each |
over the aggregated traffic |
loyal to each pool |
the aggregated traffic leaving |
to each pool remain |
aggregated traffic leaving the |
traffic leaving the datacenter |
each pool remain constant |
pool remain constant throughout |
remain constant throughout the |
constant throughout the game |
time progresses in rounds |
maelstrom uses a new |
uses a new encoding |
a new encoding scheme |
new encoding scheme called |
encoding scheme called layered |
scheme called layered interleaving |
let s be a |
s be a constant |
be a constant integer |
a constant integer large |
designed especially for time |
constant integer large enough |
hashcash a denial of |
a denial of service |
denial of service counter |
sensitive packet recovery in |
integer large enough that |
packet recovery in the |
large enough that revenue |
recovery in the presence |
enough that revenue can |
in the presence of |
the presence of bursty |
presence of bursty loss |
that revenue can be |
revenue can be approximated |
can be approximated as |
be approximated as its |
approximated as its convergence |
as its convergence limit |
the contributions of this |
contributions of this paper |
of this paper are |
this paper are as |
paper are as follows |
in each round the |
each round the system |
round the system takes |
the system takes s |
system takes s steps |
takes s steps and |
s steps and then |
steps and then a |
and then a single |
then a single pool |
end fec for long |
picked with a round |
distance communication between datacenters |
may change its infiltration |
and argue that the |
change its infiltration rates |
argue that the rate |
its infiltration rates of |
that the rate sensitivity |
infiltration rates of all |
the rate sensitivity of |
rates of all other |
rate sensitivity of fec |
of all other pools |
sensitivity of fec codes |
of fec codes and |
fec codes and the |
codes and the opacity |
and the opacity of |
the total revenue of |
the opacity of their |
total revenue of each |
opacity of their implementations |
revenue of each step |
of their implementations present |
of each step is |
their implementations present major |
each step is normalized |
implementations present major obstacles |
step is normalized to |
present major obstacles to |
major obstacles to their |
obstacles to their usage |
a gateway appliance that |
so the revenue per |
gateway appliance that transparently |
the revenue per round |
appliance that transparently aggregates |
revenue per round is |
that transparently aggregates traffic |
per round is one |
transparently aggregates traffic and |
aggregates traffic and encodes |
traffic and encodes over |
and encodes over the |
encodes over the resulting |
over the resulting high |
the pool taking a |
pool taking a step |
taking a step knows |
a step knows the |
step knows the rate |
knows the rate of |
the rate of infiltrators |
rate of infiltrators attacking |
of infiltrators attacking it |
we describe layered interleaving |
though not their identity |
on subversive miner strategies |
a new fec scheme |
subversive miner strategies and |
and the revenue rates |
new fec scheme used |
miner strategies and block |
strategies and block withholding |
and block withholding attack |
block withholding attack in |
withholding attack in bitcoin |
attack in bitcoin digital |
in bitcoin digital currency |
the revenue rates of |
fec scheme used by |
revenue rates of each |
scheme used by maelstrom |
rates of each of |
used by maelstrom where |
of each of the |
by maelstrom where for |
each of the other |
maelstrom where for constant |
of the other pools |
where for constant encoding |
for constant encoding overhead |
constant encoding overhead the |
encoding overhead the latency |
this knowledge is required |
overhead the latency of |
knowledge is required to |
the latency of packet |
is required to optimize |
latency of packet recovery |
required to optimize a |
of packet recovery degrades |
to optimize a pool |
packet recovery degrades gracefully |
optimize a pool s |
a pool s revenue |
recovery degrades gracefully as |
degrades gracefully as losses |
gracefully as losses get |
as losses get burstier |
as we see next |
we discuss implementation considerations |
we explain in section |
explain in section viii |
in section viii how |
section viii how a |
viii how a pool |
we built two versions |
how a pool can |
built two versions of |
a pool can technically |
two versions of maelstrom |
pool can technically obtain |
can technically obtain this |
technically obtain this knowledge |
one runs in user |
runs in user mode |
while the other runs |
the other runs within |
general analysis recall that |
other runs within the |
analysis recall that mi |
runs within the linux |
recall that mi is |
within the linux kernel |
that mi is the |
mi is the number |
is the number of |
the number of miners |
number of miners loyal |
of miners loyal to |
we evaluate maelstrom on |
miners loyal to pool |
loyal to pool i |
evaluate maelstrom on emulab |
how incentivize large bitcoin |
incentivize large bitcoin mining |
large bitcoin mining http |
is the number of |
and show that it |
the number of miners |
show that it provides |
number of miners used |
that it provides near |
of miners used by |
it provides near lossless |
miners used by pool |
provides near lossless tcp |
used by pool i |
by pool i to |
pool i to infiltrate |
i to infiltrate pool |
to infiltrate pool j |
infiltrate pool j at |
ip throughput and latency |
pool j at step |
j at step t |
throughput and latency over |
and latency over lossy |
latency over lossy links |
the mining rate of |
mining rate of pool |
and recovers packets with |
rate of pool i |
recovers packets with latency |
of pool i is |
packets with latency independent |
pool i is therefore |
with latency independent of |
i is therefore the |
latency independent of the |
is therefore the number |
independent of the rtt |
therefore the number of |
of the rtt of |
the number of its |
the rtt of the |
number of its loyal |
rtt of the link |
of its loyal miners |
of the link and |
its loyal miners minus |
the link and the |
loyal miners minus the |
link and the rate |
miners minus the miners |
and the rate in |
minus the miners it |
the rate in any |
rate in any single |
in any single channel |
the miners it uses |
miners it uses for |
it uses for infiltration |
this effective mining rate |
effective mining rate is |
model our focus is |
mining rate is divided |
our focus is on |
rate is divided by |
achieving serializability with low |
focus is on pairs |
is divided by the |
serializability with low latency |
is on pairs of |
divided by the total |
with low latency in |
on pairs of geographically |
by the total mining |
low latency in geodistributed |
pairs of geographically distant |
the total mining rate |
latency in geodistributed storage |
of geographically distant datacenters |
total mining rate in |
in geodistributed storage systems |
geographically distant datacenters that |
mining rate in the |
distant datacenters that coordinate |
rate in the system |
datacenters that coordinate with |
in proceedings of the |
that coordinate with each |
coordinate with each other |
with each other in |
each other in real |
namely the number of |
the number of all |
number of all miners |
of all miners that |
all miners that do |
th acm symposium on |
miners that do not |
acm symposium on operating |
that do not engage |
symposium on operating systems |
do not engage in |
this has long been |
on operating systems principles |
not engage in block |
has long been a |
engage in block withholding |
long been a critical |
been a critical distributed |
a critical distributed computing |
critical distributed computing paradigm |
distributed computing paradigm in |
computing paradigm in application |
paradigm in application domains |
in application domains such |
denote the direct mining |
application domains such as |
the direct mining rate |
domains such as finance |
direct mining rate of |
such as finance and |
mining rate of pool |
as finance and aerospace |
rate of pool i |
of pool i at |
pool i at step |
i at step t |
at step t by |
step t by pp |
t by pp mi |
by pp mi j |
similar requirements are arising |
requirements are arising across |
are arising across the |
arising across the board |
across the board as |
the board as globalized |
board as globalized enterprises |
as globalized enterprises rely |
globalized enterprises rely on |
enterprises rely on networks |
rely on networks for |
on networks for high |
speed communication and collaboration |
the most general case |
most general case of |
general case of inter |
cluster communication is one |
communication is one where |
is one where any |
one where any node |
where any node in |
any node in one |
node in one cluster |
in one cluster can |
one cluster can communicate |
cluster can communicate with |
can communicate with any |
communicate with any node |
with any node in |
any node in the |
node in the other |
k the revenue of |
in the other cluster |
the revenue of pool |
revenue of pool i |
of pool i in |
pool i in step |
i in step t |
we make no assumptions |
in step t taken |
make no assumptions about |
step t taken through |
no assumptions about the |
t taken through infiltration |
assumptions about the type |
taken through infiltration from |
about the type of |
through infiltration from pool |
the type of traffic |
infiltration from pool j |
type of traffic flowing |
from pool j s |
of traffic flowing through |
pool j s revenue |
acm transactions on database |
traffic flowing through the |
j s revenue in |
transactions on database systems |
flowing through the link |
s revenue in step |
revenue in step t |
critical applications could send |
applications could send dynamically |
could send dynamically generated |
send dynamically generated real |
time data such as |
data such as stock |
such as stock quotes |
financial transactions and battleground |
transactions and battleground location |
and battleground location updates |
pool i distributes this |
i distributes this revenue |
distributes this revenue among |
this revenue among its |
revenue among its mi |
while enterprise applications could |
enterprise applications could send |
applications could send voip |
could send voip streams |
ssh sessions and synchronous |
i members loyal and |
sessions and synchronous file |
members loyal and infiltrators |
and synchronous file updates |
synchronous file updates between |
file updates between offices |
define the p p |
the p p infiltration |
p p infiltration matrix |
p infiltration matrix by |
infiltration matrix by its |
matrix by its i |
packet loss typically occurs |
loss typically occurs at |
typically occurs at two |
occurs at two points |
at two points in |
two points in an |
points in an end |
end communication path between |
communication path between two |
path between two datacenters |
as shown in figure |
i ij the revenue |
ij the revenue density |
the revenue density of |
revenue density of pool |
density of pool i |
area network connecting them |
of pool i at |
network connecting them and |
pool i at the |
connecting them and at |
i at the end |
them and at the |
at the end of |
and at the receiving |
the end of step |
at the receiving end |
end of step t |
of step t is |
step t is its |
t is its revenue |
is its revenue from |
its revenue from direct |
revenue from direct mining |
from direct mining together |
direct mining together with |
loss in the lambda |
mining together with its |
in the lambda link |
together with its revenue |
the lambda link can |
with its revenue from |
lambda link can occur |
its revenue from infiltrated |
link can occur for |
can occur for many |
occur for many reasons |
revenue from infiltrated pools |
divided by the number |
by the number of |
the number of its |
number of its loyal |
of its loyal miners |
its loyal miners together |
loyal miners together with |
miners together with block |
dirty or degraded fiber |
withholding infiltrators that attack |
malfunctioning or misconfigured equipment |
infiltrators that attack it |
low receiver power and |
receiver power and burst |
power and burst switching |
and burst switching contention |
burst switching contention are |
switching contention are some |
contention are some reasons |
loss can also occur |
can also occur at |
also occur at receiving |
occur at receiving endhosts |
at receiving endhosts within |
receiving endhosts within the |
endhosts within the destination |
within the destination datacenter |
shoring up persistent applications |
these are usually cheap |
are usually cheap commodity |
usually cheap commodity machines |
in proceedings of the |
cheap commodity machines prone |
and the revenue vector |
commodity machines prone to |
the revenue vector at |
machines prone to temporary |
revenue vector at step |
prone to temporary overloads |
vector at step t |
to temporary overloads that |
at step t is |
temporary overloads that cause |
step t is hereinafter |
overloads that cause packets |
t is hereinafter we |
that cause packets to |
acm sigmod international conference |
is hereinafter we move |
cause packets to be |
sigmod international conference on |
hereinafter we move to |
packets to be dropped |
international conference on management |
we move to a |
to be dropped by |
conference on management of |
on management of data |
be dropped by the |
move to a static |
dropped by the kernel |
by the kernel in |
the kernel in bursts |
to a static state |
a static state analysis |
static state analysis and |
state analysis and omit |
analysis and omit the |
and omit the t |
omit the t argument |
the t argument in |
t argument in the |
argument in the expressions |
this loss mode occurs |
loss mode occurs with |
mode occurs with udp |
based traffic but not |
traffic but not with |
but not with tcp |
which advertises receiver windows |
advertises receiver windows to |
receiver windows to prevent |
windows to prevent end |
what are typical loss |
are typical loss rates |
typical loss rates on |
loss rates on long |
one source of information |
source of information is |
of information is teragrid |
since the row sums |
the row sums of |
row sums of the |
sums of the infiltration |
of the infiltration matrix |
the infiltration matrix are |
infiltration matrix are smaller |
matrix are smaller than |
are smaller than one |
its largest eigenvalue is |
largest eigenvalue is smaller |
eigenvalue is smaller than |
an optical network interconnecting |
optical network interconnecting major |
network interconnecting major supercomputing |
interconnecting major supercomputing sites |
major supercomputing sites in |
supercomputing sites in the |
sites in the us |
recall that difficulty is |
that difficulty is only |
difficulty is only adjusted |
is only adjusted periodically |
teragrid has a monitoring |
has a monitoring framework |
a monitoring framework within |
monitoring framework within which |
framework within which ten |
and there are transient |
within which ten sites |
there are transient effects |
which ten sites periodically |
are transient effects that |
ten sites periodically send |
transient effects that are |
sites periodically send each |
effects that are not |
periodically send each other |
that are not covered |
are not covered by |
not covered by this |
covered by this stable |
gbps streams of udp |
streams of udp packets |
of udp packets and |
udp packets and measure |
packets and measure the |
and measure the resulting |
measure the resulting loss |
the resulting loss rate |
we discuss this in |
discuss this in section |
this in section viii |
miners miners miners the |
miners miners the revenue |
miners the revenue its |
the revenue its infiltrators |
revenue its infiltrators obtained |
its infiltrators obtained from |
infiltrators obtained from pool |
each site measures the |
site measures the loss |
measures the loss rate |
the loss rate to |
loss rate to every |
rate to every other |
to every other site |
every other site once |
other site once an |
site once an hour |
resulting in a total |
in a total of |
loss rate measurements collected |
rate measurements collected across |
the revenue per loyal |
measurements collected across the |
revenue per loyal pool |
collected across the network |
across the network every |
the network every hour |
miner is therefore r |
controls its infiltration rate |
its infiltration rate of |
infiltration rate of pool |
and will choose the |
will choose the value |
choose the value that |
the value that maximizes |
value that maximizes the |
that maximizes the revenue |
efficient optimistic concurrency control |
maximizes the revenue density |
optimistic concurrency control using |
of all such measurements |
concurrency control using loosely |
all such measurements were |
such measurements were over |
control using loosely synchronized |
using loosely synchronized clocks |
on the first round |
the first round of |
first round of the |
round of the pool |
of the pool game |
the value of r |
is maximized at a |
maximized at a single |
at a single point |
a single point in |
single point in the |
point in the feasible |
in the feasible range |
of them were over |
after eliminating a single |
eliminating a single site |
cannot not react to |
not react to pool |
that dropped incoming packets |
dropped incoming packets steadily |
incoming packets steadily at |
packets steadily at a |
steadily at a rate |
at a rate of |
this point is the |
point is the stable |
is the stable state |
the stable state of |
stable state of the |
state of the system |
and we denote the |
we denote the value |
denote the value of |
the value of x |
a scalable system for |
scalable system for consistently |
of the remainder were |
the remainder were over |
system for consistently caching |
for consistently caching dynamic |
consistently caching dynamic web |
caching dynamic web data |
and the values of |
the values of the |
values of the corresponding |
of the corresponding revenues |
the corresponding revenues of |
corresponding revenues of the |
revenues of the pools |
of the pools with |
the pools with r |
substituting the stable value |
the stable value x |
these numbers reflect the |
numbers reflect the loss |
reflect the loss rate |
the loss rate experienced |
loss rate experienced for |
rate experienced for udp |
experienced for udp traffic |
for udp traffic on |
udp traffic on an |
traffic on an end |
we obtain the revenues |
obtain the revenues of |
the revenues of the |
revenues of the two |
of the two pools |
end path and may |
all are given in |
path and may not |
a scalable web cache |
are given in figure |
and may not generalize |
scalable web cache consistency |
may not generalize to |
web cache consistency architecture |
not generalize to tcp |
generalize to tcp packets |
sigcomm computer communications review |
we do not know |
to simplify the expressions |
do not know if |
not know if packets |
know if packets were |
if packets were dropped |
packets were dropped within |
were dropped within the |
dropped within the optical |
within the optical network |
the optical network or |
optical network or at |
network or at intermediate |
or at intermediate devices |
at intermediate devices within |
intermediate devices within either |
devices within either datacenter |
though it s unlikely |
it s unlikely that |
s unlikely that they |
unlikely that they were |
that they were dropped |
they were dropped at |
no attack if no |
were dropped at the |
attack if no pool |
dropped at the end |
if no pool engages |
no pool engages in |
pool engages in block |
engages in block withholding |
many of the mea |
surements lost just one |
lost just one or |
just one or two |
one or two packets |
or two packets whereas |
two packets whereas kernel |
nic losses are known |
losses are known to |
are known to be |
known to be bursty |
and we have i |
loss occurred on paths |
occurred on paths where |
on paths where levels |
paths where levels of |
where levels of optical |
levels of optical link |
of optical link utilization |
each miner s revenue |
miner s revenue is |
s revenue is proportional |
based cache management for |
revenue is proportional to |
cache management for dynamic |
management for dynamic web |
for dynamic web content |
is proportional to its |
proportional to its power |
be it in a |
it in a pool |
in a pool or |
were consistently lower than |
a pool or working |
pool or working solo |
o ne attacker we |
ne attacker we begin |
attacker we begin our |
we begin our analysis |
begin our analysis with |
our analysis with a |
ruling out congestion as |
out congestion as a |
congestion as a possible |
as a possible cause |
analysis with a simplified |
with a simplified game |
a simplified game of |
simplified game of two |
game of two pools |
a conclusion supported by |
conclusion supported by dialogue |
supported by dialogue with |
by dialogue with the |
dialogue with the network |
with the network administrators |
alternative architectures and protocols |
architectures and protocols for |
and protocols for providing |
protocols for providing strong |
for providing strong consistency |
providing strong consistency in |
strong consistency in dynamic |
consistency in dynamic web |
in dynamic web applications |
world wide web journal |
points are provided by |
are provided by the |
provided by the back |
bone networks of tier |
miners outside both pools |
outside both pools mine |
both pools mine solo |
global crossing reports average |
crossing reports average loss |
reports average loss rates |
average loss rates between |
or with closed pools |
with closed pools that |
closed pools that do |
pools that do not |
that do not attack |
do not attack and |
not attack and cannot |
attack and cannot be |
and cannot be attacked |
this scenario is illustrated |
scenario is illustrated in |
is illustrated in figure |
the dashed red arrow |
dashed red arrow indicates |
red arrow indicates that |
arrow indicates that x |
on four of its |
four of its six |
of its six inter |
s mining power infiltrates |
mining power infiltrates pool |
haul links for the |
links for the month |
for the month of |
the month of december |
with a block withholding |
a block withholding attack |
does not engage in |
not engage in block |
engage in block withholding |
all of its m |
loyal miners work on |
miners work on its |
work on its behalf |
qwest reports loss rates |
reports loss rates of |
on the other hand |
the other hand does |
other hand does not |
hand does not employ |
does not employ x |
of its loyal miners |
cache coherence in distributed |
coherence in distributed systems |
and its direct mining |
its direct mining power |
direct mining power is |
mining power is only |
power is only m |
in either direction on |
either direction on its |
direction on its trans |
pacific link for the |
link for the same |
the bitcoin system normalizes |
for the same month |
bitcoin system normalizes these |
system normalizes these rates |
normalizes these rates by |
these rates by the |
rates by the total |
by the total number |
the total number of |
total number of miners |
number of miners that |
of miners that publish |
miners that publish full |
that publish full proofs |
namely all miners but |
all miners but x |
we expect privately managed |
expect privately managed lambdas |
privately managed lambdas to |
managed lambdas to exhibit |
lambdas to exhibit higher |
to exhibit higher loss |
exhibit higher loss rates |
higher loss rates due |
loss rates due to |
rates due to the |
due to the inherent |
to the inherent trade |
the pools direct revenues |
pools direct revenues are |
direct revenues are therefore |
revenues are therefore m |
equipment quality and cost |
a global cache coherent |
global cache coherent file |
cache coherent file system |
as well as the |
well as the difficulty |
as the difficulty of |
the difficulty of performing |
difficulty of performing routine |
of performing routine maintenance |
performing routine maintenance on |
routine maintenance on longdistance |
maintenance on longdistance links |
end paths as dropping |
paths as dropping packets |
as dropping packets at |
dropping packets at rates |
packets at rates of |
a distributed memory object |
distributed memory object caching |
memory object caching system |
divides its revenue among |
its revenue among its |
revenue among its loyal |
among its loyal miners |
to capture a wide |
its loyal miners and |
capture a wide range |
loyal miners and the |
a wide range of |
miners and the miners |
wide range of deployed |
range of deployed networks |
and the miners that |
the miners that infiltrated |
miners that infiltrated it |
its revenue density is |
revenue density is therefore |
existing reliability options tcp |
density is therefore r |
ip is the default |
is the default reliable |
the default reliable communication |
default reliable communication option |
reliable communication option for |
communication option for contemporary |
option for contemporary networked |
for contemporary networked applications |
exclusive embeddings in commodity |
embeddings in commodity operating |
in commodity operating systems |
commodity operating systems and |
operating systems and networking |
systems and networking apis |
most applications requiring reliable |
applications requiring reliable communication |
requiring reliable communication over |
reliable communication over any |
communication over any form |
distributed data structures for |
over any form of |
data structures for internet |
any form of network |
structures for internet service |
form of network use |
for internet service construction |
of network use tcp |
in proceedings of the |
th conference on symposium |
conference on symposium on |
on symposium on operating |
symposium on operating system |
on operating system design |
the problem with commodity |
problem with commodity tcp |
ip uses positive acknowledgments |
uses positive acknowledgments and |
positive acknowledgments and retransmissions |
acknowledgments and retransmissions to |
and retransmissions to ensure |
retransmissions to ensure reliability |
to ensure reliability the |
ensure reliability the sender |
reliability the sender buffers |
the sender buffers packets |
sender buffers packets until |
buffers packets until their |
packets until their receipt |
until their receipt is |
their receipt is acknowledged |
receipt is acknowledged by |
is acknowledged by the |
acknowledged by the receiver |
and resends if an |
resends if an acknowledgment |
if an acknowledgment is |
an acknowledgment is not |
acknowledgment is not received |
is not received within |
not received within some |
received within some time |
within some time period |
a lost packet is |
lost packet is received |
packet is received in |
is received in the |
received in the form |
in the form of |
game progress bitcoin network |
the form of a |
progress bitcoin network figure |
form of a retransmission |
of a retransmission that |
a retransmission that arrives |
retransmission that arrives no |
that arrives no earlier |
arrives no earlier than |
rtts after the original |
after the original send |
the original send event |
we obtain the expression |
obtain the expression for |
the expression for r |
the sender has to |
sender has to buffer |
has to buffer each |
to buffer each packet |
buffer each packet until |
each packet until it |
packet until it s |
until it s acknowledged |
rtt in lossless operation |
and it has to |
it has to perform |
has to perform additional |
to perform additional work |
perform additional work to |
additional work to retransmit |
work to retransmit the |
to retransmit the packet |
retransmit the packet if |
the packet if it |
packet if it does |
if it does not |
it does not receive |
does not receive the |
not receive the acknowledgment |
any packets that arrive |
packets that arrive with |
that arrive with higher |
arrive with higher sequence |
with higher sequence numbers |
higher sequence numbers than |
sequence numbers than that |
numbers than that of |
than that of a |
that of a lost |
of a lost packet |
a lost packet must |
lost packet must be |
packet must be queued |
must be queued while |
be queued while the |
queued while the receiver |
while the receiver waits |
the receiver waits for |
receiver waits for the |
waits for the lost |
for the lost packet |
the lost packet to |
lost packet to arrive |
throughput financial banking application |
financial banking application running |
banking application running in |
application running in a |
running in a datacenter |
in a datacenter in |
a datacenter in new |
datacenter in new york |
divides its revenue among |
in new york city |
its revenue among its |
revenue among its registered |
among its registered miners |
sending updates to a |
updates to a sister |
to a sister site |
a sister site in |
the revenue includes both |
sister site in switzerland |
revenue includes both its |
includes both its direct |
both its direct mining |
its direct mining revenue |
direct mining revenue and |
mining revenue and b |
the rtt value between |
rtt value between these |
value between these two |
between these two centers |
these two centers is |
two centers is typically |
numerical analysis we analyze |
analysis we analyze this |
we analyze this game |
analyze this game numerically |
this game numerically by |
game numerically by finding |
numerically by finding the |
by finding the x |
and substituting this value |
substituting this value for |
this value for r |
in the case of |
the case of a |
case of a lost |
of a lost packet |
tier database caching for |
database caching for e |
all packets received within |
packets received within the |
we vary the sizes |
vary the sizes of |
the sizes of the |
sizes of the pools |
of the pools through |
the pools through the |
in international conference on |
pools through the entire |
international conference on management |
through the entire feasible |
conference on management of |
on management of data |
the entire feasible range |
milliseconds between the original |
entire feasible range and |
between the original packet |
feasible range and depict |
the original packet send |
range and depict the |
original packet send and |
packet send and the |
send and the a |
and depict the optimal |
depict the optimal x |
and the corresponding revenues |
the corresponding revenues in |
corresponding revenues in figure |
h ets are generated |
each point in each |
ets are generated from |
point in each graph |
are generated from alternate |
in each graph represents |
generated from alternate disjoint |
each graph represents the |
from alternate disjoint sub |
graph represents the equilibrium |
represents the equilibrium point |
the equilibrium point of |
equilibrium point of a |
point of a game |
streams of data rather |
of a game with |
of data rather than |
data rather than from |
a game with the |
rather than from consecutive |
than from consecutive packets |
game with the corresponding |
with the corresponding m |
with an interleave index |
an interleave index of |
where we normalize m |
the encoder would a |
the top right half |
top right half of |
right half of the |
half of the range |
of the range in |
g create correction packets |
the range in all |
create correction packets separately |
range in all graphs |
correction packets separately from |
in all graphs is |
packets separately from three |
separately from three disjoint |
from three disjoint sub |
all graphs is not |
graphs is not feasible |
consistent and scalable cache |
and scalable cache replication |
scalable cache replication for |
cache replication for multi |
as the sum of |
the sum of m |
the first containing data |
first containing data packets |
containing data packets numbered |
data packets numbered a |
packets numbered a c |
numbered a c e |
a c e g |
c e g x |
e g x x |
we use this range |
use this range as |
this range as a |
range as a reference |
as a reference color |
and we use a |
we use a dashed |
use a dashed line |
a dashed line to |
dashed line to show |
line to show the |
to show the bound |
show the bound between |
the bound between this |
bound between this value |
between this value within |
this value within the |
value within the feasible |
within the feasible range |
a shows the optimal |
shows the optimal infiltration |
the optimal infiltration rate |
in the entire feasible |
the entire feasible range |
entire feasible range we |
feasible range we see |
range we see that |
we see that pool |
chooses a strictly positive |
a strictly positive value |
strictly positive value for |
positive value for x |
the second with data |
second with data packets |
with data packets numb |
data packets numb d |
packets numb d f |
numb d f h |
d f h x |
f h x x |
h x x bered |
the revenue of pool |
is depicted in figure |
consistent and scalable caching |
and scalable caching in |
scalable caching in multitier |
caching in multitier architectures |
b and in the |
and in the entire |
in the entire feasible |
the entire feasible region |
entire feasible region it |
the international journal on |
feasible region it is |
region it is strictly |
it is strictly larger |
is strictly larger than |
international journal on very |
journal on very large |
on very large data |
very large data bases |
which the pool would |
the pool would have |
pool would have gotten |
would have gotten without |
have gotten without attacking |
and the third with |
the third with data |
third with data b |
c depicts the revenue |
depicts the revenue of |
the revenue of pool |
which is strictly smaller |
is strictly smaller than |
in the entire range |
note that the total |
that the total system |
the total system mining |
total system mining power |
system mining power is |
mining power is reduced |
power is reduced when |
is reduced when pool |
chooses to infiltrate pool |
the revenue of third |
revenue of third parties |
miners not in either |
not in either pool |
interleaving adds burst tolerance |
adds burst tolerance to |
burst tolerance to fec |
tolerance to fec but |
to fec but exacerbates |
fec but exacerbates its |
but exacerbates its sensitivfigure |
improving application throughput with |
application throughput with enterprise |
throughput with enterprise javabeans |
with enterprise javabeans caching |
separate encoding for ity |
encoding for ity to |
for ity to sending |
in international conference on |
ity to sending rate |
international conference on distributed |
to sending rate with |
therefore pays for the |
conference on distributed computing |
sending rate with an |
pays for the increased |
on distributed computing systems |
rate with an interleave |
for the increased revenue |
with an interleave index |
the increased revenue of |
an interleave index of |
increased revenue of its |
interleave index of i |
revenue of its attacker |
index of i and |
of its attacker and |
of i and an |
its attacker and everyone |
i and an encoding |
attacker and everyone else |
and an encoding rate |
and everyone else in |
an encoding rate of |
everyone else in the |
else in the system |
implications to the general |
to the general case |
the general case consider |
general case consider the |
case consider the case |
consider the case of |
the case of p |
case of p pools |
the sender would have |
sender would have to |
would have to wait |
have to wait for |
to wait for odd |
for any choice of |
wait for odd and |
any choice of the |
for odd and even |
choice of the pools |
odd and even packets |
of the pools sizes |
the pools sizes m |
and even packets i |
packets before sending any |
before sending any redundancy |
sending any redundancy information |
currency serializability for middle |
at least one pool |
receipt of its retransmission |
least one pool will |
of its retransmission have |
tier caching and replication |
one pool will choose |
its retransmission have to |
pool will choose to |
retransmission have to be |
in international conference on |
will choose to perform |
have to be buffered |
international conference on management |
choose to perform block |
to be buffered at |
conference on management of |
to perform block withholding |
be buffered at the |
on management of data |
buffered at the rethese |
at the rethese two |
the rethese two obstacles |
rethese two obstacles to |
two obstacles to using |
obstacles to using fec |
to using fec in |
using fec in time |
tings rate sensitivity and |
rate sensitivity and burst |
sensitivity and burst susceptibility |
and burst susceptibility are |
burst susceptibility are innotice |
susceptibility are innotice that |
are innotice that for |
innotice that for this |
that for this commonplace |
for this commonplace scenario |
the loss of terlinked |
loss of terlinked through |
of terlinked through the |
terlinked through the tuning |
through the tuning knobs |
an interleave of i |
interleave of i and |
of i and a |
i and a single |
and a single packet |
a single packet stops |
single packet stops all |
packet stops all traffic |
stops all traffic in |
all traffic in the |
traffic in the channel |
in the channel to |
the channel to the |
channel to the apa |
to the apa rate |
the apa rate of |
a ppendix we now |
ppendix we now prove |
we now prove theorem |
provides tolerance to a |
tolerance to a burst |
to a burst of |
a burst of up |
burst of up to |
of up to c |
up to c i |
to c i plication |
c i plication for |
i plication for a |
plication for a seventh |
for a seventh of |
a seventh of a |
seventh of a second |
cache with unbounded cache |
a sequence of such |
with unbounded cache size |
sequence of such consecutive |
unbounded cache size and |
of such consecutive packets |
cache size and unbounded |
size and unbounded dependency |
and unbounded dependency lists |
unbounded dependency lists implements |
dependency lists implements cache |
the burst tolerance of |
burst tolerance of blocks |
tolerance of blocks can |
of blocks can have |
blocks can have devastating |
since we assume that |
can have devastating effect |
we assume that the |
have devastating effect on |
assume that the transactional |
devastating effect on a |
that the transactional db |
effect on a high |
the transactional db is |
transactional db is serializable |
throughput an fec code |
an fec code can |
the operations in an |
fec code can be |
operations in an execution |
code can be changed |
in an execution of |
can be changed by |
an execution of update |
be changed by modulating |
execution of update transactions |
changed by modulating either |
of update transactions update |
by modulating either the |
update transactions update can |
modulating either the c |
transactions update can be |
either the c system |
update can be serialized |
the c system where |
can be serialized as |
c system where every |
be serialized as some |
system where every spare |
where every spare cycle |
every spare cycle counts |
serialized as some serial |
as some serial execution |
the next claim trivially |
next claim trivially follows |
in applior the i |
applior the i parameters |
claim trivially follows from |
trivially follows from the |
follows from the definition |
from the definition of |
the definition of the |
increasing c enhances burst |
definition of the database |
c enhances burst tolercations |
of the database dependency |
enhances burst tolercations with |
the database dependency list |
burst tolercations with many |
tolercations with many fine |
database dependency list specification |
a lost packet ance |
lost packet ance at |
packet ance at the |
ance at the cost |
at the cost of |
the cost of network |
if is a serialization |
cost of network and |
is a serialization of |
of network and encoding |
a serialization of the |
network and encoding overhead |
serialization of the update |
of the update transactions |
the update transactions of |
update transactions of an |
transactions of an execution |
of an execution update |
potencan potentially trigger a |
potentially trigger a butterfly |
trigger a butterfly effect |
a butterfly effect of |
butterfly effect of missed |
effect of missed deadtially |
of missed deadtially worsening |
at every step in |
missed deadtially worsening the |
deadtially worsening the packet |
worsening the packet loss |
the packet loss experienced |
packet loss experienced and |
the version dependencies of |
loss experienced and reducing |
version dependencies of every |
experienced and reducing lines |
dependencies of every object |
and reducing lines along |
of every object match |
reducing lines along a |
every object match those |
lines along a distributed |
along a distributed workflow |
object match those stored |
match those stored in |
those stored in its |
stored in its dependency |
in its dependency list |
we first describe a |
increasing i trades off |
first describe a routine |
i trades off recovery |
describe a routine for |
trades off recovery periods |
a routine for placing |
off recovery periods market |
routine for placing a |
recovery periods market crashes |
for placing a read |
periods market crashes at |
market crashes at stock |
crashes at stock exchanges |
only transaction from a |
transaction from a cache |
from a cache server |
christmas latency for better |
a cache server in |
latency for better burst |
cache server in a |
for better burst tolerance |
server in a serialization |
better burst tolerance without |
in a serialization of |
burst tolerance without adding |
a serialization of a |
tolerance without adding overhead |
serialization of a subset |
without adding overhead sales |
of a subset of |
adding overhead sales at |
overhead sales at online |
sales at online stores |
to form a serialization |
winter storms at air |
form a serialization of |
a serialization of both |
serialization of both the |
of both the update |
both the update transaction |
traffic control as mentioned |
the update transaction and |
update transaction and the |
transaction and the read |
for higher values of |
higher values of i |
the encoder has to |
encoder has to centers |
has to centers overloaded |
to centers overloaded networks |
centers overloaded networks and |
overloaded networks and end |
hosts can exhibit wait |
can exhibit wait for |
exhibit wait for more |
wait for more data |
for more data packets |
more data packets to |
data packets to be |
packets to be transmitted |
to be transmitted before |
be transmitted before it |
transmitted before it can |
before it can continuous |
it can continuous packet |
can continuous packet loss |
with each lost packet |
each lost packet driving |
lost packet driving the |
packet driving the send |
driving the send error |
the send error correction |
send error correction packets |
system further and further |
further and further out |
and further out of |
further out of sync |
out of sync with |
of sync with respect |
sync with respect to |
with respect to its |
respect to its importantly |
once the fec encoding |
the fec encoding is |
fec encoding is parameterized |
encoding is parameterized real |
with a rate and |
performing this permutation is |
a rate and an |
this permutation is one |
rate and an interleave |
permutation is one step |
and an interleave to |
is one step of |
stable state where only |
an interleave to tolerate |
one step of the |
state where only pool |
interleave to tolerate a |
step of the routine |
to tolerate a certain |
tolerate a certain burst |
a certain burst sensitive |
certain burst sensitive flow |
burst sensitive flow control |
we repeat this step |
repeat this step forming |
this step forming a |
step forming a series |
forming a series of |
a series of permutations |
ip is unable to |
each permutation is a |
is unable to distinguish |
permutation is a serialization |
unable to distinguish length |
is a serialization of |
to distinguish length b |
a serialization of update |
and each permutes a |
each permutes a range |
permutes a range of |
a range of the |
range of the transactions |
of the transactions with |
the transactions with respect |
transactions with respect to |
with respect to the |
respect to the previous |
to the previous step |
in each step the |
each step the right |
step the right end |
the right end of |
right end of the |
end of the range |
of the range is |
the range is earlier |
range is earlier than |
is earlier than in |
earlier than in the |
than in the previous |
in the previous step |
as one or more |
one or more of |
or more of the |
more of the objects |
of the objects is |
two pools where one |
the objects is closer |
to between ephemeral loss |
pools where one infiltrates |
objects is closer to |
between ephemeral loss modes |
where one infiltrates the |
is closer to the |
ephemeral loss modes due |
one infiltrates the other |
closer to the value |
loss modes due to |
to the value read |
modes due to transient |
optimal infiltration rate x |
the value read by |
due to transient contolerate |
value read by t |
to transient contolerate a |
transient contolerate a burst |
contolerate a burst of |
a burst of length |
eventually we therefore reach |
we therefore reach a |
therefore reach a permutation |
reach a permutation where |
a permutation where at |
permutation where at the |
where at the chosen |
at the chosen time |
the chosen time all |
chosen time all read |
time all read objects |
all read objects are |
read objects are at |
objects are at their |
are at their correct |
at their correct versions |
as a function of |
a function of pool |
we place t there |
function of pool sizes |
place t there to |
t there to obtain |
there to obtain the |
to obtain the desired |
obtain the desired serialization |
the desired serialization of |
desired serialization of the |
all losses occurring gestion |
serialization of the update |
of the update transactions |
the update transactions and |
update transactions and t |
or dirty fiber and |
dirty fiber and persistent |
fiber and persistent in |
permutation routine let be |
and persistent in bursts |
routine let be an |
persistent in bursts of |
let be an execution |
in bursts of size |
be an execution of |
bursts of size less |
an execution of the |
execution of the t |
of size less than |
and the lines in |
size less than or |
less than or equal |
than or equal to |
or equal to b |
equal to b are |
to b are recovered |
b are recovered with |
are recovered with congestion |
and denote by update |
denote by update the |
by update the projection |
update the projection of |
the projection of on |
the loss of one |
projection of on the |
loss of one packet |
of on the set |
show the revenue density |
of one packet out |
on the set of |
the revenue density of |
one packet out of |
the set of database |
packet out of ten |
set of database update |
of database update transactions |
out of ten thousand |
of ten thousand is |
ten thousand is sufficient |
thousand is sufficient to |
is sufficient to reduce |
transaction t reads objects |
t reads objects o |
sufficient to reduce tcp |
in a system with |
a system with p |
system with p pools |
ip throughput to a |
throughput to a third |
to a third of |
a third of its |
third of its the |
of its the same |
its the same latency |
the same latency and |
same latency and this |
latency and this latency |
and this latency depends |
this latency depends on |
latency depends on the |
depends on the i |
on the i palossless |
the i palossless maximum |
if one packet is |
is not an equilibrium |
one packet is lost |
packet is lost out |
is lost out of |
on with versions v |
lost out of a |
out of a thousand |
assume towards negation this |
towards negation this is |
negation this is not |
this is not the |
is not the case |
we d like to |
d like to parameterize |
like to parameterize the |
to parameterize the encoding |
parameterize the encoding to |
the encoding to tolerate |
encoding to tolerate a |
to tolerate a maximum |
tolerate a maximum burst |
a maximum burst length |
maximum burst length and |
burst length and then |
is an equilibrium point |
length and then have |
and then have recovthroughput |
then have recovthroughput collapses |
have recovthroughput collapses to |
now consider a setting |
recovthroughput collapses to a |
consider a setting with |
collapses to a thirtieth |
a setting with only |
take any serialization of |
to a thirtieth of |
setting with only pools |
any serialization of update |
a thirtieth of the |
thirtieth of the maximum |
one exists according to |
exists according to claim |
ery latency depend on |
latency depend on the |
depend on the actual |
on the actual burstiness |
the actual burstiness of |
actual burstiness of the |
and treat the other |
burstiness of the loss |
treat the other pools |
the other pools as |
and consider the first |
other pools as independent |
pools as independent miners |
consider the first time |
at the same time |
the first time when |
this is the setting |
first time when all |
we would like the |
time when all the |
is the setting analyzed |
would like the encoding |
when all the objects |
the setting analyzed above |
like the encoding to |
all the objects the |
setting analyzed above and |
the encoding to have |
the objects the transaction |
analyzed above and we |
encoding to have a |
objects the transaction reads |
above and we have |
the transaction reads are |
and we have seen |
transaction reads are at |
we have seen there |
reads are at a |
have seen there that |
are at a version |
seen there that pool |
at a version at |
a version at least |
version at least as |
at least as large |
least as large as |
as large as the |
can increase its revenue |
large as the versions |
increase its revenue by |
as the versions that |
fec constant rate for |
its revenue by performing |
the versions that t |
constant rate for network |
revenue by performing a |
versions that t reads |
rate for network provisioning |
by performing a block |
for network provisioning and |
performing a block withholding |
network provisioning and stability |
a block withholding attack |
at this time at |
block withholding attack on |
this time at least |
withholding attack on pool |
time at least one |
at least one object |
least one object read |
one object read by |
object read by t |
an fec scheme is |
fec scheme is required |
scheme is required where |
is required where latency |
the last written according |
required where latency of |
last written according to |
s infiltration rate by |
where latency of fec |
infiltration rate by x |
latency of fec encoders |
has the correct version |
of fec encoders are |
fec encoders are typically |
encoders are typically parameterized |
are typically parameterized with |
typically parameterized with an |
but others might not |
assume without loss of |
without loss of generality |
loss of generality that |
of generality that the |
generality that the last |
that the last version |
the last version written |
recovery degrades gracefully as |
last version written is |
degrades gracefully as losses |
version written is vn |
gracefully as losses get |
written is vn of |
as losses get burstier |
is vn of object |
take this values back |
vn of object on |
this values back to |
of object on at |
values back to the |
even tuple for each |
object on at step |
back to the setting |
tuple for each outgoing |
on at step t |
to the setting at |
for each outgoing sequence |
at step t of |
the setting at hand |
each outgoing sequence of |
setting at hand with |
outgoing sequence of r |
at hand with p |
sequence of r data |
denote by t the |
hand with p pools |
of r data packets |
by t the latest |
t the latest time |
the latest time at |
latest time at which |
the revenue of pool |
a as the encoding |
time at which a |
as the encoding overhead |
at which a wrong |
the encoding overhead stays |
which a wrong version |
is better when x |
encoding overhead stays constant |
not the one read |
the one read by |
one read by t |
c data and error |
data and error correction |
and error correction packets |
error correction packets are |
correction packets are sent |
and assume wlog it |
assume wlog it is |
wlog it is version |
it is version vn |
k of object on |
redundancy information cannot be |
information cannot be generated |
cannot be generated and |
be generated and sent |
generated and sent until |
and sent until all |
sent until all r |
rather than the desired |
until all r data |
than the desired version |
all r data packets |
the desired version vn |
r data packets are |
data packets are available |
packets are available for |
are available for sending |
the latency of packet |
latency of packet recovery |
of packet recovery is |
packet recovery is determined |
we now describe a |
recovery is determined by |
now describe a single |
is determined by the |
describe a single step |
determined by the rate |
a single step of |
by the rate at |
single step of the |
the rate at which |
step of the routine |
rate at which the |
at which the sender |
which the sender transmits |
the sender transmits data |
consider the transactions between |
the transactions between t |
transactions between t and |
between t and t |
generating error correction packets |
maelstrom design and implemenfrom |
design and implemenfrom less |
and implemenfrom less than |
implemenfrom less than r |
name size discusfish antpool |
divide these transactions into |
less than r data |
size discusfish antpool ghash |
these transactions into three |
than r data packets |
transactions into three sets |
r data packets at |
io btchine btcguild eligius |
data packets at the |
btchine btcguild eligius others |
packets at the sender |
at the sender is |
the sender is not |
sender is not a |
transactions dependent on the |
is not a viable |
dependent on the transaction |
not a viable tation |
on the transaction at |
a viable tation option |
the transaction at t |
viable tation option even |
tation option even though |
option even though the |
even though the data |
though the data rate |
the data rate in |
data rate in this |
rate in this channel |
in this channel is |
this channel is low |
transactions on which t |
on which t is |
which t is dependent |
or network could be |
network could be operating |
could be operating at |
be operating at near |
operating at near full |
at near full capacity |
near full capacity with |
full capacity with data |
capacity with data from |
with data from other |
data from other senders |
we describe the maelstrom |
transactions that do not |
describe the maelstrom appliance |
that do not belong |
the maelstrom appliance as |
do not belong to |
maelstrom appliance as a |
not belong to either |
appliance as a single |
belong to either group |
as a single machine |
a single machine fec |
single machine fec is |
machine fec is also |
fec is also very |
the following lemma states |
is also very susceptible |
following lemma states that |
also very susceptible to |
lemma states that there |
very susceptible to bursty |
states that there is |
susceptible to bursty losses |
that there is no |
there is no dependency |
is no dependency among |
no dependency among objects |
dependency among objects in |
among objects in sets |
and hence there is |
hence there is no |
there is no intersection |
is no intersection between |
no intersection between the |
intersection between the sets |
we will show how |
will show how more |
show how more machines |
how more machines can |
more machines can be |
machines can be added |
can be added to |
be added to terleaving |
if they were dependent |
is a standard encoding |
a standard encoding technique |
then version vn of |
standard encoding technique used |
version vn of object |
encoding technique used the |
vn of object on |
technique used the appliance |
of object on depends |
used the appliance to |
object on depends on |
the appliance to balance |
on depends on version |
appliance to balance encoding |
depends on version vn |
to balance encoding load |
balance encoding load and |
encoding load and scale |
load and scale to |
and scale to multo |
scale to multo combat |
to multo combat bursty |
multo combat bursty loss |
k of object on |
where error correction pack |
tiple gigabits per second |
and this dependency is |
gigabits per second of |
this dependency is reflected |
per second of traffic |
dependency is reflected in |
is reflected in their |
reflected in their t |
a b c d |
b c d x |
c d x x |
d x x e |
x x e f |
x e f g |
e f g h |
f g h x |
because they are unbounded |
g h x x |
h x x appliance |
transaction t has read |
t has read version |
has read version vn |
which is older than |
is older than vn |
the read of the |
read of the stale |
of the stale version |
the stale version vn |
would have been detected |
have been detected by |
been detected by t |
cache and the transaction |
and the transaction would |
the transaction would have |
transaction would have been |
would have been aborted |
therefore the assumption is |
the assumption is wrong |
lan mtu lambda jumbo |
mtu lambda jumbo mtu |
lambda jumbo mtu recipe |
and the sets are |
the sets are indeed |
sets are indeed independent |
jumbo mtu recipe list |
perhaps an empty set |
is unrelated to sets |
we therefore switch sets |
the six largest open |
six largest open pool |
largest open pool sizes |
maintaining a serialization of |
open pool sizes as |
a serialization of update |
pool sizes as of |
sizes as of january |
consider the following serialization |
xi denotes a transaction |
denotes a transaction x |
a transaction x in |
transaction x in set |
x in set i |
cache consistency we proceed |
consistency we proceed to |
we proceed to prove |
proceed to prove theorem |
their optimal infiltration rates |
let be an execution |
be an execution of |
an execution of the |
of each pool as |
execution of the t |
each pool as a |
pool as a fraction |
as a fraction of |
a fraction of its |
fraction of its size |
if it attacked all |
and denote by update |
it attacked all others |
denote by update the |
attacked all others without |
by update the projection |
all others without reciprocation |
update the projection of |
the projection of on |
projection of on the |
of on the set |
on the set of |
the set of database |
set of database update |
of database update transactions |
and their revenue density |
their revenue density when |
revenue density when attacking |
repair packets are injected |
packets are injected into |
are injected into stream |
injected into stream transparently |
can improve its revenue |
improve its revenue by |
its revenue by attacking |
revenue by attacking pool |
basic mechanism the basic |
mechanism the basic operation |
the basic operation of |
basic operation of maelstrom |
operation of maelstrom is |
of maelstrom is shown |
maelstrom is shown in |
is shown in figure |
tm a set of |
attacks is not an |
a set of readonly |
is not an equilibrium |
set of readonly transactions |
it intercepts outgoing data |
not an equilibrium point |
of readonly transactions performed |
intercepts outgoing data packets |
readonly transactions performed through |
outgoing data packets and |
transactions performed through a |
data packets and routes |
performed through a single |
packets and routes them |
through a single t |
and routes them to |
routes them to the |
them to the destination |
case as a test |
as a test case |
to the destination datacenter |
if the read sets |
we take the pool |
generating and injecting fec |
the read sets of |
take the pool distribution |
and injecting fec repair |
read sets of two |
the pool distribution in |
injecting fec repair packets |
sets of two transactions |
pool distribution in january |
fec repair packets into |
of two transactions include |
repair packets into the |
two transactions include the |
packets into the stream |
transactions include the same |
into the stream in |
include the same object |
the stream in their |
the same object o |
stream in their wake |
we say the one |
say the one that |
a repair packet consists |
the one that read |
repair packet consists of |
one that read a |
packet consists of a |
that read a larger |
consists of a recipe |
read a larger version |
of a recipe list |
a larger version of |
a recipe list of |
larger version of o |
recipe list of data |
version of o depends |
list of data packet |
of o depends on |
of data packet identifiers |
o depends on the |
data packet identifiers and |
depends on the other |
packet identifiers and fec |
identifiers and fec information |
and fec information generated |
fec information generated from |
information generated from these |
all transactions access the |
transactions access the same |
access the same cache |
generated from these packets |
and the cache is |
the cache is unbounded |
in the example in |
the example in figure |
we analyze the cases |
analyze the cases where |
the cases where each |
cases where each of |
where each of the |
each of the pools |
of the pools attacks |
values are only replaced |
the pools attacks all |
this information is a |
pools attacks all other |
are only replaced by |
only replaced by newer |
attacks all other open |
information is a simple |
is a simple xor |
all other open pools |
replaced by newer versions |
the size of the |
all of which behave |
so it is easy |
of which behave honestly |
size of the xor |
it is easy to |
of the xor is |
is easy to see |
the xor is equal |
note that attacking all |
easy to see that |
xor is equal to |
that attacking all pools |
to see that there |
is equal to the |
attacking all pools with |
see that there are |
equal to the mtu |
all pools with force |
that there are no |
to the mtu of |
pools with force proportional |
there are no cycles |
the mtu of the |
with force proportional to |
are no cycles such |
mtu of the datacenter |
force proportional to their |
no cycles such that |
of the datacenter network |
proportional to their size |
cycles such that two |
to their size yields |
such that two transactions |
their size yields the |
that two transactions depend |
and to avoid fragmentation |
size yields the same |
two transactions depend on |
to avoid fragmentation of |
yields the same results |
transactions depend on one |
avoid fragmentation of repair |
the same results as |
depend on one another |
fragmentation of repair packets |
same results as attacking |
of repair packets we |
results as attacking a |
repair packets we require |
as attacking a single |
the dependency graph therefore |
packets we require that |
attacking a single pool |
dependency graph therefore describes |
we require that the |
a single pool of |
graph therefore describes a |
require that the mtu |
single pool of their |
therefore describes a partial |
that the mtu of |
pool of their aggregate |
describes a partial order |
the mtu of the |
of their aggregate size |
a partial order of |
mtu of the long |
partial order of the |
order of the read |
plugging in the numbers |
haul network be set |
network be set to |
in the numbers into |
be set to a |
the numbers into the |
set to a slightly |
numbers into the analysis |
and we choose an |
into the analysis above |
to a slightly larger |
we choose an arbitrary |
the analysis above shows |
a slightly larger value |
analysis above shows that |
choose an arbitrary total |
above shows that a |
an arbitrary total ordering |
shows that a larger |
arbitrary total ordering that |
that a larger pool |
this requirement is usually |
total ordering that respects |
a larger pool needs |
requirement is usually satisfied |
ordering that respects this |
larger pool needs to |
is usually satisfied in |
that respects this partial |
pool needs to use |
usually satisfied in practical |
respects this partial order |
needs to use a |
satisfied in practical deployments |
to use a smaller |
use a smaller ratio |
assume wlog the order |
a smaller ratio of |
wlog the order is |
the order is t |
smaller ratio of its |
since gigabit links very |
ratio of its mining |
gigabit links very often |
of its mining power |
links very often use |
its mining power for |
very often use jumbo |
mining power for infiltration |
often use jumbo frames |
power for infiltration and |
use jumbo frames of |
for infiltration and can |
jumbo frames of up |
infiltration and can increase |
frames of up to |
and can increase its |
can increase its revenue |
increase its revenue density |
its revenue density more |
revenue density more than |
density more than a |
more than a small |
than a small pool |
we take an initial |
take an initial arbitrary |
an initial arbitrary serialization |
achieves its optimum attack |
its optimum attack rate |
optimum attack rate at |
of and permute it |
and permute it according |
permute it according to |
it according to the |
according to the route |
to the route above |
the route above to |
route above to place |
above to place t |
while lan networks have |
lan networks have standard |
networks have standard mtus |
of the pool s |
have standard mtus of |
the pool s mining |
pool s mining power |
increasing its revenue by |
its revenue by almost |
the result is a |
result is a permutation |
this amounts to a |
at the receiving datacenter |
amounts to a daily |
to a daily revenue |
a daily revenue increase |
daily revenue increase of |
revenue increase of b |
the appliance examines incoming |
appliance examines incoming repair |
we take all transactions |
examines incoming repair packets |
take all transactions that |
incoming repair packets and |
all transactions that precede |
repair packets and uses |
transactions that precede t |
packets and uses them |
and uses them to |
uses them to recover |
them to recover missing |
to recover missing data |
recover missing data packets |
does not depend on |
not depend on them |
the data packet is |
data packet is injected |
packet is injected transparently |
usd at the exchange |
is injected transparently into |
and place them after |
place them after t |
injected transparently into the |
at the exchange rate |
transparently into the stream |
the exchange rate on |
exchange rate on that |
rate on that date |
into the stream to |
the stream to the |
we call this permutation |
stream to the receiving |
to the receiving end |
this represents a considerable |
represents a considerable increase |
a considerable increase of |
considerable increase of the |
increase of the pools |
of the pools net |
the pools net revenue |
next we place t |
recovered data packets will |
data packets will typically |
packets will typically arrive |
will typically arrive out |
for the smallest pool |
can be placed immediately |
the attack is much |
be placed immediately after |
but this behavior is |
attack is much less |
placed immediately after t |
this behavior is expected |
is much less profitable |
behavior is expected by |
is expected by communication |
expected by communication stacks |
by communication stacks designed |
communication stacks designed for |
to reach the optimum |
we place it there |
reach the optimum it |
stacks designed for the |
place it there to |
the optimum it needs |
designed for the commodity |
it there to form |
optimum it needs almost |
for the commodity internet |
it needs almost a |
needs almost a third |
almost a third of |
a third of its |
flow control while relaying |
third of its power |
control while relaying tcp |
of its power for |
its power for attacking |
is independent of t |
power for attacking but |
for attacking but increases |
attacking but increases its |
but increases its revenue |
increases its revenue density |
then all its preceding |
maelstrom has two flow |
its revenue density by |
all its preceding transactions |
has two flow control |
revenue density by merely |
two flow control modes |
according to the dependency |
to the dependency graph |
are unrelated to t |
and are therefore located |
are therefore located after |
therefore located after it |
the permutations required are |
permutations required are therefore |
required are therefore after |
are therefore after t |
the appliance routes packets |
appliance routes packets through |
routes packets through without |
packets through without modification |
control between the endhosts |
all relevant update transactions |
relevant update transactions are |
update transactions are located |
transactions are located after |
are located after t |
the appliance acts as |
appliance acts as a |
acts as a tcp |
terminating connections and sending |
connections and sending back |
and therefore the permutations |
and sending back acks |
therefore the permutations required |
sending back acks immediately |
the permutations required are |
back acks immediately before |
permutations required are all |
acks immediately before relaying |
required are all after |
are all after t |
immediately before relaying data |
before relaying data on |
relaying data on appliance |
since in all cases |
in all cases the |
all cases the permutations |
cases the permutations are |
this is particularly useful |
the permutations are after |
is particularly useful for |
permutations are after t |
particularly useful for applications |
useful for applications with |
for applications with short |
lived flows that need |
flows that need to |
that need to ramp |
need to ramp up |
to ramp up throughput |
ramp up throughput quickly |
they do not affect |
up throughput quickly and |
do not affect the |
throughput quickly and avoid |
not affect the correctness |
quickly and avoid the |
affect the correctness of |
and avoid the slow |
the correctness of t |
two attacking pools system |
start effects of tcp |
we take the resulting |
ip on a long |
on a long link |
take the resulting permutation |
the resulting permutation that |
resulting permutation that we |
permutation that we call |
the performance advantages of |
performance advantages of splitting |
advantages of splitting longdistance |
of splitting longdistance connections |
splitting longdistance connections into |
longdistance connections into multiple |
connections into multiple hops |
and move all transactions |
into multiple hops are |
move all transactions that |
multiple hops are well |
all transactions that neither |
hops are well known |
transactions that neither t |
depend on to right |
on to right after |
to right after t |
as a function of |
and orthogonal to this |
a function of pool |
orthogonal to this work |
function of pool sizes |
the resulting permutation is |
we are primarily interested |
are primarily interested in |
primarily interested in isolating |
interested in isolating the |
in isolating the impact |
isolating the impact of |
the impact of rapid |
we repeat this process |
impact of rapid and |
repeat this process until |
of rapid and transparent |
this process until we |
rapid and transparent recovery |
process until we place |
and transparent recovery of |
until we place all |
we place all read |
transparent recovery of lost |
recovery of lost packets |
of lost packets by |
lost packets by maelstrom |
packets by maelstrom on |
by maelstrom on tcp |
this is a serialization |
rather than the buffering |
than the buffering and |
the buffering and slow |
is a serialization of |
a serialization of the |
serialization of the update |
of the update transactions |
the update transactions in |
start avoidance benefits of |
update transactions in and |
avoidance benefits of generic |
benefits of generic performance |
transactions in and all |
in and all read |
only transactions that accessed |
transactions that accessed the |
that accessed the same |
in the remainder of |
the remainder of the |
remainder of the paper |
accessed the same cache |
we describe maelstrom with |
describe maelstrom with end |
we have therefore shown |
have therefore shown that |
therefore shown that in |
shown that in any |
that in any execution |
in any execution of |
any execution of t |
cache the update transactions |
the update transactions can |
update transactions can be |
transactions can be serialized |
can be serialized with |
be serialized with readonly |
serialized with readonly transactions |
with readonly transactions that |
readonly transactions that accessed |
transactions that accessed a |
while maelstrom respects end |
that accessed a single |
accessed a single cache |
which means that t |
end flow control connections |
cache implements cache serializability |
or splits them and |
splits them and implements |
them and implements its |
and implements its own |
implements its own proxy |
proxy flow control as |
flow control as described |
control as described above |
t wo p ools |
wo p ools we |
p ools we proceed |
ools we proceed to |
we proceed to analyze |
it is not designed |
proceed to analyze the |
is not designed for |
to analyze the case |
not designed for routinely |
analyze the case where |
designed for routinely congested |
the case where two |
for routinely congested networks |
case where two pools |
where two pools may |
two pools may attack |
pools may attack each |
may attack each other |
the addition of fec |
attack each other and |
addition of fec under |
each other and the |
of fec under tcp |
other and the other |
and the other miners |
the other miners mine |
other miners mine solo |
ip flow control allows |
flow control allows it |
control allows it to |
again we have pool |
allows it to steal |
it to steal bandwidth |
to steal bandwidth from |
steal bandwidth from other |
bandwidth from other competing |
from other competing flows |
other competing flows running |
competing flows running without |
flows running without fec |
running without fec in |
without fec in the |
fec in the link |
though maintaining fairness versus |
maintaining fairness versus similarly |
fairness versus similarly fec |
controls its infiltration rate |
its infiltration rate x |
friendliness with conventional tcp |
also controls its infiltration |
controls its infiltration rate |
ip flows is not |
its infiltration rate x |
flows is not a |
is not a primary |
not a primary protocol |
a primary protocol design |
primary protocol design goal |
protocol design goal on |
design goal on over |
which are often dedicated |
are often dedicated to |
often dedicated to specific |
dedicated to specific highvalue |
to specific highvalue applications |
this scenario is illustrated |
scenario is illustrated in |
is illustrated in figure |
we see evidence for |
see evidence for this |
evidence for this assertion |
for this assertion in |
the total mining power |
this assertion in the |
total mining power in |
assertion in the routine |
mining power in the |
in the routine use |
power in the system |
the routine use of |
in the system is |
routine use of parallel |
use of parallel flows |
the system is m |
system is m x |
and udp blast protocols |
the direct revenues r |
of the pools from |
the pools from mining |
pools from mining are |
from mining are their |
mining are their effective |
are their effective mining |
their effective mining rates |
both in commercial deployments |
in commercial deployments and |
commercial deployments and by |
without infiltrating mining power |
deployments and by researchers |
and by researchers seeking |
by researchers seeking to |
researchers seeking to transfer |
divided by the total |
by the total mining |
seeking to transfer large |
the total mining rate |
to transfer large amounts |
transfer large amounts of |
large amounts of data |
amounts of data over |
of data over high |
layered interleaving in layered |
interleaving in layered interleaving |
an fec protocol with |
fec protocol with rate |
is produced by running |
produced by running c |
by running c multiple |
running c multiple instances |
c multiple instances of |
multiple instances of an |
fec protocol simultaneously with |
protocol simultaneously with increasing |
simultaneously with increasing interleave |
with increasing interleave indices |
increasing interleave indices i |
the total revenue of |
total revenue of each |
revenue of each pool |
of each pool is |
each pool is its |
pool is its direct |
is its direct mining |
its direct mining revenue |
two pools infiltrating each |
pools infiltrating each other |
and the infiltration revenue |
the infiltration revenue from |
infiltration revenue from the |
revenue from the previous |
from the previous round |
which is the attacked |
is the attacked pool |
the attacked pool s |
attacked pool s total |
pool s total revenue |
s total revenue multiplied |
total revenue multiplied by |
revenue multiplied by its |
multiplied by its infiltration |
by its infiltration rate |
the pool s total |
pool s total revenue |
s total revenue is |
total revenue is divided |
revenue is divided among |
is divided among its |
divided among its loyal |
among its loyal miners |
its loyal miners and |
loyal miners and miners |
miners and miners that |
and miners that infiltrated |
miners that infiltrated it |
at stable state this |
stable state this is |
state this is r |
three instances of an |
the first instance with |
first instance with interleave |
instance with interleave i |
the second with interleave |
second with interleave i |
and the third with |
the third with interleave |
third with interleave i |
fec encoding is simply |
encoding is simply an |
is simply an xor |
simply an xor of |
an xor of the |
xor of the r |
of the r data |
the r data packets |
r data packets hence |
in layered interleaving each |
layered interleaving each data |
interleaving each data packet |
each data packet is |
data packet is included |
packet is included in |
is included in c |
included in c xors |
each of which is |
we obtain the following |
of which is generated |
obtain the following closed |
which is generated at |
the following closed expressions |
is generated at different |
following closed expressions for |
closed expressions for each |
generated at different interleaves |
at different interleaves from |
different interleaves from the |
interleaves from the original |
from the original data |
we express the revenues |
the original data stream |
express the revenues as |
the revenues as functions |
revenues as functions of |
as functions of x |
as we shall describe |
we shall describe shortly |
ensures that the c |
that the c xors |
the c xors containing |
c xors containing a |
xors containing a data |
containing a data packet |
a data packet do |
data packet do not |
packet do not have |
do not have any |
not have any other |
data packet in common |
the resulting protocol effectively |
resulting protocol effectively has |
protocol effectively has a |
effectively has a rate |
has a rate of |
with each xor generated |
each xor generated from |
xor generated from r |
generated from r data |
from r data packets |
r data packets and |
data packets and each |
packets and each data |
and each data packet |
each data packet included |
data packet included in |
packet included in c |
included in c xors |
illustrates layered interleaving for |
layered interleaving for a |
each pool controls only |
pool controls only its |
controls only its own |
only its own infiltration |
its own infiltration rate |
in each round of |
each round of the |
round of the pool |
of the pool game |
each pool will optimize |
pool will optimize its |
will optimize its infiltration |
optimize its infiltration rate |
its infiltration rate of |
infiltration rate of the |
rate of the other |
acts at step t |
standard fec schemes can |
it optimizes its revenue |
fec schemes can be |
optimizes its revenue with |
schemes can be made |
its revenue with r |
can be made resistant |
be made resistant to |
made resistant to a |
resistant to a certain |
to a certain loss |
a certain loss burst |
certain loss burst length |
loss burst length at |
burst length at the |
length at the cost |
at the cost of |
the cost of increased |
cost of increased recovery |
of increased recovery latency |
increased recovery latency for |
recovery latency for all |
latency for all lost |
for all lost packets |
including smaller bursts and |
smaller bursts and singleton |
bursts and singleton drops |
layered interleaving provides graceful |
interleaving provides graceful degradation |
provides graceful degradation in |
graceful degradation in the |
degradation in the face |
in the face of |
the face of bursty |
face of bursty loss |
of bursty loss for |
bursty loss for constant |
loss for constant encoding |
for constant encoding overhead |
constant encoding overhead singleton |
encoding overhead singleton random |
overhead singleton random losses |
singleton random losses are |
random losses are recovered |
losses are recovered as |
are recovered as quickly |
recovered as quickly as |
as quickly as possible |
by xors generated with |
xors generated with an |
generated with an interleave |
with an interleave of |
and each successive layer |
each successive layer of |
successive layer of xors |
layer of xors generated |
of xors generated at |
xors generated at a |
generated at a higher |
at a higher interleave |
a higher interleave catches |
higher interleave catches larger |
interleave catches larger bursts |
catches larger bursts missed |
larger bursts missed by |
bursts missed by the |
missed by the previous |
by the previous layer |
the implementation of this |
implementation of this algorithm |
of this algorithm is |
this algorithm is simple |
algorithm is simple and |
is simple and shown |
simple and shown in |
and shown in figure |
a set of repair |
set of repair bins |
of repair bins is |
repair bins is maintained |
bins is maintained for |
is maintained for each |
maintained for each layer |
with i bins for |
i bins for a |
bins for a layer |
for a layer with |
a layer with interleave |
layer with interleave i |
acts at step t |
a repair bin consists |
repair bin consists of |
bin consists of a |
consists of a partially |
of a partially constructed |
it optimizes its revenue |
a partially constructed repair |
optimizes its revenue with |
partially constructed repair packet |
its revenue with x |
an xor and the |
xor and the recipe |
and the recipe list |
the recipe list of |
recipe list of identifiers |
list of identifiers of |
of identifiers of data |
identifiers of data packets |
of data packets that |
data packets that compose |
packets that compose the |
that compose the xor |
each intercepted data packet |
intercepted data packet is |
data packet is added |
packet is added to |
is added to each |
added to each layer |
to each layer where |
each layer where adding |
layer where adding to |
where adding to a |
adding to a layer |
to a layer simply |
a layer simply means |
layer simply means choosing |
simply means choosing a |
means choosing a repair |
choosing a repair bin |
a repair bin from |
repair bin from the |
bin from the layer |
from the layer s |
the layer s set |
incrementally updating the xor |
updating the xor with |
the xor with the |
xor with the new |
with the new data |
the new data packet |
and adding the data |
adding the data packet |
the data packet s |
data packet s header |
packet s header to |
s header to the |
header to the recipe |
to the recipe list |
a counter is incremented |
counter is incremented as |
is incremented as each |
incremented as each data |
as each data packet |
each data packet arrives |
data packet arrives at |
packet arrives at the |
arrives at the appliance |
and choosing the repair |
choosing the repair bin |
the repair bin from |
repair bin from the |
bin from the layer |
an equilibrium exists where |
from the layer s |
equilibrium exists where neither |
the layer s set |
exists where neither pool |
layer s set is |
s set is done |
set is done by |
is done by taking |
done by taking the |
by taking the modulo |
taking the modulo of |
the modulo of the |
can improve its revenue |
modulo of the counter |
improve its revenue by |
of the counter with |
its revenue by changing |
the counter with the |
revenue by changing its |
by changing its infiltration |
changing its infiltration rate |
counter with the number |
with the number of |
the number of bins |
number of bins in |
of bins in each |
bins in each layer |
any pair of values |
pair of values x |
for a layer with |
a layer with interleave |
the xth intercepted packet |
xth intercepted packet is |
intercepted packet is added |
packet is added to |
is added to the |
such that arg maxx |
when a repair bin |
a repair bin fills |
repair bin fills up |
bin fills up its |
fills up its recipe |
up its recipe list |
its recipe list contains |
recipe list contains r |
list contains r data |
contains r data packets |
r data packets it |
data packets it fires |
a repair packet is |
repair packet is generated |
packet is generated consisting |
is generated consisting of |
generated consisting of the |
consisting of the xor |
of the xor and |
the xor and the |
xor and the recipe |
and the recipe list |
the recipe list and |
recipe list and is |
list and is scheduled |
and is scheduled for |
is scheduled for sending |
while the repair bin |
the repair bin is |
repair bin is re |
initialized with an empty |
with an empty recipe |
an empty recipe list |
empty recipe list and |
recipe list and blank |
list and blank xor |
incoming repair packets are |
repair packets are processed |
packets are processed as |
are processed as follows |
if all the data |
all the data packets |
the data packets contained |
data packets contained in |
packets contained in the |
contained in the repair |
in the repair s |
the repair s recipe |
repair s recipe list |
s recipe list have |
recipe list have been |
list have been received |
have been received successfully |
the repair packet is |
repair packet is discarded |
if the repair s |
the repair s recipe |
repair s recipe list |
s recipe list contains |
recipe list contains a |
list contains a single |
contains a single missing |
a single missing data |
single missing data packet |
recovery can occur immediately |
can occur immediately by |
occur immediately by combining |
immediately by combining the |
by combining the xor |
combining the xor in |
the xor in the |
xor in the repair |
in the repair with |
the repair with layer |
the feasible region for |
feasible region for the |
region for the pool |
for the pool sizes |
the pool sizes is |
pool sizes is m |
layer with interleave of |
the revenue function for |
revenue function for ri |
function for ri is |
for ri is concave |
ri is concave in |
is concave in xi |
concave in xi for |
in xi for all |
xi for all feasible |
for all feasible values |
all feasible values of |
feasible values of the |
values of the variables |
therefore the solutions for |
the solutions for equations |
are unique and are |
unique and are either |
and are either at |
are either at the |
either at the borders |
at the borders of |
the borders of the |
borders of the feasible |
of the feasible region |
the feasible region or |
feasible region or where |
region or where ri |
from section v we |
section v we know |
v we know that |
we know that no |
attack is not an |
is not an equilibrium |
not an equilibrium point |
since each pool can |
each pool can increase |
pool can increase its |
can increase its revenue |
increase its revenue by |
its revenue by choosing |
revenue by choosing a |
by choosing a strictly |
choosing a strictly positive |
a strictly positive infiltration |
strictly positive infiltration rate |
is not a solution |
not a solution to |
a solution to equations |
the other successfully received |
other successfully received data |
successfully received data packets |
if the repair contains |
the repair contains multiple |
repair contains multiple missing |
contains multiple missing data |
multiple missing data packets |
nash equilibrium therefore exists |
equilibrium therefore exists with |
therefore exists with x |
it cannot be used |
cannot be used immediately |
be used immediately for |
used immediately for recovery |
immediately for recovery it |
for recovery it is |
recovery it is instead |
it is instead stored |
is instead stored in |
instead stored in a |
stored in a table |
in a table that |
a table that maps |
table that maps missing |
that maps missing data |
maps missing data packets |
missing data packets to |
data packets to repair |
packets to repair packets |
whenever a data packet |
a data packet is |
data packet is subsequently |
packet is subsequently received |
is subsequently received or |
subsequently received or recovered |
this table is checked |
table is checked to |
is checked to see |
checked to see if |
to see if any |
see if any xors |
if any xors now |
any xors now have |
xors now have singleton |
now have singleton losses |
have singleton losses due |
singleton losses due to |
losses due to the |
due to the presence |
to the presence of |
the presence of the |
presence of the new |
of the new packet |
the new packet and |
new packet and can |
packet and can be |
and can be used |
can be used for |
be used for recovering |
used for recovering other |
for recovering other missing |
recovering other missing packets |
xors received from different |
received from different layers |
from different layers interact |
different layers interact to |
layers interact to recover |
interact to recover missing |
to recover missing data |
recover missing data packets |
since an xor received |
an xor received at |
xor received at a |
received at a higher |
at a higher interleave |
a higher interleave can |
higher interleave can recover |
interleave can recover a |
can recover a packet |
recover a packet that |
a packet that makes |
packet that makes an |
that makes an earlier |
makes an earlier xor |
an earlier xor at |
earlier xor at a |
xor at a lower |
at a lower interleave |
a lower interleave usable |
lower interleave usable hence |
though layered interleaving is |
layered interleaving is equivalent |
interleaving is equivalent to |
is equivalent to c |
equivalent to c different |
instances in terms of |
in terms of overhead |
terms of overhead and |
of overhead and design |
its recovery power is |
recovery power is much |
power is much higher |
is much higher and |
much higher and comes |
higher and comes close |
and comes close to |
comes close to standard |
using symbolic computation tools |
we see that there |
see that there is |
that there is a |
there is a single |
is a single pair |
a single pair of |
single pair of values |
pair of values for |
of values for which |
values for which equation |
holds for any feasible |
for any feasible choice |
any feasible choice of |
feasible choice of m |
numerical analysis a numerical |
analysis a numerical analysis |
a numerical analysis confirms |
numerical analysis confirms these |
analysis confirms these observations |
we simulate the pool |
simulate the pool game |
the pool game for |
pool game for a |
game for a range |
for a range of |
a range of pool |
range of pool sizes |
for each choice of |
each choice of pool |
choice of pool sizes |
we start the simulation |
start the simulation when |
the simulation when both |
simulation when both pools |
when both pools do |
both pools do not |
pools do not infiltrate |
do not infiltrate each |
not infiltrate each other |
second set of rsized |
set of rsized xors |
of rsized xors staggered |
rsized xors staggered start |
xors staggered start xors |
and the revenue densities |
the revenue densities are |
revenue densities are r |
at each round one |
each round one pool |
round one pool chooses |
one pool chooses its |
pool chooses its optimal |
chooses its optimal infiltration |
its optimal infiltration rate |
optimal infiltration rate based |
infiltration rate based on |
rate based on the |
based on the pool |
on the pool sizes |
the pool sizes and |
pool sizes and the |
sizes and the rate |
and the rate with |
the rate with which |
rate with which it |
with which it is |
which it is infiltrated |
and we calculate the |
we calculate the revenue |
calculate the revenue after |
the revenue after convergence |
revenue after convergence with |
after convergence with equation |
recall the players in |
the players in the |
players in the pool |
in the pool game |
the pool game are |
pool game are chosen |
game are chosen with |
are chosen with the |
chosen with the round |
with the round robin |
the round robin policy |
so the pools take |
the pools take turns |
and we let the |
we let the game |
let the game run |
the game run until |
game run until convergence |
the results are illustrated |
results are illustrated in |
are illustrated in figure |
each run with some |
run with some m |
values results in a |
results in a single |
in a single point |
a single point in |
single point in each |
point in each graph |
in each graph in |
each graph in figure |
we depict the infiltration |
depict the infiltration rates |
the infiltration rates of |
infiltration rates of both |
rates of both pools |
of both pools x |
b and the pools |
and the pools revenue |
the pools revenue densities |
pools revenue densities r |
for each choice of |
each choice of m |
the values of x |
are the points in |
the points in each |
points in each of |
in each of the |
each of the graphs |
of the graphs with |
the graphs with the |
graphs with the respective |
with the respective coordinates |
j graphs we draw |
graphs we draw a |
we draw a border |
draw a border around |
a border around the |
border around the region |
around the region where |
the region where there |
region where there is |
where there is no |
attack by i in |
by i in equilibrium |
for the ri graphs |
the ri graphs we |
ri graphs we draw |
graphs we draw a |
we draw a line |
draw a line around |
a line around the |
line around the region |
around the region where |
the region where the |
region where the revenue |
where the revenue is |
the revenue is the |
revenue is the same |
is the same as |
the same as in |
same as in the |
as in the no |
we first observe that |
first observe that only |
observe that only in |
that only in extreme |
only in extreme cases |
in extreme cases a |
extreme cases a pool |
cases a pool does |
a pool does not |
pool does not attack |
does not attack its |
not attack its counterpart |
at equilibrium a pool |
equilibrium a pool will |
a pool will refrain |
pool will refrain from |
will refrain from attacking |
refrain from attacking only |
from attacking only if |
attacking only if the |
only if the other |
if the other pool |
the other pool is |
other pool is larger |
pool is larger than |
is larger than about |
of the total mining |
the total mining power |
we observe that a |
observe that a pool |
that a pool improves |
a pool improves its |
pool improves its revenue |
improves its revenue compared |
its revenue compared to |
revenue compared to the |
compared to the no |
attacks scenario only when |
scenario only when it |
only when it controls |
when it controls a |
it controls a strict |
controls a strict majority |
a strict majority of |
strict majority of the |
majority of the total |
of the total mining |
the total mining power |
these are the small |
are the small triangular |
the small triangular regions |
small triangular regions in |
triangular regions in figures |
in the rest of |
the rest of the |
rest of the space |
the trapezoids in the |
trapezoids in the figures |
the revenue of the |
revenue of the pool |
of the pool is |
the pool is inferior |
pool is inferior compared |
is inferior compared to |
inferior compared to the |
compared to the no |
the prisoner s dilemma |
prisoner s dilemma in |
s dilemma in a |
dilemma in a healthy |
in a healthy bitcoin |
a healthy bitcoin environment |
where neither pool controls |
neither pool controls a |
pool controls a strict |
controls a strict majority |
a strict majority of |
strict majority of the |
majority of the mining |
of the mining power |
both pools will earn |
pools will earn less |
will earn less at |
earn less at equilibrium |
less at equilibrium than |
at equilibrium than if |
equilibrium than if both |
than if both pools |
if both pools ran |
both pools ran without |
pools ran without attacking |
we can analyze in |
can analyze in this |
analyze in this case |
in this case a |
this case a game |
case a game where |
a game where each |
game where each pool |
where each pool chooses |
each pool chooses either |
pool chooses either to |
chooses either to attack |
either to attack and |
to attack and optimize |
attack and optimize its |
and optimize its revenue |
or to refrain from |
to refrain from attacking |
without loss of generality |
as we have seen |
we have seen in |
have seen in section |
seen in section v |
can increase its revenue |
increase its revenue above |
does attack but pool |
we denote the revenue |
denote the revenue of |
the revenue of pool |
the exact value of |
exact value of r |
depends on the values |
on the values of |
the values of m |
but it is always |
it is always smaller |
is always smaller than |
always smaller than one |
as we have seen |
we have seen above |
does choose to attack |
but does not surpass |
does not surpass one |
the game is summarized |
game is summarized in |
is summarized in figure |
this is the classical |
is the classical prisoner |
the classical prisoner s |
classical prisoner s dilemma |
attack is the dominant |
is the dominant strategy |
chooses to attack or |
to attack or not |
the revenue of pool |
is larger when attacking |
larger when attacking than |
when attacking than when |
attacking than when refraining |
than when refraining from |
when refraining from attack |
and the same for |
the same for xxx |
same for xxx xxx |
for xxx xxx pool |
no attack xxx pool |
comparison of packet recovery |
of packet recovery probability |
optimizations staggered start for |
staggered start for rate |
limiting in the naive |
in the naive implementation |
the naive implementation of |
naive implementation of the |
implementation of the layered |
of the layered interleaving |
the layered interleaving algorithm |
repair packets are transmitted |
packets are transmitted as |
are transmitted as soon |
transmitted as soon as |
as soon as repair |
soon as repair bins |
as repair bins fill |
repair bins fill and |
bins fill and allow |
fill and allow them |
and allow them to |
allow them to be |
them to be constructed |
all the repair bins |
the repair bins in |
repair bins in a |
bins in a layer |
in a layer fill |
a layer fill in |
layer fill in quick |
fill in quick succession |
the arrival of packets |
prisoner s dilemma for |
s dilemma for two |
dilemma for two pools |
the revenue density of |
revenue density of each |
density of each pool |
of each pool is |
each pool is determined |
pool is determined by |
is determined by the |
determined by the decision |
by the decision of |
the decision of both |
will successively fill the |
decision of both pools |
successively fill the four |
of both pools whether |
fill the four repair |
both pools whether to |
pools whether to attack |
whether to attack or |
to attack or not |
the four repair bins |
four repair bins in |
repair bins in layer |
the dominant strategy of |
dominant strategy of each |
strategy of each player |
of each player is |
each player is to |
player is to attack |
this behavior leads to |
behavior leads to a |
leads to a large |
however the payoff of |
to a large number |
the payoff of both |
a large number of |
payoff of both would |
large number of repair |
of both would be |
number of repair packets |
both would be larger |
of repair packets being |
would be larger if |
repair packets being generated |
be larger if they |
packets being generated and |
larger if they both |
being generated and sent |
if they both refrain |
generated and sent within |
they both refrain from |
both refrain from attacking |
and sent within a |
sent within a short |
within a short period |
a short period of |
short period of time |
which results in undesirable |
results in undesirable overhead |
in undesirable overhead and |
undesirable overhead and traffic |
overhead and traffic spikes |
at equilibrium of this |
equilibrium of this attack |
we would like to |
would like to rate |
when both pools attack |
limit transmissions of repair |
transmissions of repair packets |
of repair packets to |
repair packets to one |
packets to one for |
the revenue of each |
to one for every |
revenue of each pool |
one for every r |
for every r data |
every r data packets |
of each pool is |
each pool is smaller |
pool is smaller than |
is smaller than its |
smaller than its revenue |
than its revenue if |
this problem is fixed |
its revenue if neither |
problem is fixed by |
revenue if neither pool |
is fixed by staggering |
if neither pool attacked |
fixed by staggering the |
by staggering the starting |
staggering the starting sizes |
the starting sizes of |
starting sizes of the |
sizes of the bins |
the game is not |
game is not played |
is not played once |
analogous to the starting |
to the starting positions |
the starting positions of |
starting positions of runners |
positions of runners in |
of runners in a |
runners in a sprint |
the very first time |
very first time bin |
first time bin number |
time bin number x |
bin number x in |
number x in a |
x in a layer |
where each pool can |
in a layer of |
each pool can change |
a layer of interleave |
layer of interleave i |
of interleave i fires |
pool can change its |
can change its strategy |
change its strategy between |
its strategy between attack |
strategy between attack and |
between attack and no |
it does so at |
does so at size |
so at size x |
at size x mod |
size x mod r |
the pools can agree |
to refrain from attacking |
the first repair bin |
first repair bin in |
repair bin in the |
and in each round |
bin in the second |
in each round a |
in the second layer |
the second layer with |
each round a pool |
second layer with interleave |
round a pool can |
a pool can detect |
pool can detect whether |
can detect whether it |
would fire at size |
detect whether it is |
whether it is being |
it is being attacked |
is being attacked and |
being attacked and deduce |
attacked and deduce that |
and deduce that the |
deduce that the other |
the second would fire |
second would fire at |
would fire at size |
that the other pool |
the other pool is |
other pool is violating |
pool is violating the |
is violating the agreement |
cooperation where neither pool |
where neither pool attacks |
neither pool attacks is |
pool attacks is a |
attacks is a possible |
is a possible stable |
a possible stable state |
for the first i |
the first i data |
first i data packets |
i data packets added |
data packets added to |
packets added to a |
added to a layer |
to a layer with |
a layer with interleave |
layer with interleave i |
r fire immediately with |
fire immediately with just |
immediately with just one |
with just one packet |
just one packet in |
one packet in them |
for the next i |
the next i data |
next i data packets |
i data packets added |
despite the fact that |
the fact that the |
r fire immediately with |
fact that the single |
fire immediately with two |
that the single nash |
immediately with two data |
the single nash equilibrium |
with two data packets |
single nash equilibrium in |
nash equilibrium in every |
two data packets in |
equilibrium in every round |
data packets in them |
in every round is |
every round is to |
round is to attack |
and so on until |
so on until r |
on until r i |
until r i data |
r i data packets |
i data packets have |
data packets have been |
packets have been added |
have been added to |
case as an example |
been added to the |
as an example we |
added to the layer |
an example we take |
to the layer and |
example we take again |
the layer and all |
we take again the |
layer and all bins |
take again the pool |
and all bins have |
all bins have fired |
bins have fired exactly |
have fired exactly once |
again the pool sizes |
the pool sizes shown |
pool sizes shown in |
sizes shown in figure |
all bins fire at |
bins fire at size |
fire at size r |
and study the case |
study the case where |
the case where the |
case where the two |
where the two largest |
the two largest pools |
now that they have |
that they have been |
they have been staggered |
have been staggered at |
been staggered at the |
staggered at the start |
the optimal infiltration rates |
r fire for any |
fire for any i |
out of the total |
of the total system |
the total system mining |
total system mining power |
for any i data |
any i data packets |
the outlined scheme works |
outlined scheme works when |
scheme works when i |
works when i is |
when i is greater |
i is greater than |
is greater than or |
greater than or equal |
than or equal to |
or equal to r |
as is usually the |
is usually the case |
if i is smaller |
i is smaller than |
is smaller than r |
the bin with index |
bin with index x |
and the pools would |
the pools would lose |
with index x fires |
index x fires at |
compared to the no |
q i dentical p |
i dentical p ools |
dentical p ools let |
p ools let there |
the initial firing sizes |
ools let there be |
initial firing sizes would |
let there be q |
firing sizes would be |
there be q pools |
be q pools of |
q pools of identical |
pools of identical size |
of identical size that |
for the first bin |
identical size that engage |
the first bin and |
size that engage in |
that engage in block |
engage in block withholding |
in block withholding against |
block withholding against one |
for the second bin |
withholding against one another |
if r and i |
other miners neither attack |
r and i are |
miners neither attack nor |
and i are not |
neither attack nor are |
i are not integral |
attack nor are being |
nor are being attacked |
are not integral multiples |
not integral multiples of |
integral multiples of each |
multiples of each other |
in this case there |
this case there exists |
case there exists a |
there exists a symmetric |
exists a symmetric equilibrium |
limiting still works but |
still works but is |
works but is slightly |
but is slightly less |
is slightly less effective |
without loss of generality |
slightly less effective due |
less effective due to |
effective due to rounding |
due to rounding errors |
a step of pool |
delaying xors in the |
xors in the naive |
in the naive implementation |
it controls its attack |
controls its attack rates |
repair packets are transmitted |
its attack rates each |
packets are transmitted as |
attack rates each of |
are transmitted as soon |
rates each of the |
transmitted as soon as |
each of the other |
as soon as they |
soon as they are |
as they are generated |
of the other pools |
this results in the |
and due to symmetry |
results in the repair |
due to symmetry they |
in the repair packet |
to symmetry they are |
the repair packet leaving |
symmetry they are all |
repair packet leaving immediately |
they are all the |
packet leaving immediately after |
are all the same |
leaving immediately after the |
immediately after the last |
after the last data |
the last data packet |
last data packet that |
data packet that was |
packet that was added |
that was added to |
was added to it |
which lowers burst tolerance |
lowers burst tolerance if |
burst tolerance if the |
tolerance if the repair |
the attack rate of |
if the repair packet |
attack rate of pool |
the repair packet was |
repair packet was generated |
packet was generated at |
was generated at interleave |
generated at interleave i |
against any other pool |
the resulting protocol can |
each of the other |
resulting protocol can tolerate |
of the other pools |
protocol can tolerate a |
the other pools can |
can tolerate a burst |
other pools can attack |
tolerate a burst of |
pools can attack its |
a burst of i |
can attack its peers |
attack its peers as |
burst of i lost |
its peers as well |
of i lost data |
i lost data packets |
lost data packets excluding |
data packets excluding the |
packets excluding the repair |
all attack rates by |
but the burst could |
attack rates by all |
the burst could swallow |
rates by all attackers |
burst could swallow both |
by all attackers are |
could swallow both the |
all attackers are identical |
swallow both the repair |
both the repair and |
the repair and the |
repair and the last |
and the last data |
the last data packet |
last data packet in |
data packet in it |
packet in it as |
in it as they |
it as they are |
as they are not |
they are not separated |
the attack rate of |
are not separated by |
not separated by the |
attack rate of any |
separated by the requisite |
by the requisite interleave |
rate of any pool |
of any pool other |
any pool other than |
the solution to this |
against any other pool |
solution to this is |
to this is simple |
this is simple delay |
is simple delay sending |
simple delay sending the |
delay sending the repair |
sending the repair packet |
the repair packet generated |
repair packet generated by |
packet generated by a |
generated by a repair |
by a repair bin |
a repair bin until |
repair bin until the |
bin until the next |
until the next time |
the next time a |
next time a data |
time a data packet |
a data packet is |
data packet is added |
packet is added to |
is added to the |
added to the now |
to the now empty |
the now empty bin |
the direct revenue of |
which happens i packets |
direct revenue of each |
happens i packets later |
revenue of each of |
i packets later and |
of each of the |
packets later and introduces |
each of the other |
later and introduces the |
of the other pools |
and introduces the required |
introduces the required interleave |
the required interleave between |
required interleave between the |
similarly denote by r |
interleave between the repair |
between the repair packet |
the repair packet and |
repair packet and the |
packet and the last |
and the last data |
the last data packet |
last data packet included |
data packet included in |
packet included in it |
the revenue densities of |
revenue densities of pool |
notice that although transmitting |
that although transmitting the |
although transmitting the xor |
transmitting the xor immediately |
the xor immediately results |
xor immediately results in |
immediately results in faster |
results in faster recovery |
doing so also reduces |
so also reduces the |
also reduces the probability |
reduces the probability of |
the probability of a |
probability of a lost |
are instantiated to mi |
of a lost packet |
a lost packet being |
lost packet being recovered |
off results in a |
results in a minor |
in a minor control |
a minor control knob |
minor control knob permitting |
control knob permitting us |
knob permitting us to |
permitting us to balance |
us to balance speed |
to balance speed against |
balance speed against burst |
speed against burst tolerance |
our default configuration is |
default configuration is to |
configuration is to transmit |
is to transmit the |
to transmit the xor |
transmit the xor immediately |
envelope analysis to start |
analysis to start with |
we note that no |
note that no two |
that no two repair |
no two repair packets |
two repair packets generated |
repair packets generated at |
packets generated at different |
generated at different interleaves |
at different interleaves i |
will have more than |
have more than one |
more than one data |
than one data packet |
one data packet in |
data packet in common |
packet in common as |
in common as long |
common as long as |
as long as the |
long as the least |
as the least common |
the least common multiple |
of the interleaves is |
the interleaves is greater |
interleaves is greater than |
is greater than r |
greater than r i |
pairings of repair bins |
of repair bins in |
repair bins in two |
bins in two different |
in two different layers |
two different layers with |
different layers with interleaves |
layers with interleaves i |
a good rule of |
good rule of thumb |
rule of thumb is |
of thumb is to |
thumb is to select |
is to select interleaves |
to select interleaves that |
select interleaves that are |
interleaves that are relatively |
that are relatively prime |
are relatively prime to |
relatively prime to maximize |
prime to maximize their |
to maximize their lcm |
and also ensure that |
also ensure that the |
ensure that the larger |
that the larger interleave |
the larger interleave is |
larger interleave is greater |
interleave is greater than |
is greater than r |
let us assume that |
us assume that packets |
assume that packets are |
that packets are dropped |
packets are dropped with |
are dropped with uniform |
given a lost data |
a lost data packet |
what is the probability |
is the probability that |
the probability that we |
probability that we can |
that we can recover |
we can recover it |
we can recover a |
can recover a data |
recover a data packet |
a data packet if |
data packet if at |
packet if at least |
if at least one |
at least one of |
least one of the |
one of the c |
of the c xors |
the c xors containing |
c xors containing it |
xors containing it is |
containing it is re |
local recovery for receiver |
recovery for receiver loss |
for receiver loss ceived |
receiver loss ceived correctly |
loss ceived correctly and |
ceived correctly and usable |
all the other data |
the other data packets |
other data packets in |
data packets in it |
packets in it have |
in it have also |
it have also been |
have also been received |
also been received correctly |
the probability of in |
probability of in the |
of in the absence |
in the absence of |
the absence of intelligent |
absence of intelligent flow |
of intelligent flow control |
intelligent flow control mechanisms |
flow control mechanisms like |
control mechanisms like which |
mechanisms like which is |
like which is simply |
the probability of a |
probability of a received |
of a received tcp |
inexpensive xor being unusable |
xor being unusable is |
being unusable is the |
unusable is the complement |
and solving we obtain |
solving we obtain a |
we obtain a single |
hosts can be easily |
obtain a single expression |
can be easily overwhelmed |
a single expression for |
single expression for any |
expression for any ri |
be easily overwhelmed and |
easily overwhelmed and drop |
overwhelmed and drop packets |
and drop packets during |
drop packets during traffic |
since in the symmetric |
packets during traffic spikes |
in the symmetric case |
the symmetric case we |
during traffic spikes or |
traffic spikes or cpu |
symmetric case we have |
case we have r |
the probability x of |
probability x of a |
x of a sent |
of a sent xor |
a sent xor being |
sent xor being nance |
xor being nance tasks |
being nance tasks like |
nance tasks like garbage |
tasks like garbage collection |
the expression is shown |
expression is shown in |
is shown in equation |
reliable applicationdropped or unusable |
applicationdropped or unusable is |
or unusable is the |
unusable is the sum |
is the sum of |
the sum of the |
sum of the probability |
of the probability that |
the probability that it |
probability that it level |
that it level protocols |
it level protocols layered |
level protocols layered over |
protocols layered over udp |
layered over udp for |
over udp for reliable |
udp for reliable multiwas |
for reliable multiwas dropped |
reliable multiwas dropped and |
given any value of |
any value of q |
multiwas dropped and the |
value of q and |
of q and mi |
dropped and the probability |
and the probability that |
the probability that it |
probability that it was |
that it was received |
it was received and |
was received and cast |
or high speed data |
high speed data transfer |
the feasible range of |
feasible range of the |
range of the infiltration |
of the infiltration rates |
the infiltration rates is |
within this range ri |
this range ri is |
range ri is continuous |
and concave in x |
the optimal point for |
optimal point for pool |
since the function is |
the function is concave |
function is concave the |
is concave the equation |
concave the equation yields |
the equation yields a |
equation yields a single |
yields a single feasible |
a single feasible solution |
which is a function |
is a function of |
would ordinarily go back |
a function of the |
ordinarily go back to |
function of the attack |
go back to the |
of the attack rates |
back to the sender |
the attack rates of |
attack rates of the |
rates of the other |
of the other pools |
to the sender to |
the sender to retrieve |
sender to retrieve the |
to retrieve the lost |
retrieve the lost packet |
even though it was |
though it was dropped |
it was dropped at |
was dropped at the |
dropped at the receiver |
at the receiver after |
the receiver after since |
receiver after since it |
after since it is |
since it is easy |
it is easy to |
is easy to ensure |
easy to ensure that |
to ensure that no |
ensure that no two |
that no two xors |
to find a symmetric |
find a symmetric equilibrium |
no two xors share |
two xors share covering |
xors share covering the |
share covering the entire |
covering the entire geographical |
the entire geographical distance |
more than one data |
than one data packet |
the usability probabilities of |
usability probabilities of the |
probabilities of the maelstrom |
of the maelstrom proxy |
the maelstrom proxy acts |
maelstrom proxy acts as |
proxy acts as a |
acts as a local |
as a local packet |
a local packet cache |
stordifferent xors are independent |
the probability of all |
probability of all ing |
of all ing incoming |
all ing incoming packets |
ing incoming packets for |
incoming packets for a |
packets for a short |
for a short period |
and obtain a single |
obtain a single feasible |
a single feasible solution |
a short period of |
short period of time |
period of time and |
of time and prothe |
time and prothe c |
the equilibrium infiltration rate |
and prothe c xors |
equilibrium infiltration rate and |
prothe c xors being |
infiltration rate and the |
c xors being dropped |
rate and the matching |
xors being dropped or |
and the matching revenues |
being dropped or unusable |
dropped or unusable is |
or unusable is xc |
the matching revenues are |
matching revenues are shown |
revenues are shown in |
are shown in equation |
viding hooks that allow |
hooks that allow protocols |
that allow protocols to |
allow protocols to first |
protocols to first query |
to first query the |
first query the cache |
query the cache the |
the cache the probability |
cache the probability of |
the probability of correctly |
probability of correctly receiving |
of correctly receiving at |
correctly receiving at least |
receiving at least one |
at least one usable |
as in the two |
least one usable to |
one usable to locate |
usable to locate missing |
to locate missing packets |
locate missing packets before |
missing packets before sending |
packets before sending retransmission |
before sending retransmission xor |
sending retransmission xor is |
the revenue at the |
revenue at the symmetric |
at the symmetric equilibrium |
the symmetric equilibrium is |
symmetric equilibrium is inferior |
equilibrium is inferior to |
is inferior to the |
inferior to the no |
the probability of recovrequests |
probability of recovrequests back |
of recovrequests back to |
recovrequests back to the |
back to the sender |
future versions of maelstrom |
versions of maelstrom ering |
of maelstrom ering the |
maelstrom ering the lost |
ering the lost data |
the lost data packet |
lost data packet is |
up our analysis addresses |
our analysis addresses the |
which expands to could |
analysis addresses the eventual |
expands to could potentially |
addresses the eventual revenue |
the eventual revenue of |
eventual revenue of the |
revenue of the pools |
to could potentially use |
could potentially use knowledge |
potentially use knowledge of |
use knowledge of protocol |
knowledge of protocol internals |
assuming the mining difficulty |
of protocol internals to |
the mining difficulty is |
mining difficulty is set |
difficulty is set based |
is set based on |
set based on the |
based on the effective |
on the effective mining |
the effective mining power |
not including mining power |
including mining power used |
mining power used for |
power used for withholding |
difficulty is updated only |
is updated only periodically |
updated only periodically every |
by intercepting and this |
intercepting and this closed |
when mining power in |
mining power in the |
power in the system |
form formula only gives |
in the system is |
formula only gives us |
the system is regularly |
only gives us a |
system is regularly increasing |
gives us a lower |
us a lower bound |
a lower bound satisfying |
lower bound satisfying retransmission |
which has been true |
bound satisfying retransmission requests |
has been true for |
satisfying retransmission requests sent |
been true for the |
retransmission requests sent by |
true for the majority |
requests sent by the |
for the majority of |
sent by the receiver |
the majority of bitcoin |
by the receiver in |
majority of bitcoin s |
of bitcoin s history |
the receiver in on |
receiver in on the |
in on the recovery |
on the recovery probability |
since the xor usability |
the xor usability for |
no adjustment may be |
adjustment may be necessary |
or by resending packets |
by resending packets when |
resending packets when acmula |
packets when acmula does |
when acmula does not |
acmula does not factor |
if an attacker purchases |
does not factor in |
an attacker purchases new |
not factor in the |
attacker purchases new mining |
factor in the probability |
purchases new mining hardware |
in the probability of |
new mining hardware and |
the probability of the |
mining hardware and employs |
probability of the other |
hardware and employs it |
of the other data |
and employs it directly |
the other data knowledgments |
employs it directly for |
other data knowledgments are |
it directly for block |
data knowledgments are not |
directly for block withholding |
knowledgments are not observed |
are not observed within |
not observed within a |
observed within a certain |
within a certain time |
a certain time period |
this mining power is |
certain time period in |
time period in an |
mining power is never |
period in an ack |
power is never included |
is never included in |
never included in the |
included in the difficulty |
in the difficulty calculation |
the difficulty calculation the |
difficulty calculation the system |
calculation the system is |
packets in the xor |
the system is never |
in the xor being |
system is never aware |
the xor being dropped |
is never aware of |
xor being dropped and |
never aware of it |
being dropped and recovered |
the difficulty is therefore |
difficulty is therefore already |
is therefore already correctly |
therefore already correctly calculated |
we extend the analysis |
already correctly calculated and |
extend the analysis to |
correctly calculated and the |
the analysis to bursty |
calculated and the attack |
analysis to bursty losses |
and the attack is |
the attack is profitable |
attack is profitable immediately |
if the lost data |
the lost data packet |
lost data packet was |
data packet was part |
packet was part of |
was part of a |
part of a loss |
of a loss burst |
if the mining power |
a loss burst of |
the mining power is |
loss burst of size |
mining power is static |
burst of size b |
the attack becomes profitable |
repair packets generated at |
attack becomes profitable only |
packets generated at interleaves |
becomes profitable only after |
generated at interleaves less |
profitable only after the |
at interleaves less than |
only after the bitcoin |
interleaves less than b |
after the bitcoin system |
less than b are |
the bitcoin system has |
than b are dropped |
bitcoin system has normalized |
b are dropped or |
are dropped or useless |
system has normalized the |
dropped or useless with |
or useless with high |
useless with high probability |
has normalized the revenues |
normalized the revenues by |
the revenues by adjusting |
revenues by adjusting difficulty |
and we can discount |
we can discount them |
the revenue of an |
probability of recovering the |
revenue of an attacking |
of recovering the data |
of an attacking pool |
recovering the data packet |
an attacking pool is |
the data packet is |
attacking pool is reduced |
data packet is then |
pool is reduced due |
is reduced due to |
reduced due to the |
due to the reduction |
to the reduction in |
the reduction in block |
reduction in block generation |
in block generation of |
block generation of both |
generation of both the |
of both the attacking |
both the attacking and |
the attacking and attacked |
attacking and attacked pools |
is the number of |
the number of xors |
number of xors generated |
of xors generated at |
xors generated at interleaves |
generated at interleaves greater |
at interleaves greater than |
interleaves greater than b |
the formulae derived for |
formulae derived for xor |
derived for xor usability |
for xor usability still |
xor usability still hold |
since packet losses with |
packet losses with more |
losses with more than |
with more than b |
more than b intervening |
than b intervening packets |
b intervening packets between |
intervening packets between them |
packets between them have |
between them have independent |
them have independent probability |
there is only correlation |
is only correlation within |
only correlation within the |
correlation within the bursts |
how does this compare |
does this compare to |
this compare to traditional |
codes such as reed |
c repair packets are |
repair packets are generated |
packets are generated and |
are generated and sent |
generated and sent for |
and sent for every |
sent for every r |
for every r data |
every r data packets |
and the correct delivery |
the correct delivery of |
correct delivery of any |
delivery of any r |
of any r of |
any r of the |
r of the r |
c packets transmitted is |
packets transmitted is sufficient |
transmitted is sufficient to |
is sufficient to reconstruct |
sufficient to reconstruct the |
to reconstruct the original |
reconstruct the original r |
the original r data |
original r data packets |
given a lost data |
a lost data packet |
we can recover it |
can recover it if |
recover it if at |
it if at least |
if at least r |
at least r packets |
least r packets are |
r packets are received |
packets are received correctly |
are received correctly in |
received correctly in the |
correctly in the encoding |
in the encoding set |
the encoding set of |
encoding set of r |
c data and repair |
data and repair packets |
and repair packets that |
repair packets that the |
packets that the lost |
that the lost packet |
the lost packet belongs |
lost packet belongs to |
the probability of recovering |
probability of recovering a |
of recovering a lost |
recovering a lost packet |
a lost packet is |
lost packet is equivalent |
packet is equivalent to |
is equivalent to the |
equivalent to the probability |
to the probability of |
the probability of losing |
probability of losing c |
or less packets from |
less packets from the |
packets from the total |
from the total r |
since the number of |
the number of other |
number of other lost |
of other lost packets |
other lost packets in |
lost packets in the |
packets in the xor |
in the xor is |
the xor is a |
xor is a random |
is a random variable |
a random variable y |
random variable y and |
variable y and has |
y and has a |
and has a binomial |
has a binomial distribution |
a binomial distribution with |
binomial distribution with parameters |
is the summation z |
the summation z c |
expression for ri in |
for ri in a |
ri in a system |
in a system with |
a system with pools |
system with pools of |
with pools of equal |
pools of equal size |
we plot the recovery |
plot the recovery probability |
the recovery probability curves |
recovery probability curves for |
probability curves for layered |
curves for layered interleaving |
for layered interleaving and |
layered interleaving and reed |
solomon against uniformly random |
against uniformly random loss |
uniformly random loss rate |
note that the curves |
that the curves are |
the curves are very |
curves are very close |
are very close to |
very close to each |
close to each other |
especially in the loss |
in the loss range |
the loss range of |
loss range of interest |
range of interest between |
q mi q mi |
implementation details we initially |
details we initially implemented |
we initially implemented and |
initially implemented and evaluated |
implemented and evaluated maelstrom |
and evaluated maelstrom as |
evaluated maelstrom as a |
maelstrom as a user |
performance turned out to |
turned out to be |
out to be limited |
to be limited by |
be limited by copying |
limited by copying and |
by copying and context |
and we subsequently reimplemented |
we subsequently reimplemented the |
subsequently reimplemented the system |
reimplemented the system as |
the system as a |
system as a module |
as a module that |
a module that runs |
module that runs within |
that runs within the |
runs within the linux |
at an encoding rate |
an encoding rate of |
the experimental prototype of |
experimental prototype of the |
prototype of the kernel |
of the kernel version |
the kernel version reaches |
kernel version reaches output |
version reaches output speeds |
reaches output speeds close |
output speeds close to |
gigabit per second of |
per second of combined |
second of combined data |
of combined data and |
combined data and fec |
data and fec traffic |
limited only by the |
only by the capacity |
by the capacity of |
the capacity of the |
capacity of the outbound |
of the outbound network |
the outbound network card |
q symmetric equilibrium values |
symmetric equilibrium values for |
equilibrium values for a |
values for a system |
for a system of |
a system of q |
lambda networks are already |
system of q pools |
of q pools of |
networks are already reaching |
q pools of equal |
pools of equal sizes |
are already reaching speeds |
already reaching speeds of |
countermeasures in order to |
in order to choose |
and higher speeds are |
order to choose its |
to choose its optimal |
higher speeds are a |
choose its optimal infiltration |
its optimal infiltration rate |
speeds are a certainty |
are a certainty down |
a certainty down the |
certainty down the road |
a pool has to |
pool has to know |
has to know the |
to know the rate |
know the rate at |
the rate at which |
rate at which it |
at which it is |
which it is attacked |
we envision maelstrom as |
envision maelstrom as a |
maelstrom as a small |
as a small rack |
and the revenue density |
the revenue density of |
revenue density of potential |
density of potential victim |
of potential victim pools |
style cluster of blade |
a pool can estimate |
pool can estimate the |
can estimate the rate |
each acting as an |
estimate the rate with |
acting as an individual |
the rate with which |
as an individual proxy |
rate with which it |
with which it is |
which it is attacked |
it is attacked by |
is attacked by comparing |
traffic would be distributed |
attacked by comparing the |
would be distributed over |
by comparing the rates |
be distributed over such |
comparing the rates of |
distributed over such a |
the rates of partial |
over such a rack |
rates of partial and |
such a rack by |
of partial and full |
a rack by partitioning |
partial and full proofs |
rack by partitioning the |
and full proofs of |
by partitioning the address |
full proofs of work |
partitioning the address space |
proofs of work it |
the address space of |
of work it receives |
address space of the |
work it receives from |
space of the remote |
it receives from its |
of the remote datacenter |
receives from its miners |
the remote datacenter and |
remote datacenter and routing |
datacenter and routing different |
and routing different segments |
as explained in section |
routing different segments of |
explained in section ii |
different segments of the |
segments of the space |
of the space through |
the space through distinct |
space through distinct maelstrom |
through distinct maelstrom appliance |
distinct maelstrom appliance pairs |
in order to estimate |
order to estimate the |
to estimate the revenue |
estimate the revenue densities |
the revenue densities of |
revenue densities of the |
densities of the other |
of the other pools |
we plan to experiment |
plan to experiment with |
to experiment with such |
experiment with such configurations |
a pool can use |
pool can use one |
can use one of |
use one of two |
one of two methods |
which would also permit |
would also permit us |
also permit us to |
permit us to explore |
us to explore faulttolerance |
to explore faulttolerance issues |
pools often publish this |
if a maelstrom blade |
often publish this data |
a maelstrom blade fails |
publish this data to |
this data to demonstrate |
data to demonstrate their |
to demonstrate their honesty |
demonstrate their honesty to |
their honesty to their |
honesty to their miners |
and to support load |
balancing schemes that might |
schemes that might vary |
that might vary the |
might vary the ip |
vary the ip address |
the ip address space |
ip address space partitioning |
address space partitioning dynamically |
space partitioning dynamically to |
partitioning dynamically to spread |
dynamically to spread the |
to spread the encoding |
spread the encoding load |
the encoding load over |
encoding load over multiple |
load over multiple machines |
we present the implementation |
present the implementation and |
the implementation and performance |
implementation and performance of |
and performance of a |
performance of a single |
the kernel implementation is |
kernel implementation is a |
implementation is a module |
is a module for |
a module for linux |
a pool can infiltrate |
pool can infiltrate each |
can infiltrate each of |
infiltrate each of the |
each of the other |
of the other pools |
the other pools with |
other pools with some |
pools with some nominal |
with some nominal probing |
some nominal probing mining |
nominal probing mining power |
probing mining power and |
with hooks into the |
mining power and measure |
hooks into the kernel |
power and measure the |
into the kernel packet |
and measure the revenue |
the kernel packet filter |
measure the revenue density |
the revenue density directly |
revenue density directly by |
density directly by monitoring |
directly by monitoring the |
by monitoring the probe |
monitoring the probe s |
the probe s rewards |
probe s rewards from |
s rewards from the |
rewards from the pool |
as in the case |
maelstrom proxies work in |
in the case of |
proxies work in pairs |
the case of classical |
case of classical block |
of classical block withholding |
classical block withholding explained |
block withholding explained in |
one on each side |
withholding explained in section |
explained in section ii |
on each side of |
each side of the |
side of the long |
of the long haul |
the long haul link |
each proxy acts both |
a pool might detect |
proxy acts both as |
pool might detect that |
acts both as an |
might detect that it |
both as an ingress |
detect that it is |
that it is being |
it is being attacked |
as an ingress and |
an ingress and egress |
ingress and egress temporarily |
but cannot detect which |
cannot detect which of |
detect which of its |
which of its miners |
of its miners is |
in case all but |
its miners is the |
case all but one |
miners is the attacker |
all but one of |
but one of the |
one of the missing |
of the missing packets |
therefore a pool cannot |
the missing packets are |
a pool cannot block |
missing packets are router |
pool cannot block or |
packets are router at |
cannot block or punish |
are router at the |
block or punish withholding |
router at the same |
or punish withholding miners |
at the same time |
the same time since |
same time since they |
time since they handle |
since they handle duplex |
they handle duplex traffic |
handle duplex traffic in |
duplex traffic in received |
various techniques can be |
traffic in received later |
techniques can be used |
in received later or |
can be used to |
received later or recovered |
be used to encourage |
later or recovered through |
used to encourage miners |
or recovered through other |
to encourage miners to |
recovered through other xors |
encourage miners to submit |
miners to submit full |
to submit full blocks |
allowing the following manner |
a pool can pay |
pool can pay a |
can pay a bonus |
the recovery of the |
pay a bonus for |
recovery of the remaining |
a bonus for submitting |
of the remaining missing |
bonus for submitting a |
the remaining missing packet |
for submitting a full |
remaining missing packet from |
submitting a full proof |
a full proof of |
full proof of work |
missing packet from this |
packet from this xor |
this would increase the |
would increase the revenue |
in practice we stored |
increase the revenue of |
practice we stored data |
the revenue of the |
we stored data and |
revenue of the miner |
stored data and xor |
of the miner that |
data and xor packets |
the miner that found |
and xor packets in |
miner that found a |
xor packets in dou |
that found a block |
packets in dou the |
found a block while |
in dou the egress |
a block while reducing |
dou the egress router |
block while reducing the |
the egress router captures |
while reducing the revenue |
egress router captures ip |
reducing the revenue of |
router captures ip packets |
captures ip packets and |
ip packets and creates |
packets and creates re |
the revenue of the |
revenue of the other |
of the other miners |
the other miners from |
other miners from this |
miners from this block |
ble buffered red black |
buffered red black trees |
red black trees for |
while the average revenue |
the average revenue of |
average revenue of each |
revenue of each miner |
of each miner would |
each miner would stay |
miner would stay the |
would stay the same |
small miners will suffer |
miners will suffer from |
will suffer from higher |
suffer from higher variance |
from higher variance in |
byte packets and dundant |
packets and dundant fec |
and dundant fec packets |
higher variance in revenue |
the original ip packets |
original ip packets are |
another approach is to |
approach is to introduce |
is to introduce a |
to introduce a joining |
introduce a joining fee |
a joining fee by |
joining fee by paying |
fee by paying new |
by paying new miners |
paying new miners less |
new miners less for |
miners less for their |
less for their work |
for their work until |
entries this occupies around |
their work until they |
work until they have |
until they have established |
they have established a |
have established a reputation |
established a reputation with |
a reputation with the |
reputation with the pool |
routed through unaltered as |
through unaltered as they |
unaltered as they would |
as they would have |
miners that seek flexibility |
they would have been |
that seek flexibility may |
would have been at |
seek flexibility may not |
have been at the |
flexibility may not accept |
been at the send |
may not accept this |
not accept this policy |
accept this policy and |
this policy and choose |
policy and choose another |
and choose another pool |
the repair bins in |
repair bins in the |
bins in the layered |
in the layered interoriginally |
the pool can use |
pool can use a |
can use a honeypot |
the redundant packets are |
use a honeypot trap |
redundant packets are then |
a honeypot trap by |
packets are then forwarded |
honeypot trap by sending |
are then forwarded leaving |
trap by sending the |
then forwarded leaving scheme |
by sending the miners |
forwarded leaving scheme store |
sending the miners tasks |
leaving scheme store incrementally |
the miners tasks which |
scheme store incrementally computed |
miners tasks which it |
store incrementally computed xors |
tasks which it knows |
incrementally computed xors and |
which it knows will |
computed xors and to |
it knows will result |
xors and to the |
knows will result in |
and to the remote |
will result in a |
to the remote ingress |
result in a full |
the remote ingress router |
in a full proof |
remote ingress router via |
a full proof of |
ingress router via a |
full proof of work |
router via a udp |
via a udp channel |
lists of data packet |
of data packet headers |
without the data packet |
the data packet payloads |
resulting in low storage |
in low storage overheads |
if a miner fails |
low storage overheads for |
a miner fails to |
storage overheads for each |
miner fails to submit |
overheads for each layer |
fails to submit the |
for each layer the |
to submit the full |
each layer the ingress |
submit the full proof |
layer the ingress router |
the full proof of |
the ingress router captures |
full proof of work |
ingress router captures and |
proof of work it |
router captures and stores |
of work it is |
captures and stores ip |
work it is tagged |
and stores ip packets |
it is tagged as |
stores ip packets that |
is tagged as an |
tagged as an attacker |
ip packets that rise |
packets that rise linearly |
that rise linearly with |
rise linearly with the |
linearly with the value |
to prevent the attacker |
with the value of |
the value of the |
value of the interleave |
prevent the attacker from |
the attacker from learning |
attacker from learning them |
the coming from the |
coming from the direction |
from the direction of |
the honeypot tasks have |
the direction of the |
honeypot tasks have to |
direction of the egress |
of the egress router |
tasks have to be |
have to be regularly |
to be regularly refreshed |
upon memory footprint for |
memory footprint for a |
footprint for a long |
running proxy was around |
pools can also incorporate |
proxy was around receipt |
can also incorporate out |
was around receipt of |
also incorporate out of |
around receipt of a |
receipt of a redundant |
of a redundant packet |
incorporate out of band |
out of band mechanisms |
of band mechanisms to |
band mechanisms to deter |
mechanisms to deter attacks |
an ip packet is |
ip packet is recov |
such as verifying the |
as verifying the identity |
verifying the identity of |
the identity of miners |
identity of miners or |
of miners or using |
mb in our experiments |
miners or using trusted |
or using trusted computing |
using trusted computing technologies |
ered if there is |
if there is an |
there is an opportunity |
is an opportunity to |
an opportunity to do |
opportunity to do so |
redundant packets that can |
packets that can be |
that can be used |
can be used at |
be used at a |
used at a later |
at a later time |
a later time are |
later time are stored |
that assure no block |
assure no block withholding |
no block withholding is |
block withholding is taking |
withholding is taking place |
if the redundant packet |
the redundant packet is |
redundant packet is useless |
packet is useless it |
is useless it is |
this would require miners |
would require miners to |
useless it is immediately |
it is immediately dis |
require miners to use |
miners to use specialized |
to use specialized hardware |
use specialized hardware and |
specialized hardware and software |
an overhead miners may |
overhead miners may not |
miners may not accept |
other performance enhancing roles |
performance enhancing roles carded |
there is no known |
is no known silver |
no known silver bullet |
upon recovery the ip |
recovery the ip packet |
the ip packet is |
all these techniques reduce |
ip packet is sent |
these techniques reduce the |
packet is sent through |
techniques reduce the pool |
is sent through maelstrom |
reduce the pool s |
sent through maelstrom appliances |
the pool s attractiveness |
through maelstrom appliances can |
pool s attractiveness and |
maelstrom appliances can optionally |
s attractiveness and deter |
appliances can optionally aggregate |
attractiveness and deter miners |
can optionally aggregate small |
optionally aggregate small suba |
aggregate small suba raw |
small suba raw socket |
suba raw socket to |
raw socket to its |
socket to its intended |
to its intended destination |
block withholding recycling we |
withholding recycling we assume |
recycling we assume that |
we assume that the |
kilobyte packets from different |
assume that the infiltrating |
packets from different flows |
that the infiltrating miners |
from different flows into |
the infiltrating miners are |
different flows into larger |
infiltrating miners are loyal |
flows into larger ones |
miners are loyal to |
into larger ones for |
are loyal to the |
larger ones for using |
loyal to the attacker |
ones for using fec |
for using fec requires |
using fec requires that |
fec requires that each |
requires that each data |
that each data packet |
each data packet have |
data packet have a |
some of the pool |
packet have a unique |
of the pool s |
have a unique better |
the pool s members |
a unique better communication |
pool s members may |
unique better communication efficiency |
s members may be |
better communication efficiency over |
members may be disloyal |
may be disloyal infiltrators |
communication efficiency over the |
efficiency over the long |
when sending disloyal miners |
sending disloyal miners to |
disloyal miners to perform |
distance identifier that the |
miners to perform block |
identifier that the receiver |
to perform block withholding |
that the receiver can |
perform block withholding at |
the receiver can use |
block withholding at other |
withholding at other pools |
receiver can use to |
can use to keep |
use to keep track |
to keep track of |
keep track of re |
an attacker takes a |
attacker takes a significant |
takes a significant risk |
in split flow control |
split flow control mode |
can use a loyal |
flow control mode they |
use a loyal miner |
control mode they can |
a loyal miner w |
mode they can ceived |
loyal miner w to |
they can ceived data |
miner w to infiltrate |
can ceived data packets |
w to infiltrate pool |
ceived data packets and |
data packets and to |
packets and to identify |
and to identify missing |
to identify missing data |
identify missing data packets |
missing data packets perform |
data packets perform send |
side buffering of in |
thinking the miner is |
flight data for multiin |
the miner is loyal |
data for multiin a |
miner is loyal to |
is loyal to it |
for multiin a repair |
multiin a repair packet |
might use it to |
use it to attack |
it to attack pool |
if we had access |
we had access to |
had access to end |
the miner m can |
miner m can perform |
m can perform honest |
can perform honest mining |
we gigabyte flows that |
perform honest mining for |
gigabyte flows that exceed |
honest mining for pool |
flows that exceed the |
that exceed the sending |
exceed the sending end |
host s buffercould have |
s buffercould have added |
rather than withhold its |
buffercould have added a |
than withhold its blocks |
have added a header |
added a header to |
a header to each |
header to each packet |
to each packet with |
and not return any |
each packet with a |
not return any revenue |
packet with a unique |
return any revenue to |
any revenue to pool |
with a unique ing |
a unique ing capacity |
maelstrom appliances can act |
appliances can act as |
can act as mulsequence |
it will take its |
act as mulsequence number |
will take its share |
take its share of |
its share of pool |
which thinks the miner |
thinks the miner is |
the miner is loyal |
miner is loyal to |
is loyal to it |
and deliver it back |
deliver it back to |
it back to pool |
we intercept traffic trans |
to avoid such a |
avoid such a risk |
appliances send multicast packparently |
send multicast packparently and |
a pool needs a |
multicast packparently and need |
pool needs a sufficient |
packparently and need to |
needs a sufficient number |
and need to route |
a sufficient number of |
need to route it |
sufficient number of verified |
to route it without |
number of verified miners |
route it without modification |
of verified miners miners |
it without modification or |
verified miners miners that |
without modification or addi |
miners miners that it |
miners that it knows |
that it knows to |
it knows to be |
knows to be loyal |
ets to each other |
to each other across |
each other across the |
other across the long |
the optimal infiltration rate |
optimal infiltration rate may |
infiltration rate may be |
rate may be as |
may be as high |
be as high as |
we identify ip multicast |
of the pool size |
but this is only |
this is only in |
is only in extreme |
only in extreme cases |
in extreme cases when |
extreme cases when pools |
cases when pools are |
when pools are large |
to spread them within |
for practical pool sizes |
spread them within their |
them within their datacenters |
ip packets by a |
packets by a tuple |
by a tuple consisting |
a pool may need |
a tuple consisting of |
pool may need up |
tuple consisting of the |
may need up to |
consisting of the source |
of the source and |
the source and des |
appliances can take on |
can take on other |
of its mining power |
take on other existing |
its mining power for |
on other existing roles |
mining power for infiltration |
other existing roles in |
existing roles in the |
roles in the tination |
in the tination ip |
the tination ip address |
pools typically have loyal |
typically have loyal mining |
have loyal mining power |
size of the ip |
loyal mining power either |
of the ip datacenter |
mining power either run |
power either run directly |
either run directly by |
run directly by the |
acting as security and |
directly by the pool |
as security and vpn |
by the pool owners |
security and vpn gateways |
the pool owners or |
and vpn gateways and |
pool owners or sold |
vpn gateways and as |
owners or sold as |
gateways and as header |
or sold as a |
and as header plus |
sold as a service |
as header plus data |
as a service but |
a service but run |
service but run on |
but run on the |
run on the pool |
and a checksum over |
on the pool owners |
the pool owners hardware |
a checksum over the |
checksum over the ip |
over the ip data |
the ip data pay |
conventional performance enhancing proxies |
the checksum over the |
checksum over the payload |
over the payload is |
however the size of |
the payload is necessary |
the size of this |
payload is necessary since |
size of this mining |
is necessary since the |
of this mining power |
necessary since the ip |
this mining power is |
since the ip identification |
mining power is considered |
the ip identification field |
power is considered a |
ip identification field is |
is considered a trade |
identification field is only |
considered a trade secret |
a trade secret and |
trade secret and is |
secret and is not |
and is not published |
bits long and a |
long and a single |
and a single pair |
a single pair of |
single pair of end |
block withholding in practice |
withholding in practice long |
in practice long term |
hosts communicating at high |
practice long term block |
communicating at high speeds |
at high speeds will |
long term block withholding |
term block withholding attacks |
block withholding attacks are |
withholding attacks are difficult |
attacks are difficult to |
are difficult to hide |
evaluation use the same |
use the same identifier |
the same identifier for |
since miners using an |
same identifier for different |
miners using an attacked |
identifier for different data |
using an attacked pool |
for different data packets |
an attacked pool would |
different data packets within |
attacked pool would notice |
data packets within a |
pool would notice the |
packets within a fairly |
would notice the reduced |
within a fairly short |
notice the reduced revenue |
a fairly short interval |
the reduced revenue density |
fairly short interval unless |
short interval unless the |
interval unless the checksum |
unless the checksum is |
the checksum is added |
checksum is added to |
is added to we |
such attacks are rarely |
added to we evaluated |
attacks are rarely reported |
to we evaluated maelstrom |
we evaluated maelstrom on |
evaluated maelstrom on the |
maelstrom on the emulab |
on the emulab testbed |
and we can therefore |
the emulab testbed at |
we can therefore conclude |
emulab testbed at utah |
can therefore conclude that |
testbed at utah differentiate |
at utah differentiate between |
utah differentiate between them |
therefore conclude that they |
conclude that they are |
that they are indeed |
they are indeed rare |
a recent exception is |
recent exception is an |
exception is an attack |
is an attack on |
an attack on the |
attack on the eligius |
on the eligius pool |
the eligius pool performed |
eligius pool performed in |
pool performed in may |
performed in may and |
in may and june |
for all the experiments |
we used a dumbbell |
used a dumbbell topoltifiers |
a dumbbell topoltifiers result |
dumbbell topoltifiers result in |
topoltifiers result in garbled |
result in garbled recovery |
in garbled recovery by |
garbled recovery by maelstrom |
an event ogy of |
event ogy of two |
ogy of two clusters |
of two clusters of |
two clusters of nodes |
clusters of nodes connected |
of nodes connected via |
nodes connected via routing |
connected via routing nodes |
via routing nodes which |
routing nodes which will |
nodes which will be |
which will be caught |
will be caught by |
bitcoin before detecting the |
before detecting the attack |
be caught by higher |
caught by higher level |
by higher level checksums |
higher level checksums designed |
level checksums designed with |
at which point payouts |
checksums designed with a |
which point payouts to |
designed with a high |
point payouts to the |
payouts to the attackers |
to the attackers were |
the attackers were blocked |
latency link in between |
link in between them |
the attackers continued the |
attackers continued the attack |
designed to emto deal |
to emto deal with |
emto deal with tranmission |
deal with tranmission errors |
with tranmission errors on |
tranmission errors on commodity |
errors on commodity networks |
on commodity networks ulate |
commodity networks ulate the |
networks ulate the setup |
ulate the setup in |
the setup in figure |
more bitcoin before realizing |
bitcoin before realizing they |
before realizing they were |
realizing they were not |
they were not receiving |
were not receiving their |
not receiving their payout |
and ran the proxy |
ran the proxy code |
the proxy code on |
proxy code on and |
code on and hence |
the reasons the attack |
on and hence does |
reasons the attack was |
and hence does not |
the attack was so |
hence does not have |
attack was so easily |
does not have significant |
was so easily subverted |
not have significant consequences |
so easily subverted is |
have significant consequences unless |
easily subverted is the |
significant consequences unless the |
subverted is the limited |
consequences unless the routers |
is the limited efforts |
the limited efforts of |
limited efforts of the |
efforts of the attackers |
of the attackers to |
the attackers to hide |
attackers to hide themselves |
they have only used |
have only used two |
only used two payout |
shows the performance of |
used two payout addresses |
the performance of the |
two payout addresses to |
performance of the kernel |
payout addresses to collect |
addresses to collect their |
to collect their payouts |
of the kernel version |
the kernel version at |
kernel version at gigabit |
version at gigabit speeds |
and so it was |
so it was possible |
the remainder of the |
it was possible for |
remainder of the graphs |
was possible for the |
of the graphs it |
possible for the alert |
the graphs it occurs |
for the alert pool |
graphs it occurs frequently |
the alert pool manager |
alert pool manager to |
pool manager to cluster |
manager to cluster the |
the kernel version of |
to cluster the attacking |
kernel version of maelstrom |
cluster the attacking miners |
version of maelstrom can |
the attacking miners and |
of maelstrom can generate |
attacking miners and obtain |
maelstrom can generate up |
miners and obtain a |
can generate up to |
and obtain a statistically |
generate up to a |
obtain a statistically significant |
up to a show |
a statistically significant proof |
to a show the |
statistically significant proof of |
a show the performance |
significant proof of their |
show the performance of |
proof of their wrongdoing |
the performance of the |
performance of the user |
it is unknown whether |
is unknown whether this |
space version at slower |
unknown whether this was |
version at slower gigabit |
whether this was a |
at slower gigabit per |
this was a classical |
slower gigabit per second |
was a classical block |
gigabit per second of |
a classical block withholding |
per second of data |
classical block withholding attack |
second of data and |
of data and fec |
data and fec traffic |
with the goal of |
the goal of sabotage |
or a more elaborate |
a more elaborate scheme |
to emulate the mtu |
to verify the effectiveness |
emulate the mtu difference |
verify the effectiveness of |
the mtu difference between |
the effectiveness of block |
mtu difference between the |
effectiveness of block withholding |
difference between the longput |
of block withholding for |
between the longput data |
block withholding for profit |
the longput data rate |
longput data rate depending |
data rate depending on |
rate depending on the |
depending on the encoding |
on the encoding rate |
haul link and the |
link and the datacenter |
and the datacenter network |
implemented an experimental bitcoin |
an experimental bitcoin test |
experimental bitcoin test network |
bitcoin test network and |
test network and demonstrated |
network and demonstrated the |
and demonstrated the practicality |
demonstrated the practicality of |
the practicality of the |
practicality of the attack |
we were able to |
were able to saturate |
able to saturate the |
to saturate the outgoing |
saturate the outgoing card |
the outgoing card at |
outgoing card at set |
bitcoin s health large |
card at set an |
s health large pools |
at set an mtu |
health large pools hinder |
set an mtu of |
large pools hinder bitcoin |
pools hinder bitcoin s |
hinder bitcoin s distributed |
bitcoin s distributed nature |
s distributed nature as |
distributed nature as they |
nature as they put |
as they put a |
they put a lot |
put a lot of |
a lot of mining |
lot of mining power |
of mining power in |
mining power in the |
power in the hands |
in the hands of |
bytes on the network |
the hands of a |
on the network connecting |
hands of a few |
of a few pool |
a few pool managers |
the network connecting the |
network connecting the rates |
connecting the rates as |
the rates as high |
rates as high as |
this has been mostly |
has been mostly addressed |
been mostly addressed by |
mostly addressed by community |
addressed by community pressure |
by community pressure on |
community pressure on miners |
pressure on miners to |
on miners to avoid |
miners to avoid forming |
to avoid forming large |
avoid forming large pools |
with cpu overload occurring |
cpu overload occurring at |
overload occurring at end |
hosts to the proxy |
to the proxy and |
however such recommendations had |
the proxy and an |
proxy and an mtu |
and an mtu of |
such recommendations had only |
recommendations had only had |
had only had limited |
only had limited success |
and mining is still |
mining is still dominated |
is still dominated by |
still dominated by a |
dominated by a small |
by a small number |
a small number of |
small number of large |
number of large pools |
as a characteristic example |
in the period of |
the period of november |
where each incoming data |
each incoming data packet |
incoming data packet had |
data packet had to |
packet had to be |
had to be xored |
to be xored long |
haul link between proxies |
the only exception is |
only exception is figure |
three pools generated over |
where we maintained equal |
we maintained equal mtus |
maintained equal mtus of |
of the proofs of |
the proofs of work |
the fact that block |
fact that block withholding |
that block withholding attacks |
block withholding attacks are |
withholding attacks are rarely |
attacks are rarely observed |
are rarely observed may |
rarely observed may indicate |
observed may indicate that |
may indicate that the |
indicate that the active |
that the active pools |
the active pools have |
active pools have reached |
pools have reached an |
have reached an implicit |
reached an implicit or |
an implicit or explicit |
implicit or explicit agreement |
or explicit agreement not |
explicit agreement not to |
agreement not to attack |
not to attack one |
to attack one another |
throughput metrics at the |
metrics at the receive |
an attacked pool cannot |
attacked pool cannot detect |
incoming data packets are |
pool cannot detect which |
data packets are buffered |
cannot detect which of |
packets are buffered so |
detect which of its |
are buffered so that |
which of its miners |
of its miners are |
buffered so that they |
its miners are attacking |
miners are attacking it |
so that they can |
that they can be |
they can be used |
can be used in |
be used in conjunction |
let alone which pool |
alone which pool controls |
used in conjunction with |
in conjunction with figures |
which pool controls the |
pool controls the miners |
at some point a |
some point a pool |
point a pool might |
a pool might miscalculate |
pool might miscalculate and |
might miscalculate and decide |
show that commodity tcp |
miscalculate and decide to |
and decide to try |
decide to try to |
to try to increase |
try to increase its |
to increase its revenue |
ip throughxors to recover |
throughxors to recover missing |
to recover missing data |
recover missing data packets |
one pool might be |
pool might be enough |
might be enough to |
be enough to break |
enough to break the |
to break the agreement |
any received put collapses |
received put collapses in |
put collapses in the |
possibly leading to a |
collapses in the presence |
leading to a constant |
in the presence of |
the presence of non |
to a constant rate |
a constant rate of |
constant rate of attacks |
rate of attacks among |
of attacks among pools |
attacks among pools and |
among pools and a |
pools and a reduced |
and a reduced revenue |
and xor that is |
xor that is missing |
that is missing more |
is missing more than |
if open pools reach |
missing more than one |
open pools reach a |
more than one data |
pools reach a state |
than one data packet |
reach a state where |
one data packet is |
a state where their |
data packet is stored |
state where their revenue |
packet is stored that |
where their revenue density |
is stored that maelstrom |
their revenue density is |
stored that maelstrom successfully |
revenue density is reduced |
that maelstrom successfully masks |
density is reduced due |
maelstrom successfully masks loss |
is reduced due to |
successfully masks loss and |
reduced due to attacks |
masks loss and prevents |
loss and prevents this |
miners will leave them |
will leave them in |
leave them in favor |
them in favor of |
in favor of other |
favor of other available |
of other available options |
miners of sufficient size |
of sufficient size can |
sufficient size can mine |
size can mine solo |
smaller miners can form |
miners can form private |
can form private pools |
form private pools with |
private pools with closed |
pools with closed access |
limited to trusted participants |
such a change may |
a change may be |
change may be in |
may be in favor |
be in favor of |
in favor of bitcoin |
favor of bitcoin as |
of bitcoin as a |
bitcoin as a whole |
since they require such |
they require such intimate |
require such intimate trust |
private pools are likely |
pools are likely to |
are likely to be |
likely to be smaller |
and form a fine |
form a fine grained |
a fine grained distribution |
fine grained distribution of |
grained distribution of mining |
distribution of mining power |
of mining power with |
mining power with many |
power with many small |
with many small pools |
many small pools and |
small pools and solo |
pools and solo miners |
a pool may engage |
pool may engage in |
may engage in an |
engage in an attack |
in an attack against |
an attack against another |
attack against another pool |
against another pool not |
another pool not to |
pool not to increase |
not to increase its |
to increase its absolute |
increase its absolute revenue |
but rather to attract |
rather to attract miners |
to attract miners by |
attract miners by temporarily |
miners by temporarily increasing |
by temporarily increasing its |
temporarily increasing its revenue |
increasing its revenue relative |
its revenue relative to |
revenue relative to a |
relative to a competing |
to a competing pool |
recent work has investigated |
work has investigated the |
has investigated the motivation |
investigated the motivation of |
the motivation of pools |
motivation of pools to |
of pools to utilize |
pools to utilize part |
ip no loss maelstrom |
to utilize part of |
no loss maelstrom no |
utilize part of their |
loss maelstrom no loss |
part of their resources |
maelstrom no loss maelstrom |
of their resources towards |
their resources towards sabotage |
resources towards sabotage attacks |
towards sabotage attacks against |
sabotage attacks against each |
attacks against each other |
the model of those |
model of those works |
of those works is |
those works is different |
works is different from |
is different from the |
different from the pool |
from the pool game |
the pool game model |
pool game model in |
game model in two |
model in two major |
in two major ways |
two major ways a |
major ways a sabotage |
ways a sabotage attack |
a sabotage attack does |
sabotage attack does not |
attack does not transfer |
does not transfer revenue |
not transfer revenue from |
transfer revenue from victim |
revenue from victim to |
from victim to attacker |
and migrating miners switch |
migrating miners switch to |
miners switch to less |
switch to less attacked |
to less attacked pools |
changing pool sizes and |
pool sizes and hence |
sizes and hence revenues |
and hence revenues until |
hence revenues until convergence |
the model is parametrized |
model is parametrized by |
is parametrized by the |
parametrized by the cost |
by the cost of |
the cost of the |
cost of the attack |
of the attack and |
the attack and by |
attack and by the |
and by the mobility |
by the mobility of |
the mobility of the |
mobility of the miners |
and the analysis demonstrates |
the analysis demonstrates that |
analysis demonstrates that when |
demonstrates that when considering |
that when considering only |
when considering only sabotage |
considering only sabotage attacks |
only sabotage attacks there |
sabotage attacks there are |
attacks there are regions |
there are regions where |
are regions where no |
attack is the best |
is the best strategy |
the miner s dilemma |
miner s dilemma is |
s dilemma is therefore |
dilemma is therefore not |
is therefore not manifested |
therefore not manifested in |
not manifested in that |
manifested in that model |
pool competition for miners |
competition for miners is |
for miners is an |
miners is an incentive |
is an incentive in |
an incentive in and |
incentive in and of |
in and of its |
and of its own |
of its own for |
its own for mutual |
own for mutual attacks |
and a pool may |
a pool may therefore |
pool may therefore choose |
may therefore choose to |
therefore choose to perform |
choose to perform block |
to perform block withholding |
perform block withholding even |
block withholding even if |
withholding even if its |
even if its revenue |
if its revenue would |
its revenue would increase |
revenue would increase only |
would increase only after |
increase only after the |
only after the next |
after the next difficult |
the next difficult adjustment |
the two models are |
two models are therefore |
models are therefore complementary |
the analysis of their |
analysis of their combination |
of their combination is |
their combination is left |
combination is left for |
is left for future |
left for future work |
we assumed in our |
assumed in our analysis |
in our analysis that |
our analysis that pools |
analysis that pools do |
that pools do not |
pools do not charge |
do not charge fees |
not charge fees from |
charge fees from their |
fees from their members |
from their members since |
their members since such |
members since such fees |
since such fees are |
such fees are typically |
fees are typically nominal |
of a pool s |
a pool s revenue |
the model can be |
model can be extended |
can be extended to |
be extended to include |
extended to include pools |
to include pools fees |
fees would add a |
would add a friction |
add a friction element |
a friction element to |
friction element to the |
element to the flow |
to the flow of |
the flow of revenue |
flow of revenue among |
of revenue among infiltrated |
revenue among infiltrated and |
among infiltrated and infiltrating |
infiltrated and infiltrating pools |
would change to take |
change to take into |
to take into account |
take into account a |
into account a pool |
account a pool fee |
a pool fee of |
pool fee of f |
fee of f pp |
of f pp ri |
a pool with a |
pool with a fee |
with a fee of |
a fee of f |
fee of f is |
of f is a |
f is a less |
is a less attractive |
a less attractive target |
less attractive target for |
attractive target for block |
target for block withholding |
since the attacker s |
the attacker s revenue |
attacker s revenue is |
s revenue is reduced |
revenue is reduced by |
is reduced by f |
however it is also |
it is also less |
is also less attractive |
also less attractive for |
less attractive for miners |
attractive for miners in |
for miners in general |
trading off the two |
off the two for |
the two for best |
two for best protection |
for best protection is |
best protection is left |
protection is left for |
is left for future |
left for future work |
as part of the |
part of the treatment |
of the treatment of |
the treatment of the |
treatment of the miner |
r elated w ork |
elated w ork a |
the block withholding attack |
block withholding attack the |
withholding attack the danger |
attack the danger of |
the danger of a |
danger of a block |
of a block withholding |
a block withholding attack |
block withholding attack is |
tcp no loss maelstrom |
withholding attack is as |
no loss maelstrom no |
attack is as old |
loss maelstrom no loss |
is as old as |
maelstrom no loss maelstrom |
as old as bitcoin |
old as bitcoin pools |
the attack was described |
attack was described by |
was described by rosenfeld |
as pools were becoming |
pools were becoming a |
were becoming a dominant |
becoming a dominant player |
a dominant player in |
dominant player in the |
player in the bitcoin |
in the bitcoin world |
the paper described the |
paper described the standard |
described the standard attack |
used by a miner |
by a miner to |
a miner to sabotage |
miner to sabotage a |
to sabotage a pool |
sabotage a pool at |
a pool at the |
pool at the cost |
at the cost of |
the cost of reducing |
cost of reducing its |
of reducing its own |
reducing its own revenue |
a more general view |
more general view of |
general view of fairness |
view of fairness in |
of fairness in proof |
fairness in proof of |
in proof of work |
proof of work schemes |
of work schemes was |
work schemes was discussed |
schemes was discussed in |
in the context of |
the context of the |
context of the hashcash |
of the hashcash system |
one way link latency |
early work did not |
work did not address |
did not address the |
not address the possibility |
address the possibility of |
the possibility of pools |
possibility of pools infiltrating |
of pools infiltrating other |
pools infiltrating other pools |
infiltrating other pools for |
other pools for block |
pools for block withholding |
experimentally demonstrate that block |
demonstrate that block withholding |
that block withholding can |
block withholding can increase |
withholding can increase the |
can increase the attacker |
increase the attacker s |
the attacker s revenue |
way latency collapse from |
latency collapse from occurring |
they do not address |
do not address the |
not address the question |
address the question of |
the question of mutual |
question of mutual attacks |
shows the performance of |
the performance of the |
performance of the user |
space version on a |
have recently noted that |
mbps link and figure |
recently noted that a |
noted that a pool |
that a pool can |
a pool can increase |
pool can increase its |
can increase its overall |
increase its overall revenue |
shows the kernel version |
its overall revenue with |
the kernel version on |
overall revenue with block |
kernel version on a |
revenue with block withholding |
with block withholding if |
block withholding if all |
withholding if all other |
if all other mining |
all other mining is |
other mining is performed |
mining is performed by |
is performed by honest |
performed by honest pools |
the experiment in each |
experiment in each case |
in each case involves |
each case involves running |
case involves running iperf |
we consider the general |
consider the general case |
the general case where |
general case where not |
case where not all |
where not all mining |
not all mining is |
all mining is performed |
mining is performed through |
is performed through public |
performed through public pools |
and analyze situations where |
flows from one node |
analyze situations where pools |
from one node to |
situations where pools can |
one node to another |
where pools can attack |
node to another across |
pools can attack one |
to another across the |
can attack one another |
another across the long |
the discrepancy between the |
discrepancy between the calculations |
distance link with and |
between the calculations of |
link with and without |
with and without intermediary |
and without intermediary maelstrom |
without intermediary maelstrom proxies |
intermediary maelstrom proxies and |
maelstrom proxies and measuring |
proxies and measuring obtained |
and measuring obtained throughput |
measuring obtained throughput while |
obtained throughput while varying |
throughput while varying loss |
while varying loss rate |
left graph on each |
for the special case |
graph on each figure |
the special case analyzed |
special case analyzed there |
case analyzed there and |
analyzed there and our |
there and our results |
and our results can |
our results can be |
results can be explained |
can be explained by |
be explained by the |
explained by the strong |
by the strong approximations |
the strong approximations in |
strong approximations in that |
approximations in that work |
the error bars on |
error bars on the |
we calculate exactly how |
bars on the graphs |
calculate exactly how infiltrating |
on the graphs to |
exactly how infiltrating miners |
the graphs to the |
how infiltrating miners reduce |
graphs to the left |
infiltrating miners reduce the |
to the left are |
miners reduce the revenue |
the left are standard |
reduce the revenue density |
left are standard errors |
the revenue density of |
are standard errors of |
revenue density of the |
standard errors of the |
density of the infiltrated |
errors of the throughput |
of the throughput over |
the throughput over ten |
throughput over ten runs |
of the infiltrated pool |
temporary block withholding in |
block withholding in the |
withholding in the block |
ip s cache of |
in the block withholding |
s cache of tuning |
the block withholding attack |
cache of tuning parameters |
block withholding attack discussed |
of tuning parameters to |
withholding attack discussed in |
tuning parameters to allow |
parameters to allow for |
to allow for repeatable |
allow for repeatable results |
attack discussed in this |
discussed in this work |
in this work the |
this work the withheld |
work the withheld blocks |
the clients in the |
clients in the experiment |
in the experiment are |
the experiment are running |
experiment are running tcp |
the withheld blocks are |
withheld blocks are never |
blocks are never published |
ip reno on a |
reno on a linux |
blocks can be withheld |
can be withheld temporarily |
not following the bitcoin |
following the bitcoin protocol |
to improve an attacker |
improve an attacker s |
an attacker s revenue |
a miner or a |
miner or a pool |
or a pool can |
a pool can perform |
pool can perform a |
can perform a selfish |
the maelstrom parameters used |
perform a selfish mining |
maelstrom parameters used are |
parameters used are r |
a selfish mining attack |
with selfish mining the |
selfish mining the attacker |
mining the attacker increases |
the attacker increases its |
attacker increases its revenue |
increases its revenue by |
its revenue by temporarily |
revenue by temporarily withholding |
by temporarily withholding its |
temporarily withholding its blocks |
withholding its blocks and |
its blocks and publishing |
blocks and publishing them |
and publishing them in |
publishing them in response |
them in response to |
in response to block |
response to block publication |
to block publication by |
block publication by other |
publication by other pools |
by other pools and |
other pools and miners |
this attack is independent |
attack is independent of |
is independent of the |
independent of the block |
of the block withholding |
the block withholding attack |
block withholding attack we |
withholding attack we discuss |
attack we discuss here |
we discuss here and |
discuss here and the |
here and the two |
and the two can |
the two can be |
two can be performed |
can be performed in |
be performed in concert |
space version involved running |
version involved running a |
involved running a single |
an attacker can also |
attacker can also perform |
can also perform a |
also perform a double |
perform a double spending |
a double spending attack |
double spending attack as |
spending attack as follows |
second iperf flow from |
iperf flow from one |
flow from one node |
from one node to |
one node to another |
node to another with |
to another with and |
another with and without |
with and without maelstrom |
and without maelstrom running |
without maelstrom running on |
maelstrom running on the |
running on the routers |
he intentionally generates two |
on the routers and |
intentionally generates two conflicting |
the routers and measuring |
generates two conflicting transactions |
routers and measuring throughput |
and measuring throughput while |
measuring throughput while varying |
throughput while varying the |
places one in a |
while varying the random |
one in a block |
varying the random loss |
in a block it |
a block it withholds |
the random loss rate |
random loss rate on |
loss rate on the |
rate on the link |
on the link and |
and publishes the other |
publishes the other transaction |
the link and the |
link and the one |
after the recipient sees |
the recipient sees the |
recipient sees the published |
sees the published transaction |
to test the kernel |
test the kernel version |
the kernel version at |
the attacker publishes the |
kernel version at gigabit |
version at gigabit speeds |
attacker publishes the withheld |
publishes the withheld block |
the withheld block to |
withheld block to revoke |
block to revoke the |
to revoke the former |
revoke the former transaction |
we ran eight parallel |
ran eight parallel iperf |
eight parallel iperf flows |
parallel iperf flows from |
iperf flows from one |
this attack is performed |
flows from one node |
attack is performed by |
from one node to |
one node to another |
node to another for |
is performed by miners |
performed by miners or |
by miners or pools |
miners or pools against |
or pools against service |
pools against service providers |
against service providers that |
service providers that accept |
providers that accept bitcoin |
and it not directly |
it not directly related |
not directly related to |
directly related to this |
related to this work |
the curves obtained from |
curves obtained from the |
obtained from the two |
from the two versions |
the two versions are |
two versions are almost |
versions are almost identical |
block withholding defense most |
withholding defense most crypto |
we present both to |
present both to show |
both to show that |
currencies use a proof |
to show that the |
show that the kernel |
that the kernel version |
the kernel version successfully |
kernel version successfully scales |
version successfully scales up |
successfully scales up the |
scales up the performance |
up the performance of |
the performance of the |
work architecture similar to |
architecture similar to bitcoin |
performance of the user |
where finding proof of |
space version to hundreds |
finding proof of work |
version to hundreds of |
proof of work is |
to hundreds of megabits |
of work is the |
hundreds of megabits of |
work is the result |
of megabits of traffic |
megabits of traffic per |
of traffic per second |
is the result of |
the result of solution |
result of solution guessing |
of solution guessing and |
solution guessing and checking |
all of the algorithms |
of the algorithms we |
the algorithms we are |
algorithms we are aware |
we are aware of |
are aware of are |
aware of are susceptible |
of are susceptible to |
are susceptible to the |
susceptible to the block |
to the block withholding |
the block withholding attack |
as in all of |
in all of them |
all of them the |
of them the miner |
them the miner can |
the miner can check |
miner can check whether |
can check whether she |
check whether she found |
whether she found a |
she found a full |
we show how tcp |
found a full or |
a full or a |
full or a partial |
or a partial proof |
a partial proof of |
partial proof of work |
ip performance degrades on |
performance degrades on a |
prominent examples are litecoin |
ms link as the |
link as the loss |
as the loss rate |
the loss rate is |
loss rate is increased |
rate is increased from |
it is possible to |
maelstrom masks loss up |
is possible to use |
masks loss up to |
possible to use an |
to use an alternative |
use an alternative proof |
an alternative proof of |
alternative proof of work |
proof of work mechanism |
of work mechanism in |
without significant throughput degradation |
work mechanism in which |
mechanism in which miners |
in which miners would |
which miners would not |
miners would not be |
with the kernel version |
would not be able |
the kernel version achieving |
not be able to |
kernel version achieving two |
be able to distinguish |
version achieving two orders |
able to distinguish partial |
achieving two orders of |
to distinguish partial from |
two orders of magnitude |
distinguish partial from full |
orders of magnitude higher |
partial from full proofs |
from full proofs of |
full proofs of work |
of magnitude higher throughput |
magnitude higher throughput that |
higher throughput that conventional |
throughput that conventional tcp |
the graphs on the |
graphs on the right |
on the right side |
the right side of |
right side of figures |
ip throughput declining on |
throughput declining on a |
declining on a link |
on a link of |
a link of increasing |
link of increasing length |
of increasing length when |
increasing length when subjected |
length when subjected to |
when subjected to uniform |
subjected to uniform loss |
to uniform loss rates |
uniform loss rates of |
such a solution could |
a solution could reduce |
solution could reduce or |
could reduce or remove |
reduce or remove the |
or remove the danger |
remove the danger of |
the danger of block |
danger of block withholding |
making such a change |
such a change may |
a change may not |
change may not be |
may not be in |
not be in the |
be in the interest |
in the interest of |
the top line in |
the interest of the |
interest of the community |
top line in the |
line in the graphs |
in the graphs is |
the graphs is the |
graphs is the performance |
is the performance of |
the performance of tcp |
or even its potential |
ip without loss and |
without loss and provides |
loss and provides an |
could lead to a |
and provides an upper |
lead to a reduction |
provides an upper bound |
to a reduction of |
a reduction of pool |
reduction of pool sizes |
an upper bound for |
upper bound for performance |
bound for performance on |
for performance on the |
performance on the link |
as explained in section |
explained in section ix |
space and kernel versions |
maelstrom masks packet loss |
decentralized pools although most |
masks packet loss and |
pools although most pools |
packet loss and tracks |
although most pools use |
loss and tracks the |
most pools use a |
pools use a centralized |
and tracks the lossless |
use a centralized manager |
tracks the lossless line |
the lossless line closely |
a prominent exception is |
prominent exception is p |
lagging only when the |
only when the link |
when the link latency |
the link latency is |
link latency is low |
latency is low and |
pool a distributed pool |
is low and tcp |
a distributed pool architecture |
distributed pool architecture with |
pool architecture with no |
architecture with no central |
with no central manager |
ip s throughput is |
s throughput is very |
throughput is very high |
ip to attain very |
to attain very high |
attain very high speeds |
very high speeds on |
high speeds on the |
speeds on the gi |
but the question of |
the question of whether |
question of whether a |
of whether a pool |
whether a pool is |
a pool is run |
pool is run by |
is run by a |
run by a centralized |
by a centralized manager |
a centralized manager or |
centralized manager or with |
manager or with a |
or with a decentralized |
with a decentralized architecture |
a decentralized architecture is |
decentralized architecture is almost |
architecture is almost immaterial |
is almost immaterial for |
almost immaterial for the |
immaterial for the attack |
for the attack we |
the attack we describe |
pool group can be |
group can be infiltrated |
can be infiltrated and |
be infiltrated and attacked |
pool code can be |
code can be changed |
can be changed to |
be changed to support |
changed to support attacks |
to support attacks against |
support attacks against other |
attacks against other pools |
on the other hand |
pool can be used |
can be used by |
be used by groups |
used by groups of |
by groups of miners |
groups of miners to |
of miners to easily |
miners to easily form |
to easily form closed |
easily form closed pools |
these do not accept |
do not accept untrusted |
not accept untrusted miners |
and are therefore protected |
are therefore protected against |
therefore protected against block |
protected against block withholding |
c onclusion we explored |
onclusion we explored a |
we explored a block |
explored a block withholding |
a block withholding attack |
block withholding attack among |
withholding attack among bitcoin |
attack among bitcoin mining |
among bitcoin mining pools |
bitcoin mining pools an |
mining pools an attack |
pools an attack that |
an attack that is |
attack that is possible |
that is possible in |
is possible in any |
possible in any similar |
in any similar system |
any similar system that |
similar system that rewards |
system that rewards for |
that rewards for proof |
rewards for proof of |
for proof of work |
such systems are gaining |
systems are gaining popularity |
running most digital currencies |
most digital currencies and |
digital currencies and related |
currencies and related services |
we observe that no |
attacks is not a |
is not a nash |
not a nash equilibrium |
if none of the |
none of the other |
of the other pools |
the other pools attack |
a pool can increase |
pool can increase its |
can increase its revenue |
increase its revenue by |
its revenue by attacking |
revenue by attacking the |
by attacking the others |
when two pools can |
two pools can attack |
pools can attack each |
can attack each other |
they face a version |
face a version of |
a version of the |
version of the prisoner |
of the prisoner s |
the prisoner s dilemma |
if one pool chooses |
one pool chooses to |
pool chooses to attack |
the victim s revenue |
victim s revenue is |
s revenue is reduced |
and it can retaliate |
it can retaliate by |
can retaliate by attacking |
retaliate by attacking and |
by attacking and increase |
attacking and increase its |
and increase its revenue |
at nash equilibrium both |
nash equilibrium both earn |
equilibrium both earn less |
both earn less than |
earn less than they |
less than they would |
than they would have |
they would have if |
would have if neither |
have if neither attacked |
with multiple pools of |
multiple pools of equal |
pools of equal size |
of equal size a |
equal size a similar |
size a similar situation |
a similar situation arises |
similar situation arises with |
situation arises with a |
arises with a symmetric |
with a symmetric equilibrium |
the fact that block |
fact that block withholding |
that block withholding is |
block withholding is not |
withholding is not common |
is not common may |
not common may be |
common may be explained |
may be explained by |
be explained by modeling |
explained by modeling the |
by modeling the attack |
modeling the attack decisions |
the attack decisions as |
attack decisions as an |
decisions as an iterative |
as an iterative prisoner |
an iterative prisoner s |
iterative prisoner s dilemma |
we argue that the |
argue that the situation |
that the situation is |
the situation is unstable |
situation is unstable since |
is unstable since the |
unstable since the attack |
since the attack can |
the attack can be |
attack can be done |
can be done anonymously |
one pool may decide |
pool may decide to |
may decide to increase |
decide to increase its |
to increase its revenue |
increase its revenue and |
its revenue and drag |
revenue and drag the |
and drag the others |
drag the others to |
the others to attack |
others to attack as |
to attack as well |
ending with a reduced |
with a reduced revenue |
a reduced revenue for |
reduced revenue for all |
the inferior revenue would |
inferior revenue would push |
revenue would push miners |
would push miners to |
push miners to join |
miners to join private |
to join private pools |
which can verify that |
can verify that their |
verify that their registered |
that their registered miners |
their registered miners do |
registered miners do not |
miners do not withhold |
do not withhold blocks |
this would lead to |
would lead to smaller |
lead to smaller pools |
and so ultimately to |
so ultimately to a |
ultimately to a better |
to a better environment |
a better environment for |
better environment for bitcoin |
environment for bitcoin as |
for bitcoin as a |
bitcoin as a whole |
for their valuable advice |
the author is grateful |
author is grateful to |
is grateful to ken |
grateful to ken birman |
emin gu n sirer |
and the paper shepherd |
the paper shepherd joseph |
paper shepherd joseph bonneau |
way delivery latency against |
delivery latency against loss |
latency against loss rate |
peer electronic cash system |
ebay s paypal unit |
s paypal unit to |
paypal unit to start |
unit to start accepting |
to start accepting bitcoin |
start accepting bitcoin payments |
google adds bitcoin currency |
adds bitcoin currency conversion |
bitcoin currency conversion to |
currency conversion to search |
repurposing bitcoin work for |
bitcoin work for data |
work for data preservation |
in proceedings of the |
proceedings of the ieee |
of the ieee symposium |
the ieee symposium on |
ieee symposium on security |
symposium on security and |
on security and privacy |
packet delivery latencies gabit |
delivery latencies gabit link |
we had to set |
had to set the |
to set the mtu |
set the mtu of |
the mtu of the |
mtu of the entire |
of the entire path |
the entire path to |
entire path to be |
path to be the |
to be the maximum |
namecoin dns dotbit project |
which meant that the |
meant that the longhaul |
that the longhaul link |
the longhaul link had |
longhaul link had the |
link had the same |
had the same mtu |
the same mtu as |
same mtu as the |
mtu as the inter |
this resulted in the |
resulted in the fragmentation |
in the fragmentation of |
the fragmentation of repair |
fragmentation of repair packets |
of repair packets sent |
repair packets sent over |
packets sent over udp |
sent over udp on |
over udp on the |
udp on the long |
haul link into two |
link into two ip |
into two ip packet |
two ip packet fragments |
since the loss of |
the loss of a |
loss of a single |
of a single fragment |
a single fragment resulted |
single fragment resulted in |
fragment resulted in the |
resulted in the loss |
in the loss of |
the loss of the |
loss of the repair |
we observed a higher |
observed a higher loss |
a higher loss rate |
higher loss rate for |
loss rate for repairs |
rate for repairs than |
for repairs than for |
repairs than for data |
than for data packets |
a next generation smart |
we expect performance to |
next generation smart contract |
expect performance to be |
performance to be better |
to be better on |
be better on a |
better on a network |
on a network where |
a network where the |
network where the mtu |
where the mtu of |
the mtu of the |
mtu of the long |
haul link is truly |
link is truly larger |
is truly larger than |
truly larger than the |
larger than the mtu |
than the mtu within |
the mtu within each |
mtu within each cluster |
latency metrics to measure |
metrics to measure the |
to measure the latency |
measure the latency effects |
the latency effects of |
latency effects of tcp |
mbps stream between two |
stream between two nodes |
between two nodes over |
two nodes over a |
and simultaneously ran a |
mbps flow alongside on |
flow alongside on the |
alongside on the same |
on the same link |
the same link to |
same link to simulate |
link to simulate a |
to simulate a real |
analysis of bitcoin pooled |
of bitcoin pooled mining |
time stream combined with |
bitcoin pooled mining reward |
stream combined with other |
pooled mining reward systems |
combined with other intercluster |
with other intercluster traffic |
shows the average delivery |
the average delivery latency |
average delivery latency of |
level packets in the |
as loss rates go |
loss rates go up |
shows the same scenario |
the same scenario with |
same scenario with a |
scenario with a constant |
with a constant uniformly |
a constant uniformly random |
constant uniformly random loss |
uniformly random loss rate |
random loss rate of |
and varying oneway latency |
maelstrom s delivery latency |
s delivery latency is |
delivery latency is almost |
latency is almost exactly |
is almost exactly equal |
almost exactly equal to |
exactly equal to the |
equal to the one |
way latency on the |
latency on the link |
ip takes more than |
takes more than twice |
more than twice as |
than twice as long |
twice as long once |
as long once one |
way latencies go past |
research perspectives on bitcoin |
perspectives on bitcoin and |
on bitcoin and secondgeneration |
bitcoin and secondgeneration cryptocurrencies |
in ieee symposium on |
ieee symposium on security |
symposium on security and |
on security and privacy |
plots delivery latency against |
delivery latency against message |
latency against message identifier |
the spikes in latency |
spikes in latency are |
in latency are triggered |
latency are triggered by |
are triggered by losses |
triggered by losses that |
by losses that lead |
losses that lead to |
that lead to packets |
lead to packets piling |
to packets piling up |
packets piling up at |
piling up at the |
up at the receiver |
a key point is |
key point is that |
point is that we |
is that we are |
that we are plotting |
we are plotting the |
are plotting the delivery |
plotting the delivery latency |
the delivery latency of |
delivery latency of all |
latency of all packets |
not just lost ones |
ip delays correctly received |
delays correctly received packets |
correctly received packets while |
received packets while waiting |
packets while waiting for |
while waiting for missing |
waiting for missing packets |
for missing packets sequenced |
missing packets sequenced earlier |
packets sequenced earlier by |
sequenced earlier by the |
earlier by the sender |
by the sender the |
the sender the effect |
sender the effect of |
the effect of this |
effect of this is |
of this is shown |
this is shown in |
is shown in figure |
where single packet losses |
single packet losses cause |
packet losses cause spikes |
losses cause spikes in |
cause spikes in delivery |
spikes in delivery latency |
in delivery latency that |
delivery latency that last |
latency that last for |
that last for hundreds |
last for hundreds of |
for hundreds of packets |
the low data rate |
low data rate in |
data rate in the |
rate in the flow |
in the flow of |
the flow of roughly |
kb packets per rtt |
packets per rtt makes |
per rtt makes tcp |
ip flow control delays |
flow control delays at |
control delays at the |
delays at the sender |
at the sender unlikely |
given that the congestion |
that the congestion control |
the congestion control algorithm |
congestion control algorithm is |
control algorithm is reno |
which implements fast recovery |
implements fast recovery and |
fast recovery and halves |
recovery and halves the |
and halves the congestion |
halves the congestion window |
the congestion window on |
congestion window on packet |
window on packet loss |
on packet loss rather |
packet loss rather than |
loss rather than resetting |
rather than resetting it |
than resetting it completely |
the maelstrom configuration used |
maelstrom configuration used is |
information propagation in the |
propagation in the bitcoin |
in the bitcoin network |
th ieee international conference |
ieee international conference on |
international conference on peer |
bitcoin and the age |
and the age of |
the age of bespoke |
age of bespoke silicon |
in proceedings of the |
international conference on compilers |
architectures and synthesis for |
and synthesis for embedded |
synthesis for embedded systems |
into the bitcoin mines |
relatively prime interleaves offer |
prime interleaves offer better |
interleaves offer better performance |
offer better performance r |
layered interleaving and bursty |
interleaving and bursty loss |
and bursty loss thus |
bursty loss thus far |
loss thus far we |
thus far we have |
far we have shown |
we have shown how |
have shown how maelstrom |
shown how maelstrom effectively |
how maelstrom effectively hides |
maelstrom effectively hides loss |
effectively hides loss from |
hides loss from tcp |
ip for packets dropped |
for packets dropped with |
packets dropped with uniform |
dropped with uniform randomness |
we examine the performance |
examine the performance of |
the performance of the |
performance of the layered |
of the layered interleaving |
the layered interleaving algorithm |
showing how different parameterizations |
how different parameterizations handle |
different parameterizations handle bursty |
parameterizations handle bursty loss |
handle bursty loss patterns |
we use a loss |
use a loss model |
a loss model where |
loss model where packets |
model where packets are |
where packets are dropped |
packets are dropped in |
are dropped in bursts |
dropped in bursts of |
in bursts of fixed |
bursts of fixed length |
allowing us to study |
us to study the |
to study the impact |
study the impact of |
the impact of burst |
impact of burst length |
of burst length on |
burst length on performance |
the link has a |
link has a one |
ms and a loss |
and a loss rate |
a loss rate of |
where it is varied |
mbps flow of udp |
flow of udp packets |
of udp packets is |
udp packets is sent |
packets is sent over |
is sent over it |
we show that our |
show that our observation |
that our observation in |
our observation in section |
is correct for high |
correct for high loss |
for high loss rates |
high loss rates if |
loss rates if the |
rates if the interleaves |
if the interleaves are |
the interleaves are relatively |
interleaves are relatively prime |
performance improves substantially when |
improves substantially when loss |
substantially when loss rates |
when loss rates are |
loss rates are high |
rates are high and |
are high and losses |
high and losses are |
and losses are bursty |
the graph plots the |
graph plots the percentage |
plots the percentage of |
the percentage of lost |
percentage of lost packets |
of lost packets successfully |
lost packets successfully recovered |
packets successfully recovered on |
successfully recovered on the |
recovered on the y |
axis against an x |
axis of loss rates |
of loss rates on |
loss rates on a |
rates on a log |
on a log scale |
the maelstrom configuration used |
maelstrom configuration used is |
configuration used is r |
we show the ability |
show the ability of |
the ability of layered |
ability of layered interleaving |
of layered interleaving to |
layered interleaving to provide |
interleaving to provide gracefully |
to provide gracefully degrading |
provide gracefully degrading performance |
gracefully degrading performance in |
degrading performance in the |
performance in the face |
in the face of |
the face of bursty |
face of bursty loss |
we plot the percentage |
plot the percentage of |
the percentage of lost |
percentage of lost packets |
of lost packets successfully |
lost packets successfully recovered |
packets successfully recovered against |
successfully recovered against the |
recovered against the length |
against the length of |
the length of loss |
length of loss bursts |
of loss bursts for |
loss bursts for two |
bursts for two different |
for two different sets |
two different sets of |
different sets of interleaves |
and in the bottom |
in the bottom graph |
the bottom graph we |
bottom graph we plot |
graph we plot the |
we plot the average |
plot the average latency |
the average latency at |
average latency at which |
latency at which the |
at which the packets |
which the packets were |
the packets were recovered |
recovery latency is defined |
latency is defined as |
is defined as the |
defined as the difference |
as the difference between |
the difference between the |
difference between the eventual |
between the eventual delivery |
the eventual delivery time |
eventual delivery time of |
delivery time of the |
time of the recovered |
of the recovered packet |
the recovered packet and |
recovered packet and the |
packet and the one |
way latency of the |
latency of the link |
we confirmed that the |
confirmed that the emulab |
that the emulab link |
the emulab link had |
emulab link had almost |
link had almost no |
had almost no jitter |
almost no jitter on |
no jitter on correctly |
jitter on correctly delivered |
on correctly delivered packets |
way latency an accurate |
latency an accurate estimate |
an accurate estimate of |
accurate estimate of expected |
estimate of expected lossless |
of expected lossless delivery |
expected lossless delivery time |
increasing the interleaves results |
the interleaves results in |
interleaves results in much |
results in much higher |
in much higher recovery |
much higher recovery percentages |
higher recovery percentages at |
how a mining monopoly |
recovery percentages at large |
a mining monopoly can |
percentages at large burst |
mining monopoly can attack |
at large burst sizes |
monopoly can attack bitcoin |
but percentage of packets |
percentage of packets recovered |
percentage of packets recovered |
majority is not enough |
bitcoin mining is vulnerable |
in financial cryptography and |
financial cryptography and data |
cryptography and data security |
cooperative equilibrium for supergames |
the review of economic |
review of economic studies |
term competition a game |
layered interleaving recovery percentage |
interleaving recovery percentage and |
recovery percentage and latency |
percentage and latency comes |
and latency comes at |
latency comes at the |
comes at the cost |
at the cost of |
the cost of higher |
cost of higher recovery |
of higher recovery latency |
set of interleaves catches |
of interleaves catches almost |
interleaves catches almost all |
catches almost all packets |
almost all packets in |
all packets in an |
packets in an extended |
in an extended burst |
an extended burst of |
packets at an average |
at an average latency |
an average latency of |
average latency of around |
while repairing all random |
repairing all random singleton |
all random singleton losses |
random singleton losses within |
the graphs also show |
graphs also show recovery |
also show recovery latency |
show recovery latency rising |
recovery latency rising gracefully |
latency rising gracefully with |
rising gracefully with the |
gracefully with the increase |
with the increase in |
the increase in loss |
increase in loss burst |
in loss burst length |
the longer the burst |
the longer it takes |
longer it takes to |
it takes to recover |
takes to recover the |
to recover the lost |
recover the lost packets |
the maelstrom configuration used |
maelstrom configuration used is |
configuration used is r |
io bitcoin mining pool |
we show histograms of |
show histograms of recovery |
histograms of recovery latencies |
of recovery latencies for |
recovery latencies for the |
latencies for the two |
for the two interleave |
the two interleave configurations |
two interleave configurations under |
interleave configurations under different |
configurations under different burst |
under different burst lengths |
the histograms confirm the |
histograms confirm the trends |
confirm the trends described |
the trends described above |
packet recoveries take longer |
recoveries take longer from |
take longer from left |
longer from left to |
from left to right |
left to right as |
to right as we |
right as we increase |
as we increase loss |
we increase loss burst |
increase loss burst length |
and from top to |
from top to bottom |
top to bottom as |
to bottom as we |
bottom as we increase |
as we increase the |
we increase the interleave |
increase the interleave values |
illustrates the difference between |
the difference between a |
difference between a traditional |
between a traditional fec |
a traditional fec code |
traditional fec code and |
fec code and layered |
code and layered interleaving |
and layered interleaving by |
layered interleaving by plotting |
interleaving by plotting a |
element moving average of |
moving average of recovery |
average of recovery latencies |
of recovery latencies for |
recovery latencies for both |
latencies for both codes |
the channel is configured |
channel is configured to |
is configured to lose |
configured to lose singleton |
to lose singleton packets |
lose singleton packets randomly |
singleton packets randomly at |
packets randomly at a |
randomly at a loss |
at a loss rate |
a loss rate of |
and additionally lose long |
additionally lose long bursts |
lose long bursts of |
packets at occasional intervals |
both codes recovery latency |
reed solomon layered interleaving |
kncminer bitcoin mining cloud |
bitcoin mining cloud mining |
solomon versus layered interleaving |
versus layered interleaving are |
layered interleaving are configured |
interleaving are configured with |
are configured with r |
and recover all lost |
recover all lost packets |
all lost packets reed |
solomon uses an interleave |
uses an interleave of |
and layered interleaving uses |
layered interleaving uses interleaves |
interleaving uses interleaves of |
and consequently both have |
consequently both have a |
both have a maximum |
have a maximum tolerable |
a maximum tolerable burst |
maximum tolerable burst length |
tolerable burst length of |
we use a publicly |
use a publicly available |
a publicly available implementation |
publicly available implementation of |
available implementation of a |
implementation of a reed |
an authorization architecture for |
solomon code based on |
authorization architecture for trustworthy |
code based on vandermonde |
architecture for trustworthy computing |
based on vandermonde matrices |
in proceedings of the |
proceedings of the twenty |
third acm symposium on |
acm symposium on operating |
symposium on operating systems |
on operating systems principles |
the code is plugged |
code is plugged into |
is plugged into maelstrom |
plugged into maelstrom instead |
into maelstrom instead of |
maelstrom instead of layered |
instead of layered interleaving |
showing that we can |
that we can use |
we can use new |
can use new encodings |
use new encodings within |
new encodings within the |
encodings within the same |
within the same framework |
the same framework seamlessly |
solomon code recovers all |
code recovers all lost |
recovers all lost packets |
all lost packets with |
lost packets with roughly |
packets with roughly the |
with roughly the same |
roughly the same latency |
the same latency whereas |
same latency whereas layered |
latency whereas layered interleaving |
whereas layered interleaving recovers |
layered interleaving recovers singleton |
interleaving recovers singleton losses |
recovers singleton losses almost |
singleton losses almost immediately |
losses almost immediately and |
almost immediately and exhibits |
immediately and exhibits latency |
and exhibits latency spikes |
exhibits latency spikes whenever |
latency spikes whenever the |
spikes whenever the longer |
whenever the longer loss |
the longer loss burst |
longer loss burst occurs |
related work a significant |
work a significant body |
a significant body of |
significant body of work |
body of work on |
of work on application |
work on application and |
on application and tcp |
ip performance over high |
distance networks exists in |
networks exists in the |
exists in the context |
in the context of |
the context of high |
the use of parallel |
use of parallel sockets |
of parallel sockets for |
parallel sockets for higher |
sockets for higher throughput |
for higher throughput in |
higher throughput in the |
throughput in the face |
in the face of |
the face of non |
congestion loss was proposed |
loss was proposed in |
was proposed in psockets |
a number of protocols |
number of protocols have |
of protocols have been |
protocols have been suggested |
have been suggested as |
been suggested as replacements |
suggested as replacements for |
as replacements for tcp |
ip in such settings |
in such settings xcp |
are a few but |
a few but all |
few but all require |
but all require modifications |
all require modifications to |
require modifications to end |
or the intervening network |
on power splitting games |
power splitting games in |
some approaches seek to |
splitting games in distributed |
approaches seek to differentiate |
games in distributed computation |
seek to differentiate between |
to differentiate between congestion |
differentiate between congestion and |
between congestion and non |
the case of bitcoin |
case of bitcoin pooled |
of bitcoin pooled mining |
maelstrom is a transparent |
is a transparent performance |
a transparent performance enhancing |
transparent performance enhancing proxy |
as defined in rfc |
numerous implementations of peps |
implementations of peps exist |
of peps exist for |
peps exist for improving |
exist for improving tcp |
for improving tcp performance |
improving tcp performance on |
tcp performance on satellite |
but we are not |
we are not aware |
are not aware of |
not aware of any |
aware of any peps |
of any peps that |
any peps that use |
peps that use fec |
that use fec to |
use fec to mask |
fec to mask errors |
to mask errors on |
mask errors on long |
based fec for reliable |
fec for reliable communication |
weekly bitcoin network statistics |
for reliable communication was |
reliable communication was first |
communication was first explored |
was first explored by |
first explored by rizzo |
suggested the use of |
the use of fec |
use of fec for |
of fec for tcp |
ip retransmissions over aggregated |
retransmissions over aggregated traffic |
over aggregated traffic within |
aggregated traffic within an |
traffic within an overlay |
within an overlay network |
an overlay network in |
overlay network in the |
network in the commodity |
in the commodity internet |
uses fec for real |
modulating the rate of |
the rate of encoding |
rate of encoding adaptively |
the use of end |
host fec under tcp |
ip has been explored |
has been explored in |
a multitude of different |
multitude of different fec |
of different fec encodings |
different fec encodings exist |
fec encodings exist in |
encodings exist in literature |
they can broadly be |
can broadly be categorized |
broadly be categorized into |
be categorized into optimal |
categorized into optimal erasure |
into optimal erasure codes |
optimal erasure codes and |
erasure codes and near |
known optimal code is |
optimal code is reed |
which we described previously |
we described previously as |
described previously as generating |
previously as generating c |
as generating c repair |
generating c repair packets |
c repair packets from |
repair packets from r |
packets from r source |
theoretic analysis of ddos |
from r source packets |
analysis of ddos attacks |
of ddos attacks against |
ddos attacks against bitcoin |
attacks against bitcoin mining |
against bitcoin mining pools |
any r of the |
r of the resulting |
of the resulting r |
in workshop on bitcoin |
workshop on bitcoin research |
c packets can be |
packets can be used |
can be used to |
be used to reconstruct |
used to reconstruct the |
to reconstruct the r |
reconstruct the r source |
the r source packets |
optimal codes such as |
codes such as tornado |
such as tornado and |
as tornado and lt |
off encoding speed for |
encoding speed for large |
speed for large data |
for large data sizes |
large data sizes against |
data sizes against a |
sizes against a loss |
against a loss of |
a loss of optimality |
loss of optimality the |
of optimality the receiver |
optimality the receiver needs |
the receiver needs to |
receiver needs to receive |
needs to receive slightly |
to receive slightly more |
when bitcoin mining pools |
receive slightly more than |
bitcoin mining pools run |
slightly more than r |
mining pools run dry |
more than r source |
than r source or |
r source or repair |
source or repair packets |
or repair packets to |
in workshop on bitcoin |
repair packets to regenerate |
workshop on bitcoin research |
packets to regenerate the |
to regenerate the original |
regenerate the original r |
the original r data |
original r data packets |
optimal codes are extremely |
codes are extremely fast |
are extremely fast for |
extremely fast for encoding |
fast for encoding over |
for encoding over large |
encoding over large sets |
over large sets of |
large sets of data |
sets of data but |
of data but not |
data but not of |
but not of significant |
not of significant importance |
of significant importance for |
significant importance for real |
since optimal codes perform |
optimal codes perform equally |
codes perform equally well |
perform equally well with |
equally well with small |
well with small data |
with small data sizes |
comparison of mining pools |
of particular relevance are |
particular relevance are growth |
relevance are growth codes |
which use multiple encoding |
use multiple encoding rates |
multiple encoding rates for |
encoding rates for different |
rates for different overhead |
comparison of mining pools |
for different overhead levels |
layered interleaving uses multiple |
interleaving uses multiple interleaves |
uses multiple interleaves for |
multiple interleaves for different |
interleaves for different burst |
for different burst resilience |
different burst resilience levels |
burst resilience levels without |
resilience levels without modulating |
levels without modulating the |
without modulating the encoding |
modulating the encoding rate |
the effect of random |
effect of random losses |
of random losses on |
random losses on tcp |
ip has been studied |
has been studied in |
been studied in depth |
studied in depth by |
in depth by lakshman |
hashcash amortizable publicly auditable |
amortizable publicly auditable cost |
padhye s analytical model |
provides a means to |
a means to gauge |
means to gauge the |
to gauge the impact |
gauge the impact of |
the impact of packet |
impact of packet loss |
of packet loss on |
packet loss on tcp |
while most published studies |
most published studies of |
published studies of packet |
studies of packet loss |
of packet loss are |
packet loss are based |
loss are based on |
are based on the |
based on the commodity |
on the commodity internet |
the commodity internet rather |
commodity internet rather than |
internet rather than highspeed |
rather than highspeed lambda |
than highspeed lambda links |
study the sprint backbone |
the sprint backbone and |
sprint backbone and make |
backbone and make two |
and make two observations |
make two observations that |
two observations that could |
observations that could be |
that could be explained |
could be explained by |
be explained by non |
links are rarely loaded |
are rarely loaded at |
rarely loaded at more |
loaded at more than |
hashcash a denial of |
a denial of service |
denial of service counter |
of capacity and b |
packet reordering events occur |
reordering events occur for |
events occur for some |
occur for some flows |
possibly indicating packet loss |
indicating packet loss followed |
packet loss followed by |
loss followed by retransmissions |
future work scaling maelstrom |
work scaling maelstrom to |
scaling maelstrom to multiple |
maelstrom to multiple gigabits |
to multiple gigabits per |
multiple gigabits per second |
gigabits per second of |
per second of traffic |
second of traffic will |
of traffic will require |
traffic will require small |
will require small rack |
style clusters of tens |
clusters of tens of |
of tens of machines |
tens of machines to |
of machines to distribute |
machines to distribute encoding |
to distribute encoding load |
distribute encoding load over |
we need to design |
need to design intelligent |
to design intelligent load |
over mechanisms for such |
mechanisms for such a |
for such a scheme |
we have described layered |
have described layered interleaving |
described layered interleaving with |
layered interleaving with fixed |
and the next step |
the next step in |
next step in extending |
step in extending this |
in extending this protocol |
extending this protocol is |
this protocol is to |
protocol is to make |
is to make it |
to make it adaptive |
changing interleaves and rate |
interleaves and rate as |
and rate as loss |
rate as loss patterns |
as loss patterns in |
loss patterns in the |
on subversive miner strategies |
patterns in the link |
subversive miner strategies and |
in the link change |
miner strategies and block |
strategies and block withholding |
and block withholding attack |
block withholding attack in |
withholding attack in bitcoin |
attack in bitcoin digital |
in bitcoin digital currency |
conclusion modern distributed systems |
modern distributed systems are |
distributed systems are compelled |
systems are compelled by |
are compelled by real |
world imperatives to coordinate |
imperatives to coordinate across |
to coordinate across datacenters |
coordinate across datacenters separated |
across datacenters separated by |
how to disincentivize large |
to disincentivize large bitcoin |
disincentivize large bitcoin mining |
large bitcoin mining pools |
latency histograms for i |
latency histograms for i |
packet loss cripples the |
loss cripples the performance |
cripples the performance notes |
the performance notes of |
performance notes of such |
notes of such systems |
and reliability and flow |
are increasingly popular and |
increasingly popular and designed |
popular and designed for |
and designed for lans |
designed for lans and |
or the commodity internet |
the commodity internet fail |
commodity internet fail to |
internet fail to used |
fail to used for |
to used for applications |
used for applications such |
for applications such as |
applications such as efficiently |
such as efficiently distributing |
as efficiently distributing bulk |
efficiently distributing bulk data |
achieve optimal performance on |
optimal performance on the |
performance on the high |
it is not obvious |
is not obvious that |
not obvious that these |
obvious that these have |
that these have utility |
these have utility in |
have utility in real |
time communi lambda networks |
communi lambda networks linking |
lambda networks linking datacenters |
protocols is not an |
is not an option |
not an option for |
an option for commodity |
option for commodity clusters |
for commodity clusters where |
commodity clusters where standardization |
clusters where standardization is |
where standardization is critical |
standardization is critical for |
is critical for cost |
critical for cost mitigation |
maelstrom is an edge |
is an edge appliance |
an edge appliance that |
edge appliance that uses |
appliance that uses forward |
that uses forward error |
uses forward error correction |
forward error correction references |
error correction references to |
correction references to mask |
references to mask packet |
to mask packet loss |
mask packet loss from |
packet loss from end |
global crossing current network |
crossing current network performance |
ip throughput and latency |
throughput and latency by |
and latency by orders |
latency by orders of |
by orders of magninetwork |
last tude when loss |
tude when loss occurs |
maelstrom is easy to |
is easy to install |
easy to install and |
to install and accessed |
install and accessed feb |
and is completely transparent |
is completely transparent to |
completely transparent to applications |
transparent to applications and |
qwest ip network statistics |
protocols literally providing reliability |
literally providing reliability in |
providing reliability in an |
reliability in an inexpennet |
acknowledgments we would like |
we would like to |
would like to thank |
like to thank our |
to thank our shepherd |
thank our shepherd robert |
our shepherd robert morris |
shepherd robert morris and |
robert morris and the |
morris and the other |
and the other reviewers |
the other reviewers for |
other reviewers for extensive |
reviewers for extensive comments |
for extensive comments that |
extensive comments that significantly |
comments that significantly shaped |
that significantly shaped the |
significantly shaped the final |
shaped the final version |
the final version of |
final version of the |
version of the paper |
vidhyashankar venkataraman and vivek |
venkataraman and vivek vishnumurthy |
and vivek vishnumurthy provided |
vivek vishnumurthy provided useful |
vishnumurthy provided useful comments |
tom boures provided valuable |
boures provided valuable insight |
provided valuable insight into |
valuable insight into the |
insight into the quality |
into the quality of |
the quality of existing |
quality of existing fiber |
of existing fiber links |
stanislav shalunov provided information |
shalunov provided information on |
provided information on loss |
information on loss rates |
on loss rates on |
loss rates on internet |
and paul wefel gave |
paul wefel gave us |
wefel gave us access |
gave us access to |
us access to teragrid |
access to teragrid loss |
to teragrid loss measurements |
nat and packet mangling |
and packet mangling for |
packet mangling for linux |
lateral error correction for |
error correction for timecritical |
correction for timecritical multicast |
fourth usenix symposium on |
usenix symposium on networked |
symposium on networked systems |
on networked systems design |
networked systems design and |
systems design and implementation |
performance enhancing proxies intended |
enhancing proxies intended to |
proxies intended to mitigate |
intended to mitigate link |
enhanced loss differentiation algorithms |
loss differentiation algorithms for |
differentiation algorithms for use |
algorithms for use in |
for use in tcp |
use in tcp sources |
in tcp sources over |
tcp sources over heterogeneous |
sources over heterogeneous wireless |
over heterogeneous wireless networks |
ieee global telecommunications conference |
flow aggregation for enhanced |
aggregation for enhanced tcp |
for enhanced tcp over |
enhanced tcp over wide |
tcp over wide area |
over wide area wireless |
vice president of research |
president of research and |
of research and t |
multicast routing in datagram |
routing in datagram internetworks |
in datagram internetworks and |
datagram internetworks and extended |
internetworks and extended lans |
level traffic measurements from |
traffic measurements from the |
measurements from the sprint |
from the sprint ip |
the sprint ip backbone |
a transport protocol for |
transport protocol for grid |
protocol for grid computing |
journal of grid computing |
optical domain performance monitoring |
optical fiber communication conference |
end performance effects of |
performance effects of parallel |
effects of parallel tcp |
of parallel tcp sockets |
parallel tcp sockets on |
tcp sockets on a |
sockets on a lossy |
on a lossy wide |
the effects of systemic |
effects of systemic packet |
of systemic packet loss |
systemic packet loss on |
packet loss on aggregate |
loss on aggregate tcp |
on aggregate tcp flows |
ieee conference on supercomputing |
predictable high performance bulk |
high performance bulk data |
performance bulk data transfer |
ieee international conference on |
international conference on cluster |
conference on cluster computing |
the case for packet |
case for packet level |
for packet level fec |
proceedings of the tc |
fifth international workshop on |
international workshop on protocols |
workshop on protocols for |
on protocols for high |
gigabit ethernet on commodity |
ethernet on commodity systems |
where did my performance |
did my performance go |
rate limiting rears its |
limiting rears its ugly |
rears its ugly head |
isn t quite enough |
modified tcp congestion avoidance |
tcp congestion avoidance algorithm |
physical layer impact upon |
layer impact upon packet |
impact upon packet errors |
passive and active measurement |
and active measurement workshop |
maximizing sensor network data |
sensor network data persistence |
in proceedings of acm |
proceedings of acm sigcomm |
congestion control for high |
control for high bandwidth |
and protocols for computer |
protocols for computer communications |
journal of lightwave technology |
a cross layer study |
cross layer study of |
layer study of packet |
study of packet loss |
of packet loss in |
packet loss in all |
the performance of tcp |
ip for networks with |
for networks with high |
networks with high bandwidth |
delay products and random |
products and random loss |
acm transactions on networking |
rd annual ieee symposium |
annual ieee symposium on |
ieee symposium on foundations |
symposium on foundations of |
on foundations of computer |
foundations of computer science |
end forward error correction |
international zurich seminar on |
zurich seminar on communications |
rateless codes and big |
codes and big downloads |
paritybased loss recovery for |
loss recovery for reliable |
recovery for reliable multicast |
for reliable multicast transmission |
in proceedings of the |
proceedings of the acm |
of the acm sigcomm |
a simple model and |
simple model and its |
model and its empirical |
and its empirical validation |
an adaptive forward error |
adaptive forward error correction |
forward error correction protocol |
error correction protocol for |
correction protocol for end |
computer communications and networks |
th international conference on |
businesses see the light |
effective erasure codes for |
erasure codes for reliable |
codes for reliable computer |
for reliable computer communication |
reliable computer communication protocols |
on the feasibility of |
the feasibility of software |
feasibility of software fec |
the case for application |
level network striping for |
network striping for data |
striping for data intensive |
for data intensive applications |
data intensive applications using |
intensive applications using high |
applications using high speed |
using high speed wide |
high speed wide area |
speed wide area networks |
ieee conference on supercomputing |
google s secret plans |
s secret plans for |
secret plans for all |
plans for all that |
for all that dark |
all that dark fiber |
an overlay based architecture |
overlay based architecture for |
based architecture for enhancing |
architecture for enhancing internet |
for enhancing internet qos |
first usenix symposium on |
usenix symposium on networked |
symposium on networked systems |
on networked systems design |
networked systems design and |
systems design and implementation |
udp bandwidth measurement tool |
a tcp performance enhancing |
tcp performance enhancing proxy |
performance enhancing proxy for |
enhancing proxy for satellite |
proxy for satellite links |
proceedings of the second |
of the second international |
the second international ifip |
networking conference on networking |
conference on networking technologies |
performance of computer and |
of computer and communication |
computer and communication networks |
and mobile and wireless |
mobile and wireless communications |
tsunami file transfer protocol |
workshop on protocols for |
on protocols for fast |
protocols for fast longdistance |
for fast longdistance networks |
the university of illinois |
university of illinois national |
of illinois national center |
illinois national center for |
national center for supercomputing |
center for supercomputing applications |
an integrated experimental environment |
integrated experimental environment for |
experimental environment for distributed |
environment for distributed systems |
for distributed systems and |
distributed systems and networks |
of the fifth symposium |
the fifth symposium on |
fifth symposium on operating |
symposium on operating systems |
on operating systems design |
operating systems design and |
systems design and implementation |
solomon codes and their |
codes and their applications |
the: 7744
of: 4121
a: 3489
to: 3267
and: 3193
in: 2417
is: 1914
for: 1353
that: 1296
with: 990
on: 965
are: 940
we: 925
as: 917
it: 771
by: 745
be: 695
this: 681
data: 659
pool: 659
s: 654
an: 631
at: 630
can: 574
system: 559
its: 497
from: 483
not: 466
or: 452
r: 429
which: 418
each: 404
file: 401
all: 377
systems: 372
m: 368
if: 362
figure: 357
x: 343
one: 338
time: 335
nodes: 332
pools: 325
revenue: 325
t: 325
packets: 320
rate: 320
other: 317
cache: 310
but: 309
our: 291
have: 289
work: 287
i: 282
performance: 281
such: 271
when: 268
loss: 258
p: 258
miners: 257
has: 252
bandwidth: 251
packet: 244
c: 243
network: 242
their: 236
will: 232
attack: 230
block: 230
mining: 230
j: 228
these: 223
than: 222
only: 221
end: 220
use: 220
they: 218
more: 216
node: 209
bitcoin: 204
no: 204
high: 203
number: 202
services: 200
latency: 197
b: 196
may: 193
transactions: 192
would: 192
server: 190
two: 188
where: 188
update: 187
power: 182
between: 180
used: 177
based: 174
d: 173
over: 171
any: 170
different: 169
distributed: 166
miner: 164
new: 164
single: 164
also: 162
client: 161
some: 161
large: 159
tcp: 159
was: 157
web: 157
since: 153
objects: 152
application: 150
ip: 150
service: 150
using: 149
updates: 146
e: 144
protocol: 144
them: 144
set: 140
read: 139
access: 138
applications: 137
process: 137
files: 134
transaction: 133
maelstrom: 132
traffic: 132
there: 131
operating: 130
gossip: 129
however: 129
into: 129
section: 129
withholding: 129
rates: 128
example: 127
mfs: 127
so: 127
communication: 124
consistency: 124
both: 122
same: 122
size: 122
object: 121
priorities: 120
repair: 120
even: 119
peer: 119
throughput: 119
chain: 118
g: 116
control: 114
failure: 114
information: 113
while: 113
first: 112
recovery: 112
were: 112
results: 111
database: 110
protocols: 110
case: 109
acm: 106
link: 106
low: 106
therefore: 106
then: 105
without: 105
rpcs: 104
small: 104
clients: 103
writes: 103
networks: 102
could: 101
second: 101
ms: 100
rpc: 99
do: 98
many: 98
asynchronous: 97
writeback: 97
log: 96
model: 96
within: 96
group: 95
send: 95
does: 94
proof: 94
most: 93
up: 93
memory: 92
state: 91
user: 91
average: 90
content: 90
version: 90
level: 89
order: 89
back: 87
been: 87
support: 87
less: 86
through: 85
windows: 85
available: 84
groups: 84
long: 84
out: 84
see: 84
after: 83
another: 83
approach: 83
fec: 83
full: 83
how: 83
against: 82
architecture: 82
infiltration: 82
message: 82
o: 82
disk: 81
k: 81
messages: 81
need: 81
qsm: 81
about: 80
kb: 80
multicast: 80
sender: 80
uses: 80
management: 79
n: 79
q: 79
way: 79
delivery: 78
total: 78
disks: 77
experiments: 77
possible: 77
write: 77
design: 76
higher: 76
delay: 75
increase: 75
layer: 75
local: 75
research: 75
scale: 75
upload: 75
computer: 74
f: 74
implementation: 74
might: 74
source: 74
analysis: 73
burst: 73
live: 73
probability: 73
workloads: 73
accesses: 72
because: 72
every: 72
lost: 72
solution: 71
cost: 70
due: 70
step: 70
v: 70
value: 70
equilibrium: 69
like: 69
multiple: 69
manager: 68
per: 68
priority: 68
well: 68
list: 67
overhead: 67
computing: 66
load: 66
running: 66
cornell: 65
http: 65
mechanism: 65
opportunistic: 65
run: 65
should: 65
under: 65
attacking: 64
cluster: 64
operations: 64
proceedings: 64
servers: 64
show: 64
shown: 64
test: 64
attacks: 63
layered: 63
much: 63
paper: 63
partial: 63
received: 63
stream: 63
attacker: 62
dependency: 62
kernel: 62
point: 62
solutions: 62
event: 61
l: 61
maximum: 61
must: 61
scheme: 61
shows: 61
sizes: 61
code: 60
game: 60
interleave: 60
prefetching: 60
proofs: 60
replication: 60
result: 60
round: 60
storage: 60
us: 60
perform: 59
sending: 59
very: 59
inconsistency: 58
once: 58
provide: 58
threshold: 58
users: 58
workload: 58
flow: 57
scalable: 57
sent: 57
similar: 57
streaming: 57
being: 56
conference: 56
symposium: 56
download: 55
nt: 55
problem: 55
random: 55
receiver: 55
reduce: 55
requests: 55
until: 55
before: 54
correct: 54
factor: 54
global: 54
graph: 54
interleaving: 54
larger: 54
open: 54
processes: 54
range: 54
retrieved: 54
among: 53
auditing: 53
caching: 53
cannot: 53
h: 53
scenario: 53
center: 52
fetch: 52
mode: 52
real: 52
synchronous: 52
take: 52
transactional: 52
according: 51
form: 51
ieee: 51
increasing: 51
internet: 51
make: 51
present: 51
reads: 51
xor: 51
able: 50
algorithm: 50
current: 50
had: 50
inconsistencies: 50
mobile: 50
store: 50
components: 49
contention: 49
either: 49
existing: 49
hence: 49
mechanisms: 49
mi: 49
now: 49
three: 49
whether: 49
encoding: 48
general: 48
host: 48
implemented: 48
own: 48
prefetch: 48
recovered: 48
reliable: 48
values: 48
although: 47
important: 47
pp: 47
side: 47
w: 47
behavior: 46
execution: 46
fixed: 46
function: 46
generated: 46
graphs: 46
operation: 46
platform: 46
request: 46
speed: 46
th: 46
consider: 45
designed: 45
shared: 45
significant: 45
university: 45
victim: 45
window: 45
attacked: 44
blocks: 44
com: 44
community: 44
delays: 44
density: 44
error: 44
future: 44
hosted: 44
kbps: 44
left: 44
pages: 44
smaller: 44
standard: 44
wide: 44
appliance: 43
changes: 43
component: 43
costs: 43
events: 43
fig: 43
here: 43
makes: 43
ratio: 43
required: 43
right: 43
thus: 43
y: 43
allows: 42
amount: 42
change: 42
developers: 42
distribution: 42
experiment: 42
just: 42
often: 42
rather: 42
scalability: 42
seen: 42
start: 42
tasks: 42
uniform: 42
what: 42
cloud: 41
effect: 41
mafs: 41
missing: 41
numbers: 41
optimal: 41
reduced: 41
simple: 41
ssa: 41
strategy: 41
across: 40
address: 40
detection: 40
down: 40
further: 40
given: 40
hosts: 40
lower: 40
next: 40
non: 40
receive: 40
sends: 40
software: 40
token: 40
typically: 40
allow: 39
caches: 39
cases: 39
congestion: 39
directly: 39
entire: 39
increases: 39
independent: 39
interface: 39
length: 39
original: 39
overheads: 39
part: 39
structure: 39
times: 39
compared: 38
described: 38
direct: 38
during: 38
microsoft: 38
region: 38
require: 38
revenues: 38
world: 38
accessed: 37
al: 37
birman: 37
centers: 37
codes: 37
effective: 37
enough: 37
et: 37
improve: 37
losses: 37
membership: 37
provides: 37
xors: 37
better: 36
clusters: 36
consistent: 36
describe: 36
expected: 36
fact: 36
fast: 36
loyal: 36
previous: 36
related: 36
relative: 36
techniques: 36
today: 36
types: 36
adaptation: 35
additional: 35
almost: 35
commodity: 35
environment: 35
https: 35
international: 35
last: 35
links: 35
longer: 35
minimum: 35
needs: 35
org: 35
performed: 35
proxy: 35
reliability: 35
still: 35
though: 35
above: 34
acid: 34
earlier: 34
evaluation: 34
factors: 34
injection: 34
interleaves: 34
ken: 34
levels: 34
managed: 34
percentage: 34
remote: 34
sets: 34
several: 34
virtual: 34
www: 34
constant: 33
denote: 33
limited: 33
note: 33
overall: 33
particular: 33
receiving: 33
top: 33
working: 33
assume: 32
become: 32
clustered: 32
detect: 32
find: 32
including: 32
period: 32
replicated: 32
scenarios: 32
seconds: 32
space: 32
stable: 32
table: 32
technologies: 32
technology: 32
those: 32
trace: 32
versions: 32
added: 31
ensure: 31
evaluate: 31
finally: 31
follows: 31
immediately: 31
invalidations: 31
least: 31
mb: 31
overlay: 31
prediction: 31
principles: 31
receivers: 31
requires: 31
runs: 31
sirp: 31
target: 31
udp: 31
achieve: 30
adaptive: 30
amazon: 30
auditors: 30
collaboration: 30
correction: 30
dropped: 30
good: 30
incoming: 30
lists: 30
members: 30
performs: 30
policy: 30
recent: 30
ri: 30
schemes: 30
security: 30
share: 30
third: 30
usenix: 30
z: 30
associated: 29
availability: 29
close: 29
commit: 29
complete: 29
delivered: 29
edu: 29
head: 29
key: 29
later: 29
platforms: 29
pull: 29
receives: 29
recover: 29
reducing: 29
science: 29
settings: 29
task: 29
varying: 29
xi: 29
best: 28
configuration: 28
core: 28
dilemma: 28
feasible: 28
implements: 28
issues: 28
making: 28
never: 28
obtain: 28
occur: 28
others: 28
repository: 28
resulting: 28
style: 28
tools: 28
transport: 28
type: 28
area: 27
async: 27
build: 27
building: 27
cached: 27
capacity: 27
collection: 27
failures: 27
following: 27
forward: 27
framework: 27
increased: 27
instance: 27
major: 27
mbps: 27
needed: 27
neighbors: 27
nov: 27
participants: 27
reader: 27
resources: 27
solo: 27
speedup: 27
stored: 27
tm: 27
vn: 27
writer: 27
abort: 26
basic: 26
becomes: 26
benefit: 26
bin: 26
bins: 26
certain: 26
channel: 26
classical: 26
common: 26
consumption: 26
controls: 26
discuss: 26
entry: 26
fault: 26
haul: 26
instead: 26
means: 26
mostly: 26
online: 26
os: 26
solomon: 26
sources: 26
tolerance: 26
vol: 26
allowing: 25
backend: 25
bursts: 25
bursty: 25
choice: 25
conditions: 25
considered: 25
contribution: 25
copy: 25
cs: 25
difficulty: 25
distance: 25
estimate: 25
figures: 25
flows: 25
found: 25
goal: 25
hardware: 25
invalidation: 25
machines: 25
observed: 25
properties: 25
queue: 25
rain: 25
reduces: 25
report: 25
spent: 25
split: 25
takes: 25
transmission: 25
abstract: 24
algorithms: 24
around: 24
blockchain: 24
cause: 24
closed: 24
committed: 24
contrast: 24
correctly: 24
corresponding: 24
critical: 24
datacenters: 24
demand: 24
developed: 24
done: 24
equal: 24
experimental: 24
front: 24
having: 24
idea: 24
individual: 24
infiltrating: 24
knowledge: 24
limit: 24
lock: 24
logs: 24
ny: 24
observe: 24
oriented: 24
peers: 24
place: 24
processing: 24
strategies: 24
streams: 24
structured: 24
too: 24
view: 24
again: 23
approaches: 23
called: 23
choose: 23
create: 23
datacenter: 23
detected: 23
duration: 23
include: 23
layers: 23
likely: 23
majority: 23
member: 23
monitoring: 23
ones: 23
powered: 23
product: 23
reed: 23
sufficient: 23
sync: 23
traditional: 23
unix: 23
ways: 23
workshop: 23
york: 23
alternative: 22
always: 22
avoid: 22
built: 22
certification: 22
connected: 22
consequently: 22
context: 22
defined: 22
dependencies: 22
development: 22
efficient: 22
generate: 22
handle: 22
he: 22
made: 22
maintain: 22
mtu: 22
net: 22
occurs: 22
off: 22
optical: 22
parameters: 22
patterns: 22
popular: 22
problems: 22
revision: 22
starts: 22
via: 22
who: 22
wireless: 22
bandwidths: 21
buffers: 21
centralized: 21
configurations: 21
contains: 21
dynamic: 21
easily: 21
easy: 21
equation: 21
etc: 21
garbage: 21
hand: 21
hash: 21
identical: 21
iii: 21
index: 21
let: 21
life: 21
line: 21
mashup: 21
milliseconds: 21
old: 21
path: 21
private: 21
project: 21
proxies: 21
reasons: 21
response: 21
short: 21
significantly: 21
social: 21
staleness: 21
video: 21
vogels: 21
wait: 21
writing: 21
aggregate: 20
analyze: 20
architectures: 20
benefits: 20
buffering: 20
changing: 20
choosing: 20
consists: 20
four: 20
generates: 20
google: 20
grep: 20
honest: 20
inter: 20
interest: 20
interfaces: 20
library: 20
machine: 20
manner: 20
minibrowser: 20
multi: 20
none: 20
potentially: 20
presence: 20
proc: 20
quality: 20
queued: 20
racs: 20
rapidly: 20
references: 20
respectively: 20
simply: 20
site: 20
stale: 20
together: 20
traces: 20
updated: 20
usa: 20
usage: 20
yet: 20
actually: 19
adding: 19
appropriate: 19
clear: 19
convergence: 19
course: 19
currently: 19
digital: 19
expect: 19
explained: 19
extended: 19
extremely: 19
faster: 19
features: 19
inconsistent: 19
investigation: 19
issue: 19
kind: 19
know: 19
mine: 19
month: 19
natural: 19
networking: 19
om: 19
partition: 19
periods: 19
potential: 19
propagation: 19
proposed: 19
query: 19
quickly: 19
re: 19
recipe: 19
reply: 19
sequence: 19
serializability: 19
soc: 19
speeds: 19
subscribers: 19
taken: 19
towards: 19
transfer: 19
transient: 19
upon: 19
whereas: 19
works: 19
active: 18
aggregation: 18
aware: 18
background: 18
bad: 18
chooses: 18
cpu: 18
databases: 18
difficult: 18
divided: 18
drop: 18
earn: 18
effectiveness: 18
employ: 18
eventually: 18
exchange: 18
failed: 18
feb: 18
fiber: 18
foreground: 18
generation: 18
get: 18
gigabit: 18
heavy: 18
history: 18
identify: 18
includes: 18
initial: 18
itself: 18
known: 18
lambda: 18
latencies: 18
logging: 18
moreover: 18
nash: 18
neither: 18
noted: 18
participating: 18
propose: 18
publish: 18
queries: 18
repositories: 18
roundtrip: 18
sigcomm: 18
specific: 18
study: 18
subversion: 18
summary: 18
switching: 18
synthetic: 18
tokens: 18
topics: 18
unique: 18
varied: 18
various: 18
vary: 18
actual: 17
automatically: 17
chosen: 17
coda: 17
communications: 17
developer: 17
device: 17
devices: 17
don: 17
early: 17
enabled: 17
environments: 17
errors: 17
experience: 17
focus: 17
fraction: 17
functionality: 17
grows: 17
highest: 17
highly: 17
hosting: 17
ii: 17
impact: 17
implement: 17
infrastructure: 17
lines: 17
maintains: 17
modified: 17
module: 17
necessary: 17
offer: 17
partitioning: 17
periodically: 17
products: 17
progress: 17
projects: 17
ran: 17
raps: 17
reach: 17
remain: 17
setting: 17
sharing: 17
simultaneous: 17
specifically: 17
standards: 17
subservice: 17
successfully: 17
supported: 17
switch: 17
symmetric: 17
term: 17
transmitted: 17
van: 17
variety: 17
years: 17
acts: 16
already: 16
axis: 16
believe: 16
buffer: 16
bytes: 16
call: 16
chainsaw: 16
clustering: 16
compare: 16
configured: 16
conflicts: 16
connectivity: 16
contents: 16
conventional: 16
created: 16
define: 16
demonstrate: 16
difference: 16
efficacy: 16
examples: 16
fees: 16
few: 16
ghash: 16
go: 16
hard: 16
illustrated: 16
implications: 16
indeed: 16
infiltrate: 16
integration: 16
io: 16
kinds: 16
lead: 16
limiting: 16
linux: 16
little: 16
main: 16
market: 16
measurements: 16
modes: 16
operate: 16
options: 16
page: 16
parallel: 16
physical: 16
points: 16
prisoner: 16
publishes: 16
registered: 16
requirements: 16
serialization: 16
simulation: 16
simultaneously: 16
status: 16
strong: 16
sub: 16
subsystem: 16
themselves: 16
timestamp: 16
topic: 16
transparently: 16
typical: 16
accessing: 15
adapt: 15
applied: 15
arrive: 15
atp: 15
attempt: 15
bar: 15
bc: 15
comparison: 15
containing: 15
contribute: 15
corba: 15
department: 15
describes: 15
diff: 15
edge: 15
effects: 15
exists: 15
fire: 15
gc: 15
greater: 15
idle: 15
included: 15
infiltrated: 15
info: 15
june: 15
latter: 15
linear: 15
maintaining: 15
namely: 15
november: 15
oms: 15
optimize: 15
outgoing: 15
paths: 15
performing: 15
practice: 15
predictable: 15
programming: 15
regions: 15
regular: 15
renesse: 15
represents: 15
respond: 15
sense: 15
sirer: 15
slightly: 15
slow: 15
statistics: 15
stores: 15
supercomputing: 15
teragrid: 15
thousands: 15
tier: 15
underlying: 15
uniformly: 15
whole: 15
accept: 14
addition: 14
arise: 14
balancing: 14
bounded: 14
channels: 14
class: 14
combined: 14
commits: 14
conclusion: 14
concurrency: 14
currency: 14
decrease: 14
deliver: 14
destination: 14
detects: 14
differentiated: 14
directory: 14
dissemination: 14
distinct: 14
drops: 14
engineering: 14
entries: 14
exactly: 14
explore: 14
fail: 14
fifo: 14
financial: 14
finding: 14
growing: 14
guarantees: 14
handling: 14
help: 14
improves: 14
infiltrators: 14
integrated: 14
interested: 14
involves: 14
malicious: 14
modern: 14
option: 14
perfect: 14
policies: 14
prevent: 14
previously: 14
prior: 14
program: 14
providing: 14
published: 14
question: 14
relatively: 14
sabotage: 14
search: 14
singleton: 14
soon: 14
spread: 14
starting: 14
temporarily: 14
thread: 14
tolerate: 14
turn: 14
useful: 14
xj: 14
zookeeper: 14
achieved: 13
add: 13
afs: 13
annual: 13
avg: 13
balance: 13
bound: 13
collaborative: 13
collect: 13
compile: 13
completely: 13
computation: 13
concurrent: 13
consisting: 13
constraints: 13
decreases: 13
depends: 13
determine: 13
determined: 13
did: 13
digests: 13
discarded: 13
dominant: 13
dynamically: 13
embedded: 13
epidemic: 13
exception: 13
eyal: 13
face: 13
far: 13
field: 13
free: 13
frequently: 13
fundamental: 13
generating: 13
gives: 13
grained: 13
growth: 13
his: 13
hit: 13
instances: 13
intended: 13
interactive: 13
interval: 13
intervals: 13
introduction: 13
join: 13
lack: 13
location: 13
magnitude: 13
measure: 13
mentioned: 13
missed: 13
modeless: 13
nfm: 13
normal: 13
notice: 13
ntfs: 13
overview: 13
paradigm: 13
positive: 13
proportional: 13
provided: 13
public: 13
radient: 13
receipt: 13
registers: 13
relevant: 13
remains: 13
researchers: 13
robust: 13
rtt: 13
semantics: 13
setup: 13
shares: 13
situations: 13
special: 13
static: 13
structures: 13
subscribe: 13
thousand: 13
tool: 13
track: 13
trade: 13
tree: 13
unbounded: 13
written: 13
aborted: 12
academic: 12
acceptable: 12
account: 12
accurate: 12
balakrishnan: 12
base: 12
below: 12
ca: 12
capable: 12
caused: 12
challenges: 12
classes: 12
conclude: 12
copies: 12
creating: 12
cumulative: 12
date: 12
decentralized: 12
decide: 12
dedicated: 12
definition: 12
delayed: 12
despite: 12
efficiency: 12
element: 12
enterprise: 12
facts: 12
fairly: 12
footprint: 12
generally: 12
half: 12
hashcash: 12
html: 12
ing: 12
interactions: 12
introduced: 12
iv: 12
ix: 12
jan: 12
java: 12
keep: 12
leads: 12
lfs: 12
li: 12
limitations: 12
locking: 12
maintained: 12
near: 12
obtained: 12
opportunity: 12
oracle: 12
ordering: 12
orders: 12
otherwise: 12
overlap: 12
particularly: 12
past: 12
practical: 12
primary: 12
purposes: 12
push: 12
reason: 12
recently: 12
redundant: 12
rescue: 12
review: 12
roles: 12
roughly: 12
sort: 12
specify: 12
spikes: 12
stack: 12
subset: 12
taking: 12
technique: 12
ten: 12
tms: 12
unif: 12
whenever: 12
you: 12
achieving: 11
acknowledgments: 11
activity: 11
advance: 11
aggregated: 11
argue: 11
authors: 11
automated: 11
basis: 11
begin: 11
callback: 11
complex: 11
converge: 11
crash: 11
deal: 11
deployed: 11
detail: 11
details: 11
detecting: 11
discussion: 11
energy: 11
ensures: 11
equivalent: 11
evict: 11
fa: 11
fee: 11
fetches: 11
fetching: 11
finds: 11
flexible: 11
focused: 11
heterogeneous: 11
hot: 11
hundreds: 11
identifiers: 11
illustrates: 11
implementing: 11
improvements: 11
incorporates: 11
increasingly: 11
indicate: 11
insufficient: 11
interact: 11
introduce: 11
involved: 11
ithaca: 11
largest: 11
layout: 11
linearly: 11
logged: 11
maintenance: 11
measurement: 11
metadata: 11
modal: 11
moving: 11
name: 11
orkut: 11
osdi: 11
pair: 11
parameter: 11
partitioned: 11
prefetches: 11
privacy: 11
processed: 11
prove: 11
providers: 11
randomly: 11
readers: 11
realistic: 11
recovers: 11
replace: 11
replicas: 11
requiring: 11
responsible: 11
retransmission: 11
role: 11
routine: 11
sample: 11
scheduling: 11
sensitive: 11
sep: 11
separate: 11
serve: 11
simplicity: 11
stock: 11
strictly: 11
subsequent: 11
supporting: 11
supports: 11
true: 11
trusted: 11
try: 11
unlikely: 11
unshared: 11
valid: 11
ve: 11
white: 11
whose: 11
xml: 11
accuracy: 10
ack: 10
additionally: 10
along: 10
appear: 10
arg: 10
assumption: 10
asynchronously: 10
becoming: 10
bitcoins: 10
bottleneck: 10
cc: 10
cdn: 10
check: 10
checks: 10
compares: 10
conflict: 10
constructed: 10
continuous: 10
dec: 10
decision: 10
decisions: 10
degenerate: 10
degree: 10
depend: 10
desired: 10
detailed: 10
discussed: 10
distinguish: 10
distributes: 10
effectively: 10
efforts: 10
elements: 10
enable: 10
enhancing: 10
epidemics: 10
especially: 10
evaluated: 10
eventual: 10
examine: 10
expensive: 10
experienced: 10
fairness: 10
false: 10
final: 10
fine: 10
forms: 10
give: 10
gracefully: 10
ideal: 10
improving: 10
infiltrates: 10
injected: 10
ipc: 10
isn: 10
journal: 10
language: 10
leverage: 10
map: 10
maps: 10
mesh: 10
methods: 10
microbenchmarks: 10
minutes: 10
miss: 10
networked: 10
nevertheless: 10
news: 10
novel: 10
overlapping: 10
paxos: 10
pending: 10
possibility: 10
presented: 10
presents: 10
proceed: 10
pushing: 10
rapid: 10
really: 10
recall: 10
remainder: 10
reported: 10
reports: 10
representing: 10
rest: 10
return: 10
rise: 10
robbert: 10
router: 10
routes: 10
routing: 10
runtime: 10
sampling: 10
scales: 10
segment: 10
situation: 10
studies: 10
subject: 10
success: 10
synchrony: 10
tail: 10
team: 10
technical: 10
tests: 10
theorem: 10
think: 10
thresholds: 10
throughout: 10
tion: 10
transparent: 10
unlike: 10
unstable: 10
usually: 10
utility: 10
variant: 10
visible: 10
volume: 10
widely: 10
zone: 10
achieves: 9
advantages: 9
alarm: 9
allowed: 9
appliances: 9
apply: 9
assigned: 9
assigns: 9
assumed: 9
attackers: 9
august: 9
behave: 9
beneficial: 9
break: 9
causing: 9
central: 9
changed: 9
co: 9
commercial: 9
companies: 9
comparing: 9
competing: 9
compute: 9
computers: 9
confidence: 9
connections: 9
continue: 9
curves: 9
daily: 9
default: 9
degradation: 9
depending: 9
deterministic: 9
developing: 9
dirty: 9
discards: 9
disconnected: 9
domain: 9
ec: 9
ee: 9
efficiently: 9
eliminate: 9
en: 9
entirely: 9
execute: 9
explicit: 9
explored: 9
expression: 9
fails: 9
fashion: 9
fd: 9
force: 9
forming: 9
forwarding: 9
frequency: 9
fully: 9
functions: 9
grant: 9
grow: 9
gw: 9
hide: 9
hierarchy: 9
hold: 9
id: 9
ideally: 9
identifier: 9
importance: 9
incentive: 9
inexpensive: 9
infocom: 9
initially: 9
intel: 9
investigate: 9
iperf: 9
iterative: 9
john: 9
kept: 9
knows: 9
leaving: 9
loads: 9
lose: 9
lossless: 9
lowest: 9
maximize: 9
media: 9
merlin: 9
middle: 9
middleware: 9
minor: 9
modifications: 9
modify: 9
modules: 9
nsdi: 9
owner: 9
pattern: 9
permutation: 9
places: 9
plan: 9
profitable: 9
programmer: 9
prone: 9
proportion: 9
quite: 9
reached: 9
reading: 9
records: 9
reflect: 9
respect: 9
retry: 9
ricochet: 9
rings: 9
risk: 9
rounds: 9
rw: 9
sampled: 9
sd: 9
sec: 9
segments: 9
selected: 9
self: 9
session: 9
shall: 9
showing: 9
sigmod: 9
sosp: 9
specialized: 9
steps: 9
susceptible: 9
terms: 9
threads: 9
tolerant: 9
topology: 9
transfers: 9
transformation: 9
transitions: 9
transmitting: 9
trip: 9
ttl: 9
unacknowledged: 9
unless: 9
upcall: 9
vi: 9
viii: 9
want: 9
werner: 9
academia: 8
accordingly: 8
activities: 8
adds: 8
advantage: 8
affect: 8
affects: 8
afrl: 8
aggregates: 8
allocation: 8
amounts: 8
api: 8
arbitrary: 8
arrays: 8
arxiv: 8
aspects: 8
attributes: 8
auditor: 8
automatic: 8
away: 8
backup: 8
behind: 8
beyond: 8
breaks: 8
buffered: 8
byte: 8
cbcb: 8
challenge: 8
charge: 8
checking: 8
clock: 8
collected: 8
combine: 8
come: 8
competition: 8
compromised: 8
concurrently: 8
confirm: 8
congested: 8
connecting: 8
consecutive: 8
contributing: 8
coordinator: 8
corresponds: 8
creates: 8
currencies: 8
dark: 8
david: 8
de: 8
degrades: 8
densities: 8
deploy: 8
deployment: 8
detectors: 8
differs: 8
dogecoin: 8
drag: 8
driven: 8
earns: 8
eligius: 8
employed: 8
engage: 8
equations: 8
equipment: 8
essentially: 8
ever: 8
exact: 8
executed: 8
exist: 8
extensions: 8
external: 8
extreme: 8
favor: 8
filesystem: 8
gib: 8
greatly: 8
header: 8
histograms: 8
honestly: 8
ideas: 8
improvement: 8
input: 8
insight: 8
interaction: 8
jgroups: 8
largely: 8
leader: 8
leading: 8
leave: 8
lemma: 8
light: 8
limits: 8
litecoin: 8
look: 8
mashups: 8
masks: 8
massive: 8
match: 8
max: 8
mc: 8
measuring: 8
method: 8
minimize: 8
misbehaving: 8
mixed: 8
modeled: 8
models: 8
modification: 8
move: 8
neighbor: 8
nominal: 8
nor: 8
notification: 8
notifications: 8
oct: 8
offers: 8
ordered: 8
papers: 8
participation: 8
payoff: 8
perhaps: 8
perturbed: 8
placed: 8
played: 8
plot: 8
portion: 8
prioritised: 8
purpose: 8
put: 8
react: 8
recovering: 8
refer: 8
refrain: 8
replaced: 8
reservation: 8
reservations: 8
retransmissions: 8
returned: 8
reward: 8
robin: 8
sdn: 8
sections: 8
senders: 8
september: 8
series: 8
serves: 8
similarly: 8
simulate: 8
sink: 8
sm: 8
solve: 8
sometimes: 8
span: 8
specification: 8
specified: 8
stacks: 8
staggered: 8
stepwise: 8
strict: 8
subsequently: 8
substituting: 8
suitable: 8
surprisingly: 8
timer: 8
turns: 8
twice: 8
unusable: 8
verify: 8
views: 8
wang: 8
weak: 8
weather: 8
weekly: 8
wiki: 8
ws: 8
xc: 8
aborting: 7
abstraction: 7
acceptors: 7
acknowledged: 7
air: 7
apache: 7
applying: 7
architectural: 7
arises: 7
array: 7
attractive: 7
belongs: 7
big: 7
broadcast: 7
bsd: 7
byzantine: 7
calculate: 7
card: 7
causes: 7
cb: 7
characteristics: 7
cisco: 7
claim: 7
cleaning: 7
cleanup: 7
clearly: 7
closer: 7
collapse: 7
communities: 7
completion: 7
complexity: 7
concept: 7
conflicting: 7
connection: 7
considering: 7
consistently: 7
construct: 7
contemporary: 7
contributions: 7
cooperative: 7
counter: 7
creation: 7
dashed: 7
december: 7
decoupling: 7
degraded: 7
delaying: 7
demers: 7
deploying: 7
dept: 7
derived: 7
designs: 7
deter: 7
determining: 7
display: 7
disrupt: 7
doesn: 7
doing: 7
dolev: 7
downstream: 7
easier: 7
elapsed: 7
electronic: 7
eliminated: 7
eliminates: 7
employs: 7
empty: 7
emulab: 7
enabling: 7
equally: 7
everything: 7
exchanges: 7
experiences: 7
exploit: 7
exponential: 7
extend: 7
feature: 7
feedback: 7
feeds: 7
fires: 7
fit: 7
follow: 7
furthermore: 7
gaining: 7
gray: 7
guarantee: 7
handles: 7
hour: 7
huge: 7
ignored: 7
image: 7
implementations: 7
importantly: 7
incoherent: 7
incorporate: 7
inferior: 7
introduces: 7
inventory: 7
isis: 7
isolation: 7
january: 7
joseph: 7
keys: 7
latest: 7
lbfs: 7
located: 7
locations: 7
ma: 7
mahesh: 7
mapping: 7
mica: 7
mines: 7
ml: 7
mod: 7
modeling: 7
modifying: 7
motivation: 7
multicasts: 7
nature: 7
negligible: 7
nothing: 7
nsf: 7
oblivious: 7
observation: 7
obvious: 7
october: 7
operator: 7
optimistic: 7
overloaded: 7
partitions: 7
percentile: 7
persistent: 7
plus: 7
poorly: 7
population: 7
pre: 7
prefer: 7
press: 7
prime: 7
programs: 7
propagated: 7
prototype: 7
publishing: 7
rack: 7
rarely: 7
rc: 7
reaches: 7
record: 7
reflects: 7
regarding: 7
register: 7
regularly: 7
relaying: 7
rely: 7
repeated: 7
represent: 7
represented: 7
requesting: 7
resistant: 7
restart: 7
retailer: 7
retrieve: 7
rich: 7
route: 7
routers: 7
san: 7
savings: 7
scaling: 7
seamlessly: 7
seek: 7
sharply: 7
she: 7
simplify: 7
simulations: 7
simulator: 7
sites: 7
socket: 7
sockets: 7
springer: 7
stop: 7
storing: 7
subservices: 7
substantially: 7
successful: 7
timeout: 7
tradeoff: 7
trans: 7
transition: 7
transmit: 7
treat: 7
treated: 7
trees: 7
trust: 7
twenty: 7
u: 7
ubiquitous: 7
unaware: 7
unchanged: 7
useless: 7
utilization: 7
validation: 7
variation: 7
vendor: 7
vendors: 7
versus: 7
vii: 7
violating: 7
wan: 7
year: 7
yield: 7
yields: 7
yu: 7
accommodate: 6
achievable: 6
acknowledgement: 6
act: 6
acting: 6
addressed: 6
addresses: 6
adjustment: 6
administrators: 6
affected: 6
afosr: 6
agreement: 6
alternatives: 6
analyzed: 6
apparently: 6
appropriately: 6
argument: 6
arrives: 6
assumptions: 6
atomic: 6
averages: 6
avoids: 6
bars: 6
begins: 6
berkeley: 6
bonneau: 6
bottom: 6
boundaries: 6
broadly: 6
bullet: 6
california: 6
calls: 6
catch: 6
checkpoint: 6
checksum: 6
chen: 6
closely: 6
clouds: 6
clr: 6
coherent: 6
combination: 6
combines: 6
comes: 6
command: 6
comprised: 6
comput: 6
concave: 6
conducted: 6
conjunction: 6
consensus: 6
conservative: 6
considerable: 6
contain: 6
controlled: 6
coordinate: 6
copying: 6
corporation: 6
costly: 6
coupled: 6
crashed: 6
cross: 6
curve: 6
customize: 6
darpa: 6
dates: 6
db: 6
debian: 6
denotes: 6
dependent: 6
deployments: 6
description: 6
detector: 6
determines: 6
develop: 6
digest: 6
discusfish: 6
disjoint: 6
disseminate: 6
divides: 6
document: 6
draw: 6
druschel: 6
durability: 6
earth: 6
ed: 6
effort: 6
egress: 6
eliminating: 6
email: 6
erasure: 6
established: 6
expressions: 6
facebook: 6
feasibility: 6
feed: 6
fifth: 6
fill: 6
focuses: 6
forced: 6
forks: 6
former: 6
foundation: 6
frame: 6
france: 6
gbps: 6
generic: 6
hashing: 6
heartbeat: 6
hidden: 6
histories: 6
hop: 6
huang: 6
imposed: 6
improved: 6
inc: 6
indicates: 6
industry: 6
ingress: 6
intensive: 6
intercepted: 6
intermediate: 6
issued: 6
issuing: 6
kncminer: 6
krzysztof: 6
lacking: 6
lamport: 6
lan: 6
lans: 6
lcm: 6
ledger: 6
loaded: 6
locally: 6
logical: 6
lt: 6
luu: 6
manage: 6
master: 6
matching: 6
maxx: 6
mean: 6
measured: 6
min: 6
mission: 6
modifies: 6
moment: 6
moves: 6
mp: 6
multicasting: 6
my: 6
naturally: 6
nd: 6
nearly: 6
necessarily: 6
noble: 6
nonce: 6
normalizes: 6
notion: 6
numbered: 6
numerical: 6
numerous: 6
occurring: 6
older: 6
operators: 6
optimized: 6
oscillatory: 6
ostrowski: 6
outlined: 6
outperforms: 6
output: 6
overload: 6
pareto: 6
parties: 6
passive: 6
pdf: 6
perfectly: 6
permit: 6
perspective: 6
phase: 6
play: 6
pooled: 6
possibly: 6
powerful: 6
predict: 6
prefix: 6
price: 6
primarily: 6
procedure: 6
produced: 6
publicly: 6
publisher: 6
pulls: 6
punish: 6
questions: 6
queuing: 6
qwest: 6
reachable: 6
red: 6
reduction: 6
redundancy: 6
reject: 6
repeatedly: 6
replay: 6
replicate: 6
resource: 6
resulted: 6
returns: 6
reveals: 6
rewards: 6
rig: 6
ring: 6
rj: 6
rules: 6
samples: 6
satyanarayanan: 6
say: 6
seattle: 6
seems: 6
select: 6
sensitivity: 6
serialized: 6
six: 6
slowly: 6
snapshot: 6
solving: 6
somewhat: 6
sorts: 6
standardized: 6
states: 6
straightforward: 6
stratum: 6
suggested: 6
summarize: 6
sun: 6
super: 6
supplies: 6
switzerland: 6
synchronization: 6
targeted: 6
tat: 6
thank: 6
theory: 6
timeouts: 6
tit: 6
toolkit: 6
tracking: 6
transmits: 6
trigger: 6
trying: 6
txnid: 6
ultimately: 6
understand: 6
unfortunately: 6
unreliable: 6
usability: 6
usable: 6
validate: 6
variability: 6
variations: 6
vast: 6
vector: 6
vldb: 6
welsh: 6
why: 6
withheld: 6
won: 6
workers: 6
worse: 6
xxx: 6
yahoo: 6
zhang: 6
zhu: 6
ability: 5
absence: 5
absolute: 5
acknowledgements: 5
acknowledgment: 5
acl: 5
adapting: 5
adjusting: 5
ago: 5
agree: 5
ahead: 5
allocated: 5
anyone: 5
append: 5
approximate: 5
arbitrarily: 5
arguably: 5
arrival: 5
assuming: 5
atkin: 5
atomicity: 5
attempts: 5
author: 5
avoiding: 5
award: 5
backed: 5
band: 5
behalf: 5
behaves: 5
belong: 5
bit: 5
bits: 5
blocking: 5
brevity: 5
brewer: 5
broken: 5
broker: 5
buildings: 5
bus: 5
calculated: 5
calculates: 5
capture: 5
carry: 5
catches: 5
caught: 5
chains: 5
cheap: 5
cheriton: 5
classified: 5
cloned: 5
collapses: 5
comments: 5
communicate: 5
communicates: 5
compose: 5
concern: 5
concerns: 5
consequence: 5
considerations: 5
constitutes: 5
constrained: 5
consume: 5
consuming: 5
contact: 5
continuously: 5
coolstreaming: 5
cope: 5
count: 5
counts: 5
crashes: 5
credentials: 5
crossing: 5
customers: 5
danny: 5
datagram: 5
day: 5
dealing: 5
defense: 5
demands: 5
demonstrated: 5
denial: 5
denoted: 5
depict: 5
describing: 5
df: 5
dialog: 5
direction: 5
division: 5
dramatic: 5
dsn: 5
edition: 5
eferences: 5
eight: 5
elaborate: 5
elated: 5
embedding: 5
emerging: 5
employing: 5
encourage: 5
encryption: 5
enforce: 5
ephemeral: 5
estimated: 5
ethernet: 5
exceed: 5
except: 5
exhibit: 5
explain: 5
explicitly: 5
exploiting: 5
exploring: 5
expressed: 5
falls: 5
faults: 5
faulty: 5
filtering: 5
firing: 5
flexibility: 5
forces: 5
forwarded: 5
frames: 5
freebsd: 5
frequent: 5
gap: 5
generality: 5
geo: 5
geographical: 5
gigabits: 5
gribble: 5
grid: 5
ground: 5
gu: 5
hacker: 5
health: 5
hierarchical: 5
highperformance: 5
hints: 5
hooks: 5
human: 5
identification: 5
illustrate: 5
imagine: 5
implicit: 5
implies: 5
inaccurate: 5
incorrect: 5
incrementally: 5
incurs: 5
independently: 5
informatik: 5
informatique: 5
innovation: 5
instantly: 5
integrating: 5
interesting: 5
interference: 5
internal: 5
interoperability: 5
invalidate: 5
investigated: 5
investigator: 5
iptv: 5
isps: 5
ittay: 5
johnson: 5
joining: 5
joins: 5
katz: 5
kleinberg: 5
lakshmi: 5
languages: 5
layouts: 5
learn: 5
leases: 5
lengths: 5
lightweight: 5
linking: 5
liu: 5
locality: 5
locks: 5
logic: 5
looking: 5
lossy: 5
mar: 5
marketing: 5
mask: 5
messaging: 5
metrics: 5
minibrowsers: 5
minority: 5
monitors: 5
monthly: 5
moved: 5
mutual: 5
national: 5
normally: 5
ntroduction: 5
observations: 5
obtains: 5
occasional: 5
occurred: 5
offered: 5
omit: 5
omitted: 5
onclusion: 5
onon: 5
optimization: 5
optimizations: 5
optimizes: 5
organization: 5
organizations: 5
organized: 5
originally: 5
orthogonal: 5
outside: 5
overcome: 5
pairs: 5
partially: 5
passed: 5
pause: 5
pay: 5
pays: 5
peerto: 5
people: 5
percentages: 5
permitting: 5
personal: 5
pf: 5
phenomenon: 5
placement: 5
plots: 5
poison: 5
positives: 5
precisely: 5
predictor: 5
predictors: 5
probabilistic: 5
proceeds: 5
progresses: 5
property: 5
proprietary: 5
protected: 5
proved: 5
punished: 5
quick: 5
raise: 5
rare: 5
raw: 5
rd: 5
reaching: 5
regardless: 5
release: 5
reliance: 5
remaining: 5
remove: 5
repairs: 5
repeat: 5
requested: 5
requirement: 5
resilience: 5
resolve: 5
respects: 5
responsiveness: 5
restrictions: 5
reveal: 5
rizzo: 5
road: 5
robustness: 5
satisfactory: 5
satisfied: 5
save: 5
saving: 5
scheduled: 5
schneider: 5
seem: 5
sees: 5
selecting: 5
sensors: 5
separated: 5
separately: 5
sequenced: 5
serializable: 5
sessions: 5
shard: 5
shards: 5
signal: 5
simulated: 5
sized: 5
slack: 5
slower: 5
sophisticated: 5
soule: 5
speculative: 5
sprint: 5
stated: 5
stay: 5
studied: 5
submit: 5
substantial: 5
subsystems: 5
suffer: 5
sum: 5
summarized: 5
suspicion: 5
switched: 5
switches: 5
tech: 5
tell: 5
temporary: 5
texture: 5
thinking: 5
tightly: 5
timely: 5
timestamps: 5
tocs: 5
topologies: 5
tracks: 5
transform: 5
transformations: 5
trap: 5
treating: 5
trend: 5
trends: 5
triggers: 5
tune: 5
tuning: 5
turned: 5
unable: 5
unexpected: 5
unit: 5
unpredictable: 5
untrusted: 5
updating: 5
upper: 5
ut: 5
utilisation: 5
verifying: 5
viewed: 5
volatile: 5
vs: 5
waiting: 5
walk: 5
wall: 5
wants: 5
win: 5
wired: 5
writers: 5
ycsb: 5
yx: 5
ab: 4
abbadi: 4
ac: 4
accepted: 4
acks: 4
action: 4
actions: 4
adopted: 4
advanced: 4
advances: 4
agrawal: 4
alone: 4
alongside: 4
alternate: 4
alvisi: 4
amir: 4
amortizable: 4
analogous: 4
andrew: 4
answer: 4
antpool: 4
applicable: 4
approximated: 4
april: 4
archive: 4
arrow: 4
aspect: 4
assign: 4
assumes: 4
athey: 4
awkward: 4
bahack: 4
ballots: 4
banking: 4
behaviour: 4
benchmark: 4
bernstein: 4
biersack: 4
bifurcations: 4
bitcointalk: 4
blade: 4
blast: 4
blend: 4
blocked: 4
blogspot: 4
border: 4
boxes: 4
brief: 4
briefly: 4
broadcasts: 4
broader: 4
bulk: 4
burstier: 4
calculator: 4
callbacks: 4
caller: 4
capitalization: 4
captures: 4
carefully: 4
certainly: 4
charts: 4
chat: 4
city: 4
clean: 4
clones: 4
closes: 4
cms: 4
collecting: 4
collector: 4
combining: 4
coming: 4
comparable: 4
compensated: 4
compiles: 4
completed: 4
complicated: 4
componentized: 4
composition: 4
comprehensive: 4
compromise: 4
conduct: 4
confirms: 4
consequences: 4
consist: 4
contained: 4
continued: 4
converges: 4
cooperation: 4
correlated: 4
correlation: 4
courtois: 4
crucial: 4
cryptography: 4
cutting: 4
cycle: 4
danger: 4
das: 4
debugging: 4
decades: 4
decided: 4
declining: 4
deep: 4
deferred: 4
defines: 4
demonstrates: 4
desktop: 4
devastating: 4
di: 4
differences: 4
differentiate: 4
differently: 4
directed: 4
directions: 4
directories: 4
disaster: 4
discovery: 4
discussions: 4
disloyal: 4
disrupted: 4
disruption: 4
distant: 4
distribute: 4
distributions: 4
double: 4
ease: 4
ebay: 4
economic: 4
editor: 4
educause: 4
elsewhere: 4
emin: 4
emulate: 4
encoder: 4
encodes: 4
endhosts: 4
enforcing: 4
engages: 4
engineer: 4
enhanced: 4
enormous: 4
enter: 4
envelope: 4
epi: 4
eprint: 4
esb: 4
evenly: 4
evicting: 4
evidence: 4
evil: 4
ex: 4
examines: 4
exchanged: 4
exclusive: 4
executing: 4
existence: 4
export: 4
exposed: 4
extensive: 4
faber: 4
faced: 4
familiar: 4
february: 4
filter: 4
firewalls: 4
five: 4
fixing: 4
flowing: 4
flush: 4
flushes: 4
focusing: 4
followed: 4
formula: 4
foster: 4
fourth: 4
fragmentation: 4
francis: 4
francisco: 4
freedman: 4
friendly: 4
gain: 4
games: 4
gateways: 4
gathered: 4
gets: 4
globally: 4
goes: 4
going: 4
graceful: 4
grants: 4
grateful: 4
grossklags: 4
hackingdistributed: 4
hakim: 4
heartbeats: 4
heavily: 4
helps: 4
hint: 4
holds: 4
home: 4
honeypot: 4
hope: 4
horus: 4
hweather: 4
identified: 4
identifying: 4
identity: 4
ids: 4
ignore: 4
images: 4
impacted: 4
impossible: 4
inaccessible: 4
incorporating: 4
incurred: 4
incurring: 4
inform: 4
initiated: 4
initiative: 4
inquiry: 4
integral: 4
integrate: 4
intelligent: 4
intend: 4
interacts: 4
interrupted: 4
intervening: 4
intrinsically: 4
introducing: 4
invalidated: 4
invalidating: 4
isbn: 4
javascript: 4
jboss: 4
jelasity: 4
joint: 4
jose: 4
july: 4
jumbo: 4
jxta: 4
kaashoek: 4
keeping: 4
keidar: 4
kermarrec: 4
kj: 4
korn: 4
lacks: 4
lakshman: 4
laszka: 4
leaders: 4
learning: 4
leaves: 4
letting: 4
leveraging: 4
libraries: 4
lies: 4
listed: 4
lived: 4
ll: 4
logsim: 4
longdistance: 4
looked: 4
lru: 4
maid: 4
malfunctioning: 4
managing: 4
marked: 4
matched: 4
matrix: 4
maximizes: 4
measures: 4
merely: 4
methodology: 4
miles: 4
military: 4
miller: 4
mind: 4
minimal: 4
minted: 4
minute: 4
misconfigured: 4
misses: 4
mix: 4
mixture: 4
mobility: 4
modulating: 4
monitor: 4
monopoly: 4
moore: 4
msg: 4
mtus: 4
naive: 4
nak: 4
namecoin: 4
narrow: 4
netfilter: 4
normalized: 4
notable: 4
notify: 4
numerically: 4
obstacles: 4
obtaining: 4
offering: 4
ools: 4
operates: 4
optimum: 4
organofcorti: 4
ork: 4
outstanding: 4
owners: 4
ownership: 4
packages: 4
parameterized: 4
participate: 4
parts: 4
patient: 4
paul: 4
payments: 4
payout: 4
payouts: 4
peak: 4
penalty: 4
peps: 4
periodic: 4
permacoin: 4
permutations: 4
person: 4
perspectives: 4
pfldnet: 4
phenomena: 4
php: 4
pi: 4
picked: 4
pictures: 4
placing: 4
player: 4
plotting: 4
podc: 4
poll: 4
polling: 4
positions: 4
practicality: 4
predicted: 4
preferred: 4
preprint: 4
pressure: 4
prevents: 4
pricing: 4
primitives: 4
principle: 4
probabilities: 4
probably: 4
prominent: 4
promising: 4
protect: 4
provisioned: 4
provisioning: 4
pub: 4
publication: 4
pulled: 4
purely: 4
qi: 4
qp: 4
quantities: 4
quantum: 4
querying: 4
readset: 4
reality: 4
realize: 4
reasonable: 4
reference: 4
reflected: 4
regional: 4
rejoin: 4
relation: 4
relations: 4
reliably: 4
removed: 4
reno: 4
repairing: 4
replacement: 4
replicating: 4
repo: 4
reserved: 4
residing: 4
resilient: 4
resolution: 4
resolved: 4
responding: 4
responses: 4
retailers: 4
retransmit: 4
rev: 4
revoke: 4
rigs: 4
rk: 4
robert: 4
root: 4
rosenfeld: 4
routed: 4
rover: 4
rowstron: 4
rule: 4
safety: 4
saturate: 4
saw: 4
scan: 4
secondary: 4
secret: 4
secure: 4
secured: 4
seeks: 4
selective: 4
selfish: 4
separates: 4
sequences: 4
sequentially: 4
sha: 4
shasha: 4
shenker: 4
shift: 4
shut: 4
silver: 4
situational: 4
sixteenth: 4
smalltalk: 4
smart: 4
smr: 4
sold: 4
spanning: 4
spun: 4
srm: 4
standardization: 4
statistically: 4
steadily: 4
steady: 4
stress: 4
stronger: 4
strongly: 4
students: 4
subjected: 4
subsection: 4
successes: 4
suffers: 4
sums: 4
suspected: 4
symmetry: 4
synchronized: 4
tackle: 4
tailored: 4
technological: 4
tens: 4
terminology: 4
terrain: 4
terry: 4
tested: 4
theoretic: 4
things: 4
threaded: 4
took: 4
tornado: 4
touched: 4
towsley: 4
traced: 4
tradeoffs: 4
trading: 4
translate: 4
travel: 4
treats: 4
tremendous: 4
trial: 4
truly: 4
tuple: 4
understanding: 4
undesirable: 4
units: 4
unknown: 4
unusual: 4
upstream: 4
usd: 4
usual: 4
utilities: 4
vahdat: 4
valuable: 4
variable: 4
variants: 4
varies: 4
vfs: 4
viable: 4
vice: 4
visual: 4
von: 4
vulnerable: 4
waits: 4
walsh: 4
washington: 4
weakly: 4
weatherspoon: 4
weight: 4
wish: 4
withhold: 4
worked: 4
worst: 4
worth: 4
writeset: 4
wrong: 4
xk: 4
xkj: 4
yearly: 4
your: 4
abadi: 3
aborts: 3
accepting: 3
accepts: 3
accountable: 3
accumulated: 3
accusations: 3
acidrain: 3
acknowledge: 3
acquire: 3
actively: 3
adapts: 3
adequate: 3
adjust: 3
advancing: 3
advantageous: 3
advertisements: 3
afec: 3
affecting: 3
aggregating: 3
agnostic: 3
aguilera: 3
aimed: 3
aircraft: 3
alan: 3
albeit: 3
alberta: 3
album: 3
alert: 3
alerts: 3
alleviate: 3
amenable: 3
analytical: 3
analytically: 3
anomaly: 3
anything: 3
app: 3
apparent: 3
appears: 3
ar: 3
areas: 3
aren: 3
argued: 3
arising: 3
arla: 3
assertion: 3
assess: 3
assigning: 3
assignment: 3
association: 3
assure: 3
attribute: 3
auditable: 3
aug: 3
authorization: 3
avatars: 3
avoided: 3
backbone: 3
balancer: 3
baltimore: 3
banks: 3
bases: 3
basically: 3
batch: 3
bayou: 3
beginning: 3
bianchini: 3
black: 3
boards: 3
bone: 3
boston: 3
boures: 3
breaking: 3
bring: 3
browser: 3
builds: 3
busy: 3
calling: 3
cambridge: 3
camera: 3
cap: 3
care: 3
carries: 3
categories: 3
chandra: 3
characteristic: 3
checked: 3
children: 3
chronous: 3
chubby: 3
churn: 3
circumvent: 3
claims: 3
clement: 3
closing: 3
colleagues: 3
colorado: 3
column: 3
combinations: 3
commands: 3
commerce: 3
commonly: 3
commun: 3
communicating: 3
company: 3
compatible: 3
compete: 3
competitive: 3
compilers: 3
compiling: 3
completes: 3
composed: 3
compound: 3
comprises: 3
compulsory: 3
computational: 3
computed: 3
con: 3
conceived: 3
concentrate: 3
concluded: 3
conclusions: 3
condition: 3
configurable: 3
confirmed: 3
conservation: 3
conserve: 3
consideration: 3
constructing: 3
construction: 3
consumed: 3
consumes: 3
continues: 3
convinced: 3
convoy: 3
cooperate: 3
coordinates: 3
coordination: 3
copper: 3
corbett: 3
corfu: 3
corporate: 3
correcting: 3
correctness: 3
correlate: 3
criterion: 3
culler: 3
culprit: 3
cumulus: 3
curious: 3
custer: 3
custom: 3
customer: 3
customized: 3
cycles: 3
dahlin: 3
dangers: 3
davis: 3
ddos: 3
deadline: 3
deadlines: 3
deals: 3
decade: 3
deciding: 3
decker: 3
decreasing: 3
deering: 3
degrade: 3
demonstrating: 3
dependable: 3
designers: 3
desire: 3
determinism: 3
deviation: 3
dialogue: 3
differentiation: 3
dis: 3
discover: 3
discrepancy: 3
disruptions: 3
disruptive: 3
disseminated: 3
distributing: 3
dns: 3
documentation: 3
documents: 3
dollars: 3
dominated: 3
dos: 3
drawn: 3
driver: 3
dropping: 3
drpm: 3
dying: 3
economics: 3
eded: 3
edward: 3
eicken: 3
el: 3
elastic: 3
elastras: 3
election: 3
electrical: 3
elegant: 3
else: 3
emergence: 3
emotional: 3
empire: 3
encodings: 3
encouraging: 3
ending: 3
endpoint: 3
endto: 3
enhances: 3
ensuring: 3
enterprises: 3
entities: 3
entity: 3
essay: 3
essential: 3
evaluates: 3
evaluating: 3
eventing: 3
evident: 3
evolution: 3
evolved: 3
exacerbates: 3
excellent: 3
excluding: 3
executes: 3
expands: 3
expelled: 3
experimentally: 3
experimented: 3
expires: 3
explorer: 3
exposes: 3
extends: 3
extension: 3
extent: 3
extra: 3
extracted: 3
fascinating: 3
fewer: 3
fg: 3
fields: 3
fifteenth: 3
finite: 3
firm: 3
flash: 3
flat: 3
fledged: 3
flight: 3
folder: 3
folders: 3
forest: 3
format: 3
formation: 3
franklin: 3
freeloaders: 3
french: 3
friend: 3
fs: 3
ganesh: 3
gargamel: 3
gather: 3
geographically: 3
gf: 3
gfgf: 3
goals: 3
gossiped: 3
government: 3
gps: 3
graphically: 3
great: 3
greatest: 3
grids: 3
grossman: 3
gt: 3
guaranteed: 3
gui: 3
gummadi: 3
guo: 3
gurumurthi: 3
hall: 3
handlers: 3
handley: 3
happen: 3
happy: 3
hauser: 3
haven: 3
heap: 3
helland: 3
helping: 3
hibernator: 3
hierarchically: 3
hierarchies: 3
highspeed: 3
homogeneous: 3
howard: 3
hypothesis: 3
ibm: 3
icdcs: 3
ics: 3
ignores: 3
ignoring: 3
il: 3
immersed: 3
implicitly: 3
impose: 3
imposes: 3
incentives: 3
income: 3
incorporated: 3
incorrectly: 3
incremental: 3
incremented: 3
incur: 3
indiana: 3
indicating: 3
informed: 3
inherits: 3
initiate: 3
innovations: 3
inside: 3
insights: 3
instability: 3
instantaneous: 3
institute: 3
intent: 3
interacting: 3
intercept: 3
intercepting: 3
intercepts: 3
interconnect: 3
interconnected: 3
internals: 3
internetworks: 3
interplay: 3
intervene: 3
inversion: 3
irregular: 3
iteration: 3
jbosscache: 3
jim: 3
jitter: 3
keeps: 3
kenneth: 3
knowing: 3
kuenning: 3
landscape: 3
laptops: 3
larson: 3
late: 3
lateral: 3
lesson: 3
linkage: 3
linked: 3
lions: 3
lloyd: 3
logically: 3
london: 3
looks: 3
loop: 3
losing: 3
lot: 3
luby: 3
mainframe: 3
mainsoft: 3
mainwin: 3
malkhi: 3
malo: 3
managers: 3
marian: 3
markets: 3
marks: 3
mashed: 3
matter: 3
mazie: 3
mckusick: 3
meanwhile: 3
memcached: 3
michael: 3
microbenchmark: 3
mid: 3
migration: 3
minimizing: 3
mirror: 3
mitigate: 3
modular: 3
montresor: 3
motivated: 3
motivates: 3
mountain: 3
movies: 3
multithreaded: 3
nat: 3
native: 3
neighborhood: 3
ninja: 3
nonetheless: 3
noticeable: 3
null: 3
observing: 3
obstruction: 3
occasionally: 3
odel: 3
oneway: 3
onto: 3
opened: 3
operational: 3
opposed: 3
optimizing: 3
optionally: 3
organize: 3
orientation: 3
ousterhout: 3
outbound: 3
outdated: 3
overflows: 3
overlapped: 3
overloads: 3
overwhelmed: 3
pacific: 3
padhye: 3
pai: 3
paid: 3
parameterize: 3
participant: 3
passing: 3
paying: 3
payload: 3
pc: 3
pedone: 3
penalties: 3
perceived: 3
percent: 3
perez: 3
personalities: 3
personalization: 3
pervasive: 3
peter: 3
phone: 3
pinheiro: 3
pisa: 3
players: 3
pleisch: 3
poisson: 3
polls: 3
poor: 3
popek: 3
popularity: 3
portions: 3
pose: 3
posed: 3
positioning: 3
prabhakaran: 3
prebuilt: 3
precedence: 3
predominate: 3
preferentially: 3
prefetched: 3
presentation: 3
preserved: 3
president: 3
preventing: 3
principal: 3
probing: 3
processor: 3
produce: 3
productivity: 3
programmers: 3
promise: 3
propagate: 3
properly: 3
protection: 3
proves: 3
provider: 3
psockets: 3
publishers: 3
punishment: 3
purchases: 3
pushed: 3
putting: 3
qpqp: 3
quanta: 3
queueing: 3
queues: 3
raises: 3
ramp: 3
ranging: 3
rao: 3
raptor: 3
reacting: 3
readonly: 3
realizing: 3
reasoning: 3
recipient: 3
recognize: 3
recommendations: 3
reconstruct: 3
recoveries: 3
reflecting: 3
rejoins: 3
relate: 3
relates: 3
released: 3
releases: 3
remarkably: 3
removing: 3
replaces: 3
replacing: 3
replayed: 3
replica: 3
replies: 3
representation: 3
representative: 3
res: 3
reserve: 3
resort: 3
resp: 3
respective: 3
responds: 3
restarting: 3
restored: 3
restricted: 3
retain: 3
retransmitted: 3
retrieves: 3
revisions: 3
rewarding: 3
rfc: 3
rises: 3
risks: 3
safely: 3
saint: 3
satisfies: 3
satisfy: 3
satisfying: 3
scaled: 3
scene: 3
school: 3
scope: 3
script: 3
semantic: 3
sensor: 3
separation: 3
sequential: 3
served: 3
seventeenth: 3
seventh: 3
severe: 3
sharded: 3
shepherd: 3
showed: 3
sight: 3
simpler: 3
simplest: 3
sinfonia: 3
situated: 3
sixth: 3
sleep: 3
slight: 3
smallest: 3
soft: 3
song: 3
sorrosal: 3
sound: 3
spanner: 3
spare: 3
spatial: 3
specifications: 3
speculatively: 3
splitstream: 3
splitting: 3
sporadic: 3
spot: 3
sprite: 3
sr: 3
ssrn: 3
stability: 3
standardize: 3
started: 3
stays: 3
stems: 3
streamed: 3
struggled: 3
subnet: 3
subscriber: 3
substrate: 3
successive: 3
suddenly: 3
sufficiently: 3
suggests: 3
suite: 3
supplied: 3
surprising: 3
suspicions: 3
symbolic: 3
symbols: 3
synchronously: 3
sys: 3
syst: 3
tables: 3
tacc: 3
tagging: 3
talk: 3
tape: 3
targets: 3
taylor: 3
teams: 3
television: 3
tends: 3
terminating: 3
testing: 3
text: 3
thin: 3
thinks: 3
thirty: 3
thomson: 3
threat: 3
tight: 3
tiled: 3
tolerable: 3
ton: 3
tput: 3
tr: 3
traditionally: 3
train: 3
transferred: 3
transferring: 3
transmissions: 3
triggered: 3
triggering: 3
trivial: 3
trouble: 3
truncation: 3
tsunami: 3
tudor: 3
tudorm: 3
turning: 3
uk: 3
unacceptable: 3
unavailable: 3
unblocks: 3
uncommittable: 3
undetected: 3
unexpectedly: 3
unfairly: 3
unmanaged: 3
unperturbed: 3
unrelated: 3
utah: 3
utilize: 3
uwin: 3
validated: 3
valuation: 3
variables: 3
variance: 3
verified: 3
verlag: 3
viewing: 3
vigfusson: 3
visualization: 3
vt: 3
wake: 3
walli: 3
wanted: 3
warns: 3
watch: 3
wefel: 3
wei: 3
whatever: 3
widespread: 3
winner: 3
wisdom: 3
wobber: 3
worlds: 3
worry: 3
wv: 3
yxyx: 3
zero: 3
zhao: 3
zurich: 3
abandon: 2
abc: 2
abilene: 2
abstractions: 2
academics: 2
acceptance: 2
accessible: 2
accomplished: 2
accordance: 2
accounting: 2
accumulating: 2
accurately: 2
accusation: 2
accused: 2
acquisition: 2
adam: 2
addison: 2
addressing: 2
adjusted: 2
adjusts: 2
administrative: 2
adopt: 2
advancements: 2
advent: 2
adversary: 2
advertises: 2
advice: 2
advocated: 2
aegis: 2
affinity: 2
aforementioned: 2
age: 2
agents: 2
aggregator: 2
aggressive: 2
aict: 2
aim: 2
ajax: 2
akamai: 2
alaska: 2
alberto: 2
albums: 2
alive: 2
alleviating: 2
allocates: 2
allocating: 2
alloscomp: 2
alternatively: 2
altruistic: 2
ame: 2
amortizes: 2
amounting: 2
analyzes: 2
anchorage: 2
anderson: 2
andresen: 2
anomalies: 2
anonymity: 2
anonymously: 2
anti: 2
anticipate: 2
antony: 2
apart: 2
apis: 2
appealing: 2
appeared: 2
appends: 2
applicationindependent: 2
approximately: 2
approximation: 2
approximations: 2
apr: 2
archetypal: 2
argues: 2
arguing: 2
arithmetic: 2
arose: 2
arraystructured: 2
arrived: 2
art: 2
article: 2
articles: 2
asia: 2
asics: 2
asks: 2
asp: 2
aspx: 2
assists: 2
assurance: 2
astonishingly: 2
asynch: 2
atm: 2
attach: 2
attached: 2
attain: 2
attempted: 2
attempting: 2
attestation: 2
attract: 2
attraction: 2
attractiveness: 2
audio: 2
aumann: 2
authentication: 2
automate: 2
autonomous: 2
autotuning: 2
avoidance: 2
ba: 2
baa: 2
babaoglu: 2
bach: 2
backlashed: 2
bailey: 2
baker: 2
ban: 2
barb: 2
barr: 2
barrier: 2
bast: 2
basu: 2
began: 2
behaved: 2
behaving: 2
belonging: 2
benjamin: 2
berlin: 2
bernoulli: 2
besides: 2
bespoke: 2
bhargava: 2
bifurcation: 2
billion: 2
binary: 2
binomial: 2
birrell: 2
bitcoinfoundation: 2
bjornstad: 2
blank: 2
bloomberg: 2
blow: 2
blumenthal: 2
boils: 2
bolton: 2
bonus: 2
borders: 2
box: 2
boycott: 2
bridge: 2
brings: 2
brokerage: 2
bronson: 2
browsers: 2
bruijn: 2
btcguild: 2
btchine: 2
buf: 2
bulpin: 2
burden: 2
burstiness: 2
business: 2
businesses: 2
buterin: 2
butterfly: 2
bypass: 2
caja: 2
calculation: 2
calculations: 2
calvin: 2
came: 2
canada: 2
candidates: 2
canonical: 2
capabilities: 2
carey: 2
carolina: 2
carrera: 2
cash: 2
castro: 2
casual: 2
casually: 2
category: 2
cation: 2
cdns: 2
cease: 2
ceases: 2
ceived: 2
census: 2
centralization: 2
century: 2
certainty: 2
cfm: 2
challenging: 2
chance: 2
characterise: 2
cheaply: 2
checkout: 2
checksums: 2
chemical: 2
chicago: 2
chief: 2
china: 2
chowdhry: 2
cidr: 2
circle: 2
circulate: 2
circulates: 2
circumstances: 2
cited: 2
clark: 2
click: 2
cloudviews: 2
coarse: 2
coexist: 2
coherence: 2
cohort: 2
colarelli: 2
colocated: 2
color: 2
combat: 2
comer: 2
committing: 2
commons: 2
communicated: 2
compelled: 2
compensate: 2
compensation: 2
complement: 2
composing: 2
computes: 2
concentrated: 2
concert: 2
conclusive: 2
confident: 2
confined: 2
connect: 2
cons: 2
considerably: 2
considers: 2
constitute: 2
constraint: 2
contacted: 2
contexts: 2
contract: 2
contradicts: 2
controlling: 2
conversion: 2
converted: 2
converting: 2
cooling: 2
cooper: 2
cording: 2
corner: 2
correspond: 2
countermeasures: 2
counterpart: 2
covered: 2
covering: 2
cox: 2
cpus: 2
credited: 2
credits: 2
cripple: 2
cripples: 2
criticism: 2
croquet: 2
crypto: 2
cryptocurrencies: 2
cryptographic: 2
cryptology: 2
ct: 2
cubic: 2
cypherspace: 2
cz: 2
damage: 2
dan: 2
dangerous: 2
daniel: 2
dast: 2
datasets: 2
datta: 2
days: 2
dbms: 2
dce: 2
dd: 2
deadlocks: 2
dealbook: 2
deceased: 2
decodes: 2
deduce: 2
deeper: 2
deeply: 2
defanti: 2
defects: 2
defer: 2
degradations: 2
degrading: 2
deleting: 2
deliberately: 2
deliberation: 2
delivering: 2
delta: 2
demanding: 2
dentical: 2
dependence: 2
depicted: 2
depicts: 2
deplist: 2
deplistcurr: 2
derive: 2
descriptor: 2
designing: 2
deterministically: 2
deviate: 2
devise: 2
dfreedman: 2
didn: 2
differentiable: 2
diminish: 2
dinosaur: 2
diot: 2
disabled: 2
disadvantage: 2
disadvantages: 2
discount: 2
disincentivize: 2
disrupting: 2
disseminates: 2
dissertation: 2
distances: 2
distinction: 2
diverse: 2
divide: 2
dividing: 2
doi: 2
dok: 2
dominate: 2
dot: 2
dotbit: 2
doubt: 2
downloaded: 2
downloads: 2
downside: 2
drives: 2
driving: 2
dry: 2
ds: 2
dubbed: 2
dugan: 2
dumbbell: 2
duplex: 2
duplicating: 2
dutta: 2
eagerly: 2
earned: 2
earnings: 2
ebling: 2
echo: 2
ecoop: 2
ecosystem: 2
eigenvalue: 2
eighteenth: 2
einar: 2
einstein: 2
elmore: 2
embeddings: 2
emphasis: 2
empirical: 2
enables: 2
encapsulate: 2
encapsulated: 2
encoders: 2
encounter: 2
encountered: 2
encrypted: 2
endhost: 2
ends: 2
ensemble: 2
ent: 2
enters: 2
envision: 2
equate: 2
equilibria: 2
era: 2
ered: 2
esbs: 2
escriva: 2
esign: 2
establishing: 2
estimates: 2
ethereum: 2
ethereumwhitepaper: 2
ets: 2
european: 2
eurosys: 2
ev: 2
eva: 2
eve: 2
eventbased: 2
everyone: 2
eviction: 2
evictions: 2
evolve: 2
exabytes: 2
exceeds: 2
exceptionally: 2
excessively: 2
executions: 2
exerted: 2
exerting: 2
exhibits: 2
expectation: 2
expectations: 2
expedite: 2
explains: 2
explores: 2
exponentially: 2
exports: 2
express: 2
expurging: 2
extensibility: 2
extremes: 2
eyes: 2
facilitate: 2
facto: 2
fair: 2
fall: 2
falling: 2
farms: 2
faulttolerance: 2
featuring: 2
fekete: 2
felser: 2
felten: 2
feng: 2
ferguson: 2
fetched: 2
fills: 2
finer: 2
finished: 2
finishes: 2
fired: 2
firewalling: 2
firoiu: 2
fits: 2
fix: 2
fl: 2
flag: 2
flexibly: 2
fluctuations: 2
fluid: 2
flushed: 2
fold: 2
forbes: 2
forcing: 2
forgo: 2
formats: 2
formulae: 2
forrestv: 2
forth: 2
foundations: 2
fox: 2
fragment: 2
fragments: 2
fraleigh: 2
frameworks: 2
fred: 2
fresh: 2
friction: 2
friedman: 2
friendliness: 2
frobenius: 2
fundamentally: 2
funded: 2
funds: 2
furman: 2
gained: 2
gaps: 2
garbled: 2
gateway: 2
gehrke: 2
generalize: 2
generations: 2
genesis: 2
geometric: 2
ghz: 2
gibbs: 2
gibson: 2
gigabyte: 2
github: 2
glick: 2
globalcrossing: 2
globe: 2
gossiper: 2
gossips: 2
gotten: 2
gr: 2
gradient: 2
gradually: 2
granted: 2
grapevine: 2
grasp: 2
greene: 2
griner: 2
guerraoui: 2
guesses: 2
guessing: 2
gupta: 2
guruprasad: 2
habel: 2
hackers: 2
halfway: 2
halted: 2
handful: 2
hands: 2
happens: 2
harder: 2
harley: 2
harm: 2
hashrate: 2
headers: 2
healthy: 2
heidelberg: 2
height: 2
held: 2
helpful: 2
hereinafter: 2
heterogenous: 2
hey: 2
hibler: 2
hiccups: 2
hides: 2
highlight: 2
highvalue: 2
hinder: 2
histogram: 2
hits: 2
hoarding: 2
hobor: 2
holy: 2
homepage: 2
honesty: 2
honeyman: 2
hops: 2
hotcloud: 2
huitema: 2
hundred: 2
hungry: 2
hurdle: 2
hurwitz: 2
huston: 2
iacr: 2
ic: 2
iciw: 2
icmp: 2
identities: 2
idit: 2
ie: 2
ifip: 2
ih: 2
ij: 2
ill: 2
illinois: 2
im: 2
imagined: 2
immaterial: 2
immediate: 2
immersion: 2
imperatives: 2
imperfect: 2
imply: 2
imposing: 2
impractical: 2
impulse: 2
incorporation: 2
indices: 2
indirection: 2
inefficiency: 2
inefficient: 2
infer: 2
influence: 2
influenced: 2
infrastructures: 2
ingrid: 2
ings: 2
inherent: 2
ining: 2
initialized: 2
initiates: 2
injecting: 2
insertion: 2
inspect: 2
install: 2
installed: 2
instantiated: 2
institutions: 2
instruction: 2
instructs: 2
integer: 2
intentionally: 2
interconnecting: 2
interconnection: 2
interconnects: 2
interix: 2
intermediary: 2
interpret: 2
intersection: 2
intervention: 2
intimate: 2
intricacies: 2
invalid: 2
investment: 2
invoke: 2
invokes: 2
ipdps: 2
iptps: 2
irish: 2
iscussion: 2
island: 2
italy: 2
itcoin: 2
ith: 2
ity: 2
izs: 2
james: 2
javabeans: 2
jimenez: 2
joglekar: 2
jonsson: 2
jsp: 2
juels: 2
justify: 2
kandemir: 2
kansas: 2
kapritsos: 2
karlsson: 2
katabi: 2
kazar: 2
kemme: 2
keycurr: 2
kharif: 2
kiawah: 2
kilper: 2
kim: 2
kimsas: 2
kjkj: 2
knob: 2
knobs: 2
kojo: 2
kroll: 2
krzys: 2
kurose: 2
labelled: 2
labs: 2
lagged: 2
lagging: 2
lake: 2
lambdarail: 2
lambdas: 2
landing: 2
landolsi: 2
laptop: 2
lastly: 2
lastop: 2
launching: 2
laying: 2
le: 2
lease: 2
legitimate: 2
leigh: 2
lepreau: 2
leskovec: 2
leslie: 2
leverages: 2
levy: 2
lightly: 2
lightwave: 2
likewise: 2
lin: 2
liskov: 2
literally: 2
literature: 2
lm: 2
locate: 2
lockstep: 2
longest: 2
longhaul: 2
loops: 2
loosely: 2
louise: 2
lowering: 2
lowers: 2
lows: 2
ltd: 2
luck: 2
lundqvist: 2
lynch: 2
lyon: 2
madden: 2
madhow: 2
magharei: 2
mance: 2
mandatory: 2
mangling: 2
manifested: 2
manipulate: 2
manually: 2
march: 2
marshaling: 2
martinez: 2
matrices: 2
matthews: 2
mature: 2
maximal: 2
maximized: 2
maya: 2
md: 2
mdcc: 2
meant: 2
megabits: 2
memoryless: 2
menees: 2
metacdn: 2
micro: 2
microsystems: 2
migrating: 2
milestone: 2
mined: 2
minimising: 2
minimizes: 2
minus: 2
mirroring: 2
miscalculate: 2
mismatched: 2
mistakenly: 2
mit: 2
mitigated: 2
mitigation: 2
mlml: 2
modem: 2
moderately: 2
modest: 2
modulo: 2
mohr: 2
montenegro: 2
monterey: 2
moshe: 2
mrc: 2
mst: 2
multigrep: 2
multiples: 2
multiplied: 2
multithreading: 2
muri: 2
muthitacharoen: 2
myriad: 2
nakamoto: 2
named: 2
names: 2
naming: 2
nandi: 2
narayanan: 2
naughton: 2
navathe: 2
ncsa: 2
ne: 2
nec: 2
needham: 2
negation: 2
neil: 2
nejdl: 2
newbold: 2
newer: 2
newly: 2
ngan: 2
nic: 2
nichols: 2
nishtala: 2
nity: 2
nlanr: 2
nonequilibrium: 2
nonnenmacher: 2
nontransactional: 2
normalization: 2
normalize: 2
normalizing: 2
normed: 2
notably: 2
notes: 2
notifies: 2
notifying: 2
noting: 2
nutshell: 2
nytimes: 2
obsolete: 2
obviously: 2
occasion: 2
occupies: 2
odd: 2
offices: 2
ofwork: 2
ole: 2
olympics: 2
omi: 2
omits: 2
ool: 2
ooled: 2
op: 2
opacity: 2
opening: 2
operated: 2
opportunities: 2
opt: 2
opted: 2
ordinarily: 2
organizational: 2
organizing: 2
originate: 2
orities: 2
oscillates: 2
ostar: 2
ourselves: 2
outages: 2
outs: 2
outsource: 2
outsources: 2
overcast: 2
overflow: 2
overlays: 2
overqos: 2
pairings: 2
pam: 2
panacea: 2
par: 2
paradigms: 2
parameshwaran: 2
parameterizations: 2
parametrized: 2
parity: 2
park: 2
parno: 2
pass: 2
pathway: 2
patterson: 2
payloads: 2
paypal: 2
pcb: 2
pdcs: 2
pdfs: 2
penalised: 2
pentium: 2
peration: 2
perfor: 2
peris: 2
permits: 2
permitted: 2
perpetrators: 2
perron: 2
petersen: 2
ph: 2
phanishayee: 2
phases: 2
physically: 2
picture: 2
piggybacked: 2
piling: 2
pipeline: 2
pipelines: 2
pitfalls: 2
plague: 2
plays: 2
plication: 2
plugged: 2
plugging: 2
poolattacks: 2
popper: 2
port: 2
porto: 2
ports: 2
poses: 2
position: 2
powersaving: 2
pplive: 2
practically: 2
preceding: 2
precise: 2
precludes: 2
predecessor: 2
predefined: 2
predetermined: 2
predictive: 2
predicts: 2
preferable: 2
prefers: 2
preiss: 2
preliminary: 2
premise: 2
presenting: 2
preservation: 2
preserve: 2
preserving: 2
prevail: 2
prevention: 2
primitive: 2
prioritising: 2
prioritization: 2
prioritize: 2
prioritizing: 2
privately: 2
probabilistically: 2
probe: 2
problematic: 2
processors: 2
produces: 2
professor: 2
profile: 2
profiler: 2
profit: 2
projection: 2
promote: 2
proper: 2
proposals: 2
pros: 2
proving: 2
pseudo: 2
pullbased: 2
pulling: 2
pulse: 2
purchase: 2
purchasing: 2
pure: 2
pursue: 2
pushes: 2
pvldb: 2
qhuang: 2
qin: 2
qmi: 2
qos: 2
quasistatic: 2
quicksilver: 2
quote: 2
race: 2
racticalities: 2
radc: 2
radical: 2
radio: 2
raised: 2
ramakrishnan: 2
ramamritham: 2
randomness: 2
ranges: 2
rank: 2
rateless: 2
rational: 2
ratios: 2
reacts: 2
readily: 2
ready: 2
realizes: 2
rears: 2
reasonably: 2
reception: 2
reciprocation: 2
recognition: 2
recompute: 2
reconciliation: 2
recycling: 2
redesigned: 2
redesigning: 2
reductions: 2
referred: 2
refraining: 2
refreshed: 2
regard: 2
registration: 2
reimplemented: 2
reissue: 2
reissued: 2
rejaie: 2
rejected: 2
rejecting: 2
rejects: 2
relaxed: 2
relay: 2
relevance: 2
relied: 2
relies: 2
reliminaries: 2
relying: 2
remarks: 2
remotely: 2
removes: 2
rendering: 2
rep: 2
repaired: 2
repeatable: 2
repetitions: 2
reporting: 2
repurposing: 2
reputation: 2
requisite: 2
resending: 2
resends: 2
reserves: 2
resident: 2
resolutions: 2
respecting: 2
responsive: 2
restarted: 2
retaliate: 2
retried: 2
retries: 2
reverse: 2
reviewers: 2
revisiting: 2
rewarded: 2
rewritten: 2
reykjavik: 2
reynolds: 2
ricci: 2
rightful: 2
rigid: 2
rigorous: 2
rigorously: 2
rimon: 2
rising: 2
rivalry: 2
roberts: 2
rohrs: 2
rome: 2
roots: 2
rose: 2
rosenblum: 2
ross: 2
rounding: 2
routinely: 2
row: 2
rp: 2
rsized: 2
rt: 2
rtts: 2
ruling: 2
runners: 2
rvr: 2
sabul: 2
safe: 2
saha: 2
said: 2
sake: 2
sales: 2
satellite: 2
saxena: 2
scans: 2
scatter: 2
scattered: 2
scheduler: 2
schedules: 2
sci: 2
scientist: 2
scratch: 2
scsi: 2
seamless: 2
secondgeneration: 2
seda: 2
seeking: 2
seemingly: 2
seldom: 2
seltzer: 2
seminar: 2
senior: 2
sensible: 2
sept: 2
serial: 2
serialize: 2
serially: 2
serious: 2
serverlets: 2
serverpull: 2
settle: 2
shape: 2
shapley: 2
sharp: 2
sheds: 2
shelby: 2
shell: 2
shi: 2
shieh: 2
shifts: 2
shokrollahi: 2
shortcut: 2
shortly: 2
sidebotham: 2
sigact: 2
signs: 2
sigops: 2
sigurbjornsson: 2
silicon: 2
silverlight: 2
simplification: 2
simplified: 2
simulating: 2
singh: 2
sinks: 2
sister: 2
sivakumar: 2
slashdot: 2
slashes: 2
slows: 2
slush: 2
smooth: 2
soar: 2
soars: 2
soas: 2
societies: 2
society: 2
solaris: 2
solely: 2
solheim: 2
solid: 2
solved: 2
something: 2
son: 2
sonic: 2
sons: 2
soper: 2
sourceforge: 2
south: 2
sovran: 2
specialists: 2
speedups: 2
spend: 2
spending: 2
splits: 2
spreads: 2
spreitzer: 2
srs: 2
st: 2
stabilizes: 2
staggering: 2
standalone: 2
stands: 2
stat: 2
statement: 2
station: 2
statqwest: 2
stats: 2
steal: 2
stefan: 2
stephen: 2
stocks: 2
stoller: 2
stops: 2
stories: 2
storms: 2
story: 2
stripe: 2
striping: 2
sturgis: 2
styles: 2
su: 2
subgroup: 2
submits: 2
submitted: 2
submitting: 2
subscribed: 2
subversive: 2
subverted: 2
succession: 2
successively: 2
sudden: 2
suffice: 2
suggest: 2
suggestions: 2
suited: 2
summation: 2
sundr: 2
supergame: 2
supergames: 2
supervisors: 2
supply: 2
suppose: 2
sure: 2
surpass: 2
surprise: 2
survey: 2
susceptibility: 2
suspect: 2
sustain: 2
swallow: 2
swanson: 2
swapping: 2
swift: 2
swinehart: 2
swiss: 2
sybil: 2
synchronize: 2
synthesis: 2
systematic: 2
systemic: 2
tablets: 2
tagged: 2
tailer: 2
tandard: 2
tango: 2
tardos: 2
tation: 2
tauber: 2
tc: 2
technically: 2
tempted: 2
tend: 2
termed: 2
testbed: 2
tgperf: 2
theimer: 2
thick: 2
thing: 2
thirtieth: 2
thumb: 2
tiered: 2
ties: 2
tional: 2
tions: 2
tirumala: 2
toappliance: 2
tock: 2
tods: 2
tolerates: 2
tolerating: 2
topeer: 2
topologically: 2
totally: 2
touch: 2
touches: 2
toueg: 2
tough: 2
toy: 2
tp: 2
tracing: 2
tracked: 2
trades: 2
tragedy: 2
tranmission: 2
transact: 2
transcoded: 2
transforming: 2
trapezoids: 2
treatment: 2
tremel: 2
triangular: 2
tricky: 2
tries: 2
trivially: 2
trol: 2
trsu: 2
trustworthy: 2
tuft: 2
tuned: 2
tv: 2
twentieth: 2
twofold: 2
ugly: 2
umich: 2
un: 2
unaltered: 2
unattacked: 2
unattractive: 2
uncommitable: 2
uncommitted: 2
unexplored: 2
unfavorable: 2
uninvolved: 2
uniquely: 2
united: 2
unlucky: 2
unnecessarily: 2
unnecessary: 2
unreachable: 2
unresponsive: 2
uploading: 2
urgent: 2
userspace: 2
validating: 2
vancouver: 2
vandermeer: 2
vandermonde: 2
vasek: 2
vcurr: 2
vein: 2
venkataramani: 2
ver: 2
verby: 2
versioned: 2
vicinity: 2
viewers: 2
violations: 2
virtualized: 2
visibility: 2
visited: 2
vko: 2
volumes: 2
vpn: 2
wakeup: 2
walks: 2
wallace: 2
war: 2
warehouse: 2
warship: 2
watching: 2
wattenhofer: 2
weakens: 2
weaker: 2
weaknesses: 2
websphere: 2
welch: 2
wesley: 2
west: 2
whereby: 2
whom: 2
wicker: 2
wider: 2
width: 2
wiley: 2
williams: 2
willing: 2
willner: 2
winter: 2
withholds: 2
witnessed: 2
wizkid: 2
wlog: 2
wo: 2
wolfgang: 2
wong: 2
words: 2
workflow: 2
worsening: 2
wrapping: 2
wrongdoing: 2
wvwv: 2
xie: 2
xored: 2
xp: 2
xth: 2
yang: 2
ymir: 2
yxxy: 2
zoom: 2
aaron: 1
abbreviations: 1
abilalso: 1
absorbs: 1
abstracts: 1
abu: 1
accompanied: 1
accomplish: 1
accomplishing: 1
accounted: 1
accumulate: 1
aceves: 1
acheive: 1
acheived: 1
acls: 1
acmula: 1
acomplish: 1
acquires: 1
acquiring: 1
acthe: 1
activated: 1
actuators: 1
acwe: 1
adamic: 1
adaptable: 1
adapted: 1
adaptively: 1
adaptivity: 1
addi: 1
addiserver: 1
additive: 1
addressable: 1
adelaide: 1
adhere: 1
adjacent: 1
administers: 1
admit: 1
admits: 1
adoption: 1
ads: 1
advertise: 1
advertised: 1
adya: 1
aelstrom: 1
aerospace: 1
af: 1
affirmative: 1
afford: 1
afforded: 1
afraid: 1
afterward: 1
agenda: 1
agent: 1
ager: 1
aggregat: 1
aggregrate: 1
aggressively: 1
aggressiveness: 1
agreements: 1
ahnn: 1
aid: 1
aims: 1
airplane: 1
ak: 1
akkus: 1
alarms: 1
albatross: 1
albrecht: 1
alegre: 1
alexander: 1
alfetching: 1
allavena: 1
allegedly: 1
alleviated: 1
alleviates: 1
allo: 1
alloc: 1
allocations: 1
allthingsdistributed: 1
alof: 1
alpha: 1
altogether: 1
altrustic: 1
am: 1
amar: 1
ambitious: 1
america: 1
amortized: 1
ample: 1
amplify: 1
amsden: 1
analogy: 1
analyse: 1
analyst: 1
analytics: 1
ance: 1
anceaume: 1
anddrop: 1
andersen: 1
andre: 1
andreas: 1
ann: 1
anne: 1
annie: 1
annotated: 1
announce: 1
announcement: 1
announces: 1
annoyingly: 1
annually: 1
anonymous: 1
answered: 1
answers: 1
anticipated: 1
antipolis: 1
antonio: 1
anytime: 1
anyway: 1
anywhere: 1
ap: 1
apa: 1
appeal: 1
appendix: 1
applaud: 1
appleton: 1
appli: 1
applicationdropped: 1
appliit: 1
applior: 1
applithe: 1
approaching: 1
approximates: 1
apps: 1
apthe: 1
arbor: 1
architects: 1
archival: 1
archives: 1
argus: 1
arisen: 1
armies: 1
arranging: 1
arrivals: 1
artifact: 1
artificial: 1
ase: 1
aside: 1
ask: 1
asking: 1
assembling: 1
assist: 1
assisted: 1
associates: 1
associating: 1
astrolabe: 1
aswhen: 1
asynbandwidth: 1
asynchrony: 1
asynmicrobenchmarks: 1
asynmobile: 1
asynqueue: 1
ata: 1
atomically: 1
atr: 1
att: 1
attaches: 1
attaching: 1
attar: 1
attention: 1
attr: 1
attracted: 1
attributable: 1
attributed: 1
atypical: 1
audit: 1
audits: 1
australia: 1
authoritative: 1
authorized: 1
autodesk: 1
automation: 1
autotion: 1
autowriteback: 1
autumn: 1
ava: 1
availto: 1
avatar: 1
awaiting: 1
awareness: 1
axes: 1
azim: 1
backgound: 1
backgrounds: 1
backlog: 1
backlogs: 1
backnt: 1
backups: 1
bakalova: 1
balancers: 1
ballance: 1
ballot: 1
banaei: 1
bandcontain: 1
bandhowever: 1
bandwhile: 1
bandwidthdelay: 1
bandwidthsensitive: 1
banff: 1
barcelona: 1
bare: 1
barracuda: 1
barriers: 1
baseline: 1
bastion: 1
batched: 1
batches: 1
batching: 1
batkin: 1
battle: 1
battlefield: 1
battleground: 1
bayeux: 1
bcc: 1
bcq: 1
became: 1
befriended: 1
behren: 1
beijing: 1
belief: 1
believed: 1
beloved: 1
benchmarking: 1
benchmarks: 1
benenational: 1
benoit: 1
bered: 1
berkeleydb: 1
bernd: 1
bershad: 1
bership: 1
bert: 1
bertier: 1
bhattacharjee: 1
biggest: 1
bile: 1
bill: 1
billed: 1
billing: 1
bills: 1
bindel: 1
binds: 1
bio: 1
bitcoinmines: 1
bk: 1
blades: 1
blame: 1
ble: 1
bleeding: 1
blending: 1
blogs: 1
boa: 1
board: 1
body: 1
bold: 1
bologna: 1
bond: 1
bonding: 1
bonn: 1
book: 1
boosting: 1
boot: 1
borisov: 1
bortnikov: 1
boss: 1
bostic: 1
bother: 1
bottlenecks: 1
bought: 1
boundary: 1
bounds: 1
boys: 1
bq: 1
br: 1
brahms: 1
brazil: 1
breakthrough: 1
bregni: 1
breslau: 1
brian: 1
bridging: 1
bright: 1
british: 1
broad: 1
broberg: 1
brokers: 1
brought: 1
bruno: 1
budget: 1
budgets: 1
buffercould: 1
bug: 1
buggy: 1
bugs: 1
builders: 1
burdens: 1
bureau: 1
burgess: 1
buried: 1
burrows: 1
busnel: 1
button: 1
buyer: 1
buying: 1
buys: 1
cabrera: 1
cacheable: 1
cacheserializability: 1
caise: 1
cal: 1
calability: 1
camargos: 1
cameras: 1
cancelled: 1
candidate: 1
cannes: 1
cantwell: 1
capability: 1
cappuccino: 1
captured: 1
caratti: 1
carded: 1
cardinality: 1
cardoso: 1
career: 1
careers: 1
careful: 1
caribbean: 1
carl: 1
carried: 1
carrier: 1
carrying: 1
cart: 1
carzaniga: 1
cas: 1
cascades: 1
cast: 1
castor: 1
catalog: 1
catastrophe: 1
catastrophic: 1
cated: 1
categorized: 1
cations: 1
causal: 1
cbb: 1
cbbc: 1
cbqpcb: 1
ccb: 1
ccdf: 1
cdf: 1
cdrom: 1
ceives: 1
cell: 1
centered: 1
ceremony: 1
certify: 1
certifying: 1
cess: 1
chained: 1
chainlink: 1
chair: 1
chakka: 1
chakravorty: 1
challenger: 1
chang: 1
changtao: 1
chapman: 1
characterised: 1
chawathe: 1
cheat: 1
checkouts: 1
chenchu: 1
cheslack: 1
chicken: 1
chiefly: 1
chien: 1
chinese: 1
ching: 1
chockler: 1
choke: 1
chose: 1
chow: 1
christened: 1
christmas: 1
chronously: 1
chrony: 1
chu: 1
chuck: 1
chunk: 1
chunks: 1
cincilla: 1
circuitous: 1
circulation: 1
circumvented: 1
circumvents: 1
citation: 1
cites: 1
cities: 1
clara: 1
clarity: 1
cleaned: 1
cleanly: 1
clearer: 1
clements: 1
clientserver: 1
clocks: 1
clone: 1
closest: 1
cloudifying: 1
clutter: 1
cmu: 1
codaniques: 1
coded: 1
coding: 1
coexistence: 1
coexists: 1
coherency: 1
coincide: 1
collaborate: 1
collaborating: 1
collapsing: 1
collateral: 1
collects: 1
collusions: 1
columbia: 1
comings: 1
comitting: 1
comm: 1
commatic: 1
commence: 1
commences: 1
commentary: 1
commenting: 1
commercially: 1
commonplace: 1
communal: 1
communi: 1
commutative: 1
comp: 1
compact: 1
comparably: 1
comparaour: 1
comparative: 1
comparisons: 1
compelling: 1
compensating: 1
competitors: 1
compiler: 1
complementary: 1
complementing: 1
complexities: 1
complimentary: 1
comply: 1
compositional: 1
compressed: 1
compresses: 1
comprising: 1
compromises: 1
compromising: 1
computationally: 1
computfits: 1
comsium: 1
concentrates: 1
concentration: 1
conception: 1
conceptually: 1
concludes: 1
concrete: 1
concurby: 1
concuruses: 1
conditioned: 1
conext: 1
configure: 1
conform: 1
conios: 1
conitbased: 1
conjecture: 1
connec: 1
connectionless: 1
conner: 1
consequent: 1
conserving: 1
consisted: 1
consisupdate: 1
consortium: 1
constantly: 1
constituent: 1
constrain: 1
constructs: 1
consulting: 1
consumer: 1
consumers: 1
contacting: 1
contally: 1
contemplate: 1
contemplated: 1
contemplating: 1
contending: 1
conthe: 1
contingencies: 1
contolerate: 1
contractors: 1
contravention: 1
contributes: 1
controller: 1
controversy: 1
convenience: 1
convention: 1
converged: 1
converging: 1
conversely: 1
converts: 1
conveyed: 1
convoys: 1
cooperating: 1
coordinated: 1
coordinating: 1
coordinators: 1
copied: 1
coping: 1
cops: 1
cornelldeveloped: 1
corr: 1
corrected: 1
corrections: 1
correspondingly: 1
corroborated: 1
corrupted: 1
cotton: 1
counters: 1
counting: 1
couple: 1
crashing: 1
crawl: 1
credit: 1
credo: 1
crete: 1
crippling: 1
criteria: 1
critically: 1
crossroads: 1
crowcroft: 1
csd: 1
cstr: 1
culled: 1
cumulatively: 1
customised: 1
customizability: 1
customizable: 1
customization: 1
customizing: 1
customuserserviceapp: 1
cutler: 1
cuts: 1
cybercaf: 1
cyrus: 1
czerwinski: 1
dallas: 1
danielsson: 1
danilov: 1
dantzig: 1
dat: 1
datagrams: 1
dataset: 1
datastores: 1
daunting: 1
dave: 1
davoli: 1
dazzling: 1
dc: 1
dead: 1
deadlock: 1
deadly: 1
deadtially: 1
dean: 1
debatable: 1
debate: 1
debated: 1
debilitating: 1
debris: 1
debugger: 1
declare: 1
declaring: 1
declines: 1
decode: 1
decoder: 1
decoding: 1
decoupled: 1
decouples: 1
decreased: 1
defend: 1
deferplications: 1
deferrable: 1
defers: 1
defgh: 1
defining: 1
definitions: 1
degrees: 1
deit: 1
delegation: 1
delete: 1
deleted: 1
delivers: 1
demanded: 1
demise: 1
demonstration: 1
denning: 1
dennis: 1
denoting: 1
dep: 1
depart: 1
departments: 1
dependalso: 1
dependenrate: 1
depsa: 1
depth: 1
dequeued: 1
der: 1
dered: 1
deregistered: 1
derivative: 1
derivatives: 1
derives: 1
deriving: 1
des: 1
descending: 1
deserver: 1
deshow: 1
designated: 1
designating: 1
designer: 1
desirability: 1
desirable: 1
desktops: 1
destabilize: 1
destinations: 1
detailing: 1
deterioration: 1
determinants: 1
detrimental: 1
dev: 1
developerhours: 1
deviations: 1
devised: 1
devlin: 1
devoted: 1
devoting: 1
dewitt: 1
dexa: 1
dhs: 1
dht: 1
diamond: 1
dictionaries: 1
diego: 1
dies: 1
differany: 1
differential: 1
differentiates: 1
differentiating: 1
differenwith: 1
differing: 1
diffs: 1
dimensions: 1
dimov: 1
ding: 1
directing: 1
directs: 1
disable: 1
disables: 1
disappearance: 1
disappeared: 1
disastrous: 1
disbut: 1
discard: 1
disconnections: 1
disconnects: 1
discontinuing: 1
discotheque: 1
discounts: 1
discovered: 1
discusses: 1
discussing: 1
disguise: 1
disinclined: 1
disjoing: 1
dislike: 1
dismiss: 1
dispatch: 1
dispatcher: 1
dispatching: 1
displayed: 1
displaying: 1
displays: 1
dispools: 1
disregarded: 1
dissatisfied: 1
disseminating: 1
distill: 1
distorted: 1
diverges: 1
diversion: 1
diversity: 1
dll: 1
dlls: 1
documented: 1
dol: 1
domains: 1
dominating: 1
donet: 1
door: 1
dou: 1
doubled: 1
douceur: 1
doug: 1
downgraded: 1
downtime: 1
dozen: 1
dr: 1
drain: 1
drained: 1
drawbacks: 1
drawing: 1
draws: 1
drift: 1
drifting: 1
drive: 1
drivers: 1
drone: 1
duals: 1
ducing: 1
dummynet: 1
dundant: 1
dunn: 1
duplicates: 1
dur: 1
durably: 1
durations: 1
dwarf: 1
dx: 1
dynamics: 1
ear: 1
eastham: 1
economies: 1
ecution: 1
edd: 1
edges: 1
edit: 1
editors: 1
edits: 1
eds: 1
eduardo: 1
educational: 1
edutella: 1
eed: 1
efficacies: 1
eg: 1
egemen: 1
egg: 1
ekin: 1
elasticity: 1
elect: 1
electric: 1
electricity: 1
electronics: 1
eleventh: 1
eliability: 1
elicit: 1
eliciting: 1
elkin: 1
embark: 1
embody: 1
embrace: 1
embraces: 1
emerged: 1
emerges: 1
emitting: 1
emmanuelle: 1
empted: 1
emto: 1
emulating: 1
encapsulates: 1
encod: 1
encode: 1
encompass: 1
encourages: 1
encrypt: 1
encrypts: 1
endeavor: 1
endowment: 1
endpoints: 1
endtransaction: 1
endtransport: 1
enemy: 1
enforced: 1
engine: 1
engineers: 1
enlarge: 1
enlarges: 1
enlarging: 1
enputing: 1
entail: 1
entering: 1
entitled: 1
entropy: 1
enumerate: 1
episodes: 1
epositories: 1
epstein: 1
equeation: 1
equipped: 1
erally: 1
erates: 1
erent: 1
ering: 1
erodes: 1
erred: 1
errorprone: 1
ery: 1
eschewing: 1
esprit: 1
establish: 1
establishes: 1
etup: 1
eugster: 1
evalappended: 1
eventoriented: 1
everyday: 1
everywhere: 1
evocative: 1
evolves: 1
evolving: 1
exacerbating: 1
exaggerated: 1
examined: 1
excellently: 1
excess: 1
exchanging: 1
excitement: 1
exclusively: 1
exec: 1
execusystem: 1
executable: 1
exemplifies: 1
exercise: 1
exhibitthe: 1
existed: 1
exited: 1
exiting: 1
expects: 1
expedited: 1
expense: 1
experiencing: 1
expert: 1
expire: 1
explanation: 1
explanatory: 1
exploits: 1
exponentiated: 1
exporting: 1
expose: 1
exposition: 1
expunged: 1
expurge: 1
exsmart: 1
exspecified: 1
extending: 1
extensible: 1
extensively: 1
extracting: 1
extracts: 1
facilitates: 1
facilitating: 1
facing: 1
factory: 1
failing: 1
failuredetectio: 1
failwhen: 1
faithful: 1
faloutsos: 1
falsely: 1
faltered: 1
famous: 1
farber: 1
farewell: 1
farm: 1
farnoush: 1
farther: 1
fascination: 1
fashioned: 1
fastest: 1
featured: 1
federal: 1
federica: 1
feel: 1
feels: 1
feet: 1
feldman: 1
fell: 1
fellow: 1
fellowship: 1
felt: 1
ferred: 1
ferris: 1
ffs: 1
fic: 1
fidelity: 1
fifteen: 1
figured: 1
fikes: 1
filing: 1
filled: 1
films: 1
filtered: 1
filters: 1
finance: 1
findings: 1
finegrained: 1
finergrained: 1
finish: 1
fiorano: 1
firewall: 1
firms: 1
fisher: 1
fitted: 1
fitting: 1
fixes: 1
flavors: 1
flaws: 1
flickr: 1
flood: 1
flooding: 1
fluctuating: 1
fluke: 1
flushing: 1
fly: 1
focs: 1
football: 1
forcibly: 1
forecast: 1
foregoing: 1
foremost: 1
foresee: 1
forgoing: 1
fork: 1
forked: 1
formal: 1
formally: 1
formed: 1
formerly: 1
fort: 1
fortune: 1
forwards: 1
founder: 1
fractions: 1
franke: 1
frans: 1
freed: 1
freenix: 1
freeze: 1
freshness: 1
fricano: 1
friends: 1
friendship: 1
frightening: 1
frontend: 1
frost: 1
ftcs: 1
fugal: 1
fulfilling: 1
fulfillment: 1
functional: 1
fund: 1
funding: 1
furniture: 1
furthest: 1
gabit: 1
gabriel: 1
gallager: 1
garbinato: 1
garcia: 1
gates: 1
gathering: 1
gauge: 1
gauthier: 1
gave: 1
gb: 1
gbit: 1
gcheap: 1
geambasu: 1
generalized: 1
genercache: 1
generous: 1
geneva: 1
genit: 1
genuinely: 1
geodistributed: 1
geoff: 1
geoffrey: 1
gershinsky: 1
gestion: 1
getting: 1
ghemawat: 1
gi: 1
gian: 1
giardullo: 1
gifford: 1
gilbert: 1
ginting: 1
ginza: 1
gis: 1
gist: 1
giving: 1
glade: 1
glance: 1
glitches: 1
globalized: 1
globecom: 1
goel: 1
gold: 1
goldberg: 1
googles: 1
gov: 1
governed: 1
governmental: 1
grade: 1
gradual: 1
grande: 1
granting: 1
granularity: 1
graphical: 1
grc: 1
gree: 1
greece: 1
grepwe: 1
griffioen: 1
grimm: 1
grips: 1
grochowski: 1
grounds: 1
groundwork: 1
grove: 1
gruber: 1
grunwald: 1
guadelope: 1
gubarev: 1
guha: 1
guis: 1
gunnar: 1
gunnars: 1
gurevich: 1
gurus: 1
guy: 1
guyadec: 1
hacked: 1
hacking: 1
hadoop: 1
haifa: 1
halem: 1
halt: 1
halts: 1
halves: 1
hampering: 1
hanan: 1
handing: 1
handled: 1
handler: 1
handoff: 1
handurukande: 1
hang: 1
hansell: 1
happened: 1
haridasan: 1
haridasana: 1
harmful: 1
harnessing: 1
harpaz: 1
hartman: 1
harwood: 1
hasn: 1
hawaii: 1
hazard: 1
hazards: 1
headquarters: 1
heard: 1
heart: 1
heartbeatmonitor: 1
heat: 1
heavyweight: 1
hegde: 1
hegedu: 1
heidemann: 1
heiser: 1
helen: 1
hellerstein: 1
helped: 1
henceforth: 1
her: 1
herein: 1
hersonissos: 1
hesitate: 1
heterogeneity: 1
heuristic: 1
heuristics: 1
hiding: 1
highassurance: 1
highbandwidth: 1
highbut: 1
highlights: 1
highlyavailable: 1
highpower: 1
highquality: 1
hik: 1
hill: 1
him: 1
himself: 1
hisgen: 1
hjelm: 1
hochschild: 1
holbrook: 1
holder: 1
holding: 1
holte: 1
homes: 1
homomorphic: 1
honolulu: 1
honored: 1
hoon: 1
hopcroft: 1
hoping: 1
horizontal: 1
horn: 1
horrible: 1
hospital: 1
hotice: 1
hotly: 1
hotnets: 1
hotzone: 1
hours: 1
howa: 1
hpca: 1
hreshold: 1
hrs: 1
hsieh: 1
htm: 1
hu: 1
huazhong: 1
huberman: 1
hungary: 1
hurricane: 1
hurting: 1
hyder: 1
hyper: 1
ically: 1
icccn: 1
iceland: 1
icomp: 1
icons: 1
icpads: 1
icsoc: 1
idempotence: 1
iden: 1
idenhigh: 1
identically: 1
idigest: 1
ied: 1
igniting: 1
ignorance: 1
ihkj: 1
iis: 1
ilability: 1
illegal: 1
illusion: 1
imaginable: 1
imation: 1
imc: 1
imlocal: 1
immature: 1
immersive: 1
impacts: 1
implememted: 1
implemenfrom: 1
implemenshared: 1
implementapackages: 1
implosion: 1
implying: 1
import: 1
imported: 1
imporwriteback: 1
imposition: 1
impossibility: 1
imprecise: 1
impressive: 1
improv: 1
inadequacy: 1
inadequate: 1
inadequately: 1
inapplicable: 1
inappropriate: 1
inbound: 1
incentivize: 1
inception: 1
inclusive: 1
incompatible: 1
incomplete: 1
incon: 1
inconsisto: 1
incorpoas: 1
increasess: 1
incredibly: 1
incrementing: 1
indefinitely: 1
independence: 1
indexing: 1
indicators: 1
indictment: 1
indirect: 1
indirectly: 1
indispensable: 1
individinterference: 1
individuals: 1
indranil: 1
induced: 1
industrystandard: 1
inefficiencies: 1
inevitable: 1
inexpennet: 1
inf: 1
infect: 1
infected: 1
infection: 1
infectious: 1
inference: 1
inferred: 1
inflate: 1
inflexibility: 1
inflight: 1
informa: 1
informatics: 1
informing: 1
informs: 1
infrequent: 1
infused: 1
inherited: 1
inhibit: 1
inhibiting: 1
inicontention: 1
initialised: 1
initialize: 1
initiating: 1
initiation: 1
inject: 1
injects: 1
inner: 1
innetwork: 1
innotice: 1
innovate: 1
inprocess: 1
inputs: 1
insecure: 1
insensitive: 1
inseparable: 1
inserted: 1
inserts: 1
inspection: 1
inspired: 1
instruct: 1
instructed: 1
instructing: 1
instructions: 1
instrument: 1
instrumented: 1
instrumenting: 1
int: 1
integers: 1
integrity: 1
intellection: 1
intellectual: 1
intelligence: 1
intem: 1
intense: 1
intensity: 1
intention: 1
intentions: 1
interadditional: 1
intercluster: 1
interdependence: 1
interests: 1
interfere: 1
interferes: 1
interject: 1
interlinked: 1
intermittent: 1
intermittently: 1
internally: 1
internships: 1
interoriginally: 1
interpreted: 1
interpreting: 1
interprocess: 1
interrogates: 1
interrupt: 1
interrupts: 1
interspersed: 1
intervenallows: 1
intra: 1
intractable: 1
intrapartition: 1
intrusion: 1
intrusions: 1
intuitively: 1
invaluable: 1
invariant: 1
invest: 1
investigations: 1
invisible: 1
invocation: 1
invocations: 1
involve: 1
involvement: 1
involving: 1
iperfthe: 1
ipto: 1
irections: 1
irregularity: 1
irregularly: 1
irrelevant: 1
irrespectively: 1
isca: 1
isola: 1
isolate: 1
isolated: 1
isolating: 1
israel: 1
istemi: 1
istva: 1
itcc: 1
items: 1
iterators: 1
ities: 1
itors: 1
iyengar: 1
jacobson: 1
jade: 1
jain: 1
janne: 1
jannotti: 1
jansch: 1
jared: 1
jari: 1
je: 1
jerian: 1
jesi: 1
jian: 1
jiang: 1
jin: 1
jini: 1
jit: 1
jiun: 1
jk: 1
jmc: 1
jms: 1
job: 1
jobs: 1
joined: 1
jones: 1
jong: 1
jorge: 1
jostling: 1
jr: 1
judicious: 1
julkunen: 1
jump: 1
jumpy: 1
juni: 1
junqueira: 1
justice: 1
justification: 1
kallman: 1
kalogeras: 1
kamilmani: 1
kaminsky: 1
kamra: 1
kanthak: 1
karamanolis: 1
karels: 1
karp: 1
kashani: 1
katti: 1
kay: 1
kde: 1
keidl: 1
keith: 1
keller: 1
kempe: 1
kent: 1
kerberos: 1
kernels: 1
kernilized: 1
kevin: 1
keyboard: 1
khan: 1
khorlin: 1
kib: 1
kick: 1
kicking: 1
kicks: 1
kid: 1
kilobyte: 1
kilobytes: 1
kirk: 1
kistler: 1
kit: 1
kk: 1
kleiman: 1
kliot: 1
kloc: 1
knights: 1
knowledgments: 1
kodali: 1
kogan: 1
korhonen: 1
koskela: 1
kostic: 1
kouznetsov: 1
kr: 1
kramer: 1
kraska: 1
krishnakumar: 1
krishnamurthy: 1
kristjan: 1
kristjanvj: 1
krohn: 1
kubiatowicz: 1
kulkarni: 1
kumar: 1
kupfer: 1
kwiatkowski: 1
la: 1
laboratories: 1
laboratory: 1
lacked: 1
ladis: 1
lafayette: 1
lag: 1
lags: 1
laid: 1
laing: 1
lamb: 1
land: 1
landlord: 1
lands: 1
landscapes: 1
lapping: 1
larry: 1
lasting: 1
lauderdale: 1
launch: 1
law: 1
lay: 1
layed: 1
layering: 1
lazy: 1
leaks: 1
learned: 1
learners: 1
leasing: 1
lecture: 1
led: 1
lee: 1
leff: 1
legends: 1
lenient: 1
lenz: 1
lesser: 1
lets: 1
lever: 1
levin: 1
libdeh: 1
liberal: 1
licensing: 1
lie: 1
lied: 1
lieu: 1
lighter: 1
likelihood: 1
limitation: 1
linden: 1
lindsay: 1
linearizable: 1
linger: 1
linkages: 1
lion: 1
listen: 1
listening: 1
littered: 1
liveness: 1
lives: 1
livestreaming: 1
livny: 1
lkjj: 1
ln: 1
lo: 1
localized: 1
localizes: 1
lockfree: 1
logarithmic: 1
logarithmically: 1
logics: 1
login: 1
logstructured: 1
lombard: 1
longput: 1
longrunning: 1
lookup: 1
loose: 1
loses: 1
loudifying: 1
louisiana: 1
lowand: 1
lowbandwidth: 1
lowerfile: 1
lowmfs: 1
lowney: 1
lowpower: 1
lpdc: 1
lr: 1
lugano: 1
luna: 1
luo: 1
lying: 1
lyles: 1
maarten: 1
mach: 1
macrobenchmarks: 1
maffeis: 1
magazine: 1
maglaris: 1
magnetic: 1
magnified: 1
magninetwork: 1
mahajan: 1
maheshwari: 1
mailing: 1
mailoth: 1
mainly: 1
mainteconsequently: 1
mak: 1
maki: 1
malloth: 1
man: 1
mandated: 1
mandreoli: 1
manipulates: 1
manm: 1
mann: 1
mao: 1
marandi: 1
marchukov: 1
marcon: 1
margo: 1
marie: 1
marin: 1
mario: 1
markoff: 1
markus: 1
marrying: 1
marshal: 1
martignon: 1
maryland: 1
marzullo: 1
mash: 1
mashing: 1
masking: 1
mass: 1
massachussetts: 1
matches: 1
materials: 1
matically: 1
matskin: 1
matt: 1
maxcontiguous: 1
maxim: 1
maximizing: 1
maymounkov: 1
mazieres: 1
mazon: 1
mbit: 1
mcauliffe: 1
mcelroy: 1
mckenney: 1
mea: 1
meaning: 1
meaningfully: 1
meantime: 1
mechacies: 1
medians: 1
mediarich: 1
mediately: 1
medical: 1
meet: 1
meetings: 1
megabytes: 1
megastore: 1
meirong: 1
melnik: 1
mem: 1
memcache: 1
memcopy: 1
memoryrelated: 1
men: 1
mendel: 1
menus: 1
mer: 1
merchant: 1
merging: 1
meta: 1
metaphor: 1
miami: 1
miao: 1
mib: 1
migrate: 1
mika: 1
mike: 1
mikhail: 1
mile: 1
milestones: 1
millions: 1
mimic: 1
mindset: 1
minh: 1
minimise: 1
minimised: 1
minimized: 1
minitransactions: 1
minjun: 1
minneapolis: 1
minnesota: 1
minobrowser: 1
mirrored: 1
mirrors: 1
misbehavior: 1
misfinally: 1
miskin: 1
mislove: 1
misra: 1
mistakes: 1
mitigates: 1
mitigating: 1
mitzenmacher: 1
mixes: 1
mmcn: 1
moderate: 1
modewell: 1
modularity: 1
mofavouring: 1
mohan: 1
moll: 1
moments: 1
monetary: 1
monin: 1
monnet: 1
mono: 1
monolithic: 1
monotonically: 1
moon: 1
morning: 1
morris: 1
mostlyreads: 1
motes: 1
motivating: 1
motivations: 1
mounted: 1
mouse: 1
movement: 1
mpi: 1
mplementation: 1
mscorwks: 1
msgs: 1
msiegen: 1
msn: 1
mts: 1
mulsequence: 1
multiframed: 1
multigigabit: 1
multiin: 1
multimedia: 1
multiplexing: 1
multispeed: 1
multitier: 1
multitude: 1
multiversioning: 1
multiwas: 1
multo: 1
mummert: 1
murderous: 1
music: 1
mutexes: 1
mvs: 1
mwaura: 1
na: 1
naaman: 1
nagle: 1
nahrstedt: 1
naively: 1
naks: 1
namespace: 1
nance: 1
napper: 1
narada: 1
nary: 1
nate: 1
nats: 1
naval: 1
navigate: 1
navy: 1
nawab: 1
nca: 1
ndi: 1
neatly: 1
necessity: 1
needing: 1
needless: 1
needlessly: 1
negative: 1
negatively: 1
negligibly: 1
negotiate: 1
nei: 1
neighboring: 1
nelson: 1
ness: 1
netecon: 1
neting: 1
neufeld: 1
newarr: 1
newscasts: 1
nextgeneration: 1
nfs: 1
ngas: 1
nicely: 1
niche: 1
nick: 1
night: 1
nikolov: 1
nimble: 1
ninth: 1
nishimura: 1
nishita: 1
nism: 1
nization: 1
noisy: 1
nomad: 1
nonblockingtransport: 1
noncongestion: 1
nondeterministic: 1
nonempty: 1
nossdav: 1
noteworthy: 1
notified: 1
notoriously: 1
nowadays: 1
np: 1
npapers: 1
nsfc: 1
ntserver: 1
num: 1
numb: 1
numeric: 1
nutanong: 1
nygren: 1
obeyed: 1
obeys: 1
obfuscates: 1
objective: 1
objectivity: 1
objectoriented: 1
obligation: 1
obliging: 1
observable: 1
observes: 1
obsessively: 1
obsoleted: 1
obstacle: 1
obstructs: 1
occurrence: 1
ofc: 1
offenders: 1
offerings: 1
officially: 1
offline: 1
offset: 1
oforder: 1
ogy: 1
oltp: 1
omitting: 1
oneanother: 1
ongoing: 1
ontology: 1
onwards: 1
opearting: 1
openafs: 1
opennt: 1
opens: 1
opensketch: 1
operafrom: 1
operamfs: 1
operat: 1
operatencies: 1
opposing: 1
opposition: 1
optimality: 1
optimally: 1
optimisation: 1
optimisations: 1
optimise: 1
optimised: 1
optimistically: 1
optional: 1
orchestrating: 1
orderings: 1
organised: 1
origin: 1
originated: 1
originating: 1
originator: 1
ority: 1
orks: 1
orleans: 1
orma: 1
ormance: 1
ortributed: 1
orts: 1
oscillate: 1
oscillating: 1
ost: 1
otivation: 1
ource: 1
ous: 1
outcontent: 1
outlook: 1
outof: 1
outperform: 1
outperformed: 1
outright: 1
outweighs: 1
overcomes: 1
overhaul: 1
overlaid: 1
overlayed: 1
overloading: 1
overpredict: 1
overprediction: 1
overreach: 1
overrequest: 1
overrequesting: 1
oversight: 1
oversold: 1
overweigh: 1
overwritten: 1
owing: 1
owned: 1
ozalp: 1
ozsu: 1
pa: 1
pack: 1
package: 1
packparently: 1
packs: 1
pain: 1
pairing: 1
paleczny: 1
pallickara: 1
palossless: 1
pan: 1
panel: 1
pang: 1
panning: 1
pans: 1
paolo: 1
papazoglou: 1
parallelising: 1
parallelism: 1
parallelized: 1
parallelizing: 1
parameterizing: 1
parent: 1
parents: 1
paritybased: 1
parliament: 1
parsa: 1
partitionable: 1
partners: 1
party: 1
passes: 1
pastry: 1
pat: 1
paterson: 1
patin: 1
patino: 1
pavlo: 1
pdc: 1
peculiar: 1
peek: 1
peep: 1
peking: 1
penalized: 1
pennsylvania: 1
penzo: 1
perceive: 1
perceiving: 1
percentiles: 1
perception: 1
perdichizzi: 1
perf: 1
performances: 1
perience: 1
periments: 1
permission: 1
permute: 1
permutes: 1
permuting: 1
perobject: 1
persisted: 1
persistence: 1
persisting: 1
personalize: 1
personalized: 1
personally: 1
pertaining: 1
perturbances: 1
perturbation: 1
perturbations: 1
perv: 1
pervasively: 1
pervasiveness: 1
petko: 1
petrov: 1
pfhsn: 1
phani: 1
philadelphia: 1
philosophical: 1
phones: 1
phpmyadmin: 1
physician: 1
pianese: 1
picconi: 1
picking: 1
picks: 1
piece: 1
pieces: 1
piles: 1
pirahesh: 1
pittsburgh: 1
pivotal: 1
plain: 1
planes: 1
planned: 1
planning: 1
plans: 1
plants: 1
plasma: 1
plat: 1
plausible: 1
please: 1
plentiful: 1
plotted: 1
pluggable: 1
plugins: 1
pn: 1
podcasts: 1
pointer: 1
pointers: 1
poirier: 1
police: 1
political: 1
portability: 1
portal: 1
porting: 1
portob: 1
portray: 1
posix: 1
post: 1
postava: 1
posted: 1
posters: 1
postponed: 1
postponing: 1
potencan: 1
potholes: 1
powell: 1
powerhouse: 1
powering: 1
powers: 1
powersavings: 1
ppendix: 1
practices: 1
pratt: 1
pray: 1
preallocating: 1
precede: 1
precious: 1
precision: 1
preclude: 1
predetermining: 1
predictability: 1
predicting: 1
predictions: 1
predominant: 1
predominantly: 1
preempted: 1
preexisting: 1
preferences: 1
prefetchthe: 1
preguica: 1
preparation: 1
prepare: 1
prepared: 1
prepares: 1
preprocessing: 1
pressing: 1
presume: 1
pretend: 1
pretending: 1
pretends: 1
prevailing: 1
prevalent: 1
previewer: 1
previwhen: 1
prices: 1
prifetched: 1
priin: 1
primaldual: 1
princehouse: 1
priorispeedup: 1
prioritized: 1
priortwo: 1
prito: 1
priviledges: 1
privileged: 1
pro: 1
probed: 1
probes: 1
procedures: 1
proceedin: 1
professional: 1
profiled: 1
profiles: 1
profiling: 1
programmed: 1
progressed: 1
prohibited: 1
prohibitive: 1
prohibitively: 1
prohibits: 1
projected: 1
proliferating: 1
prometheus: 1
prominently: 1
promised: 1
promotional: 1
promptly: 1
pronounced: 1
propagates: 1
propagating: 1
propel: 1
propgated: 1
proportionally: 1
proposal: 1
proposes: 1
prothe: 1
prototypes: 1
protruding: 1
proverbial: 1
provers: 1
provisions: 1
provoke: 1
proximity: 1
prudently: 1
pruned: 1
ptions: 1
publications: 1
publishsubscribe: 1
pubsub: 1
punishing: 1
purchased: 1
purdue: 1
putation: 1
puter: 1
puts: 1
puzar: 1
pvalues: 1
pvm: 1
pvt: 1
qnx: 1
qppq: 1
qu: 1
quadrant: 1
quadrants: 1
qualities: 1
quantified: 1
quantity: 1
quarterman: 1
quarters: 1
quasi: 1
quema: 1
questionable: 1
quinlan: 1
quired: 1
quorum: 1
quotes: 1
raab: 1
races: 1
rachid: 1
racing: 1
radar: 1
radi: 1
rago: 1
raid: 1
rails: 1
raincloud: 1
raisepriority: 1
raising: 1
rameter: 1
ramifications: 1
randomised: 1
ranks: 1
ratnasamy: 1
ratner: 1
rayfield: 1
rbudp: 1
rchitecture: 1
reaction: 1
reactive: 1
readiness: 1
readwrite: 1
realistically: 1
realities: 1
realization: 1
realtime: 1
reardon: 1
reboot: 1
rebuilt: 1
recast: 1
rechecks: 1
recipients: 1
recite: 1
reclamation: 1
recode: 1
recommendation: 1
recommended: 1
recommends: 1
reconfigurable: 1
reconfiguration: 1
reconfigure: 1
reconfiguring: 1
reconnect: 1
reconstruction: 1
recorded: 1
recov: 1
recovrequests: 1
recovthroughput: 1
recurrence: 1
recursive: 1
recursively: 1
redesign: 1
redirect: 1
redirected: 1
redirecting: 1
redisplaying: 1
reedsolomon: 1
referenced: 1
referring: 1
refers: 1
refines: 1
reflectsan: 1
refuse: 1
reg: 1
regenerate: 1
registering: 1
registry: 1
regularity: 1
reid: 1
reiher: 1
reimplementing: 1
relational: 1
relationship: 1
relationships: 1
relayed: 1
religion: 1
relinked: 1
remained: 1
removal: 1
ren: 1
rename: 1
rency: 1
render: 1
renderers: 1
renders: 1
rendezvous: 1
renessea: 1
renewed: 1
rently: 1
reordering: 1
replacements: 1
replicates: 1
reportedly: 1
repre: 1
reprefetch: 1
representa: 1
reproduced: 1
rescuer: 1
researcher: 1
resembles: 1
resend: 1
resetting: 1
reside: 1
resides: 1
resiliency: 1
resolves: 1
resolving: 1
resorting: 1
respected: 1
restarts: 1
restrict: 1
restructured: 1
resumed: 1
retained: 1
retaining: 1
retains: 1
rethese: 1
rethink: 1
retriev: 1
retrieving: 1
reusability: 1
reusable: 1
reuse: 1
reused: 1
revalidate: 1
revealed: 1
revealing: 1
reverts: 1
revisited: 1
revived: 1
revoked: 1
revolution: 1
rewind: 1
rexford: 1
rhee: 1
ricardo: 1
ride: 1
righteous: 1
rio: 1
rive: 1
rlogin: 1
ro: 1
roads: 1
rockell: 1
rocky: 1
rodrigues: 1
rodriguez: 1
rolig: 1
roll: 1
rollback: 1
rolled: 1
rollout: 1
room: 1
rooted: 1
roselli: 1
rotating: 1
rotations: 1
rough: 1
roundrobin: 1
roussel: 1
roy: 1
rss: 1
ru: 1
rubenstein: 1
ruichuan: 1
rush: 1
rushed: 1
rx: 1
saab: 1
sacrificing: 1
saikat: 1
saito: 1
sakoda: 1
salt: 1
sambamurthy: 1
samet: 1
sandber: 1
sankaran: 1
santa: 1
sarana: 1
sarcasm: 1
saroiu: 1
sata: 1
satpep: 1
saturated: 1
saturates: 1
saturating: 1
saturation: 1
savage: 1
saying: 1
says: 1
scaleable: 1
scarce: 1
scenarmobile: 1
scenes: 1
schedule: 1
schematic: 1
schiper: 1
schizophrenic: 1
schlosser: 1
schmidt: 1
schroeder: 1
schuh: 1
schultz: 1
sciascia: 1
scott: 1
screen: 1
screenshots: 1
scripting: 1
scripts: 1
scrutiny: 1
sdns: 1
seagate: 1
searchers: 1
searching: 1
searing: 1
sears: 1
secretly: 1
seeing: 1
seely: 1
seemed: 1
selec: 1
selection: 1
selectively: 1
selfinterested: 1
selforganizing: 1
semanticweb: 1
senses: 1
sensitivfigure: 1
sentative: 1
separating: 1
sequencing: 1
ser: 1
serialised: 1
sericola: 1
serverless: 1
servicing: 1
serving: 1
setceiver: 1
setups: 1
seven: 1
severely: 1
shacham: 1
shaded: 1
shah: 1
shahabi: 1
shalunov: 1
shao: 1
shaped: 1
shapers: 1
shapiro: 1
sharding: 1
shayee: 1
shed: 1
shelf: 1
shenghua: 1
shielding: 1
shim: 1
shipping: 1
shirriff: 1
shop: 1
shopping: 1
shoring: 1
shortcomings: 1
shorter: 1
shouldn: 1
shraer: 1
shrideep: 1
shtml: 1
shupp: 1
shutdown: 1
si: 1
sidebar: 1
siegenthaler: 1
siena: 1
sig: 1
sigkdd: 1
sigmetrics: 1
signature: 1
signed: 1
signers: 1
significance: 1
signing: 1
silberstein: 1
silently: 1
similarities: 1
similarity: 1
simon: 1
simulaneous: 1
simulates: 1
singhai: 1
sintek: 1
sishim: 1
sistencies: 1
sitaraman: 1
sits: 1
sivasubramaniam: 1
sive: 1
sjsu: 1
skepticism: 1
sky: 1
skyrocketing: 1
slash: 1
slay: 1
sleeps: 1
slide: 1
slip: 1
slowdowns: 1
sluggish: 1
smartphone: 1
smith: 1
smoothly: 1
snapshots: 1
snooping: 1
soa: 1
socc: 1
sock: 1
softway: 1
solicited: 1
solves: 1
somefound: 1
sonicmq: 1
sonicsoftware: 1
sooner: 1
sophia: 1
sorted: 1
sought: 1
spain: 1
spanningtree: 1
spans: 1
spared: 1
spawned: 1
speaking: 1
speaks: 1
speci: 1
specializes: 1
specificity: 1
specifies: 1
specifying: 1
spectrum: 1
speech: 1
spends: 1
spielman: 1
spiked: 1
spinning: 1
spirits: 1
spix: 1
splay: 1
splitx: 1
sponse: 1
sporting: 1
sports: 1
sprays: 1
spring: 1
squash: 1
squirrelmail: 1
sridharan: 1
srrs: 1
srsr: 1
ssh: 1
stabilizing: 1
stadrops: 1
stafford: 1
stand: 1
standardizing: 1
stanford: 1
stanislav: 1
stanton: 1
startup: 1
starved: 1
starving: 1
statemachine: 1
statems: 1
stationary: 1
statistic: 1
stead: 1
steen: 1
steep: 1
steer: 1
steere: 1
stepping: 1
stipulating: 1
sto: 1
stochastic: 1
stocking: 1
stodolsky: 1
stoica: 1
stor: 1
stordifferent: 1
straight: 1
street: 1
strengths: 1
stretagy: 1
strike: 1
strikes: 1
strikingly: 1
strings: 1
stripped: 1
strongest: 1
stronglytraces: 1
structuring: 1
studying: 1
stumbled: 1
stylistic: 1
suba: 1
subing: 1
subintervals: 1
subj: 1
subkilobyte: 1
submission: 1
subnetworks: 1
subordinate: 1
subramanian: 1
subscribes: 1
subscription: 1
subscriptions: 1
subsections: 1
subsequence: 1
subservicecontrol: 1
subserviceprocess: 1
substandard: 1
substitute: 1
substreams: 1
subtitles: 1
subtransactions: 1
succeeds: 1
successor: 1
sue: 1
suffix: 1
suggestive: 1
sul: 1
summarise: 1
summarised: 1
summarizes: 1
summarizing: 1
summer: 1
sunos: 1
sup: 1
superbowl: 1
superceded: 1
superfluous: 1
superimpose: 1
superior: 1
superlinearly: 1
supersede: 1
superseded: 1
supervisory: 1
supplementing: 1
supposed: 1
suppressing: 1
surely: 1
surements: 1
surge: 1
surplus: 1
surrounding: 1
surtani: 1
survivors: 1
suryanarayana: 1
suspended: 1
sussman: 1
sw: 1
swart: 1
swivel: 1
swws: 1
sympo: 1
syn: 1
synchro: 1
synchronise: 1
synchronizing: 1
systraditional: 1
szeged: 1
szymaniak: 1
tablet: 1
tag: 1
tags: 1
tailor: 1
tains: 1
talking: 1
talks: 1
tam: 1
tamma: 1
tan: 1
tance: 1
tanin: 1
tao: 1
tapped: 1
tapping: 1
tari: 1
taught: 1
tdi: 1
te: 1
technetwork: 1
technicalsessions: 1
technion: 1
technologists: 1
telecommunications: 1
telemetry: 1
templates: 1
tempt: 1
tems: 1
tency: 1
tended: 1
tension: 1
tent: 1
tentative: 1
tention: 1
terleaving: 1
terlinked: 1
terminal: 1
terminate: 1
terminated: 1
testimony: 1
textures: 1
thanks: 1
thefly: 1
ther: 1
thereafter: 1
thereby: 1
thorsten: 1
thrashing: 1
threading: 1
threetier: 1
throughputs: 1
throughxors: 1
tial: 1
tiate: 1
tib: 1
ticast: 1
ticipant: 1
ticker: 1
ticular: 1
tied: 1
tightrope: 1
timecritical: 1
timeframe: 1
timeline: 1
timers: 1
timescales: 1
timesharing: 1
timo: 1
tination: 1
tings: 1
tiple: 1
titles: 1
tity: 1
tively: 1
tivity: 1
todd: 1
todisincentivize: 1
toend: 1
toknow: 1
tokyo: 1
tolercations: 1
tom: 1
tone: 1
tons: 1
toplas: 1
topoltifiers: 1
tortures: 1
touted: 1
traceroute: 1
traf: 1
trafficshaping: 1
trailing: 1
tranasctions: 1
transactionally: 1
transactypes: 1
transcoding: 1
transformed: 1
transis: 1
transit: 1
transitional: 1
translated: 1
translates: 1
translation: 1
translators: 1
translucence: 1
transportation: 1
transporting: 1
travels: 1
treatments: 1
tri: 1
trials: 1
tricks: 1
tried: 1
trillion: 1
trim: 1
trips: 1
truncate: 1
truncated: 1
truong: 1
tsatalos: 1
ttls: 1
tude: 1
tudy: 1
tunable: 1
tung: 1
tuples: 1
turing: 1
tus: 1
tweb: 1
twelth: 1
twin: 1
twitter: 1
twotier: 1
tx: 1
txcache: 1
txn: 1
typed: 1
typwrite: 1
ual: 1
ubicomm: 1
ubiquitously: 1
ublcs: 1
uc: 1
ucb: 1
ufrgs: 1
ular: 1
ulate: 1
ultrareliable: 1
unacceptably: 1
unavoidable: 1
unbe: 1
uncertain: 1
unclustered: 1
unconstrained: 1
uncontrolled: 1
uncorrelated: 1
underdog: 1
undergo: 1
underlies: 1
underneath: 1
underperform: 1
understandable: 1
understanddevices: 1
understands: 1
understood: 1
undertake: 1
undertaken: 1
underutilisation: 1
underway: 1
undesired: 1
undisturbed: 1
unencrypted: 1
unflattering: 1
unfortunate: 1
unidirectional: 1
unifies: 1
uniformally: 1
unilaterally: 1
unintended: 1
uninterference: 1
unites: 1
univ: 1
univerisity: 1
universal: 1
universe: 1
universita: 1
unjustified: 1
unlink: 1
unlinking: 1
unmodified: 1
unnoticed: 1
unparalleled: 1
unrealistic: 1
unreasonable: 1
unrecoverable: 1
unreserved: 1
unresilient: 1
unresolved: 1
unresponsiveness: 1
unrpcs: 1
unspecified: 1
unsuitable: 1
untouched: 1
unusually: 1
unwilling: 1
upare: 1
upcalls: 1
upcan: 1
upcations: 1
upcoming: 1
upgrade: 1
upgrades: 1
upisting: 1
uploads: 1
upreader: 1
upson: 1
upstudies: 1
upthe: 1
upwidth: 1
ures: 1
urgently: 1
url: 1
urls: 1
usages: 1
usdoj: 1
usefully: 1
usermakes: 1
uservisible: 1
utilising: 1
utilized: 1
utilizes: 1
utilizing: 1
utrsut: 1
utt: 1
uture: 1
uut: 1
uutt: 1
validations: 1
vanilla: 1
vanishes: 1
var: 1
variances: 1
vasilatos: 1
vb: 1
vectored: 1
vehicles: 1
veitch: 1
velenis: 1
venkataraman: 1
vercurr: 1
verification: 1
versa: 1
vertical: 1
victims: 1
videos: 1
vidhyashankar: 1
viding: 1
viewer: 1
viewsynchronous: 1
violated: 1
violates: 1
viral: 1
vironment: 1
virtually: 1
virtue: 1
vishnumurthy: 1
visibly: 1
vision: 1
visit: 1
visiting: 1
vista: 1
visualize: 1
visualizing: 1
vital: 1
vivek: 1
vlo: 1
vmm: 1
vms: 1
voelker: 1
vogel: 1
voip: 1
vollset: 1
voluntary: 1
vrable: 1
wa: 1
waived: 1
wallach: 1
wans: 1
warp: 1
wasn: 1
wasteful: 1
wastes: 1
wasting: 1
watches: 1
wave: 1
wcnc: 1
weaken: 1
weakness: 1
webfs: 1
weblogs: 1
webtier: 1
weeks: 1
weighed: 1
welldefined: 1
weng: 1
wes: 1
westerlund: 1
wg: 1
whatsnew: 1
whatsoever: 1
wheeler: 1
whereupon: 1
whiteboard: 1
whitepaper: 1
whithout: 1
wholefile: 1
wicom: 1
widens: 1
wieloch: 1
wilma: 1
windowsnt: 1
winnie: 1
wires: 1
withuated: 1
witnesses: 1
wolf: 1
wonder: 1
woo: 1
woodford: 1
worker: 1
worries: 1
worrisome: 1
worthwhile: 1
wouldn: 1
wrapped: 1
wraps: 1
writclassification: 1
writeaccesses: 1
writeinvalidations: 1
writequirement: 1
writeserver: 1
writetive: 1
writewrite: 1
wrote: 1
wscompatible: 1
wsdl: 1
wspds: 1
wstransactions: 1
wu: 1
xaxis: 1
xcp: 1
xiao: 1
xisting: 1
xperimental: 1
xu: 1
xyx: 1
yaghmazadeh: 1
yann: 1
yaxes: 1
yee: 1
yielded: 1
yin: 1
ylianttila: 1
youtube: 1
yuanchao: 1
yuanyuan: 1
yum: 1
yushprakh: 1
zahorjan: 1
zdonik: 1
zeal: 1
zealous: 1
zelenka: 1
zephyr: 1
zettabyte: 1
zhen: 1
zhenqi: 1
zhou: 1
zhuang: 1
zoomed: 1
zooming: 1
zooms: 1
zou: 1
zuck: 1
zwilling: 1
